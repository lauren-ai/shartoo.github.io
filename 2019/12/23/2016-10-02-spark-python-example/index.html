<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>大数据：spark mllib python使用示例 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据：spark mllib python使用示例">
<meta property="og:url" content="http://shartoo.github.com/2019/12/23/2016-10-02-spark-python-example/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="大数据">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-23T10:45:59.345Z">
<meta property="article:modified_time" content="2019-12-23T09:55:14.203Z">
<meta property="article:author" content="shartoo">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shartoo.github.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2016-10-02-spark-python-example" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2016-10-02-spark-python-example/" class="article-date">
  <time datetime="2019-12-23T10:45:59.345Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      大数据：spark mllib python使用示例
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="机器学习的背景知识"><a href="#机器学习的背景知识" class="headerlink" title="机器学习的背景知识"></a>机器学习的背景知识</h2><blockquote>
<p>监督学习的中点是** 在规则化参数的同时最小化误差<strong>。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但</strong>训练误差<strong>小并不是我们的最终目标，我们的目标是希望模型的</strong>测试误差**小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。要知道，有时候人的先验是非常重要的。</p>
</blockquote>
<p>来源于：   <a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="noopener">http://blog.csdn.net/zouxy09/article/details/24971995</a></p>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="一-数学公式"><a href="#一-数学公式" class="headerlink" title="一  数学公式"></a>一  数学公式</h2><p>&emsp; &emsp; 许多机器学习方法都可以被转换为一个凸函数优化问题，比如查找凸函数f（自变量是w，在代码中称为权重，自变量有d维）最小值。通常，我们可以将这些写成 $ min_{w\epsilon R^d}f(w) $ ，其目标函数是以下形式<br>$$  f(w) := \lambda, R(w) +\ frac1n \sum_{i=1}^n L(w;x_i,y_i) \label{eq:regPrimal}$$</p>
<p>&emsp;<br>&emsp;<br>此处向量$x_{i}\epsilon R^d$是训练数据，对于$1\leq i\leq n$ 和 $y_{i}\epsilon R$是我们需要预测的标签。如果$L(w;x,y)$可以被表示为 $W^T x$和$y$的函数，则可以调用 <strong><em>linear</em></strong>方法。</p>
<p>&emsp;<br>&emsp;<br>目标函数<strong><em>f</em></strong>分为两部分：控制模型复杂度的正则化部分，模型在训练数据集上误差评估的损失度量部分。损失度量函数$L(w;.)$是一个在域$w$上的凸函数。固定的正则化参数$\lambda \geq 0$(代码中是参数<strong><em>regParam</em></strong>)定义了权衡最小化损失（比如训练误差）和最小化模型复杂度（比如，防止过拟合）之间的平衡。</p>
<h2 id="二-误差函数"><a href="#二-误差函数" class="headerlink" title="二 误差函数"></a>二 误差函数</h2><p>下表概括了损失函数和它们在spark.mllib支持的的梯度和分梯度方法. </p>
<table>
<thead>
<tr>
<th>损失</th>
<th>loss function $L(w;x,y)$$;;;;;;;;$</th>
<th>梯度或分梯度</th>
</tr>
</thead>
<tbody><tr>
<td>hinge loss(SVM)</td>
<td>$max${0,$1-yw^{T}x$},$y \epsilon$ {-1,+1}</td>
<td>$ -y\cdot x (if, yw^Tx &lt; 1);; 0 (otherwise))$</td>
</tr>
<tr>
<td>logstic loss(逻辑回归)</td>
<td>$log(1+exp(-yw^Tx)),y\epsilon {-1,+1}$</td>
<td>$-y(1-\frac {1}{1+exp(-yw^Tx)})\cdot x$</td>
</tr>
<tr>
<td>squared loss(最小二乘)</td>
<td>$\frac {1}{2}(w^Tx-y)^2, y\epsilon R$</td>
<td>$(w^Tx-y)\cdot x$</td>
</tr>
</tbody></table>
<h2 id="三-规则化"><a href="#三-规则化" class="headerlink" title="三 规则化"></a>三 规则化</h2><p>  &emsp;&emsp;规则化的目的是简化模型并避免过拟合，规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同<br>&emsp;&emsp;关于L1范式和L2范式:</p>
<ul>
<li>L0 范式：L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说，让参数W是稀疏的。</li>
<li>L1 范式:  L1范数是指向量中各个元素绝对值之和，也称叫“稀疏规则算子”（Lasso regularization）。<br>既然L0可以实现稀疏，为什么不用L0，而要用L1呢？一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。</li>
<li>L2范式：<br>在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。它的作用是改善过拟合。<br>L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。<br>&emsp;&emsp;目前在 spark.mllib中支持的正则化如下：<table>
<thead>
<tr>
<th>范式</th>
<th>regularizer $R(w)$</th>
<th>梯度或子梯度</th>
</tr>
</thead>
<tbody><tr>
<td>zero(unregularized)</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>L2</td>
<td>$\frac{1}{2}$||w||$_2^2$</td>
<td>w</td>
</tr>
<tr>
<td>L1</td>
<td>$||w||_1$</td>
<td>$sign(w)$</td>
</tr>
<tr>
<td>elastic net</td>
<td>$\alpha||w||_1+(1-\alpha)\frac{1}{2}||w||_2^2$</td>
<td>$\alpha sign(w)+(1-\alpha)w$</td>
</tr>
</tbody></table>
</li>
</ul>
<p><em>此处$sign(w)$是由项$w$中所有$sign(-1,+1)$组成的向量</em></p>
<h2 id="三-优化"><a href="#三-优化" class="headerlink" title="三 优化"></a>三 优化</h2><p>&emsp;&emsp;线性方法使用凸函数来优化目标函数. spark.mllib使用两个方法，SGD和LBFGS（Limited-Memory Quasi-Newton Method）。当前，大多数算法API都支持Stochastic Gradient Descent（随机梯度下降），和少部分支持LBFGS。</p>
<h2 id="四-分类"><a href="#四-分类" class="headerlink" title="四  分类"></a>四  分类</h2><p>&emsp;&emsp;分类旨在将数据项切分到不同类别。<strong><em>spark.mllib</em></strong>提供了两个线性分类方法：线性SVM和逻辑回归。线性SVM只支持二分类，逻辑回归既支持二分类也支持多分类。这两种方法，<strong><em>spark.mllib</em></strong>都支持L1和L2范式规则化。在<strong>MLlib</strong>中训练数据集合以 <em>LabeledPoint</em>类型的RDD代表，其中label（标签）是从0开始0,1,2…的类别索引。<br>&emsp;&emsp;<strong>注意</strong>：指导手册中的，二分类标签$y$要么是 1 要么是 -1，这是为了方便在公式里，但是在<strong>*spark.mllib</strong>里面是以0代表公式中的-1的</p>
<h2 id="五-线性SVM"><a href="#五-线性SVM" class="headerlink" title="五 线性SVM"></a>五 线性SVM</h2><h3 id="5-1-线性SVM概要"><a href="#5-1-线性SVM概要" class="headerlink" title="5.1 线性SVM概要"></a>5.1 线性SVM概要</h3><p>&emsp;&amp;emsp线性SVM的误差函数是由hingle loss给出:$$L(w;x,y) :=max\lbrace0,1-yw^Tx\rbrace$$<br>线性SVM默认使用L2范式规则化训练数据，同时是支持L1范式规则的，此时就变成一个线性问题。<br>线性SVM的输出是一个SVM模型。对于一个新数据点，以$x$表示，模型将会基于$w^Tx$的值预测。默认情况下，如果$w^Tx\geq 0$那么输出为1，否则输出为0。</p>
<h3 id="5-2-示例代码"><a href="#5-2-示例代码" class="headerlink" title="5.2 示例代码"></a>5.2 示例代码</h3><p>&emsp;&emsp;一下代码展示了如何载入数据，创建SVM模型，根据模型预测并计算训练误差。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">学习使用spark.mllib中 SVM模型。代码展示了如何载入数据，创建SVM模型，根据模型预测并计算训练误差。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from pyspark.mllib.classification import SVMWithSGD,SVMModel</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestSVM&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line">#载入数据并解析</span><br><span class="line">def parsePoint(line):</span><br><span class="line">  values &#x3D; [float(x) for x in line.split(&quot; &quot;)]</span><br><span class="line">    return LabeledPoint(values[0],values[1:])</span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;sample_svm_data.txt&quot;)</span><br><span class="line"></span><br><span class="line">parseData &#x3D; data.map(parsePoint)</span><br><span class="line">#创建SVM模型</span><br><span class="line">model &#x3D; SVMWithSGD.train(parseData,iterations&#x3D;100)</span><br><span class="line"># 评估模型</span><br><span class="line">labelsAndPoints &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">trainError &#x3D; labelsAndPoints.filter(lambda (v,p):v!&#x3D;p).count()&#x2F;float(parseData.count())</span><br><span class="line">mylog.append(&quot;SVM模型测试，训练误差是:&quot;)</span><br><span class="line">mylog.append(str(trainError))</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;log&#x2F;&quot;)</span><br><span class="line">#存储和载入模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;SVMModelSave&quot;)</span><br><span class="line">sameModel &#x3D; SVMModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;SVMModelSave&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="六-逻辑回归"><a href="#六-逻辑回归" class="headerlink" title="六 逻辑回归"></a>六 逻辑回归</h2><h3 id="6-1-逻辑回归概要"><a href="#6-1-逻辑回归概要" class="headerlink" title="6.1 逻辑回归概要"></a>6.1 逻辑回归概要</h3><p>&emsp;&emsp;逻辑回归在二分类预测中广泛使用，其误差函数是下式 logistic loss:$$L(w;x,y):=log(1+\exp(-yw^Tx))$$<br>对于二分类问题，算法的输出结果是一个二项式逻辑回归模型。对于给定新数据点，以$x$表示，使用logistic函数来预测:$$f(z) =\frac{1}{1+e^{-z}}$$。其中$z =w^Tx$，如果$f(w^Tx)&gt;0.5$输出为正，否则为负。与线性SVM不同的是，逻辑回归模型的原始输出有一个概率解释（即，x是正的概率）。<br>&emsp;&emsp;二项式逻辑回归可以生成多项式逻辑回归并用来训练和预测多分类问题。比如说，对于<strong>K</strong>个可能的输出结果，其中一个可以选定为轴，其余<strong>k-1</strong>则与此轴对立。在spark.mllib中第一个被选中的类0就是轴类。<br>&emsp;&emsp;对于多分类问题，算法将会输出一个多项式逻辑回归模型，包含了<strong>k-1</strong>个与第一个类对立的二项式逻辑回归模型。对于新数据点，<strong>k-1</strong>个模型将会运行，其中有最大概率的模型即预测的模型。<br>&emsp;&emsp;spark中实现了两个算法来解决逻辑回归问题：mini-batch gradient（梯度下降）和L-BFGS。参考<a href="http://www.bubuko.com/infodetail-898846.html" target="_blank" rel="noopener">batch-GD， SGD， Mini-batch-GD， Stochastic GD， Online-GD区别</a>  spark推荐L-BFGS梯度下降以获得更快的收敛。</p>
<h3 id="6-2-逻辑回归代码"><a href="#6-2-逻辑回归代码" class="headerlink" title="6.2 逻辑回归代码"></a>6.2 逻辑回归代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">测试逻辑回归代码</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from pyspark.mllib.classification import  LogisticRegressionWithSGD,LogisticRegressionModel</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(</span><br><span class="line">&quot;TestLogisticRegression&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#载入和解析数据</span><br><span class="line">def parsePoint(line):</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">values &#x3D; [float(x) for x in line.split(&quot; &quot;)]</span><br><span class="line">return LabeledPoint(values[0],values[1:])</span><br><span class="line"></span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;sample_svm_data.txt&quot;)</span><br><span class="line">parseData  &#x3D; data.map(parsePoint)</span><br><span class="line">#创建模型</span><br><span class="line">model &#x3D; LogisticRegressionWithSGD.train(parseData)</span><br><span class="line"></span><br><span class="line">#评估模型</span><br><span class="line">labelaAndPoints &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">trainError &#x3D; labelaAndPoints.filter(lambda (k,v):k!&#x3D;v).count()</span><br><span class="line">&#x2F;</span><br><span class="line">float(parseData.count())</span><br><span class="line">mylog.append(&quot;逻辑回归的误差是:&quot;)</span><br><span class="line">mylog.append(trainError)</span><br><span class="line"></span><br><span class="line"># 存储和载入模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;logisticregression_model&#x2F;&quot;)</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;log&#x2F;&quot;)</span><br><span class="line">logisticregression_model &#x3D; LogisticRegressionModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;logisticregression_model&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="7-回归"><a href="#7-回归" class="headerlink" title="7 回归"></a>7 回归</h2><p>&emsp;&emsp;<strong>Linear least squares, Lasso, and ridge regression</strong><br>&emsp;&emsp;Linear least squares是回归问题最常用的公式，其误差函数如下：$$L(w;x,y):=\frac{1}{2}(w^Tx-y)^{2}$$<br>使用不同的规则参数将会派生出不同的相关回归方法；其中的 <strong>ordinary least squares</strong>和<strong>linear least squares</strong>不使用规则参数，ridge regression(岭回归)使用L2规则参数，Lasso使用L1规则参数。所有的这些模型，其平均误差或者训练误差$$\frac{1}{n}\sum_{i=1}^n(w^Tx-y_i)^2$$ 即均方差。<br>&emsp;&emsp;一下代码展示了如何载入数据、转换为LabeledPoint类型的RDD。然后使用 LinearRegressionWithSGD来创建简单线性模型来预测标签值。最后再计算均方差来评估适应度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,LinearRegressionModel</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestSimpleLinearRegression&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line"></span><br><span class="line">#载入数据</span><br><span class="line">def parsePoint(line):</span><br><span class="line">  values &#x3D; [float(x) for x in line.replace(&#39;,&#39;,&#39; &#39;).split(&#39; &#39;)]</span><br><span class="line">    return LabeledPoint(values[0],values[1:])</span><br><span class="line"></span><br><span class="line">mylog &#x3D; []</span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;data&#x2F;lpsa.data&quot;)</span><br><span class="line">parseData &#x3D; data.map(parsePoint)</span><br><span class="line">#创建模型</span><br><span class="line">model &#x3D; LinearRegressionWithSGD.train(parseData,iterations &#x3D; 10,step&#x3D;0.000001)</span><br><span class="line">#评估模型误差</span><br><span class="line">valuesAndPres &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">MSE &#x3D; valuesAndPres.map(lambda (v,p):(v-p)**2).reduce(lambda x,y:x+y)&#x2F;valuesAndPres.count()</span><br><span class="line">mylog.append(&quot;简单线性回归误差是：&quot;)</span><br><span class="line">mylog.append(MSE)</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;log&quot;)</span><br><span class="line">#存储 和使用模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;SimpleLinearRegressionModel&quot;)</span><br><span class="line">sameMode &#x3D; LinearRegressionModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;SimpleLinearRegressionModel&quot;)&lt;&#x2F;pre&gt;</span><br></pre></td></tr></table></figure>

<h3 id="流线性回归"><a href="#流线性回归" class="headerlink" title="流线性回归"></a>流线性回归</h3><p>如果数据是以流的形式到达，在线适配回归模型、新数据到达时跟更新模型参数是很有用的。<em>spark.mllib<em>当前支持使用 *</em>ordinary least squares**的线性回归。适应过程类似于离线使用，一批新数据到达时预测适应值，以此来不断地更新流中的新数据的回应值（回归值）。<br>&emsp;&emsp;如下代码演示了如何训练和测试来自两个不同文本格式的输入流，将流解析成labeled point,使用第一个流来拟合回归模型，并在第二个流中作预测。注意，当训练目录 *”/home/xiatao/machine_learing/streaming_linear_regression/data</em> 新增数据时，相应的预测目录<em>/home/xiatao/machine_learing/streaming_linear_regression/predict</em>就会产生相应的结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># 流线性回归模型测试</span><br><span class="line">from pyspark.mllib.linalg import Vectors</span><br><span class="line">from pyspark.mllib.regression import StreamingLinearRegressionWithSGD</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestStreamLinearRegression&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line"></span><br><span class="line">#第一步创建 StreamingContextssc &#x3D; StreamingContext(sc,1)</span><br><span class="line"></span><br><span class="line">#载入和解析数据</span><br><span class="line">def parse(lp):</span><br><span class="line">  label &#x3D; float(lp[ lp.find(&#39;(&#39;)+1:lp.find(&#39;,&#39;)  ])</span><br><span class="line">    vec &#x3D; Vectors.dense(lp[lp.find(&#39;[&#39;)+1:lp.find(&#39;,&#39;)].split(&#39;,&#39;))</span><br><span class="line">    return LabeledPoint(label,vec)</span><br><span class="line"># 训练集和测试集的数据每行的格式为 (y,[x1,x2,x3]),其中y是标签，x1,x2,x3是特征。</span><br><span class="line"># 训练集中数据更新时，测试集目录就会出现预测值。并且数据越多，预测越准确</span><br><span class="line">trainingData &#x3D; ssc.textFileStream(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;streaming_linear_regression&#x2F;data&quot;).map(parse).cache()</span><br><span class="line">testData &#x3D; ssc.textFileStream(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;streaming_linear_regression&#x2F;predict&quot;).map(parse)</span><br><span class="line">#创建 权值为0的初始化模型</span><br><span class="line">numFeatures&#x3D; 3</span><br><span class="line">model &#x3D; StreamingLinearRegressionWithSGD()</span><br><span class="line">model.setInitialWeights([0.0,0.0,0.0])</span><br><span class="line"># 为训练流和测试流登记，并启动作业</span><br><span class="line">model.trainOn(trainingData)</span><br><span class="line">mylog.append(model.predictOnValues(testData.map(lambda lp:(lp.label,lp.features))))</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()&lt;&#x2F;pre&gt;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2016-10-02-spark-python-example/" data-id="ck4ig03qy001nb8je53916khh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/12/23/2016-10-09-regularization-deeplearning/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          深度学习：正则化
        
      </div>
    </a>
  
  
    <a href="/2019/12/23/2016-10-02-spark-mllib-desciontree/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">大数据：spark mllib决策树</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/12/23/template/">博客题目</a>
          </li>
        
          <li>
            <a href="/2019/12/23/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-11-26-model-pruning/">模型剪枝和优化-torch和Tensorflow为例</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-10-28--understand-pytorch/">理解pytorch的计算逻辑</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-09-24-outlier-detection/">使用pyod做离群点检测</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 shartoo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>