<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8" >
    <meta name="baidu-site-verification" content="dIcXMeY8Ya" />
    
    <title>文章归档: 2019/12 | Hexo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0" >
    <meta name="keywords" content="Jelon, 前端, Web, 张德龙, 前端开发" >
    <meta name="description" content="Jelon个人前端小站" >

    
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml" >
    
    
    <link rel="shortcut icon" href="/favicon.ico" >
    
    
<link rel="stylesheet" href="/css/style.css">

    <!--[if lt IE 9]>
    
<script src="/js/html5.js"></script>

    <![endif]-->
    
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?fd459238242776d173cdc64918fb32f2";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


<meta name="generator" content="Hexo 4.2.0"></head>

<body class="home">
    <!--[if lt IE 9]>
    <div class="browsehappy">
        当前网页 <strong>不支持</strong>
        你正在使用的浏览器. 为了正常的访问, 请 <a href="http://browsehappy.com/" target="_blank" rel="noopener">升级你的浏览器</a>.
    </div>
    <![endif]-->

    <!-- 博客头部 -->
    <header class="header">
    <section class="container header-main">
        <div class="logo">
            <a href="/">
                <div class="cover">
                    <span class="name">Hexo</span>
                    <span class="description"></span>
                </div>
            </a>
        </div>
        <div class="dropnav icon-paragraph-justify" id="JELON__btnDropNav"></div>
        <ul class="menu hidden" id="JELON__menu">
            
            <li rel="/archives/2019/12/page/10/index.html" class="item ">
                <a href="/" title="首页" class="icon-home">&nbsp;首页</a>
            </li>
            
            <li rel="/archives/2019/12/page/10/index.html" class="item ">
                <a href="/lab/" title="实验室" class="icon-lab">&nbsp;实验室</a>
            </li>
            
            <li rel="/archives/2019/12/page/10/index.html" class="item ">
                <a href="/about/" title="关于" class="icon-about">&nbsp;关于</a>
            </li>
            
            <li rel="/archives/2019/12/page/10/index.html" class="item ">
                <a href="/comment/" title="留言" class="icon-comment">&nbsp;留言</a>
            </li>
            
        </ul>
        <div class="profile clearfix">
            <div class="feeds fl">
                
                
                <p class="links">
                    
                        <a href="https://github.com/jangdelong" target="_blank">Github</a>
                        |
                    
                        <a href="https://pages.coding.me" target="_blank">Hosted by Coding Pages</a>
                        
                    
                </p>
                <p class="sns">
                    
                        <a href="http://weibo.com/jangdelong" class="sinaweibo" target="_blank"><b>■</b> 新浪微博</a>
                    
                        <a href="https://www.facebook.com/profile.php?id=100011855760219&amp;ref=bookmarks" class="qqweibo" target="_blank"><b>■</b> Facebook</a>
                    
                    <a href="javascript: void(0);" class="wechat">
                        <b>■</b>
                        公众号
                        <span class="popover">
                            <img src="/img/wechat_mp.jpg" width="120" height="120" alt="我的微信订阅号">
                            <i class="arrow"></i>
                        </span>
                    </a>
                </p>
                
            </div>
            <div class="avatar fr">
                <img src="/img/jelon.jpg" alt="avatar" title="Jelon" >
            </div>
        </div>
    </section>
</header>


    <!-- 博客正文 -->
    <div class="container body clearfix">
        <section class="content">
            <div class="content-main widget">
                <!-- 文章归档 -->

    <h3 class="widget-hd">
        <strong>
            
                文章归档
                <!-- 文章归档，可以根据日期分类 -->
            
        </strong>
    </h3>
    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2014-05-11-javafoundamention1/">
    		Java基础笔记-Java内存区域
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.254Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="一-运行时的数据区组成"><a href="#一-运行时的数据区组成" class="headerlink" title="一 运行时的数据区组成"></a>一 运行时的数据区组成</h2><p><img src="/images/blog/java-jvm-store-model.png" alt="图示1">   </p>
<ol>
<li><p>程序计数器<br>当前线程执行的字节码的行号指示器。自己吗解释器通过改变程序计数器(PC)的值来选取下一条需要执行的字节码指令。分支、循环、跳转、线程回复等基础功能都依赖于它。<br>例如：Java多线程机制。线程轮流切换，分配CPU执行时间，任一时刻，一个CPU只会执行一条线程指令，每个线程都需要一个独立PC，以保证线程切换后能正确恢复。      </p>
</li>
<li><p><strong>虚拟机栈</strong>四点说明:</p>
<ol>
<li>生命周期随着线程存亡</li>
<li>Java方法执行的内存模型：每个方法在执行的同时会创建一个线帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。</li>
<li>每个方法从调用直至执行完成的过程，就对应着一个线帧在虚拟机栈中从入栈到出栈的过程。</li>
<li>能发生的两类异常:<ul>
<li>I：线程的请求深度大于虚拟机所允许的深度，会抛出StackOverflowError。</li>
<li>II：虚拟机栈可动态扩展时无法申请到足够的内存，就会抛出OutOfMemoryError。</li>
</ul>
</li>
</ol>
</li>
<li><p>本地方法栈<br>作用与虚拟机相同，不同的是虚拟机栈为虚拟机执行Java方法（即字节码）服务，而本地方法栈为虚拟机执行本地方法（Native）服务。    </p>
</li>
<li><p>Java堆<br>Java Heap是Java虚拟机所管理的内存中最大的一块，是所有线程共享的一块内存区域。在虚拟机启动时创建，此区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。<br>说明：</p>
<ul>
<li>是垃圾收集器管理的主要目标(GC Garbage Collected).</li>
<li>可以处于物理上不连续的内存空间中，只要逻辑上连续即可。</li>
</ul>
</li>
</ol>
<p>5.方法区<br>   与堆一样，是所有线程共享的内存区域。它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。<br>  运行时常量池(Runtime Constannt Pool)：是方法区的一部分。Class文件中除了有类的版本、方法、字段、接口等描述信息外，还有一项是常量池，用于存放编译期间生成的各种字面两和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。<br><br>&nbsp;&nbsp;&nbsp;&nbsp;运行时常量池相对于class文件常量池的另外一个特征是动态性，Java语言并不要求常量一定是编译器才能产生。运行期间可将新的常量放入池中，比如String的intern()方法就是这种特性的直接应用。</p>
<ol start="6">
<li>直接内存<br>非虚拟机运行时数据区的一部分，但频繁使用<br>来源：JDK 1.4之后引入NIO类，引入一种基于通道（Channel）与缓冲区（Buffer）的I/O方式。它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。由于避免在Java堆和Native堆中来回复制数据，在某些场合可显著提高性能。</li>
</ol>
<h2 id="二-Java内存模型与线程"><a href="#二-Java内存模型与线程" class="headerlink" title="二 Java内存模型与线程"></a>二 Java内存模型与线程</h2><ol>
<li><strong>主要目标</strong><br>定义程序各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量的底层细节。注意：</li>
</ol>
<ul>
<li>此处的主内存、工作内存与上面的Java堆、栈、方法区等不是同一层次的划分，两者没有关系</li>
</ul>
<p>2.内存模型与线程的对应关系如下图：<br><img src="/images/blog/java-jvm-store-model2.png" alt="图示2">  </p>
<ol start="3">
<li>说明<ul>
<li>Java内存模型规定所有变量都存储在主内存中，此处主内存仅是虚拟机内存的一部分.</li>
<li>每个线程还有自己的工作内存（作用类似于处理器告诉缓冲），线程的工作内存保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。</li>
</ul>
</li>
</ol>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2014-05-11-javafoundamention1/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2014-05-11-javafoundamention1/" title="Java基础笔记-Java内存区域">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-05-24-datamining-classfy-main/">
    		数据挖掘方法之分类
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.252Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>博文参考《数据挖掘概念与技术》：韩家威著（机械工业出版社）</p>
<h2 id="一-分类的概念"><a href="#一-分类的概念" class="headerlink" title="一 分类的概念"></a>一 分类的概念</h2><p>在面向对象编程（OOP）中我们说“一切皆对象”，在数据挖掘中，我们应该认为“一切皆数据”。而分类就是按照您的选择评估标准将数据进行分离，使得具有某些相同特性的数据属于一个类，不相同的数据不在一个类。</p>
<h2 id="二-分类的一般过程"><a href="#二-分类的一般过程" class="headerlink" title="二 分类的一般过程"></a>二 分类的一般过程</h2><p>分类一般分为两个阶段：<code>学习阶段(构建模型)</code>和<code>分类阶段（使用分类模型给测试数据的赋予类标号）</code>。</p>
<ul>
<li>学习阶段：通过分析或从训练集“学习”来构造分类器。训练集由数据元组和相应的类标号组成。<ul>
<li><strong>元组</strong>：是用n维向量 $X=(x_1,x_2,x_3…)$ 表示的一条数据记录，其中n维向量表示的是元组X在n个属性上的度量。例如下图中黑边框标记的一条记录的元组表示为：<em>X=(“Ricky Field”,”Middle_aged”,”Low”)</em> 该数据有4个属性，分别是 *”name”,”age”,”income”,”Loan_descision”* ，其中属性”Loan_descision”也是分类属性，类标号为”Risky”。<img src="/images/blog/classfymain1.png"></li>
<li>分类阶段，如下图所示，它属于一种映射过程。根据分类模型中的规则，给予测试数据元组X特定的类标号<img src="/images/blog/classfymain2.png">

</li>
</ul>
</li>
</ul>
<h2 id="三-分类的评估"><a href="#三-分类的评估" class="headerlink" title="三 分类的评估"></a>三 分类的评估</h2><h3 id="3-1-度量的基础术语"><a href="#3-1-度量的基础术语" class="headerlink" title="3.1 度量的基础术语"></a>3.1 度量的基础术语</h3><ul>
<li><strong>正元组</strong>：感兴趣的主要元组，即我们要在第二章图中的数据中找到类标号”Loan_edscision”为”safe”的记录。</li>
<li><strong>负元组</strong>：除去正元组以外的其他元组（或称为记录）。</li>
</ul>
<h3 id="3-2-度量的四个构件"><a href="#3-2-度量的四个构件" class="headerlink" title="3.2 度量的四个构件"></a>3.2 度量的四个构件</h3><ul>
<li><strong>真正例/真阳例（True Positive ,TP）</strong>：指的是被分类器正确分类的我们”感兴趣”的元组。</li>
<li><strong>真负例/真阴例（True Negative ,TN）</strong>：指的是被分类器正确分类的我们”不感兴趣”的元组。</li>
<li><strong>假正例/假阳例（False Positive,FP）</strong>：指的是被分类器错误的分类的元组，即将我们”不感兴趣”的元组分成了”感兴趣”的元组。</li>
<li><strong>假负例/假阳例（False Negative,FN）</strong>：指的是被分类器错误的分类的元组，即将我们”感兴趣”的元组分成了”不感兴趣”的元组。</li>
</ul>
<h3 id="3-3-评估度量"><a href="#3-3-评估度量" class="headerlink" title="3.3 评估度量"></a>3.3 评估度量</h3><p>有了度量的四个构件，我们可以得到常用的评估度量公式。如下</p>
<table>
<thead>
<tr>
<th>度量</th>
<th>公式</th>
</tr>
</thead>
<tbody><tr>
<td>准确率(识别率)</td>
<td>$\frac{TP+TN}{P+N}$</td>
</tr>
<tr>
<td>错误率(误分类率)</td>
<td>$\frac{FP+FN}{P+N}$</td>
</tr>
<tr>
<td>敏感率(真正例率、召回率)</td>
<td>$\frac{TP}{P}$</td>
</tr>
<tr>
<td>特效性(真负例率)</td>
<td>$\frac{TN}{N}$</td>
</tr>
<tr>
<td>精度</td>
<td>$\frac{TP}{TP+FP}$</td>
</tr>
<tr>
<td>F分数(精度和召回率的调和均值)</td>
<td>$\frac{2<em>精度</em>召回率}{精度+召回率}$</td>
</tr>
</tbody></table>
<p>如何理解：</p>
<ul>
<li>准确率和错误率是相对的：前者计算的是<B>全部记录</B>中，分类器正确分类的元组数量，包括正确分类的”感兴趣”元组和”不感兴趣”元组；而后者计算的是<B>全部记录</B>中分类器错误分类的元组，也包括错误分类的”感兴趣”元组和”不感兴趣”元组。</li>
<li>敏感度和特效性是相对的：前者计算的是<B>“感兴趣”元组</B>中，被正确分类的元组数量，可以理解为“我们得到的”感兴趣”元组，有多少是真正的”感兴趣”元组”；特效性计算的是<B>“不感兴趣”元组</B>中被正确分类的元组数量，可以理解为“我们的得到的”不感兴趣”元组，有多少真正是”不感兴趣”元组”。</li>
<li>精度：是一个完全关乎”感兴趣”元组的统计项，一般情况下与敏感度等同。</li>
</ul>
<h2 id="四-对模型的评估"><a href="#四-对模型的评估" class="headerlink" title="四 对模型的评估"></a>四 对模型的评估</h2><h3 id="4-1-保持-holdout-方法和随机二次抽样"><a href="#4-1-保持-holdout-方法和随机二次抽样" class="headerlink" title="4.1 保持(holdout)方法和随机二次抽样"></a>4.1 保持(holdout)方法和随机二次抽样</h3><ul>
<li><strong>保持(holdout)方法</strong> 是我们在讨论准确率时默认使用的方法。此方法中，数据会被随机地划分为两个独立的集合：<br><code>训练数据集合</code>和<code>检验数据集合</code>。通常,2/3的数据分配到训练集，其余1/3分配到检验集。</li>
<li><strong>随机二次抽样方法</strong> ，是保持方法的一种变形，只是将保持方法重复k次，总准确率取每次迭代准确率的平均值。</li>
</ul>
<h3 id="4-2-k折交叉验证"><a href="#4-2-k折交叉验证" class="headerlink" title="4.2 k折交叉验证"></a>4.2 k折交叉验证</h3><p>将数据分成互不相交的k等份 $D_1,D_2,D_3,…D_k$，训练和校验进行k次。第i次迭代时，将第i个等份(“折”)作为校验集，而其他等份(“折”)全体作为训练集合。注意，在保持方法中数据是随机分的，而此处是均分，并且每份数据集合都有一次机会作为校验集。下图显示的是第四次迭代时的一个示例：<br><img src="/images/blog/classfymain4.png"></p>
<h3 id="4-3-自助法"><a href="#4-3-自助法" class="headerlink" title="4.3 自助法"></a>4.3 自助法</h3>
            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-05-24-datamining-classfy-main/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-05-24-datamining-classfy-main/" title="数据挖掘方法之分类">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-05-16-datamining-logicregressionsample/">
    		数据挖掘方法之六：解读逻辑回归
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.248Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="一-使用数据"><a href="#一-使用数据" class="headerlink" title="一 使用数据"></a>一 使用数据</h2><p>本文着重示例如何使用逻辑回归<br><br><a herf="http://download.csdn.net/detail/huangxia73/7059709">数据来源:电信数据集合</a><br><br><B>描述：</B>电信数据，有多个属性，用来预测客户流失。<br><br>载入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">call_consumer&lt;-read.table(file&#x3D;&quot;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&quot;,header&#x3D;TRUE,sep&#x3D;&quot;,&quot;)  </span><br><span class="line">   Warning message:  </span><br><span class="line">   In read.table(file &#x3D; &quot;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&quot;, header &#x3D; TRUE,  :  </span><br><span class="line">    incomplete final line found by readTableHeader on &#39;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&#39;  </span><br><span class="line"> &gt; edit(call_consumer)</span><br></pre></td></tr></table></figure>

<img src="/images/blog/loginregressionsample1.png">

<h2 id="二-解读逻辑回归模型"><a href="#二-解读逻辑回归模型" class="headerlink" title="二 .解读逻辑回归模型"></a>二 .解读逻辑回归模型</h2><p>分三种：</p>
<ul>
<li>一个两分预测变量的模型</li>
<li>多分预测变量</li>
<li>连续的预测变量</li>
</ul>
<h3 id="2-1-两分预测变量模型"><a href="#2-1-两分预测变量模型" class="headerlink" title="2.1 两分预测变量模型"></a>2.1 两分预测变量模型</h3><p>假定唯一的预测变量是语音邮箱套餐（Intl.plan），这是一个表示是否为套餐会员的标记变量。下表显示了语音邮箱套餐会员流失情况。</p>
<table>
<thead>
<tr>
<th></th>
<th>语音邮箱=否(x=0)</th>
<th>语音邮箱=是(x=1)</th>
<th>合计</th>
</tr>
</thead>
<tbody><tr>
<td>流失=假(y=0)</td>
<td>2008</td>
<td>842</td>
<td>2850</td>
</tr>
<tr>
<td>流失=真(y=1)</td>
<td>403</td>
<td>80</td>
<td>483</td>
</tr>
<tr>
<td>合计</td>
<td>2411</td>
<td>922</td>
<td>3333</td>
</tr>
</tbody></table>
<p>似然函数可以表示为：</p>
<p>$$<br> L(b|x)=[\pi(0)]^{403}<em>[1-\pi(0)]^{2008}</em>[\pi(1)]^{80}[1-\pi(1)]^{842}<br>$$</p>
<p>使用语音邮箱套餐的客户流失的发生比＝ $\frac{\pi(1)}{1-\pi(1)} =\frac{80}{842}=0.095$</p>
<p>没有使用语音邮箱套餐的客户流失的发生比＝ $\frac{\pi(0)}{1-\pi(0)}=\frac{403}{2008}=0.2007$  </p>
<p> OR=0.095/0.2007=0.47</p>
<p>也即　使用语音邮箱套餐的客户与没有使用语音邮箱套餐的客户相比，流失概率只有47%</p>
<p>下图显示了语音套餐会员流失的逻辑回归结果<br><img src="/images/blog/loginregressionsample4.png"><br>可以得到 $b_0$＝-1.60596和 $b_1$=-0.747795。所以用于语音邮箱套餐（x=1）的客户或者没有语音套餐（x=0）的客户流失的估计值为：</p>
<p>$$<br> \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}=\frac{e^{-1.60596-0.747795}}{1+e^{-1.60596-0.747795}}<br>$$</p>
<ul>
<li><p>对于一个拥有此套餐的客户，估计他的流失概率为： $\pi(1)=0.0868$（也可以直接计算 P(流失|语音邮箱计划)=80/922=0.0868)，这一概率比数据集中给出的客户流失的总比例14.5%要小，说明开通语音邮箱套餐有利于减少客户流失。</p>
</li>
<li><p>对于一个没有拥有此套餐的客户，估计他的流失概率为：$\pi(0)=0.16715$ （也可以直接计算 P(流失\｜语音邮箱计划)=403/2411=0.16715，这一概率比数据集中给出的客户流失的总比例14.5%要高，说明没有开通语音邮箱套餐对于客户流失不大。</p>
</li>
<li><p>进一步地，可以利用Wald检验法检验语音邮箱套餐参数的显著性。这里, $b_1$ =-0.747795, $SE(b_1)$ =0.129101得 $Z_{wald}=-0.747795/0.129101=-5.79$ P值为P(|Z|&gt;-5.79)趋近于0</p>
</li>
</ul>
<h3 id="2-2-多分预测变量模型"><a href="#2-2-多分预测变量模型" class="headerlink" title="2.2 多分预测变量模型"></a>2.2 多分预测变量模型</h3><p>假定将客户服务电话数（customers services calls)看做一个新的变量<font color="red">“-CSC”</font>，分类如下：</p>
<ul>
<li>0个或1个客户服务电话：CSC＝低</li>
<li>2个或3个客户服务电话：CSC＝中</li>
<li>4个以上客户服务电话：CSC＝高</li>
</ul>
<p>此时，分析人员需要用指示变量（虚拟变量）和参考单元编码法来给数据集编码，假定选择“ＣＳＣ＝低”作为参考单元，则可把指示变量值分配给另外两个变量。使用指示变量之后：</p>
<p align="center">使用参考单元编码的客户电话指示变量</p>

<table>
<thead>
<tr>
<th></th>
<th>CSC-中</th>
<th>csc-高</th>
</tr>
</thead>
<tbody><tr>
<td>低(0个或1个电话)</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>中(2个或3个电话)</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>高( $\ge4个电话$ )</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>使用CSC展示客户流失情况列表汇总如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>CSC-低</th>
<th>CSC-中</th>
<th>CSC-高</th>
<th>合计</th>
</tr>
</thead>
<tbody><tr>
<td>流失=假(y=0)</td>
<td>1664</td>
<td>1057</td>
<td>129</td>
<td>2850</td>
</tr>
<tr>
<td>流失=真(y=1)</td>
<td>214</td>
<td>131</td>
<td>138</td>
<td>483</td>
</tr>
<tr>
<td>合计</td>
<td>1878</td>
<td>1188</td>
<td>267</td>
<td>3333</td>
</tr>
</tbody></table>
<p>此时再对数据进行逻辑回归分析，得到的结果如下（<font color="blue">注意：没有CSC－低</font>）：<br><img src="/images/blog/loginregressionsample8.png"></p>
<ul>
<li>对于CSC－中：$\bar {OR}＝ｅ^{b1}＝ｅ^{-0.03698}=0.96$</li>
<li>对于CSC－高：$\bar {OR}＝ｅ^{b2}＝ｅ^{2.11844}=8.32$</li>
</ul>
<p>这里， $b_0＝-2.501,b_1=-0.03698，b_2=2.11844$ 所以客户流失概率的估计量为：</p>
<p>$$<br>  \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}<br>  \ 其中g(x)=e^{-2.051-0.036989(csc-中)+2.11844(csc-高)}<br>$$</p>
<p>有：</p>
<ul>
<li>对于那些很少拨打客服电话的客户：$g(x)=e^{-2.051-0.036989(0)+2.11844(0)}=e^{-2.501}$ 概率为：$\pi(x)=\frac{e^{-2.501}}{1+e^{-2.501}}=0.114$<br>。此概率比全部数据样本集中客户流失的概率14.5%要小。这表明这一类客户的流失率一定程度上比总体客　　户的流失率要小。</li>
<li>对于拨打客服电话处于中等水平的客户，同上，此时<br>$g(x)=e^{-2.051-0.036989(1)+2.11844(0)}=e^{-2.088}$ <font color="blue">注意系数的差别,上一条中的系数是0，0，这个是1，0</li>
<li>对于经常拨打客服电话的客户，同上，此时<br>$g(x)=e^{-2.051-0.036989(0)+2.11844(1)}=e^{-2.501}$ 注意系数的差别,上一条中的系数是1，0，这个是0，1</font></li>
</ul>
<h4 id="Wald检验"><a href="#Wald检验" class="headerlink" title="Wald检验"></a>Wald检验</h4><p>如下：</p>
<ul>
<li>对于<font color="blue">CSC－中</font> 的参数进行Wald检验，$b_1=-0.036989,SE(b_1)=0.11771$<br>　故而，<br>$$<br>Z_{wald}＝-0.036989/0.117701=-0.31426<br>$$</li>
</ul>
<p>此时，P值P(|Z|&gt;0.31426)=0.753，不显著，所以没有证据表明<font color="blue">CSC－中</font>与<font color="blue">CSC－低</font>的差异能有效预测客户流失。</p>
<ul>
<li>对于<font color="blue">CSC－高</font>的参数进行Wald检验，$b_1=2.11844,SE(b_1)=0.142380$故而<font align="center">$Z_{wald}=2.11844/0.142380=14.88$</font><br>此时，P值P(|Z|&gt;14.88)=0.000，显著，表明<font color="blue">CSC－高</font>与<font color="blue">CSC－低</font>的差异能有效预测客户流失。</li>
</ul>
<p><B>所以，对于多分预测变量模型，关键是指示变量和参照单元编码</B>   </p>
<h3 id="2-3-解读连续预测变量模型"><a href="#2-3-解读连续预测变量模型" class="headerlink" title="2.3　解读连续预测变量模型"></a>2.3　解读连续预测变量模型</h3><p>假定我们考虑以客户日使用分钟数作为预测变量，则相应的逻辑回归分析结果如下：<br><img src="/images/blog/loginregressionsample14.png"><br>因此对于一个给定日使用分钟数的顾客，流失概率：</p>
<p>$$<br>  \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}=\frac{e^{-3.929-0.112717(日分钟数)}}{1+e^{-3.929-0.112717(日分钟数)}}<br>$$</p>
<ul>
<li>对于一个日使用分钟数为100的顾客流失的概率估计为：</li>
</ul>
<p>$$<br>ｇ(x)＝-3.9292+0.112717(100)=-2.80212<br>$$</p>
<p>概率π(100)＝0.0572,比数据集中总比例14.5%要小，表明低的日使用分钟数会在一定程度上防止顾客流失</p>
<ul>
<li>对于一个日使用分钟数为300的顾客流失的概率估计为：</li>
</ul>
<p>$$<br>ｇ(x)=-3.9292+0.0112717(300)＝-0.054778<br>$$</p>
<p>概率π(300)＝0.3664，比数据集中总比例14.5%要大，表明日使用分钟数越多顾客流失越多</p>
<p>“日使用分钟数”，这一实例的<strong>偏差Ｇ</strong>为：</p>
<p>$$<br>  G=偏差(没有预测变量的模型)-偏差(有预测变量的模型)<br>  \=-2ln\frac{没有预测变量的似然值}{有预测变量的似然值}<br>  \=2{-1307.129-[483ln(483)+2850ln(2850)-3333ln(3333)]}<br>  \=144.035<br>$$</p>
<p>对Ｇ进行卡方检验，</p>
<p>$$<br>  P(x^2)\gt G_{观测值}即P(x^2)\gt 144.035=0.0000<br>$$</p>
<p>因此强有力的证据表明日使用分钟数有助于预测顾客的流失情况。</p>
<p>对“日使用分钟数”进行Ｗａｌｄ检验，可以得到同样的结论。</p>
<h2 id="三-多元逻辑回归"><a href="#三-多元逻辑回归" class="headerlink" title="三.多元逻辑回归"></a>三.多元逻辑回归</h2><p>多元逻辑回归与简单逻辑回归十分相似，需要注意的是选择恰当的预测变量，其方法主要有</p>
<ul>
<li>针对单个变量的挑选：Wald检验某个变量是否有助于预测</li>
<li>针对多个变量总体挑选：总体显著性Ｇ</li>
</ul>
<p>下图一个简单示例：<br><img src="/images/blog/loginregressionsample18.png"><br><img src="/images/blog/loginregressionsample19.png"><br>由上面两幅图可以看出，其中的“账户时长”变量其Wald检验的Ｐ值没有拒绝零假设检验，因而需要从全体预测变量中剔除。最后的Ｇ偏差，卡方检验虽然两幅图中都能表明，多元预测变量能显著预测结果（Ｇ检验的Ｐ值＝０），但是剔除账户长度后更好。</p>
<h2 id="四-逻辑回归中引入高阶项"><a href="#四-逻辑回归中引入高阶项" class="headerlink" title="四 逻辑回归中引入高阶项"></a>四 逻辑回归中引入高阶项</h2><h4 id="为何需要高阶项"><a href="#为何需要高阶项" class="headerlink" title="为何需要高阶项"></a>为何需要高阶项</h4><p>如果逻辑回归转换函数在连续变量中不是线性的，让步比的估计和置信区间的应用可能会有问题。原因在与估计的让步比在预测变量取值域上是一个常数。例如，不论是第23分钟还是第323分钟，日使用分钟数每增加1个单位，让步比都是1.01.这种让步比为常数的假设并不总是成立。<br><br>此时，分析人员需要做一些非线性的调整，如使用指示变量（见多分预测变量模型）和高阶项（如：$x^2，x^3．．$）。<br></p>
<h4 id="高阶项的作用"><a href="#高阶项的作用" class="headerlink" title="高阶项的作用"></a>高阶项的作用</h4><p>高阶项的引入可以作为惩罚函数，减少该变量不正常的分布。使用高阶项（和起始变量一起运用）的优势在于，高阶项可以是连续的并且可以提供更严格的估计。</p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-05-16-datamining-logicregressionsample/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-05-16-datamining-logicregressionsample/" title="数据挖掘方法之六：解读逻辑回归">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-05-12-datamining-logicregression/">
    		数据挖掘方法之五：逻辑回归
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.240Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="一-为何要有逻辑回归"><a href="#一-为何要有逻辑回归" class="headerlink" title="一 为何要有逻辑回归"></a>一 为何要有逻辑回归</h2><p>假设有如下关于患者年龄与患病情况的数据集：<br><img src="/images/blog/loginregression1.png"><br>我们查看数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt; edit(patient)  </span><br><span class="line">        patient_id age if_sick  </span><br><span class="line">   [1,]          1  25       0  </span><br><span class="line">   [2,]          2  29       0  </span><br><span class="line">   [3,]          3  30       0  </span><br><span class="line">   [4,]          4  31       0  </span><br><span class="line">   [5,]          5  32       0  </span><br><span class="line">   [6,]          6  41       0  </span><br><span class="line">   [7,]          7  41       0  </span><br><span class="line">   [8,]          8  42       0  </span><br><span class="line">   [9,]          9  44       1  </span><br><span class="line">   [10,]         10  49       1  </span><br><span class="line">   [11,]         11  50       0  </span><br><span class="line">   [12,]         12  59       1  </span><br><span class="line">   [13,]         13  60       0  </span><br><span class="line">   [14,]         14  62       0  </span><br><span class="line">   [15,]         15  68       1  </span><br><span class="line">   [16,]         16  72       0  </span><br><span class="line">   [17,]         17  79       1  </span><br><span class="line">   [18,]         18  80       0  </span><br><span class="line">   [19,]         19  81       1  </span><br><span class="line">   [20,]         20  84       1  </span><br><span class="line">   &gt; p&lt;-as.data.frame(patient)  </span><br><span class="line">   &gt; plot(p$if_sick~p$age,main&#x3D;&quot;20位患者年龄与患病情况&quot;,xlab&#x3D;&quot;年龄&quot;,ylab&#x3D;&quot;患病情况&quot;)</span><br></pre></td></tr></table></figure>

<p>画出对照图看数据分布：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; plot(p$if_sick~p$age,main&#x3D;&quot;20位患者年龄与患病情况&quot;,xlab&#x3D;&quot;年龄&quot;,ylab&#x3D;&quot;患病情况&quot;)  </span><br><span class="line">&gt; lm&lt;-lm(if_sick~age,data&#x3D;p)  </span><br><span class="line">&gt; abline(lm)  </span><br><span class="line">&gt; legend(x&#x3D;65,y&#x3D;0.2,legend&#x3D;&quot;线性拟合&quot;,lty&#x3D;1)</span><br></pre></td></tr></table></figure>

<p>结果图如下：<br><img src="/images/blog/loginregression2.png"><br>此时，我们发现线性拟合完全偏离了数据分布，即使使用如对数变换也难以取得理想结果，如上图所示数据分布（不是0就是1）提醒我们应该使用逻辑回归。</p>
<h2 id="二-逻辑回归概念及认识"><a href="#二-逻辑回归概念及认识" class="headerlink" title="二 .  逻辑回归概念及认识"></a>二 .  逻辑回归概念及认识</h2><h3 id="2-1-对比线性回归"><a href="#2-1-对比线性回归" class="headerlink" title="2.1  对比线性回归"></a>2.1  对比线性回归</h3><p>线性回归是用来估计<font color="red">连续型</font>回应变量与一组预测变量之间关系的方法。<br><br>逻辑回归用来估计<font color="red">非连续型（分类型）</font>回应变量与一组预测变量之间关系的方法。</p>
<h3 id="2-2-公式"><a href="#2-2-公式" class="headerlink" title="2.2 公式"></a>2.2 公式</h3><p>逻辑回归的条件均值（给定X=x的情况下Y的条件均值表示为：E(Y|x)，以下用π(x)表示）具体如下:</p>
<p>$$<br>  \pi(x)= \frac{e^{b_0+b_1}}{1+e^{b_0+b_1}}<br>$$</p>
<p>上式形成的图形成为反曲线，是非线性的S型。它的一种逻辑转换如下，它是一种有效的逻辑转换方法：</p>
<p>$$<br>    g(x)=ln\frac{\pi(x)}{1-\pi(x)}=b_0+b_1x<br>$$</p>
<p>转换函数g(x)表现了线性逻辑回归模型的几个很好的性质，如线性、连续性、取值范围无限性。</p>
<p><strong>取值范围</strong>：$\pi(x)在 b_0+b_1x\backsim -\infty 时取最小值0，π(x)在 b_0+b_1x\backsim\infty时取最大值1$ 。</p>
<p><strong>性质</strong>：π(x)可以看做一种概率形式，其取值范围为 (0,1).其中</p>
<ul>
<li>π(x) 可以看做X=x条件下的正效应（如疾病）发生的概率</li>
<li>1-π(x) 可以看做是在这种条件下正效应没有发生的概率。</li>
</ul>
<h3 id="2-3-误差"><a href="#2-3-误差" class="headerlink" title="2.3 误差"></a>2.3 误差</h3><p>线性回归模型中，误差e服从均值为0、方差为常数的正态分布，而逻辑回归由于其回应变量的取值是二分的，其误差只有两种形式：</p>
<ol>
<li>X=x时，如果出现Y=1的情况，其概率为π(x)，误差为 e=1-π(x)</li>
<li>X=x时， 如果Y=0，其概率为1-π(x),误差为 e=0-π(x)=-π(x)</li>
</ol>
因而，逻辑回归的误差服从二项分布，其方差为π(x)(1-π(x))，这样逻辑回归的回应变量Y=π(x)+e也服从概率为π(x)的二项分布。

<h3 id="2-4-估值（最大似然估计）"><a href="#2-4-估值（最大似然估计）" class="headerlink" title="2.4 估值（最大似然估计）"></a>2.4 估值（最大似然估计）</h3><p>最大似然估计：在已经得到试验结果的情况下，我们应该寻找使这个结果出现 的可能性最大的那个  作为真的估计。</p>
<p>线性回归中使用最小二乘法有可能得到回归系数最优值的闭合形式解，但在逻辑回归中不存在，我们采用的最大似然估计法，得到的观测数据的似然参数估计值是最大的。似然函数 $l(b|x)$ 是一个参数为 $b=b_0,b_1,….$ ，用来表示被观测数据x的概率的函数。在回应变量为正相关的情况下$(X=x_i,Y_i=1)$ ，观测值会影响概率π(x)的值，在回应变量为负相关的情况下 $(X=x_i,Y_i=0)$，观测值会影响概率1-π(x)的值。因此 $Y_i=0或1$，对第i个观测值概率的影响可以表示为 :$[\pi(x_i)]^{y_i}[1-\pi(x)^{1-y_i}]$</p>
<p>假设观测值是独立的，可以把似然函数 $l(b|x)$ 表示为单个项的乘积：$(b|x)=\prod^n_{i=1}$<br>通过对 $l(b|x)$ 每个参数求微分，并令其微分等于零，可以得到最大似然估计。</p>
<h3 id="2-5-衡量回归模型显著性"><a href="#2-5-衡量回归模型显著性" class="headerlink" title="2.5 衡量回归模型显著性"></a>2.5 衡量回归模型显著性</h3><p>先见下表,患病情况与年龄的逻辑回归分析结果<br><img src="/images/blog/loginregression7.png"></p>
<h4 id="2-5-1-统计量G"><a href="#2-5-1-统计量G" class="headerlink" title="2.5.1 统计量G"></a>2.5.1 统计量G</h4><p>在简单线性回归模型中，检验统计量F=MSR/MSE 可以来判断回归模型的显著性。在逻辑回归模型中，检验的是带有某个预测变量的模型比不带该预测变量的模型是否能更好的回应变量匹配。</p>
<ul>
<li>饱和模型：包含了和数据点个数一样多的参数的模型（能完全正确估计回应变量，没有预测误差）</li>
<li>拟合模型：带有少于数据点个数的参数</li>
</ul>
<p>偏差定义如下：</p>
<p>$$<br>   偏差 D= -2ln[\frac{拟合模型似然值}{饱和模型似然值}]<br>$$</p>
<p>上述检验称为似然比值检验，其中-2ln 部分是为了方便计算。将拟合模型中对π(x)的估计值表示为π(x)’，则偏差公式变为：</p>
<p>$$<br>   偏差 D= -2ln\sum^n_{i=1}[y_iln\frac{\pi(x)’}{y_i}+(1-y_i)ln\frac{1-\pi(x)’}{1-y_i}]<br>$$</p>
<p>该偏差表示考虑了预测变量后模型的误差，它类似于线性回归中的平方和误差。</p>
<p>决定某个特定的预测变量是否重要的程序是计算出不带此预测变量模型的偏差，减去带有此预测变量模型的偏差，即:</p>
<p>$$<br>   G= 偏差(非预测模型)-偏差(预测模型)= -ln[\frac{非预测似然值}{预测似然值}]<br>$$</p>
<p><font color="blue">统计量G服从自由度为1的卡方分布</font><br><br>在患病情况例子中，从表格可以看到似然对数比是 -10.101，那么:</p>
<p>G=2{-10.101-[7ln(7)+13ln(13)-20ln(20)]}=5.696</p>
<h4 id="2-5-2-Wald检验"><a href="#2-5-2-Wald检验" class="headerlink" title="2.5.2 Wald检验"></a>2.5.2 Wald检验</h4><p>该比率为：</p>
<p>$$<br>  Z_{wald}=\frac{b_1}{SE(b_1)}<br>$$<br>服从标准正态分布，由表1提供的系数估计值及标准差：b1=0.6696和 $SE(b_1)=0.03223$，于是有: $Z_{wald}=0.6696/0.3223=2.08$ <br><br>表中P值即为P(|Z|&gt;2.08)=0.038<br>通常可以为逻辑回归系数构建一个100(1-a)%的置信区间：</p>
<p>$$<br>   [b_0-Z<em>SE(b_0),b_0</em>SE(b_0)]和[b_1-Z<em>SE(b_1),b_1+Z</em>SE(b_1)]<br>$$</p>
<h4 id="2-5-3-发生比和让步比"><a href="#2-5-3-发生比和让步比" class="headerlink" title="2.5.3 发生比和让步比"></a>2.5.3 发生比和让步比</h4><ul>
<li><strong>发生比</strong>：事件发生的概率与不发生的概率的比值。<ul>
<li>发生比告诉我们一件事情发生或者不发生哪种情况更有可能.一件 事情发生的可能性大于不发生的可能性时，发生比大于1。</li>
<li>一件事情发生的可能性小于不发生的可能性时，发生比大于1</li>
<li>一件事情很有可能发生时，发生比等于1</li>
</ul>
</li>
</ul>
<p>例如，预测的一个72岁病人换用特定病例的概率为61%，不患此病的概率为39%。因此一个72岁病人患此病的发生比=0.61/0.39=1.56。</p>
<p><strong>让步比</strong>：x=1时回应变量发生的发生比除以x=0时回应变量发生的发生比。它很简单的表达了让步比和斜率系数之间的关系<br><br>在二分预测变量的二元逻辑回归中，当x=1时，回应变量发生(y=1)的发生比为：$\frac{\pi(1)}{1-\pi(1)}=e^{b_0+b_1}$</p>
<p>相应的，当x=0时，回应变量发生的发生比为：$\frac{\pi(0)}{1-\pi(0)}=e^{b_0}$</p>
<p>则让步比公式如下：</p>
<p>$$<br>  OR= \frac{\pi(1)/[1-\pi(1)]}{\pi(0)/[1-\pi(0)]}=e^{b_0+b_1}/e^{b_0}=e^{b_1}<br>$$</p>
<p>例如：一个临床试验报告称，曾经使用过与从没有使用过雌性激素替换疗法的人中患子宫癌的让步比为5.0，这可以解释为使用雌性激素替换疗法的人得子宫癌的概率是未使用者的5倍。</p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-05-12-datamining-logicregression/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-05-12-datamining-logicregression/" title="数据挖掘方法之五：逻辑回归">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-05-04-datamining-mutillinerandselect/">
    		数据挖掘方法之四：多重共线性及变量选择方法
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.238Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>  前多重共线性： 也即使用的多个预测变量之间存在线性相关。多重共线性会导致解的不稳定，进而可能导致意外的结果。在线性代数中，基坐标必须是相互正交的，也即不相关的，此处在做多元回归预测时，必须保证预测变量之间是不相关的。</p>
<h3 id="避免不正交的方法"><a href="#避免不正交的方法" class="headerlink" title="避免不正交的方法"></a>避免不正交的方法</h3><h3 id="1分析之前"><a href="#1分析之前" class="headerlink" title="1分析之前"></a>1分析之前</h3><h4 id="a-逐个计算预测变量之间的相关系数"><a href="#a-逐个计算预测变量之间的相关系数" class="headerlink" title="a.逐个计算预测变量之间的相关系数"></a>a.逐个计算预测变量之间的相关系数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; cor(sugar$sugars,sugar$shelf)  </span><br><span class="line">[1] 0.1004379  </span><br><span class="line">&gt; cor(sugar$fiber,sugar$potass)  </span><br><span class="line">[1] 0.9033737</span><br></pre></td></tr></table></figure>

<p>可以看到纤维和钾含量存在高度相关性，需要注意</p>
<h4 id="b-为预测变量建立矩阵图"><a href="#b-为预测变量建立矩阵图" class="headerlink" title="b.为预测变量建立矩阵图"></a>b.为预测变量建立矩阵图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; #同时画多个变量的对照图需要使用 car包中的 scatterplotMatrix函数  </span><br><span class="line">&gt;install.packages(&quot;car&quot;)  </span><br><span class="line">&gt;library(car)  </span><br><span class="line">&gt;#使用谷物数据集的 “糖”，“纤维”，“钾”三列数据  </span><br><span class="line">&gt; sugar_frame&lt;-as.data.frame(sugar[,c(&quot;糖&quot;,&quot;纤维&quot;,&quot;钾&quot;)])  </span><br><span class="line">&gt;#画出对照图  </span><br><span class="line">&gt; scatterplotMatrix(sugar_frame,spread&#x3D;F,lty.smooth&#x3D;2,var.labels&#x3D;c(&quot;糖&quot;,&quot;纤维&quot;,&quot;钾&quot;))</span><br></pre></td></tr></table></figure>

<p>结果如下图：<br><img src="/images/blog/muitllinerandselect1.png"><br>可以看到第四张和第六张是纤维和钾的相关图，可以看出他们之间有很强相关性。</p>
<h3 id="1-2-分析之后：方差膨胀因子-variance-inflation-factors-VIFs"><a href="#1-2-分析之后：方差膨胀因子-variance-inflation-factors-VIFs" class="headerlink" title="1.2 分析之后：方差膨胀因子(variance inflation factors,VIFs)"></a>1.2 分析之后：方差膨胀因子(variance inflation factors,VIFs)</h3><p>$$<br>  VIF=\frac{1}{1-R^2_i}<br>$$</p>
<p>其中 $R_i^2$ 表示 $R^2$ 的值是通过在其他预测变量上回归分析 $x_i$ 得到的。假设xi和其他变量没有任何关系,那么 $R_i^2=0$ ，于是可以得到 $VIFi=\frac{1}{1-0}=1$ 。也即VIF最小值为1，没有最大值.</p>
<p> $VIF_i$ 的变化对第i个系数的变化率Sbi如何产生影响，有如下公式：</p>
<p> $$<br>   Sb_i=Sc_i=S\sqrt{\frac{1}{(n-1)S^2_i}\frac{1}{1-R^2_i}}=S\sqrt{\frac{VIF_i}{(n-1)S^2_i}}<br> $$</p>
<p>如果 $x_i$ 与其他预测变量不想管，那么 $VIF_i=1$ ，而且相关系数的标准差 $Sb_i$ 没有增大。然而如果xi与其他变量相关，那么较大的 $VIF_i$ 值会使得相关系数的标准差 $Sb_i$过度膨胀。因此，方差估计的膨胀会导致估计精度的下降。</p>
<p>粗略的经验法则如下:</p>
<ul>
<li>VIF&gt;=5  模型有中度的多重共线性（相当于 $R^2=0.08$ ）</li>
<li>VIF&gt;=10  模型中有严重多重共线性(相当于 $R^2=0.90$ )</li>
</ul>
<p>下面来查看谷物数据集中 糖、纤维、钾的膨胀因子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&gt; #回归拟合  </span><br><span class="line">&gt; fit&lt;-lm(data&#x3D;sugar,rating~sugars+fiber+potass)  </span><br><span class="line">&gt;#注意，我们只是用了sugar数据集中包含“糖”，“纤维”，“钾”三列数据的sugar_frame  </span><br><span class="line">&gt;#进行膨胀因子计算时，需要使用gvlma包中的vif函数，因此需要先安装  </span><br><span class="line">&gt; install.packages(&quot;gvlma&quot;)  </span><br><span class="line">&gt; library(gvlma)  </span><br><span class="line">Warning message:  </span><br><span class="line">程辑包‘gvlma’是用R版本3.0.2 来建造的   </span><br><span class="line">&gt;#线性模型的综合验证  </span><br><span class="line">&gt; gvlma(fit)  </span><br><span class="line">  Call:  </span><br><span class="line">  lm(formula &#x3D; rating ~ sugars + fiber + potass, data &#x3D; sugar)  </span><br><span class="line"></span><br><span class="line">    Coefficients:  </span><br><span class="line">    (Intercept)       sugars        fiber       potass    </span><br><span class="line">        52.6762      -2.0510       4.3701      -0.0543    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS  </span><br><span class="line">    USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:  </span><br><span class="line">    Level of Significance &#x3D;  0.05   </span><br><span class="line"></span><br><span class="line">    Call:  </span><br><span class="line">     gvlma(x &#x3D; fit)   </span><br><span class="line"></span><br><span class="line">                         Value p-value                   Decision  </span><br><span class="line">    Global Stat        7.24415 0.12353    Assumptions acceptable.  </span><br><span class="line">    Skewness           5.61716 0.01779 Assumptions NOT satisfied!  </span><br><span class="line">    Kurtsis           0.02125 0.88411    Assumptions acceptable.  </span><br><span class="line">    Link Function      0.40164 0.52624    Assumptions acceptable.  </span><br><span class="line">    Heteroscedasticity 1.20410 0.27250    Assumptions acceptable.  </span><br><span class="line">    &gt;#查看膨胀因子 vif  </span><br><span class="line">    &gt; vif(fit)  </span><br><span class="line">      sugars    fiber   potass   </span><br><span class="line">     1.164237 6.327286 6.204047</span><br></pre></td></tr></table></figure>

<h2 id="二-变量选择方法"><a href="#二-变量选择方法" class="headerlink" title="二.  变量选择方法"></a>二.  变量选择方法</h2><p>为帮助数据分析人员确定在多元回归模型中应该包含哪些变量，下面是几种变量选择方法</p>
<ul>
<li>向前选择</li>
<li>向后排除</li>
<li>逐步选择</li>
<li>最优子集</li>
</ul>
<p>注意四种选择方法所使用的数据集都是 “谷物数据集”。</p>
<h3 id="2-1-向前选择程序"><a href="#2-1-向前选择程序" class="headerlink" title="2.1   向前选择程序"></a>2.1   向前选择程序</h3><p>1.对于第一个加入模型的变量，选择与回应变量相关度最高的预测变量（假设为 $x_1$ ）如果所有变量对模型都不重要，则停止,否则执行2<br>2. 对其余的每个变量，F统计序列式 $F(x_2|x_1),F(x_3|x_1),F(x_4|x_1)$ .第二次通过此算法时是, $F(x_3|x_1,x_2),F(x_4|x_1,x_2)$ 。选择具有最大F统计序列的变量<br>3. 对 2 选择出来的变量，进行F统计序列的显著性检验。如果结果模型没有重大意义，则停止，否咋将从 2 得到的变量加入到模型中，然后返回2</p>
<p><strong>初始</strong>：模型中没有变量。</p>
<p><strong>过程</strong>：把与回应变量（营养级别） 密切相关的变量选出来，如果是显著的就加入到模型中。变量糖在所有预测变量中与营养级别有最高的相关系数（r=0.762）。然后进行序列F检验，例如F(纤维|糖)和F(钠|糖)等，然后看到，F(纤维|糖)显著性检验具有最高的F统计序列值，这样变量纤维作为第二个变量加入到模型中。再进行一次序列F检验，比如F(钠|糖，纤维)和F(脂肪|糖，纤维)，等等。F(钠|糖，纤维)具有最高的序列F统计值。因而钠作为第三个变量加入到模型中。</p>
<p><strong>结束</strong>：一次按照第二步进行，得到如下变量加入顺序：脂肪，蛋白质，碳水化合物，卡里路，维生素和钾。此时再也找不到其他显著的变量加入模型中才中断，此时的多元回归模型如下：<br><img src="/images/blog/muitllinerandselect4.png"><br>下图显示了一个顺序选择的模型概览:<br><img src="/images/blog/muitllinerandselect5.png"></p>
<h3 id="2-2-向后排除程序"><a href="#2-2-向后排除程序" class="headerlink" title="2.2 向后排除程序"></a>2.2 向后排除程序</h3><p>向后排除程序是从模型中所有变量或者所有用户自定义变量集开始的。步骤如下:</p>
<ol>
<li>在全模型中执行向后排除，即使用所有变量的模型。例如，可能全模型中有4个变量 $x_1,x_2,x_3,x_4$</li>
<li>对于当前模型中的每个变量，计算出它的偏F统计量。第一次是：$F(x_1,x_2,x_3,x_4)、F(x_2|x_1,x_3,x_4)、F(x_3|x_1,x_2,x_4)和F(x_4|x_1,x_2,x_3)$ 。选择具有最小偏F统计量的比那辆，其值用 $F_{min}$表示</li>
<li>检验 $F_{min}$ 的显著性。如果 $F_{min}$ 不显著，从模型中删除与Fmin对应的变量，然后返回执行2，如果 $F_{min}$ 显著，停止这个过程。</li>
</ol>
<p><strong>实例</strong>：<br>起始时模型包含了所有变量，然后计算该模型中每个变量的偏F统计量。例如，这些统计量分别是F(重量|糖，纤维，….杯子)，F(杯子|糖，纤维,…..重量|)。找到最小偏F统计量（ $F_{min}$ ）对应的变量。第一次是重量，此时 $F_{min}$ 不显著，因而从模型中去掉，接下来变量具有最小偏F统计是杯子，也是不显著的，因而需要被剔除。第三次具有最小偏F统计量的是货架2的指标变量，但是Fmin对应的p值并没有大道可以从模型中剔除，因而保留并中断。得到的模型为：</p>
<p>$$<br>  y =b_0+b_1(糖)+b_2(纤维)+b_3(钠)+b_4(脂肪)+b_5(蛋白质)+b_6(碳水化合物)+b_7(卡里路)+b_8(维生素)+b_9(钾)+b_10(货架2)+e<br>$$</p>
<img src="/images/blog/muitllinerandselect7.png">
模型1表示包含所有预测变量，模型2中剔除了重量之外所有预测变量，于是有：

<p>$$<br>  SS_{重量|所有其他变量}=SS_{所有变量}-SS_{重量以外所有变量}=12980.078-14980.005=0.073<br>$$</p>
<p>上表信息中显示，偏F统计量的结果为：</p>
<p>$$<br>  F(重量|所有其他变量)= \frac{SS_{重量|所有其他变量}}{MSE_{所有变量}}=0.073/0.261=0.0280<br>$$</p>
<p>F统计量的值0.28 落在 $F_{1,n-p-2}=F_{1,72}$ 分布的40%点处，对应的p值是0.60，因而重量不应该包含在模型中。</p>
<h3 id="2-3-逐步选择程序"><a href="#2-3-逐步选择程序" class="headerlink" title="2.3 逐步选择程序"></a>2.3 逐步选择程序</h3><p> 逐步选择程序是向前选择方法的一种改进。在向前选择中会出现这种情况，当新加入的变量加入到模型时，向前选择过程中已经加入的变量可能就显得不重要了，这在向前选择方法中是没有考虑的。逐步选择过程可以检验这种情况，方法是每一步在现有变量的基础上计算每个变量的部分平方和，执行偏F检验。如果模型中有一个变量不再是显著的，这个含有最小偏F统计的变狼就会被移出模型。当不再有变量加入或者移出模型时，结束过程并得到最终模型。</p>
<h3 id="2-4-最优子集程序"><a href="#2-4-最优子集程序" class="headerlink" title="2.4  最优子集程序"></a>2.4  最优子集程序</h3><p>对于预测变量集不是太大的数据集，最优子集是一种较好方法。但是如果预测变量超过30个，最优子集方法就会产生组合爆炸，难以控制。步骤如下：</p>
<ol>
<li>分析人员需要指定需要多少个（假设为m）供筛选的模型，以及在一个模型中含有最大预测变量个数（假设为n）</li>
<li>对于含有一个预测变量的所有模型，例如：$y=b_0+b_1(糖),y=b_0+b_1(纤维)$,….等。计算对应的 $R^2$ ,修正 $R^2$ 和S值都计算出来，最优的m个模型是基于这些统计值得到。</li>
<li>对于含有两个最优的m个模型是基于这些统计值得到。</li>
<li>重复以上，直到达到最大的预测变量（n）个数，然后分析人员把预测变量个数为1,2,,..n的最优模型罗列，以选择最佳总体模型</li>
</ol>
<p><strong>实例，下图是最优子集程序用于谷物数据集的省略概览</strong></p>
<p><font color="blue">[注意，整个过程比下图要复杂，例如变量数为1时，本应该有12行结果，下图中只简要用了两行，其他的也是]</font><br><img src="/images/blog/muitllinerandselect10.png"><br>图中，每一行代表一个不同的模型，某模型中包含了哪个变量，该变量对应的方格被涂成黑色。如，第一个模型（第一行）仅包含了变量糖；第四个模型（第四行）包含了糖和钾。其中的最优模型子集被红色覆盖的那个模型（也即那一行）。</p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-05-04-datamining-mutillinerandselect/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-05-04-datamining-mutillinerandselect/" title="数据挖掘方法之四：多重共线性及变量选择方法">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
            <a href="javascript: void(0);" class="cat">未分类</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-04-24-datamining-mutilregression/">
    		2013-04-24-datamining-mutilregression
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.225Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>title: 数据挖掘方法之三：多元回归模型<br>date: 2013-04-24 17:01:34<br>categories:</p>
<ul>
<li>数据挖掘</li>
</ul>
<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>  前面介绍了一个预测变量和一个回应变量的回归，但数据挖掘通常对一个回应变量和多个预测变量之间的关系更感兴趣，数据中可能有很多变量都与目标（回应）变量有线性关系，多元回归模型可以更加精确的预测这些关联。<br><br>多元回归模型如下：</p>
<p>$$<br>y= b_0+b_1x_1+b_2x_2+….+e<br>$$</p>
<p>其中 $b0，b1，b2…..$ 是模型参数，为常数，可以通过最小二乘法估计。关于误差项e和回应变量y的假设与简单线性回归模型一样。</p>
<h2 id="二-多元回归的推断"><a href="#二-多元回归的推断" class="headerlink" title="二 多元回归的推断"></a>二 多元回归的推断</h2><p>主要有：</p>
<ol>
<li><p>t检验，用来对预测变量xi和回应变量y之间的关系进行推断</p>
</li>
<li><p>F检验，用来对整个回归模型的显著性进行检验</p>
</li>
<li><p>$b_i$ ,第i个预测变量系数的置信区间</p>
</li>
<li><p>回应变量y的均值的置信区间，用于预测变量 $x1,x2,x3,..$ 取特定值时，对回应变量y的均值进行估计</p>
</li>
</ol>
<h3 id="2-1-y和-x-i-之间关系的t检验"><a href="#2-1-y和-x-i-之间关系的t检验" class="headerlink" title="2.1 y和 $x_i$之间关系的t检验"></a>2.1 y和 $x_i$之间关系的t检验</h3><p>假设检验如下：</p>
<ul>
<li>H0: bi=0</li>
<li>H1:  bi!=0</li>
</ul>
<p>这些假设的模型的唯一区别是第i项是否存在，其他项都是相同的。</p>
<p><strong>实例</strong>： 营养级别和糖之间关系的t检验</p>
<ul>
<li>H0: b1=0;模型 $y=b_0+b_2(纤维)+e$</li>
<li>H1:b1!=0;模型：$y=b_0+b_1(糖)+b_2(纤维)+e$</li>
</ul>
<p>还是使用数据集：谷物(在本系列文章第二篇中有下载地址)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#数据集存储在sugar中  </span><br><span class="line">#线性拟合  </span><br><span class="line">&gt;mutil_regre&lt;-lm(data&#x3D;sugar,rating~sugars+fiber)  </span><br><span class="line">#查看  </span><br><span class="line">&gt;summary(mutil_regre)    </span><br><span class="line">Call:  </span><br><span class="line">  lm(formula &#x3D; rating ~ sugars + fiber, data &#x3D; sugar)  </span><br><span class="line">  Residuals:  </span><br><span class="line">        Min      1Q  Median      3Q     Max   </span><br><span class="line">    -12.133  -4.247  -1.031   2.620  16.398   </span><br><span class="line"></span><br><span class="line">    Coefficients:  </span><br><span class="line">                Estimate Std. Error t value  </span><br><span class="line">    (Intercept)  51.6097     1.5463  33.376  </span><br><span class="line">    sugars       -2.1837     0.1621 -13.470  </span><br><span class="line">    fiber         2.8679     0.3023   9.486  </span><br><span class="line">                Pr(&gt;|t|)      </span><br><span class="line">    (Intercept)  &lt; 2e-16 ***  </span><br><span class="line">    sugars       &lt; 2e-16 ***  </span><br><span class="line">    fiber       2.02e-14 ***  </span><br><span class="line">    ---  </span><br><span class="line">    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  </span><br><span class="line"></span><br><span class="line">    Residual standard error: 6.219 on 74 degrees of freedom  </span><br><span class="line">    Multiple R-squared:  0.8092,    Adjusted R-squared:  0.804   </span><br><span class="line">    F-statistic: 156.9 on 2 and 74 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>

<p>可以得到，糖对应的稀疏为 $b_1=-2.1837$ ,对应的标准差为 $S_{bi}=0.1621$</p>
<p>T对应的t统计量，也即检验的统计量为:<br> $t=\frac{b_1}{S_{b1}}=\frac{-2.1837}{0.1621}=-13.4713$</p>
<p>P值对应的是t统计量的p值。也即:  $p=P(|t|&gt;tobs)=P(|t|&gt;-13.4713)$ 约等于0. 使用p值来检验假设，当p值很小时就可以拒绝原假设。</p>
<h3 id="2-2-整体回归模型的显著性水平检验：F检验"><a href="#2-2-整体回归模型的显著性水平检验：F检验" class="headerlink" title="2.2 整体回归模型的显著性水平检验：F检验"></a>2.2 整体回归模型的显著性水平检验：F检验</h3><p><strong>检验</strong> 是分别对每个变量，糖，纤维，….逐个检验与回应变量线性关系。即{营养级别|糖}，{营养级别|纤维}，….</p>
<p><strong>F检验</strong> 是对所有变量一起检测与回应变量关系，即{营养级别|糖，纤维，……}</p>
<p>F检验的前提是</p>
<ul>
<li><p>H0: b0=b1=……=0   也即模型为：$y=b_0+e$</p>
</li>
<li><p>H1:至少存在一个bi不等于零</p>
</li>
</ul>
<p>备选假设H1并不要求任何回归系数都不是零，而是当备选假设为真时，存在一个回归系数不是零。因此，F检验的备选假设并没有唯一确定一个模型，当一个、几个或者所有回归系数都不是零时，备选假设都是成立的。</p>
<p>F统计量为：</p>
<p>$$<br>    F= F_{obs}=\frac{MSR}{MSE} 服从F_{m,n-m-1}分布<br>$$</p>
<p> <strong>如何理解</strong>：MSE（误差平方和均值）能很好的估计总体变异 $σ^2$（不论原假设是否为真），而MSR只有当原假设为真时才是 $σ^2$ 的优良统计量，因而只有在原假设为真的情况下MSR和MSE才会比较接近，也即F很小的时候，有足够的争取表明原假设为真。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;#查看方差分析表  </span><br><span class="line">&gt;anova(mutil_regre)  </span><br><span class="line">Analysis of Variance Table  </span><br><span class="line"></span><br><span class="line">Response: rating  </span><br><span class="line">          Df Sum Sq Mean Sq F value    Pr(&gt;F)      </span><br><span class="line">sugars     1 8654.7  8654.7 223.774 &lt; 2.2e-16 ***  </span><br><span class="line">fiber      1 3480.0  3480.0  89.978 2.023e-14 ***  </span><br><span class="line">Residuals 74 2862.0    38.7                        </span><br><span class="line">---  </span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br></pre></td></tr></table></figure>

<p><font color="blue">注意：</font>方差分析表只给出了均方误差(MSE=Mean Sq=38.7) 而第一步中t检验中查看线性拟合时已经直接给出了F统计量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F-statistic: 156.9 on 2 and 74 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>

<p>其中的p值小于任何合理的显著性水平所要求的值，因而拒绝原假设。</p>
<h3 id="2-3-特定回归系数的置信区间"><a href="#2-3-特定回归系数的置信区间" class="headerlink" title="2.3 特定回归系数的置信区间"></a>2.3 特定回归系数的置信区间</h3><p>可以为某个回归系数构造一个 100(1-a)%的置信区间，与简单线性回归无异。</p>
<h3 id="2-4-给定x1-x2-x3-…-下，y均值的置信区间"><a href="#2-4-给定x1-x2-x3-…-下，y均值的置信区间" class="headerlink" title="2.4 给定x1,x2,x3,….下，y均值的置信区间"></a>2.4 给定x1,x2,x3,….下，y均值的置信区间</h3><p> 与简单线性回归类似，只不过变量增多。变为如：谷物为5.00克的糖和5.00克纤维时，营养级别的均值分布。</p>
<h2 id="三-多元回归的三个重要参数"><a href="#三-多元回归的三个重要参数" class="headerlink" title="三 多元回归的三个重要参数"></a>三 多元回归的三个重要参数</h2><h3 id="3-1-调整-R-2-：对包含无用预测变量的惩罚模式"><a href="#3-1-调整-R-2-：对包含无用预测变量的惩罚模式" class="headerlink" title="3.1 调整 $R^2$ ：对包含无用预测变量的惩罚模式"></a>3.1 调整 $R^2$ ：对包含无用预测变量的惩罚模式</h3><p>往模型里增加一个变量会增加决定系数$R^2$ 的值，不管这个变量是否有用。为了模型的简洁性，需要找到某种方法来惩罚包含无用预测变量模型 $R^2$ 的值，此即通常所说的[调整 $R^2$ ],其表达式如下：</p>
<p>$$<br>   R^2_{adj}= 1-(1-R^2)\frac{n-1}{n-m-1}<br>$$</p>
<p>如果[<em>调整</em> $R^2$ ]比 $R^2$ 小很多，则表明模型中至少有一个变量是多余的，分析人员需要考虑剔除。</p>
<h3 id="3-2-序贯的误差平方和-（sequential-sums-of-squares）"><a href="#3-2-序贯的误差平方和-（sequential-sums-of-squares）" class="headerlink" title="3.2 序贯的误差平方和 （sequential sums of squares）"></a>3.2 序贯的误差平方和 <em>（sequential sums of squares）</em></h3><p>序贯的误差平方和代表SSR中回归平方和的部分，SSR代表通过回应变量和一组预测变量的线性关系对总体变异解释的部分。序贯的误差平方和把SSR划分成各个唯一的SSR部分，分别由某个特定的预测变量来描述。因此，序贯的误差平方和的值取决于变量输入模型中的次序。下表是某种次序的序贯的武昌平方和，可以看出其实是对糖含量和营养级别的简单回归分析得到的SSR值。</p>
<p>模型：</p>
<p>$$<br>    y = b_0+b_1(糖)+b_2(纤维)+b_3(货架1)+b_4(货架4)+e<br>$$</p>
<p>的序贯平方和</p>
<table>
<thead>
<tr>
<th>来源</th>
<th>DF</th>
<th>Seq SS</th>
</tr>
</thead>
<tbody><tr>
<td>糖</td>
<td>1</td>
<td>8701.7</td>
</tr>
<tr>
<td>纤维</td>
<td>1</td>
<td>3416.1</td>
</tr>
<tr>
<td>货架1</td>
<td>1</td>
<td>0.3</td>
</tr>
<tr>
<td>货架2</td>
<td>1</td>
<td>152.4</td>
</tr>
</tbody></table>
<p>从表中可以看到第三个序贯误差平方和是对货架1的指示标量，值为0.3 ，它代表了营养级别中，在位置因素作用下货架1的变化率，这里糖和纤维的变化率已经被提取出来了，表示货架1的序贯平方和的值很小，表明这个变量很有可能是对估计营养级别是没有用的。</p>
<h3 id="3-3-偏F检验"><a href="#3-3-偏F检验" class="headerlink" title="3.3 偏F检验"></a>3.3 偏F检验</h3><p>假设模型中已经有了p个变量， $x_1,x_2,x_3,….x_p,$ 一个新的变量x<em>是否应该包含在此模型中？应该计算将x*加入到给定含有p个变量的模型中所产生的额外序列平方和，这个值表示为:<br>$$<br> SS_{extra}=SS(x</em>|x_1,x_2,x_3,….x_p)。<br>$$</p>
<p> 现在,额外序列平方和通过在全模型（包括 $x_1,x_2,x_3,….x_p$ 和x<em>）中的回归平方和计算得到，表示为 $SS_{full}=SS(x_1,x_2,x_3,….x_p,x</em>)$ ,从全模型的回归平方和中减速缩减模型（仅包含 $x_1,x_2,x_3,….x_p$）的回归平方和（表示为：$SS_{reduced}=SS(x_1,x_2,x_3,….x_p)$ ），也即：</p>
<p> $$<br>      SS_{extra}=SS_{full}-SS_{reduced}<br> $$</p>
<p>即：</p>
<p>$$<br>   SS(x<em>|x_1,x_2,….x_p)= SS(x_1,x_2,….x_p,x^</em>)-SS(x_1,x_2,..x_p)<br>$$</p>
<p>偏F检验的原假设如下：</p>
<ul>
<li><p>H0:否定 $SS_{extra}$与 $x^<em>$ 是相关的，对已经包含 $x_1,x_2,x_3,….x_p$ 的模型的回归平方和没有显著的共享。因此，模型中不应该包含 $x^</em>$ 。</p>
</li>
<li><p>H1:肯定SSextra与x<em>是相关的，对已经包含x1,x2,x3,….xp的模型的回归平方和有显著的贡献。因此，模型中应该包含x</em></p>
</li>
</ul>
<p>偏F检验的测试统计量是:</p>
<p>$$<br>  F(x^*|x_1,x_2,…x_p)=\frac{SS_{extra}}{MSE_{full}}</p>
<p>  MSE_{full}代表全模型的均方误差，包括x_1,x_2,…x_p和x^*<br>$$</p>
<p>当假设为真时，这个统计量服从 $F_{1,n-p-2}$ 的分布。因此，当 $F(x^*|x_1,x_2,x_3,….x_p)$ 值太大或者它像对应的p值太小时，有理由拒绝原假设。而偏F检验的一个可替代的方法是t检验。一个自由度为1和n-p-2的F检验等价于一个自由度为n-p-2的t检验。这是由他们之间的概率分布关系 $(F_{1,n-p-2}=(t_{n-p-2})^2)$</p>
<p><font color="blue">注意：</font>序贯平方和与部分平方和的区别如下：</p>
<table>
<thead>
<tr>
<th>变量</th>
<th>序贯平方和</th>
<th>部分平方和</th>
</tr>
</thead>
<tbody><tr>
<td>$x_1$</td>
<td>$SS(x_1)$</td>
<td>$SS(x_1|x_2,x_3,x_4)$</td>
</tr>
<tr>
<td>$x_2$</td>
<td>$SS(x_2|x_1)$</td>
<td>$SS(x_2|x_1,x_3,x_4)$</td>
</tr>
<tr>
<td>$x_3$</td>
<td>$SS(x_3|x_1,x_2)$</td>
<td>$SS(x_2|x_1,x_2,x_4)$</td>
</tr>
<tr>
<td>$x_4$</td>
<td>$SS(x_4|x_1,x_2,x_3)$</td>
<td>$SS(x_2|x_1,x_2,x_3)$</td>
</tr>
</tbody></table>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-04-24-datamining-mutilregression/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-04-24-datamining-mutilregression/" title="2013-04-24-datamining-mutilregression">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
            <a href="javascript: void(0);" class="cat">未分类</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-04-20-datamining-regression/">
    		2013-04-20-datamining-regression
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.218Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>title: 数据挖掘方法之二：回归模型（简单线性回归）<br>date: 2013-04-20 17:01:34<br>categories:</p>
<ul>
<li>数据挖掘</li>
</ul>
<h2 id="注：文中所使用代码为R"><a href="#注：文中所使用代码为R" class="headerlink" title="注：文中所使用代码为R"></a>注：文中所使用代码为R</h2><h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>简单线性回归模型是用于估计一个连续预测变量和一个连续回应变量的线性关系。回归方程或估计回归方程(estimated regression equation,ERE)：<br>$$\bar y=b_0+b_1x$$</p>
<ul>
<li>$\bar y$是回应变量的估计值</li>
<li>$b_0$是回归线在y轴上的截距</li>
<li>$b_1$是回归线的斜率</li>
<li>$b_0$和$b_1$称为回归系数</li>
</ul>
<h2 id="二-实例"><a href="#二-实例" class="headerlink" title="二 实例"></a>二 实例</h2><p>数据来源: <a href="http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html" target="_blank" rel="noopener">谷物数据集</a><br>数据描述：谷物数据集,包含了77种早餐谷物的16个属性对应的营养信息<br>首先导入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sugar&lt;-read.table(file&#x3D;&quot;&#x2F;LabData&#x2F;RData&#x2F;regression&#x2F;nutrition.txt&quot;,header&#x3D;TRUE)</span><br></pre></td></tr></table></figure>
<p>部分数据概览如下：</p>
<figure class="highlight plain"><figcaption><span>```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![数据集](&#x2F;images&#x2F;blog&#x2F;regression1.png)</span><br><span class="line"></span><br><span class="line">就给定谷物的含糖量对该谷物的营养成分进行评价，77种谷物的营养级别与含糖量的散点图和拟合回归线如下:</span><br></pre></td></tr></table></figure>
<pre><code>plot(data=sugar,rating~sugars,main=&quot;营养级别和含糖量的散点图及拟合线&quot;,xlab=&quot;含糖量&quot;,ylab=&quot;营养级别&quot;)  

lm.reg&lt;-lm(data=sugar,rating~sugars)  

abline(lm.reg,lty=4,lwd=3)   </code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![拟合](&#x2F;images&#x2F;blog&#x2F;regression2.png)</span><br><span class="line"></span><br><span class="line">使用线性回归模型拟合结果如下：</span><br></pre></td></tr></table></figure>
<pre><code>lm(data=sugar,rating~sugars)       
Call:      
lm(formula = rating ~ sugars, data = sugar)      
Coefficients:      
  (Intercept)       sugars        
   59.284       -2.401       </code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line">这里给定ERE为 ： $\bar y&#x3D;59.284-2.401*sugars$， 所以$b_0$&#x3D;59.284,$b_1$&#x3D;-2.401   </span><br><span class="line"></span><br><span class="line">### 误差残留   </span><br><span class="line"></span><br><span class="line"> **问题**：数据集中包含了一个含糖量(sugars&#x3D;1)为1的谷物 cheerios(数据概览中，黑色框部分)，其营养价值是50.765,而非估计值的56.98.</span><br><span class="line">  二者之差也即</span><br><span class="line"></span><br><span class="line">$$y-\bar y&#x3D;56.98-50.765&#x3D;6.215,$$</span><br><span class="line"></span><br><span class="line">称为预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)。</span><br><span class="line"></span><br><span class="line">为寻求这种预测误差总体尽可能小，最小二乘回归法会选择一条唯一的回归线，满足使得数据集的整体残差平方和达到最小值。有多重方法可以选择，如中位数回归方法，但最小二乘法回归是最常见的。   </span><br><span class="line">&lt;br&gt;</span><br><span class="line"></span><br><span class="line">## 三 误差评估</span><br><span class="line"></span><br><span class="line">### 1 最小二乘法估计   </span><br><span class="line"></span><br><span class="line">公式如下：</span><br><span class="line"></span><br><span class="line">$$y&#x3D;m_0+m_1x+e$$</span><br><span class="line">   &lt;font color&#x3D;&quot;blue&quot;&gt;其中误差项e引入用以解释不确定性的因素。&lt;&#x2F;font&gt;</span><br><span class="line"></span><br><span class="line">**基本假设**</span><br><span class="line"></span><br><span class="line">1. 零均值假设：误差项是期望为零的随机变量，即$E(e)&#x3D;0$</span><br><span class="line">2. 不变方差假设：误差项e的方差（用$σ^2$表示）是常数且与 $x_1,x_2,....$ 的值无关</span><br><span class="line">3. 独立性假设：e的变量是相互独立的</span><br><span class="line">4. 正态性假设：误差项e是正态随机变量,也即：误差项e的值是独立的正态分布随机变量，带有均值0和不变方差$σ^2$</span><br><span class="line"></span><br><span class="line">回应变量y的分布:</span><br><span class="line"></span><br><span class="line">(1)根据零假设，回应变量y的值均落在回归线上</span><br><span class="line">(2)根据不变方差假设，不论预测变量x1,x2,..取什么值，y的方差不变</span><br><span class="line">(3)根据独立性假设，对任意的 $x_1,x_2,..$ 取值，y的值都是相互独立的</span><br><span class="line">(4)根据正态性假设，回应变量y也是正态分布的随机变量。也即回应变量y也是独立正态变量，均值不变，方差不变。</span><br><span class="line">最小二乘回归线(least-square line)将误差的平方和最小化，总的预测误差用SSEp表示，则总的误差平方和为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SSE_p&#x3D;\sum^n_&#123;i-1&#125;\epsilon^2&#x3D;\sum^n_&#123;i-1&#125;(y_i-\beta_0-\beta_1x_i)^2</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">利用微积分，在以下微积分方程结果为0的时候, $b_0$ 和 $b_1$ 的取值会让总的误差平方和最小。关于 $b_0$ 和 $b_1$ 的偏微分方程为:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac &#123;\partial SSE_p&#125;&#123;\partial \beta_0&#125;&#x3D;-2\sum^n_&#123;i-1&#125;(y_i-\beta_0-\beta_1x_i)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac&#123;\partial SSE_p&#125;&#123;\partial \beta_1&#125;&#x3D;-2\sum^n_&#123;i-1&#125;x_i(y_i-\beta_0-\beta_1x_i)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">令上式为0，则有:&lt;br&gt;</span><br><span class="line">$$</span><br><span class="line">(y_i-\beta_0-\beta_1x_i)&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">(y_i-\beta_0-\beta_1x_i) &#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">分别求和，得到</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  \sum^n_&#123;i-1&#125;y_i-nb_0-b_1\sum^a_&#123;i-1&#125;x_i&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \sum^n_&#123;i-1&#125;x_iy_i-b_0\sum^n_&#123;i-1&#125;x_i-b_1\sum^n_&#123;i-1&#125;x^2_i&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">重新表示为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_0n+b_1\sum^n_&#123;i-1&#125;x_i&#x3D;\sum^n_&#123;i-1&#125;y_i</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   b_0\sum^n_&#123;i-1&#125;x_i+b_i\sum^n_&#123;i-1&#125;x_i^2&#x3D;\sum^n_&#123;i-1&#125;x_iy_i</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">求出 $b_0$ 和 $b_1$ 的值：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_1&#x3D;\frac&#123;\sum x_iy_i-[(\sum x_i)(\sum y_i)]&#x2F;n&#125;&#123;\sum x^2_i-(\sum x_i)^2&#x2F;n&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_0 &#x3D; \bar y-b_1\bar x (\bar x为x 的均值)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 2 决定系数</span><br><span class="line"></span><br><span class="line">$r^2$ 称为决定系数（coefficient of determination）用来衡量回归线的拟合度，也即最小二乘回归线产生的线性估计与实际观测数据的拟合程度。前面提到y^代表回应变量的估计值，$y-\bar y$ 代表预测误差或残差。</span><br><span class="line"></span><br><span class="line">#### 引子</span><br><span class="line"></span><br><span class="line">1.想象一下，如果不考虑数据集中含糖量而直接预测其营养价值，我们直观的做法是求其平均值作为预测值。假设开始为数据集里的每个记录计算(y-y&#39;)（其中y&#39;为回应变量的平均值），然后计算其平方和，这与计算误差(y-y^)，然后计算误差平方和类似。这时统计量总体误差平方和SST为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SST &#x3D;(y_1-y&#39;)^2+(y_2-y&#39;)^2+(y_3-y&#39;)^2....</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">SST，也称为总体平方和(sum of squares total，SST)是在没有考虑预测变量的情况下，衡量回应变量总体变异的统计量。</span><br><span class="line"></span><br><span class="line">2. 接下来是衡量估计回归方程能多大程度提高估计的准确度。运用回归线时的估计误差为： $y-\bar y$  ,当忽略含糖量信息时，估计误差是 $y-y&#39;$ 。因此改进量是：$\bar y-y&#39;$ .进一步基于 $\bar y-y&#39;$ 构造一个平方和的统计量，这样的统计量被称为回归平方和 *(sum of squares of regression,SSR)* ，是相对于忽略预测信息，衡量在使用回归线后预测精度提高的统计量，即：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SSR &#x3D; (\bar y_1-y&#39;)^2+(\bar y_2-y&#39;)^2+....</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">由: $y-y&#39;&#x3D;(\bar y-y&#39;)+(y-\bar y)$  两边都进行平方，然后进行总和运算，有：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  SST &#x3D;SSR +SSE</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">#### 结论   </span><br><span class="line"></span><br><span class="line">SST衡量了回应变量变异的一部分，这部分是被回应变量和预测变量之间的线性关系所解释的。然而不是所有的数据点都正好落在回归线上，这意味着还有一部分y变量的变异不能被回归线所解释。SSE可以被认为是衡量不能被x和y之间的回归线所解释的其他变异，包括随机变异。</span><br><span class="line">决定系数 $r^2$ ，它衡量了用回归线来描述预测变量和回应变量之间线性关系的符合程度</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   r^2 &#x3D;\frac&#123;SSR&#125;&#123;SST&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">### 3 估计值的标准误差</span><br><span class="line"></span><br><span class="line">**符号**：*S*</span><br><span class="line"></span><br><span class="line">**概念**：用于衡量由回归线产生估计值的精度的统计量。</span><br><span class="line">为介绍s，首先引入均方误差 *(mean squares error，MSE)* ：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    MSE &#x3D; \frac&#123;SSE&#125;&#123;n-m-1&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中，m标示预测变量的个数，简单线性回归是m&#x3D;1,多元线性回归时m大于1。与 *SSE* 一样， *MSE* 用于衡量在回应变量中没有被回归分析所解释的变异。</span><br><span class="line">标准误差的估计由下式给出:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   S &#x3D;\sqrt &#123;MSE&#125;&#x3D;\sqrt &#123;SSE&#x2F;(n-m-1)&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"> *S* 值为“典型”残差的估计，*s* 是衡量估计中的典型误差，即回应预测值与实际值之间的差异。也即标准误差能反应估计回归方程做出预测的精确度，因此 *s* 越小越好。</span><br><span class="line"></span><br><span class="line">### 4 其他评估   </span><br><span class="line"></span><br><span class="line">#### 1. 相关系数</span><br><span class="line"></span><br><span class="line">用来定义两个变量线性关系的统计量称为相关系数 *(correlation coefficient，也称皮尔森相关系数)* ，用来衡量变量之间线性关系强弱。计算公式如下：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   r &#x3D;\frac&#123;\sum (x-x&#39;)(y-y&#39;)&#125;&#123;(n-1)S_xS_y&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">+ 其中 $S_x$和 $S_y$分别代表样本x和y的标准差</span><br><span class="line">+ 相关系数r的取值范围为：(-1,1)</span><br><span class="line">+ 变量r的值越接近于1，表明二者正向相关性越大，随着x增大y也会增大。</span><br><span class="line">+ 变量r的值越接近于-1，表明二者负向相关性越大，随着x增大y会减小。</span><br><span class="line"></span><br><span class="line">#### 2. 方差分析表(ANOVA table)   </span><br><span class="line"></span><br><span class="line">一般形式如下：</span><br><span class="line"></span><br><span class="line">|变异源|平方和( *SS* )|自由度|均方差( *MS* )|F|</span><br><span class="line">|---|---|---|---|---|</span><br><span class="line">|回归| *SSR* | *m* | $MSR &#x3D;\frac&#123;SSR&#125;&#123;m&#125;$ | $F&#x3D; \frac&#123;MSR&#125;&#123;MSE&#125;$ |</span><br><span class="line">|误差| *SSE* | *n-m-1*| $MSE &#x3D;\frac&#123;SSE&#125;&#123;n-m-1&#125;$ ||</span><br><span class="line">|合计| *SST&#x3D;SSR+SSE* | *n-1* |||</span><br><span class="line"></span><br><span class="line">下面展示了 糖含量营养级别回归结果:</span><br></pre></td></tr></table></figure>
<p>anova&lt;-aov(data=sugar,rating~sugars)<br>summary(anova)<br>                 Df Sum Sq Mean Sq  F value   Pr(&gt;F)<br>    sugars       1   8655    8655   102.3    1.15e-15<br>    Residuals   75   6342      85                   </p>
<pre><code>sugars      ***  
Residuals        
           ---  </code></pre><p>  Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3. 异常点、高杠杠点和强影响观测值   </span><br><span class="line"></span><br><span class="line"> **高杠杆点** *(High leverage point)*：可以被认为是一个观测值在预测空间中的极限，也即一个高杠杆值可以被认为是不考虑y值得x变量的极限。杠杆第i个观察值hi可以被标示如下（x&#39;为平均数）：</span><br><span class="line"></span><br><span class="line"> $$</span><br><span class="line">   h_i &#x3D;\frac&#123;1&#125;&#123;h&#125;+\frac&#123;(x-x&#39;)^2&#125;&#123;\sum (x_i-x&#39;)^2&#125;</span><br><span class="line"> $$</span><br><span class="line"></span><br><span class="line">对于给定数据集,1&#x2F;n和右边分式分母都是常数，所以第i个观察的杠杆只依赖于 $(x_i-x&#39;)^2$。</span><br><span class="line">一个拥有大于 $\frac&#123;2*(m+1)&#125;&#123;n&#125;$ 和 $\frac&#123;3*(m+1)&#125;&#123;n&#125;$ 的观察点被认为是高杠杆点。</span><br><span class="line"></span><br><span class="line">**异常点** 观测到的偏离回归直（曲）线的点。一种粗略的评价观察值的方法是使用标准残留值 *(standardized residuals)*一般用:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   S_&#123;i,resid&#125;&#x3D;s\sqrt&#123;1-h_i&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">来标示第i个残留数的标准差，则hi代表第i个观测值的杠杆影响，那么标准残留值可以表示为:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    r_&#123;i,stand &#125;&#x3D; \frac&#123;(y-y&#39;_i)&#125;&#123;s_&#123;i,resid&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">如果标准残留值得绝对值超过了2，就可以认为是一个异常点,上图中观测点1和4应该是异常点。</span><br><span class="line"></span><br><span class="line"> **强影响力点** ：对数据集的分析造成较大影响的观测点。通常强影响力观测值同时有较大的残留值和较高的杠杆，但也有可能它既不是异常点也没有较高的杠杆，但两者特点组合成一个具有影响力的点。粗略估算一个观察点是否是强影响力点的方法是看它的Cook距离 *(Cook&#39;s distance)* 是否大于1.0，更确切的说，用Cook距离与F分布 *(m,n-m)* 来比较，若观测值落在分布的第一部分（低于25个百分点），就说它对整体分布只有一点点影响，若落在中点以后就说明该点是有影响力的。Cook距离将残留值和刚刚都考虑进去的，第i个观察点的距离可以为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac&#123;(y_i-y&#39;_i)^2&#125;&#123;(m+1)S^2&#125;\frac&#123;h_i&#125;&#123;1-h_i&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中 $y_i-y&#39;_i$表示第i个残留值，m表示预测变量的个数，s为标准误差的估计，hi为第i个观察点的杠杆。左边的比率含有一个元素代表了残留值，右边的函数代表了杠杆值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 四 回归推断</span><br><span class="line"></span><br><span class="line">最小二乘法回归是建立在一个假设基础上的线性回归模型，我们需要一个系统地框架来评估两个变量之间是否存在线性关系。对于最小二乘法的公式：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">     y &#x3D; m_0+m_1x+e</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">主要有以下四种方法：</span><br><span class="line"></span><br><span class="line">1.用来推断回应变量与预测变量之间关系的t检验法</span><br><span class="line">2. 斜率m1的置信区间</span><br><span class="line">3. 在给定一个特定的预测值条件下，回应变量&lt;B&gt;均值&lt;&#x2F;B&gt;的置信区间</span><br><span class="line">4. 在给定一个特定的预测值条件下，回应变量&lt;B&gt;随机值&lt;&#x2F;B&gt;的预测区间</span><br><span class="line"></span><br><span class="line">### 4.1 x和y之间线性关系的t检验</span><br><span class="line"></span><br><span class="line">对于简单线性回归t检验与F检验是等价的。&lt;br&gt;</span><br><span class="line"></span><br><span class="line">#### 对斜率的估计</span><br><span class="line"></span><br><span class="line">用最小二乘法估计的斜率m&#39;(注意m1是真实斜率)是一个统计量，像所有统计量一样服从一个特定均值和标准差的样本分布.斜率的回归推断是基于m&#39;的样本方差的点估计 Sm&#39;,Sm&#39;被解释为对斜率变异性的衡量指标，较大的Sm&#39;预示着斜率m*的估计是不稳定的。t检验基于统计量 $t&#x3D;(m&#39;-m_1)&#x2F;Sm&#39;$ ,它服从一个自由度为 $n-2$ 的t分布，当零假设为真（变量x和y之间不存在线性关系）时，检验统计量 $t&#x3D;m&#39;&#x2F;Sm&#39;$ 服从一个自由度为 *n-2* 的 t 分布。</span><br><span class="line"></span><br><span class="line">我们重新概览下77种谷物数据中营养级别与含糖量的线性回归结果：</span><br></pre></td></tr></table></figure>
<pre><code> lm.reg&lt;-lm(data=sugar,rating~sugars)  
 summary(lm.reg)  

Call:  
lm(formula = rating ~ sugars, data = sugar)  

Residuals:  
    Min      1Q  Median      3Q     Max   
-17.853  -5.677  -1.439   5.160  34.421   

Coefficients:  
            Estimate Std. Error t value Pr(&gt;|t|)      
(Intercept)  59.2844     1.9485   30.43  &lt; 2e-16 ***  
sugars       -2.4008     0.2373  -10.12 1.15e-15 ***  
---  
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  

Residual standard error: 9.196 on 75 degrees of freedom  
Multiple R-squared:  0.5771,    Adjusted R-squared:  0.5715   
F-statistic: 102.3 on 1 and 75 DF,  p-value: 1.153e-15</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">我们可以得到：</span><br><span class="line">  + 在系数列下面，找到斜率m&#39;的估计值为 -2.4008。</span><br><span class="line">  + 在SE系数列里找到斜率m&#39;的标准差Sm&#39;为 0.2373</span><br><span class="line">  + 在T值列找到t的统计量，即t检验的值 , $t&#x3D;\frac&#123;m&#39;&#125;&#123;Sm&#39;&#125;&#x3D;\frac&#123;-2.4008&#125;&#123;0.2373&#125;&#x3D;-10.1171$</span><br><span class="line">  + 在最后一列可以找到t检验的p值，是一个双尾检验，形式为：p值&#x3D; $P(|t|&gt;tobs)$ ，其中的tobs代表观测值。此处P值&#x3D; $P(|t|&gt;tobs)&#x3D;P(|t|&gt;-10.1171)$ 近似为0，小于任何显著性要求的合理界限，因此可以拒绝零假设也即认为含糖量和营养级别之前存在线性关系。</span><br><span class="line"></span><br><span class="line">### 4.2 回归直线斜率的置信区间</span><br><span class="line"></span><br><span class="line">置信区间也即在一定概率P下保证变量落在某区间 [a,b]内。对于回归直线的真实斜率 $m_1$来说，*100\*(1-c)%* 的置信区间也即有 100*(1-c)%的把握保证回归线的真实斜率位于 [ $m&#39;-(t_&#123;n-2&#125;)(Sm&#39;),m&#39;+(t_&#123;n-2&#125;)(Sm&#39;)$ ]区间。其中 $t_&#123;n-2&#125;$ 是自由度为n-2的 t 分布。</span><br><span class="line"></span><br><span class="line">例如构建一个回归直线的真实斜率 $m_1$的95%的置信区间。有一个对m1的点估计值 m&#39;&#x3D;-2.4008,对95%的之心去和自由度为 $n-2&#x3D;77-2&#x3D;75$ 的t临界值为 2.0(查表t75.95%&#x3D;2.0),从表中得到 Sm&#39;&#x3D;0.2373，因此置信区间为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    [ $m&#39;-(t_&#123;n-2&#125;)(Sm&#39;),m&#39;+(t_&#123;n-2&#125;)(Sm&#39;)$ ]</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">相关代码及结果如下：&lt;br&gt;</span><br></pre></td></tr></table></figure>
<p>lm.reg&lt;-lm(data=sugar,rating~sugars)<br>#level=0.95为置信度<br>confint(lm.reg,level=0.95)<br>                    2.5 %    97.5 %<br>    (Intercept) 55.402783 63.165952<br>    sugars      -2.873567 -1.92807</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 4.3 给定x条件下，y均值的置信区间和y随机选择值的预测区间</span><br><span class="line"></span><br><span class="line">给定x条件下，y&lt;B&gt;均值&lt;&#x2F;B&gt;的置信区间由如下公式判定：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  \bar y\pm t_&#123;n-2&#125;(s)\sqrt &#123;\frac&#123;1&#125;&#123;n&#125;+\frac&#123;(x_p-\bar x)^2&#125;&#123;\sum(x_i-\bar x)^2&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中 $\bar y$ 表示给定x值后，y的点估计值，$t_&#123;n-2&#125;$ 是与样本大小和置信水平相关联的乘数，s是估计的标准误差，$\bar x$ 是产生预测值所对应的x专指变量。</span><br><span class="line"></span><br><span class="line">给定x条件下，y随机选择值的预测区间:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \bar y\pm t_&#123;n-2&#125;(s)\sqrt &#123;1+\frac&#123;1&#125;&#123;n&#125;+\frac&#123;(x_p-\bar x)^2&#125;&#123;\sum(x_i-\bar x)^2&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"> **注意**：第二张图中表达式与第一张图中表达式相比，除了在平方根李出现了 **&quot;1+&quot;** 外完全一样，这说明比起均值估计来，对于单个y值得估计会有更大的变化范围，这也说明了预测区间总是比类似的置信区间要宽</span><br><span class="line"></span><br><span class="line">我们希望预测含糖量为 sugars&#x3D;10时该谷物的营养级别范围，实例代码如下：</span><br></pre></td></tr></table></figure>
<pre><code>&gt; point&lt;-data.frame(sugars=10)  
&gt; point  
  sugars  
1     10  
&gt; lm.reg  

Call:  
lm(formula = rating ~ sugars, data = sugar)  

Coefficients:  
(Intercept)       sugars    
     59.284       -2.401    

&gt; lm.pred&lt;-predict(lm.reg,point,interval=&quot;prediction&quot;,level=0.95)  
&gt; lm.pred  
       fit     lwr      upr  
 1 35.27617 16.7815 53.77083</code></pre><pre><code>
可以看到在95%的置信度下，含糖量为10的谷物其营养级别介于 16.7815和53.77083之间</code></pre>
            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-04-20-datamining-regression/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-04-20-datamining-regression/" title="2013-04-20-datamining-regression">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
            <a href="javascript: void(0);" class="cat">未分类</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2013-04-01-dataming-PCA/">
    		2013-04-01-dataming-PCA
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.200Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>title: 数据挖掘方法之一：主成份分析<br>date: 2013-04-01 17:01:34<br>categories:</p>
<ul>
<li>数据挖掘</li>
</ul>
<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>主成分分析（principle componentsanalysis,PCA）是指将多个变量通过线性组合，选出较少个数的重要变量集合来描述相关结构的额一种统计分析方法，这些线性组合被称为“成分”。由m个变量组成的数据集的总变异，可以由k个线性组合变量组成的子集来标示（k&lt;m）。这意味着k个变量与原来的变量反应了几乎同样多的信息  </p>
<h2 id="二-步骤"><a href="#二-步骤" class="headerlink" title="二 步骤"></a>二 步骤</h2><ol>
<li><p>原始指标数据的标准化采集p维随机向量</p>
<p>$$x = (x_1,x_2,…x_p)^T$$</p>
<p>$n$个样本<br>$x_i= (x_{i1},x_{i2},x_{i3},…x_{ip})^T,i=1,2..n, n&gt;p$</p>
<p>构造样本阵，对样本元素进行如下标准化变换</p>
</li>
</ol>
<p>$$Z_{ij}=\frac{x_{ij}-\bar x_j}{s_J},i =1,2,..\pi, j =1,2,…p$$</p>
<p>其中$$\bar x_j =\frac{\sum^n_{i-1}x_{ij}}{n},s^2_j=\frac{\sum^n_{i-1}(x_{ij}-\bar x_j)^2}{n-1}$$</p>
<p>   得标准化矩阵Z。</p>
<ol start="2">
<li><p>对标准化矩阵Z求相关系数矩阵    </p>
<p> $$R = [r_{ij}] xp =\frac{Z^TZ}{n-1}其中r_{ij}=\frac{\sum z_{kj}\cdot z_{kj}}{n-1}, i,j = 1,2…p$$</p>
</li>
<li><p>求解样本相关矩阵R的特征方程</p>
<p>$R-\lambda I_p =0$ 得 $p$ 个特征根，按<br>$$\frac{sum^m_{j-1}\lambda <em>j}{\sum^p</em>{j-1}\lambda _j}\geq 0.85<br>确定n的值，使信息的利用率达85%以上。对每个 $\lambda_j,i=1,2,..m$ 解方程组$R_b= \lambda_jb$得单位特征向量$b^0_j$$</p>
</li>
<li><p>将标准化后的指标变量转换成主成分。</p>
<p>$U_{ij}=z^Tb^0_j,j=1,2,…m$</p>
<p>$U_1$称为第一主成分，$U_2$称为第二主成分，$U_3$称为第三主成分，…</p>
<p>对n个主成份进行综合评价。对n个主成份进行加权求和，即得最终评价值，权数为每个主成份的方差贡献率。</p>
</li>
</ol>
<h2 id="三-关键性结论"><a href="#三-关键性结论" class="headerlink" title="三 关键性结论"></a>三 关键性结论</h2><p>以下结论对主成份分析非常重要</p>
<p><strong>结论1</strong>：标准化数据集州农工的总体变动性等于所有Z向量方差之和，等于每个成分方差之和，等于特征权值之和，等于变量的个数。即<br>$$<br>  \sum^m_{i-1}Var(Y_i)=\sum^m_{i-1}Var(Z_i)=\sum^m_{i-1}\lambda _i=m<br>$$</p>
<p><strong>结论2</strong>：给定成分与给定变量间的偏相关性是特征向量与特征值的函数。</p>
<p>具体来说<br>$$Corr(Y_i,Z_i)=e_{ii}\sqrt\lambda _i,i,j=1,2,..m,\lambda _1\gt\lambda _2\geq\lambda _3…\geq\lambda _m$$<br>其中 $(\lambda _1,e_1),(\lambda _2,e_2),(\lambda _3,e_3),..(\lambda _m,e_m)$<br>是相关系数矩阵p的<font color="gray">特征值-特征向量</font>对，并且偏相关系数包括了所有变量之间的影响。</p>
<p><strong>结论3</strong>：Z中第i个主成份解释了变量的总体变异的百分比，等于第i个特征根与变量个数之间的比率 $\frac{\lambda _i}{m}$</p>
<h2 id="四-应用于实际数据"><a href="#四-应用于实际数据" class="headerlink" title="四 应用于实际数据"></a>四 应用于实际数据</h2><table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>最小值</th>
<th>最大值</th>
<th>均值</th>
<th>标准差</th>
</tr>
</thead>
<tbody><tr>
<td>房屋价值中位数</td>
<td>Range</td>
<td>14999</td>
<td>500001</td>
<td>206918.067</td>
<td>115485.040</td>
</tr>
<tr>
<td>收入中位数</td>
<td>Range</td>
<td>0.500</td>
<td>15.000</td>
<td>3.873</td>
<td>1.906</td>
</tr>
<tr>
<td>平均房龄中位数</td>
<td>Range</td>
<td>1</td>
<td>52</td>
<td>28.656</td>
<td>12.582</td>
</tr>
<tr>
<td>总房间数</td>
<td>Range</td>
<td>2</td>
<td>37937</td>
<td>2621.653</td>
<td>2131.644</td>
</tr>
<tr>
<td>总卧室数</td>
<td>Range</td>
<td>1</td>
<td>6445</td>
<td>535.096</td>
<td>413.541</td>
</tr>
<tr>
<td>人口数</td>
<td>Range</td>
<td>3</td>
<td>35682</td>
<td>1418.971</td>
<td>1122.534</td>
</tr>
<tr>
<td>家庭数</td>
<td>Range</td>
<td>1</td>
<td>6082</td>
<td>479.332</td>
<td>377.378</td>
</tr>
<tr>
<td>北纬</td>
<td>Range</td>
<td>32.540</td>
<td>41.950</td>
<td>35.630</td>
<td>2.137</td>
</tr>
<tr>
<td>西经</td>
<td>Range</td>
<td>-124.350</td>
<td>-114.310</td>
<td>-119.567</td>
<td>2.003</td>
</tr>
</tbody></table>
<p>1.使用上图的均值和标准差对变量进行标准化，得到Z向量。<br>2.研究下图中变量矩阵图以检验变量间是否存在相关性。<br> 可以看到 总房数、卧室数、人口数和家庭数之间表现出正相关性，西经和北纬表现出负相关性。再来观察变量的相关性矩阵：</p>
<table>
<thead>
<tr>
<th></th>
<th>平均收入</th>
<th>平均房龄</th>
<th>总房数</th>
<th>总卧室数</th>
<th>人口数</th>
<th>家庭数</th>
<th>北纬</th>
<th>西经</th>
</tr>
</thead>
<tbody><tr>
<td>平均收入</td>
<td>1.000</td>
<td>-0.117</td>
<td>0.199</td>
<td>-0.012</td>
<td>0.002</td>
<td>0.010</td>
<td>-0.083</td>
<td>-0.012</td>
</tr>
<tr>
<td>平均房龄</td>
<td>-0.117</td>
<td>1.000</td>
<td>-0.360</td>
<td>-0.318</td>
<td>-0.292</td>
<td>-0.300</td>
<td>0.011</td>
<td>-0.107</td>
</tr>
<tr>
<td>总房数</td>
<td>0.199</td>
<td>-0.360</td>
<td>1.000</td>
<td>0.928</td>
<td>0.856</td>
<td>0.919</td>
<td>-0.035</td>
<td>0.041</td>
</tr>
<tr>
<td>总卧室数</td>
<td>-0.012</td>
<td>-0.318</td>
<td>0.928</td>
<td>1.000</td>
<td>0.878</td>
<td>0.981</td>
<td>-0.064</td>
<td>0.064</td>
</tr>
<tr>
<td>人口数</td>
<td>0.002</td>
<td>-0.292</td>
<td>0.856</td>
<td>0.878</td>
<td>1.000</td>
<td>0.907</td>
<td>-0.107</td>
<td>0.097</td>
</tr>
<tr>
<td>家庭数</td>
<td>0.010</td>
<td>-0.300</td>
<td>0.919</td>
<td>0.981</td>
<td>0.907</td>
<td>1.000</td>
<td>-0.069</td>
<td>0.051</td>
</tr>
<tr>
<td>北纬</td>
<td>-0.083</td>
<td>0.011</td>
<td>-0.035</td>
<td>-0.064</td>
<td>-0.107</td>
<td>-0.069</td>
<td>1.000</td>
<td>-0.925</td>
</tr>
<tr>
<td>西经</td>
<td>-0.012</td>
<td>-0.107</td>
<td>0.041</td>
<td>0.064</td>
<td>0.097</td>
<td>0.051</td>
<td>-0.925</td>
<td>1.000</td>
</tr>
</tbody></table>
<p> 矩阵图和相关矩阵式常用的两种方法，用来观察预测变量之间的相关性结构。<br>  如果完成一次住房价预测的多元回归分析，但不考虑数据集中的多重共线性将导致回归结果非常不稳定，预测值的微小变化将会导致回归系数的极大变化，而得不到任何结论。此时需要用主成分分析，其可以通过相关化结构，确定相关变量的基本组成部分<br>3. 采用主成分分析对房屋数据集进行分析，该要素矩阵（下图）中每个栏目代表成分Yi=e’Z中的一项。栏目中元素为成分的权重，代表了变量与成分的偏相关。结论2 表明这些成分的权重等于Corr(Yi,Zi),成分涉及第i个特征向量和特征值</p>
<table>
<thead>
<tr>
<th></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
</tr>
</thead>
<tbody><tr>
<td>平均收入</td>
<td>0.086</td>
<td>-0.058</td>
<td>0.922</td>
<td>0.370</td>
<td>-0.02</td>
<td>-0.018</td>
<td>0.037</td>
<td>-0.004</td>
</tr>
<tr>
<td>平均房龄</td>
<td>-0.429</td>
<td>0.025</td>
<td>-0.407</td>
<td>0.806</td>
<td>0.014</td>
<td>0.026</td>
<td>0.009</td>
<td>-0.001</td>
</tr>
<tr>
<td>总房数</td>
<td>0.956</td>
<td>0.100</td>
<td>0.102</td>
<td>0.104</td>
<td>0.120</td>
<td>0.162</td>
<td>-0.119</td>
<td>0.015</td>
</tr>
<tr>
<td>总卧室数</td>
<td>0.970</td>
<td>0.083</td>
<td>-0.121</td>
<td>0.056</td>
<td>0.144</td>
<td>-0.068</td>
<td>0.051</td>
<td>-0.083</td>
</tr>
<tr>
<td>人口数</td>
<td>0.933</td>
<td>0.034</td>
<td>-0.121</td>
<td>0.076</td>
<td>-0.327</td>
<td>0.034</td>
<td>0.006</td>
<td>-0.015</td>
</tr>
<tr>
<td>家庭数</td>
<td>0.972</td>
<td>0.086</td>
<td>-0.113</td>
<td>0.087</td>
<td>0.058</td>
<td>-0.112</td>
<td>0.061</td>
<td>0.083</td>
</tr>
<tr>
<td>北纬</td>
<td>-0.140</td>
<td>0.970</td>
<td>0.017</td>
<td>-0.088</td>
<td>0.017</td>
<td>0.132</td>
<td>0.113</td>
<td>0.005</td>
</tr>
<tr>
<td>西经</td>
<td>0.144</td>
<td>-0.969</td>
<td>-0.062</td>
<td>-0.063</td>
<td>0.037</td>
<td>0.136</td>
<td>0.109</td>
<td>0.007</td>
</tr>
</tbody></table>
<ol start="4">
<li>结论3表明，Z的总变异种第i个主成分所占的比例是ri/m(其中ri是特征值)，即第i个特征值与变量数的比例。由下图可以看出，第一特征值是 3.091,因为有8个预测变量，第一主成分解释 $\frac{3.091}{8}=48.767$ 的变异。</li>
</ol>
<table>
<thead>
<tr>
<th>成分</th>
<th>合计</th>
<th>变化百分比</th>
<th>累计百分比%</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>3.901</td>
<td>48.767</td>
<td>48.767</td>
</tr>
<tr>
<td>2</td>
<td>1.910</td>
<td>23.881</td>
<td>72.648</td>
</tr>
<tr>
<td>3</td>
<td>1.073</td>
<td>13.409</td>
<td>86.057</td>
</tr>
<tr>
<td>4</td>
<td>0.825</td>
<td>10.311</td>
<td>96.368</td>
</tr>
<tr>
<td>5</td>
<td>0.148</td>
<td>1.847</td>
<td>98.215</td>
</tr>
<tr>
<td>6</td>
<td>0.082</td>
<td>1.020</td>
<td>99.235</td>
</tr>
<tr>
<td>7</td>
<td>0.047</td>
<td>0.586</td>
<td>99.821</td>
</tr>
<tr>
<td>8</td>
<td>0.014</td>
<td>0.179</td>
<td>100.00</td>
</tr>
</tbody></table>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2013-04-01-dataming-PCA/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2013-04-01-dataming-PCA/" title="2013-04-01-dataming-PCA">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    

    
    <nav class="page-navigator">
        <a class="extend prev" rel="prev" href="/archives/2019/12/page/9/">前一页</a><a class="page-number" href="/archives/2019/12/">1</a><span class="space">&hellip;</span><a class="page-number" href="/archives/2019/12/page/8/">8</a><a class="page-number" href="/archives/2019/12/page/9/">9</a><span class="page-number current">10</span>
    </nav>
    


            </div>

        </section>
        <!-- 侧栏部分 -->
<aside class="sidebar">
    <section class="widget">
        <h3 class="widget-hd"><strong>文章分类</strong></h3>
        <!-- 文章分类 -->
<ul class="widget-bd">
    
    <li>
        <a href="/categories/blog/">blog</a>
        <span class="badge">(94)</span>
    </li>
    
</ul>
    </section>

    
    <section class="widget">
        <h3 class="widget-hd"><strong>热门标签</strong></h3>
        <!-- 文章标签 -->
<div class="widget-bd tag-wrap">
  
</div>
    </section>
    

    

    
    <!-- 友情链接 -->
    <section class="widget">
        <h3 class="widget-hd"><strong>友情链接</strong></h3>
        <!-- 文章分类 -->
<ul class="widget-bd">
    
        <li>
            <a href="https://jelon.top" target="_blank" title="Jelon个人前端小站">前端博客小站</a>
        </li>
    
        <li>
            <a href="https://www.baidu.com" target="_blank" title="百度搜索">百度</a>
        </li>
    
</ul>
    </section>
    
</aside>
<!-- / 侧栏部分 -->
    </div>

    <!-- 博客底部 -->
    <footer class="footer">
    &copy;
    
        2016-2019
    

    <a href="/">Jelon Loves You</a>
</footer>
<div class="back-to-top" id="JELON__backToTop" title="返回顶部">返回顶部</div>

    <!--博客js脚本 -->
    <!-- 这里放网站js脚本 -->

<script src="/js/main.js"></script>

</body>
</html>