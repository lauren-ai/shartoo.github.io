<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8" >
    <meta name="baidu-site-verification" content="dIcXMeY8Ya" />
    
    <title>`blog`分类下的文章 | Hexo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0" >
    <meta name="keywords" content="Jelon, 前端, Web, 张德龙, 前端开发" >
    <meta name="description" content="Jelon个人前端小站" >

    
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml" >
    
    
    <link rel="shortcut icon" href="/favicon.ico" >
    
    
<link rel="stylesheet" href="/css/style.css">

    <!--[if lt IE 9]>
    
<script src="/js/html5.js"></script>

    <![endif]-->
    
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?fd459238242776d173cdc64918fb32f2";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


<meta name="generator" content="Hexo 4.2.0"></head>

<body class="home">
    <!--[if lt IE 9]>
    <div class="browsehappy">
        当前网页 <strong>不支持</strong>
        你正在使用的浏览器. 为了正常的访问, 请 <a href="http://browsehappy.com/" target="_blank" rel="noopener">升级你的浏览器</a>.
    </div>
    <![endif]-->

    <!-- 博客头部 -->
    <header class="header">
    <section class="container header-main">
        <div class="logo">
            <a href="/">
                <div class="cover">
                    <span class="name">Hexo</span>
                    <span class="description"></span>
                </div>
            </a>
        </div>
        <div class="dropnav icon-paragraph-justify" id="JELON__btnDropNav"></div>
        <ul class="menu hidden" id="JELON__menu">
            
            <li rel="/categories/blog/index.html" class="item ">
                <a href="/" title="首页" class="icon-home">&nbsp;首页</a>
            </li>
            
            <li rel="/categories/blog/index.html" class="item ">
                <a href="/lab/" title="实验室" class="icon-lab">&nbsp;实验室</a>
            </li>
            
            <li rel="/categories/blog/index.html" class="item ">
                <a href="/about/" title="关于" class="icon-about">&nbsp;关于</a>
            </li>
            
            <li rel="/categories/blog/index.html" class="item ">
                <a href="/comment/" title="留言" class="icon-comment">&nbsp;留言</a>
            </li>
            
        </ul>
        <div class="profile clearfix">
            <div class="feeds fl">
                
                
                <p class="links">
                    
                        <a href="https://github.com/jangdelong" target="_blank">Github</a>
                        |
                    
                        <a href="https://pages.coding.me" target="_blank">Hosted by Coding Pages</a>
                        
                    
                </p>
                <p class="sns">
                    
                        <a href="http://weibo.com/jangdelong" class="sinaweibo" target="_blank"><b>■</b> 新浪微博</a>
                    
                        <a href="https://www.facebook.com/profile.php?id=100011855760219&amp;ref=bookmarks" class="qqweibo" target="_blank"><b>■</b> Facebook</a>
                    
                    <a href="javascript: void(0);" class="wechat">
                        <b>■</b>
                        公众号
                        <span class="popover">
                            <img src="/img/wechat_mp.jpg" width="120" height="120" alt="我的微信订阅号">
                            <i class="arrow"></i>
                        </span>
                    </a>
                </p>
                
            </div>
            <div class="avatar fr">
                <img src="/img/jelon.jpg" alt="avatar" title="Jelon" >
            </div>
        </div>
    </section>
</header>


    <!-- 博客正文 -->
    <div class="container body clearfix">
        <section class="content">
            <div class="content-main widget">
                <!-- 文章分类 -->

    <h3 class="widget-hd">
        <strong>
            
                `blog`分类下的文章
            
        </strong>
    </h3>
    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/template/">
    		博客题目
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.760Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                
            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/template/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/template/" title="博客题目">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-11-26-model-pruning/">
    		模型剪枝和优化-torch和Tensorflow为例
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.757Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1 基本概念"></a>1 基本概念</h2><h3 id="1-1-基本问题"><a href="#1-1-基本问题" class="headerlink" title="1.1 基本问题"></a>1.1 基本问题</h3><p>网络剪枝目标是</p>
<ul>
<li>更小的模型</li>
<li>更快的推理(inference)速度</li>
<li>不对准确率精度等造成过多损失</li>
</ul>
<p>相关技术有</p>
<ul>
<li>权重共享</li>
<li>量化(quantization)</li>
<li>低阶近似(Low-Rank Approximation)</li>
<li>二元/三元网络(Binary / Ternary Net)</li>
<li>Winograd Transformation</li>
</ul>
<h3 id="1-2-当前神经网络遇到的一些挑战"><a href="#1-2-当前神经网络遇到的一些挑战" class="headerlink" title="1.2 当前神经网络遇到的一些挑战"></a>1.2 当前神经网络遇到的一些挑战</h3><ol>
<li>模型变得越来越大</li>
</ol>
<p><img src="/images/blog/model_pruning_1.png" alt="模型剪枝和优化"></p>
<ol start="2">
<li>速度越来越慢</li>
</ol>
<p><img src="/images/blog/model_pruning_2.png" alt="模型剪枝和优化"></p>
<ol start="3">
<li>能源效率</li>
</ol>
<p>AlphaGo 使用了1920个CPU和280个GPU，每场比赛消耗3000美元的电力。</p>
<h3 id="1-3-网络剪枝的原理"><a href="#1-3-网络剪枝的原理" class="headerlink" title="1.3 网络剪枝的原理"></a>1.3 网络剪枝的原理</h3><p>将原本的稠密连接网络，删去不必要的连接，变成右边相对稀疏的网络。<strong>稀疏网络易于压缩，并且可以在预测时跳过零值，提高推理速度</strong>。</p>
<p><img src="/images/blog/model_pruning_3.png" alt="模型剪枝和优化"></p>
<p>如果可以对网络的所有神经元贡献度排序，我们可以删除排在末尾的神经元，这样可就可以减小网络获得更快的推理速度。</p>
<p>可以使用神经元的权重的L1/L2正则来做排序。剪枝之后，准确率将会降低。通常会执行<code>训练</code>$\rightarrow$<code>剪枝</code>$\rightarrow$<code>训练</code>$\rightarrow$<code>剪枝</code>..的循环中。如果一次剪枝过多，网络可能会损坏，无法恢复。所以在实践中，这是一个迭代执行的步骤。</p>
<h2 id="2-剪枝技术"><a href="#2-剪枝技术" class="headerlink" title="2 剪枝技术"></a>2 剪枝技术</h2><h3 id="2-1-权重剪枝"><a href="#2-1-权重剪枝" class="headerlink" title="2.1 权重剪枝"></a>2.1 权重剪枝</h3><ul>
<li>将权重矩阵中孤立(没有与其他权重项有连接的)的权重设置为0。这对应着上图中删除了连接</li>
<li>此处，为了达到k%的稀疏度，我们将孤立的权重排序。在权重矩阵中，W对应了梯度，然后将最小的k%设置为0。下面的代码演示了这个过程</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f &#x3D; h5py.File(&quot;model_weights.h5&quot;,&#39;r+&#39;)</span><br><span class="line">for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:</span><br><span class="line"> ranks &#x3D; &#123;&#125;</span><br><span class="line"> for l in list(f[‘model_weights’])[:-1]:</span><br><span class="line"> data &#x3D; f[‘model_weights’][l][l][‘kernel:0’]</span><br><span class="line"> w &#x3D; np.array(data)</span><br><span class="line"> ranks[l]&#x3D;(rankdata(np.abs(w),method&#x3D;’dense’) — 1).astype(int).reshape(w.shape)</span><br><span class="line"> lower_bound_rank &#x3D; np.ceil(np.max(ranks[l])*k).astype(int)</span><br><span class="line"> ranks[l][ranks[l]&lt;&#x3D;lower_bound_rank] &#x3D; 0</span><br><span class="line"> ranks[l][ranks[l]&gt;lower_bound_rank] &#x3D; 1</span><br><span class="line"> w &#x3D; w*ranks[l]</span><br><span class="line"> data[…] &#x3D; w</span><br></pre></td></tr></table></figure>

<h3 id="2-2-神经元剪枝"><a href="#2-2-神经元剪枝" class="headerlink" title="2.2 神经元剪枝"></a>2.2 神经元剪枝</h3><ul>
<li>将神经元对应的权重矩阵中的一整列的值全部设为0，这等同于删除了对应的输出神经元</li>
<li>此处，要达到k%的稀疏度，我们对权重矩阵的列排序，排序规则是它们的L2正则，然后删除最小的k%。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">f &#x3D; h5py.File(&quot;model_weights.h5&quot;,&#39;r+&#39;)</span><br><span class="line">for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:</span><br><span class="line"> ranks &#x3D; &#123;&#125;</span><br><span class="line"> for l in list(f[‘model_weights’])[:-1]:</span><br><span class="line">     data &#x3D; f[‘model_weights’][l][l][‘kernel:0’]</span><br><span class="line">     w &#x3D; np.array(data)</span><br><span class="line">     norm &#x3D; LA.norm(w,axis&#x3D;0)</span><br><span class="line">     norm &#x3D; np.tile(norm,(w.shape[0],1))</span><br><span class="line">     ranks[l] &#x3D; (rankdata(norm,method&#x3D;’dense’) — 1).astype(int).reshape(norm.shape)</span><br><span class="line">     lower_bound_rank &#x3D; np.ceil(np.max(ranks[l])*k).astype(int)</span><br><span class="line">     ranks[l][ranks[l]&lt;&#x3D;lower_bound_rank] &#x3D; 0</span><br><span class="line">     ranks[l][ranks[l]&gt;lower_bound_rank] &#x3D; 1</span><br><span class="line">     w &#x3D; w*ranks[l]</span><br><span class="line">     data[…] &#x3D; w</span><br></pre></td></tr></table></figure>
<p>通常随着你增加稀疏度，并且删除越来越多的神经元，模型的性能会下降，此时就需要对模型性能和稀疏度作出取舍了。</p>
<h3 id="2-3-权重稀疏和神经元稀疏的对比"><a href="#2-3-权重稀疏和神经元稀疏的对比" class="headerlink" title="2.3 权重稀疏和神经元稀疏的对比"></a>2.3 权重稀疏和神经元稀疏的对比</h3><p><img src="/images/blog/model_pruning_4.png" alt="模型剪枝和优化"></p>
<p>看起来权重稀疏更柔和一些。</p>
<p><img src="/images/blog/model_pruning_5.png" alt="模型剪枝和优化"></p>
<p>权重稀疏和神经元稀疏在减小网络尺寸上效果相同。</p>
<h3 id="2-4-剪枝的问题"><a href="#2-4-剪枝的问题" class="headerlink" title="2.4 剪枝的问题"></a>2.4 剪枝的问题</h3><p>参考自<a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" target="_blank" rel="noopener">Pruning deep neural networks to make them fast and small</a>，说明尽管有诸多剪枝的论文，但是在现实世界里很少使用剪枝，究其原因，可能有如下</p>
<ul>
<li>按照贡献度排序的方法目前为止上不够完善，精度损失过高</li>
<li>难以实现</li>
<li>一些公司使用了剪枝技术，但是没有公开这个秘密</li>
</ul>
<h2 id="3-剪枝实践"><a href="#3-剪枝实践" class="headerlink" title="3 剪枝实践"></a>3 剪枝实践</h2><h3 id="3-1-剪枝为了速度VS为了更小的模型"><a href="#3-1-剪枝为了速度VS为了更小的模型" class="headerlink" title="3.1 剪枝为了速度VS为了更小的模型"></a>3.1 剪枝为了速度VS为了更小的模型</h3><p>VGG模型90%的权重在后面的全连接层，但是只贡献了1%的浮点运算。最近，人们才开始专注裁剪全连接层，通过替换全连接层模型尺寸会大幅度缩减。此处只关注于裁剪整个卷积层，但是它有个很好的副作用就是同事减小了内存消耗，如论文<a href="https://arxiv.org/abs/1611.06440" target="_blank" rel="noopener">1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference</a>所述，网络层越深，越容易被裁剪。这表明最后的卷积层会大幅度被裁剪，全连接后面的诸多神经元也会被抛弃。</p>
<p>对卷积层裁剪时，同时也可以对每个卷积核做权重衰减，或者移除某个卷积核的某个特定维度(列)，这样会得到稀疏的卷积核，这么得来的结果无法得到计算速度的提升。最近的研究提倡<code>结构稀疏</code>,即整个卷积核被裁剪掉。</p>
<p>另外一个重要提示是<strong>通过训练然后裁剪一个大网络，尤其在迁移学习时，其结果比从头训练一个小网络要好得多</strong></p>
<h3 id="3-2-裁剪卷积核"><a href="#3-2-裁剪卷积核" class="headerlink" title="3.2 裁剪卷积核"></a>3.2 裁剪卷积核</h3><p>参考论文<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="noopener">Pruning filters for effecient convents</a>.</p>
<p>此论文提倡裁剪掉整个卷积核。裁剪一个卷积核的索引k，影响的是它所在网络层，以及后续的网络层。所有在索引k处的输入通道，在后续网络层会被移除掉，如下图。</p>
<p><img src="/images/blog/model_pruning_6.png" alt="模型剪枝和优化"></p>
<p>假若后续层是全连接层，以及feature map的通道的尺寸会是$M\times N$，那么将会从全连接层中移除$M\times N$个神经元。</p>
<p><strong>神经元的排序相当简单，即它们每个卷积核的权重的L1 norm。</strong></p>
<p>每次剪枝迭代都会对所有卷积核的权重L1 norm排序，裁剪掉末尾的m个filter，重新训练，并重复。</p>
<h3 id="3-3-结构剪枝"><a href="#3-3-结构剪枝" class="headerlink" title="3.3 结构剪枝"></a>3.3 结构剪枝</h3><p>参考论文<a href="https://arxiv.org/abs/1512.08571" target="_blank" rel="noopener">1512.08571 Structured Pruning of Deep Convolutional Neural Networks</a></p>
<p>论文内容与上面差不多，但是排序算法复杂得多。论文使用了一个有N个粒子过滤器(particle filters)的集合，保存了N个即将被裁剪的卷积核。</p>
<p>如果粒子(particle)所代表的卷积核没有被mask划出，每个粒子(particle)被分配一个基于网络在验证集上准确率的得分。然后基于新的得分，会得到新的裁剪mask。<br>由于此步骤执行起来相对繁琐，论文使用了较小的验证集以衡量粒子得分。</p>
<h3 id="3-4-nvidia裁剪：卷积核裁剪以提升资源推理效率-Resource-Efficient-Inference"><a href="#3-4-nvidia裁剪：卷积核裁剪以提升资源推理效率-Resource-Efficient-Inference" class="headerlink" title="3.4 nvidia裁剪：卷积核裁剪以提升资源推理效率(Resource Efficient Inference)"></a>3.4 nvidia裁剪：卷积核裁剪以提升资源推理效率(Resource Efficient Inference)</h3><p>参考论文<a href="https://arxiv.org/abs/1611.06440" target="_blank" rel="noopener">1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference</a>。</p>
<p>首先，他们提出了<strong>将一个裁剪问题视为某种优化问题：选取权重B的子集，如果裁剪它们使得网络的损失变化得最小</strong></p>
<p>$$<br>min _{w’}|C(D|W’)-C(D|W)|\quad s.t\quad ||W’||_0\le B<br>$$</p>
<p>注意：使用的是绝对值差异而非简单的差异，这样裁剪网络不会太多地缩减网络的性能，但是也应该不会增加。</p>
<p>这样一来，所有的排序方法可以使用此损失函数来衡量了。</p>
<h3 id="3-5-Oracle裁剪"><a href="#3-5-Oracle裁剪" class="headerlink" title="3.5  Oracle裁剪"></a>3.5  Oracle裁剪</h3><p>VGG16有4224个卷积核，完美的排序方法应该使用暴力裁剪每个卷积核，然后观察在训练集上损失函数变化，此方法称为oracle排序，最可能的排序方法。为了衡量其他排序方法的的效率，他们计算了其他方法与oracle的speraman协相关系数。令人惊讶的是，它们想到的排序方法(下文提到)与oracle协相关程度最高。</p>
<p>它们想到一个新的基于损失函数的泰勒一阶展开(代表最快的计算)神经元排序方法，裁剪一个卷积核$h$与将其清零相同。</p>
<p>$C(W,D)$是网络权重被设为W时在数据集D上的平均损失。现在，我们可以评估$C(W,D)$的在$C(W,D,h=0)$处的展开，它们 应该十分相近，因为移除单一卷积核不会对损失值造成太大影响。</p>
<p>$h$的排序为$C(W,D,h=0)-C(W,D)$的绝对值。</p>
<p>$$<br>\Theta <em>{TE}(h_i)=|\triangle C(h_i)|=|C(D,h_i)-\frac{\partial C}{\partial h_i}h_i-C(D,h_i)|=|\frac{\partial C}{\partial h_i}h_i|\<br>\Theta _{TE}(z_l ^{k})=|\frac{1}{M}\sum_m \frac{\partial C}{\partial z</em>{l,m} ^{(k)}}z_{l,m} ^{(k)}<br>$$</p>
<p>每一层的排序都会那一整层的排序的L2 norm的排序再次normalized。这有点经验主义，不太确定是否真有必要，但是极大地影响剪枝质量。</p>
<p>这种排序是相当直觉性的，我们不能同时使用排序方法本身所使用的激活函数、梯度。如果(激活函数、梯度)任意一个很高，代表其对输出有较大影响。将它们相乘，根据梯度或者激活函数值非常高或低，可以让我们得以衡量，是抛弃还是继续保留该卷积核。</p>
<p>这让我很好奇，他们到底有没有将剪枝问题视为最小化网络损失函数值差异，然后想出的泰勒展开式，还是说相反的，网络损失值差异是他们的某种备份的新方法。</p>
<h2 id="4-剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则"><a href="#4-剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则" class="headerlink" title="4 剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则"></a>4 剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则</h2><p>使用1000张狗和1000张猫的图片，对VGG模型做迁移学习训练。猫狗图片来自<a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="noopener">kaggle猫狗分类</a>,使用400张猫和400张狗的图片作为测试集。</p>
<h3 id="4-1-剪枝之后的结果说明"><a href="#4-1-剪枝之后的结果说明" class="headerlink" title="4.1 剪枝之后的结果说明"></a>4.1 剪枝之后的结果说明</h3><ul>
<li>准确率从98.7%掉到97.5%</li>
<li>网络模型从538MB减小到150MB</li>
<li>在i7 CPU上推理时间从0.78秒减小到0.227秒。基本是原来的三分之一</li>
</ul>
<h3 id="4-2-第一步-训练一个大网络"><a href="#4-2-第一步-训练一个大网络" class="headerlink" title="4.2 第一步:训练一个大网络"></a>4.2 第一步:训练一个大网络</h3><p>使用一个VGG16，然后丢弃最后三个全连接层，然后添加新的三个全连接层，此过程会freeze所有的卷积层，只训练新的三个全连接层。</p>
<p>我们先准备数据集，从kaggle下载数据之后，从总分别选取1400张猫和1400张狗，其中1000张猫和1000张狗作为训练集，放在<code>train1000</code>目录下的<code>cat</code>和<code>dog</code>目录下，另外的400张猫和400张狗放在<code>val</code>目录下的<code>cat</code>和<code>dog</code>目录下。使用Tensorflow2.0的代码示例如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from keras_applications.vgg16 import VGG16</span><br><span class="line">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><br><span class="line">from tensorflow.keras.optimizers import Adam</span><br><span class="line">from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,ReduceLROnPlateau,Callback</span><br><span class="line"></span><br><span class="line">## global parameters</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-4</span><br><span class="line">input_width,input_height &#x3D; 224,224</span><br><span class="line"></span><br><span class="line">weight_save_path &#x3D; &quot;.&#x2F;vgg16_catdog_weights&#x2F;&quot;</span><br><span class="line">record_save_path &#x3D; &quot;.&#x2F;vgg16_catdog_tensorboard&#x2F;&quot;</span><br><span class="line">model_weight_file &#x3D; weight_save_path + &quot;vgg16_catdog_binary.h5&quot;</span><br><span class="line">optimizer &#x3D; Adam(lr&#x3D;0.0001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08)</span><br><span class="line"># Callback for early stopping the training</span><br><span class="line">early_stopping &#x3D; tf.keras.callbacks.EarlyStopping(monitor&#x3D;&#39;val_accuracy&#39;, min_delta&#x3D;0, patience&#x3D;15, verbose&#x3D;1, mode&#x3D;&#39;auto&#39;)</span><br><span class="line"># set model checkpoint callback (model weights will auto save in weight_save_path)</span><br><span class="line">checkpoint &#x3D; ModelCheckpoint(model_weight_file, monitor&#x3D;&#39;val_accuracy&#39;, verbose&#x3D;1, save_best_only&#x3D;True, mode&#x3D;&#39;max&#39;, period&#x3D;1)</span><br><span class="line"># monitor a learning indicator(reduce learning rate when learning effect is stagnant)</span><br><span class="line">reduceLRcallback &#x3D; ReduceLROnPlateau(monitor&#x3D;&#39;val_acc&#39;, factor&#x3D;0.7, patience&#x3D;5,</span><br><span class="line">                                     verbose&#x3D;1, mode&#x3D;&#39;auto&#39;, cooldown&#x3D;0, min_lr&#x3D;0)</span><br><span class="line"></span><br><span class="line">class LossHistory(Callback):</span><br><span class="line">    def on_train_begin(self, logs&#x3D;&#123;&#125;):</span><br><span class="line">        self.losses &#x3D; []</span><br><span class="line">        self.val_losses &#x3D; []</span><br><span class="line">        self.acc &#x3D; []</span><br><span class="line">        self.val_acc &#x3D; []</span><br><span class="line">        self.recall &#x3D; []</span><br><span class="line"></span><br><span class="line">    def on_epoch_end(self, batch, logs&#x3D;&#123;&#125;):</span><br><span class="line">        self.losses.append(logs.get(&#39;loss&#39;))</span><br><span class="line">        self.val_losses.append(logs.get(&#39;val_loss&#39;))</span><br><span class="line">        self.acc.append(logs.get(&#39;acc&#39;))</span><br><span class="line">        self.val_acc.append(logs.get(&#39;val_accuracy&#39;))</span><br><span class="line">        self.recall.append(logs.get(&#39;recall&#39;))</span><br><span class="line"></span><br><span class="line">def build_model(input_width,input_height,drop_prob&#x3D;0.5):</span><br><span class="line">    vgg &#x3D; VGG16(include_top&#x3D;False,weights&#x3D;&quot;imagenet&quot;,classes&#x3D;2,input_shape&#x3D;(input_width,input_height,3),backend &#x3D; tf.keras.backend, layers &#x3D; tf.keras.layers, models &#x3D; tf.keras.models, utils &#x3D; tf.keras.utils)</span><br><span class="line">    for layer in vgg.layers:</span><br><span class="line">        layer.trainable &#x3D;False</span><br><span class="line">    print(vgg.summary())</span><br><span class="line">    out &#x3D; tf.keras.layers.Flatten()(vgg.output)</span><br><span class="line">    dense1 &#x3D;tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;)(out)</span><br><span class="line">    drop1 &#x3D; tf.keras.layers.Dropout(drop_prob)(dense1)</span><br><span class="line">    dense2 &#x3D;tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;)(drop1)</span><br><span class="line">    drop2 &#x3D; tf.keras.layers.Dropout(drop_prob)(dense2)</span><br><span class="line">    dense3 &#x3D;tf.keras.layers.Dense(1,activation&#x3D;&quot;sigmoid&quot;)(drop2)</span><br><span class="line">    merged_model &#x3D; tf.keras.models.Model(vgg.input,dense3)</span><br><span class="line">    print(merged_model.summary())</span><br><span class="line">    return merged_model</span><br><span class="line"></span><br><span class="line">def train_val_generator(train_img_path,val_img_path):</span><br><span class="line">    train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1 &#x2F; 255.,</span><br><span class="line">                                           rotation_range&#x3D;45,</span><br><span class="line">                                           width_shift_range&#x3D;0.2,</span><br><span class="line">                                           # degree of horizontal offset(a ratio relative to image width)</span><br><span class="line">                                           height_shift_range&#x3D;0.2,</span><br><span class="line">                                           # degree of vertical offset(a ratio relatice to image height)</span><br><span class="line">                                           shear_range&#x3D;0.2, # the range of shear transformation(a ratio in 0 ~ 1)</span><br><span class="line">                                           zoom_range&#x3D;0.25,</span><br><span class="line">                                           # degree of random zoom(the zoom range will be [1 - zoom_range, 1 + zoom_range])</span><br><span class="line">                                           horizontal_flip&#x3D;True, # whether to perform horizontal flip</span><br><span class="line">                                           vertical_flip&#x3D;True, # whether to perform vertical flip</span><br><span class="line">                                           fill_mode&#x3D;&#39;nearest&#39; # mode list: nearest, constant, reflect, wrap</span><br><span class="line">                                           )</span><br><span class="line">    val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1 &#x2F; 255.)</span><br><span class="line"></span><br><span class="line">    train_generator &#x3D; train_datagen.flow_from_directory(</span><br><span class="line">            train_img_path,</span><br><span class="line">            shuffle&#x3D;True,</span><br><span class="line">            target_size&#x3D;(input_width,input_height),</span><br><span class="line">            batch_size&#x3D;batch_size,</span><br><span class="line">            class_mode&#x3D;&#39;binary&#39;)</span><br><span class="line"></span><br><span class="line">    validation_generator &#x3D; val_datagen.flow_from_directory(</span><br><span class="line">            val_img_path,</span><br><span class="line">            target_size&#x3D;(input_width,input_height),</span><br><span class="line">            batch_size&#x3D;batch_size,</span><br><span class="line">            class_mode&#x3D;&#39;binary&#39;)</span><br><span class="line">    return train_generator,validation_generator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_model(train_img_path,val_img_path,batch_size,epochs):</span><br><span class="line">    if os.path.exists(model_weight_file):</span><br><span class="line">        model &#x3D; tf.keras.models.load_model(model_weight_file)</span><br><span class="line">    else:</span><br><span class="line">        model &#x3D; build_model(input_width,input_height)</span><br><span class="line">        model.compile(optimizer&#x3D;optimizer,</span><br><span class="line">                      loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">                      metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">    train_generator, validation_generator &#x3D; train_val_generator(train_img_path,val_img_path)</span><br><span class="line">    train_sample_count &#x3D; len(train_generator.filenames)</span><br><span class="line">    val_sample_count &#x3D; len(validation_generator.filenames)</span><br><span class="line">    print(train_sample_count, val_sample_count)</span><br><span class="line">    history &#x3D; LossHistory()</span><br><span class="line">    model.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch&#x3D;int(train_sample_count &#x2F; batch_size) + 1,</span><br><span class="line">        epochs&#x3D;epochs,</span><br><span class="line">        validation_data&#x3D;validation_generator,</span><br><span class="line">        validation_steps&#x3D;int(val_sample_count &#x2F; batch_size) + 1,</span><br><span class="line">        callbacks&#x3D;[TensorBoard(log_dir&#x3D;record_save_path), early_stopping, history, checkpoint, reduceLRcallback]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    train_set_path &#x3D; &#39;E:&#x2F;data&#x2F;images&#x2F;dogs-vs-cats&#x2F;train1000&#39;</span><br><span class="line">    valid_set_path &#x3D; &#39;E:&#x2F;data&#x2F;images&#x2F;dogs-vs-cats&#x2F;val&#39;</span><br><span class="line">    batch_size &#x3D; 8</span><br><span class="line">    epochs &#x3D; 20</span><br><span class="line">    train_model(train_set_path, valid_set_path, batch_size,epochs)</span><br></pre></td></tr></table></figure>

<p>最后的准确率，没有作者那么高，只有90%，如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">236&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3744 - accuracy: 0.8231</span><br><span class="line">237&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3738 - accuracy: 0.8233</span><br><span class="line">238&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3747 - accuracy: 0.8230</span><br><span class="line">239&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3746 - accuracy: 0.8232</span><br><span class="line">240&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3746 - accuracy: 0.8229</span><br><span class="line">241&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3747 - accuracy: 0.8231</span><br><span class="line">242&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3738 - accuracy: 0.8239</span><br><span class="line">243&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 1s - loss: 0.3735 - accuracy: 0.8236</span><br><span class="line">244&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 1s - loss: 0.3728 - accuracy: 0.8243</span><br><span class="line">245&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8240</span><br><span class="line">246&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8242</span><br><span class="line">247&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8239</span><br><span class="line">248&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3746 - accuracy: 0.8231</span><br><span class="line">249&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8228</span><br><span class="line">250&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8235</span><br><span class="line">Epoch 00020: val_accuracy did not improve from 0.90274</span><br><span class="line"></span><br><span class="line">251&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 47s 188ms&#x2F;step - loss: 0.3742 - accuracy: 0.8227 - val_loss: 0.2761 - val_accuracy: 0.8815</span><br></pre></td></tr></table></figure>
<p>查看对应的验证集的tensorboard如下</p>
<p><img src="/images/blog/model_pruning_7.jpg" alt="模型剪枝和优化"></p>
<h3 id="4-3-对卷积核排序"><a href="#4-3-对卷积核排序" class="headerlink" title="4.3  对卷积核排序"></a>4.3  对卷积核排序</h3><p>为了计算泰勒展开指标，我们需要在数据集上做一个<code>前向+后向传播</code>(可以在一个较小的数据集上)。</p>
<p>现在需要获取卷积层的梯度和激活函数。可以在梯度计算时注册一个hook，当这些东西就绪时会调用这个callback。</p>
<p>现在，我们可以从<code>self.activations</code>中获得激活函数值，当梯度就绪时会执行计算排序的方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def compute_rank(self, grad):</span><br><span class="line"> activation_index &#x3D; len(self.activations) - self.grad_index - 1</span><br><span class="line"> activation &#x3D; self.activations[activation_index]</span><br><span class="line"> values &#x3D; \</span><br><span class="line">  torch.sum((activation * grad), dim &#x3D; 0).\</span><br><span class="line">   sum(dim&#x3D;2).sum(dim&#x3D;3)[0, :, 0, 0].data</span><br><span class="line">	</span><br><span class="line"> # Normalize the rank by the filter dimensions</span><br><span class="line"> values &#x3D; \</span><br><span class="line">  values &#x2F; (activation.size(0) * activation.size(2) * activation.size(3))</span><br><span class="line"></span><br><span class="line"> if activation_index not in self.filter_ranks:</span><br><span class="line">  self.filter_ranks[activation_index] &#x3D; \</span><br><span class="line">   torch.FloatTensor(activation.size(1)).zero_().cuda()</span><br><span class="line"></span><br><span class="line"> self.filter_ranks[activation_index] +&#x3D; values</span><br><span class="line"> self.grad_index +&#x3D; 1</span><br></pre></td></tr></table></figure>

<h2 id="5-剪枝实践：使用Tensorflow-训练剪枝MNIST模型为例"><a href="#5-剪枝实践：使用Tensorflow-训练剪枝MNIST模型为例" class="headerlink" title="5. 剪枝实践：使用Tensorflow 训练剪枝MNIST模型为例"></a>5. 剪枝实践：使用Tensorflow 训练剪枝MNIST模型为例</h2><p>下面使用tensorflow api为例，其他API也有类似功能。基于keras api的权重剪枝，在训练过程中迭代的删除一些没用的连接，基于连接的梯度。下面示范通过简单的使用一种通用文件压缩算法(如zip压缩)，就可以缩减keras模型</p>
<h3 id="5-1-训练一个剪枝的模型"><a href="#5-1-训练一个剪枝的模型" class="headerlink" title="5.1 训练一个剪枝的模型"></a>5.1 训练一个剪枝的模型</h3><p>tensorflow提供一个<code>prune_low_magnitude()</code>的API来训练模型，模型中会移除一些连接。基于Keras的API可以应用于独立的网络层，或者整个网络。在高层级，此技术是在给定规划和目标稀疏度的前提下，通过迭代的移除(即zeroing out)网络层之间的连接。</p>
<p>例如，典型的配置是目标稀疏度为75%，通过每迭代100步(epoch)裁剪一些连接，从第2000步(epoch)开始。更多配置需要查看官方文档。</p>
<h3 id="5-2-一层一层的构建一个剪枝的模型"><a href="#5-2-一层一层的构建一个剪枝的模型" class="headerlink" title="5.2 一层一层的构建一个剪枝的模型"></a>5.2 一层一层的构建一个剪枝的模型</h3><p>下面展示如何在网络层层面使用API，构建一个剪枝的分类模型。</p>
<ul>
<li>此时，<code>prune_low_magnitude()</code>接收一个想要被裁剪的网络层作为参数。</li>
<li>此函数需要一个剪枝参数，配置的是在训练过程中的剪枝算法。以下是相关参数的意义<ul>
<li><strong>Sparsity</strong>: 整个训练过程中使用的是多项式递减(PolynomialDecay)。从50%的稀疏度开始，然后逐渐地训练模型以达到90%的稀疏度。x%的稀疏度代表x%的权重标量将会被裁剪掉</li>
<li><strong>Schedule</strong>：从第2000步开始到训练结束，网络层之间的连接会逐渐被裁剪掉，并且是每100步执行一次。究其原因是，要训练一个在几个步骤内稳定达到一定准确率的模型，以帮助其收敛。同时，也让模型在每次裁剪之后能恢复，所以并不是每一步都要裁剪。我们可以将裁剪频率设为100.</li>
</ul>
</li>
</ul>
<p>为了演示如何保存并重新载入裁剪的模型，我们先训练一个模型10个epoch，保存，然后载入模型并继续训练2个epoch。逐渐地稀疏，四个重要参数是<strong><code>begin_sparsity</code>,<code>final_sparsity</code>,<code>begin_step</code>,<code>end_step</code></strong>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow_model_optimization.sparsity import keras as sparsity</span><br><span class="line"></span><br><span class="line">epochs &#x3D; 12</span><br><span class="line">l &#x3D; tf.keras.layers</span><br><span class="line">num_train_samples &#x3D; x_train.shape[0]</span><br><span class="line">end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">print(&#39;End step: &#39; + str(end_step))</span><br><span class="line">pruning_params &#x3D; &#123;</span><br><span class="line">      &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                   final_sparsity&#x3D;0.90,</span><br><span class="line">                                                   begin_step&#x3D;2000,</span><br><span class="line">                                                   end_step&#x3D;end_step,</span><br><span class="line">                                                   frequency&#x3D;100)</span><br><span class="line">&#125;</span><br><span class="line">pruned_model &#x3D; tf.keras.Sequential([</span><br><span class="line">    sparsity.prune_low_magnitude(</span><br><span class="line">        l.Conv2D(32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        input_shape&#x3D;input_shape,</span><br><span class="line">        **pruning_params),</span><br><span class="line">    l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">    l.BatchNormalization(),</span><br><span class="line">    sparsity.prune_low_magnitude(</span><br><span class="line">        l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;), **pruning_params),</span><br><span class="line">    l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">    l.Flatten(),</span><br><span class="line">    sparsity.prune_low_magnitude(l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">                                 **pruning_params),</span><br><span class="line">    l.Dropout(0.4),</span><br><span class="line">    sparsity.prune_low_magnitude(l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;),</span><br><span class="line">                                 **pruning_params)</span><br><span class="line">])</span><br><span class="line">pruned_model.summary()</span><br></pre></td></tr></table></figure>

<p>作为对比，我们训练了一个MNSIT数据集的分类模型，首先，我们准备的数据和参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tempfile</span><br><span class="line">import zipfile</span><br><span class="line">import os</span><br><span class="line">import tensorboard</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow_model_optimization.sparsity import keras as sparsity</span><br><span class="line"></span><br><span class="line">## global parameters</span><br><span class="line">batch_size &#x3D; 128</span><br><span class="line">num_classes &#x3D; 10</span><br><span class="line">epochs &#x3D; 10</span><br><span class="line"># input image dimensions</span><br><span class="line">img_rows, img_cols &#x3D; 28, 28</span><br><span class="line">logdir &#x3D; tempfile.mkdtemp()</span><br><span class="line">print(&#39;Writing training logs to &#39; + logdir)</span><br><span class="line"></span><br><span class="line">def prepare_trainval(img_rows, img_cols):</span><br><span class="line">    # the data, shuffled and split between train and test sets</span><br><span class="line">    (x_train, y_train), (x_test, y_test) &#x3D; tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    if tf.keras.backend.image_data_format() &#x3D;&#x3D; &#39;channels_first&#39;:</span><br><span class="line">      x_train &#x3D; x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)</span><br><span class="line">      x_test &#x3D; x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)</span><br><span class="line">      input_shape &#x3D; (1, img_rows, img_cols)</span><br><span class="line">    else:</span><br><span class="line">      x_train &#x3D; x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)</span><br><span class="line">      x_test &#x3D; x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)</span><br><span class="line">      input_shape &#x3D; (img_rows, img_cols, 1)</span><br><span class="line"></span><br><span class="line">    x_train &#x3D; x_train.astype(&#39;float32&#39;)</span><br><span class="line">    x_test &#x3D; x_test.astype(&#39;float32&#39;)</span><br><span class="line">    x_train &#x2F;&#x3D; 255</span><br><span class="line">    x_test &#x2F;&#x3D; 255</span><br><span class="line">    print(&#39;x_train shape:&#39;, x_train.shape)</span><br><span class="line">    print(x_train.shape[0], &#39;train samples&#39;)</span><br><span class="line">    print(x_test.shape[0], &#39;test samples&#39;)</span><br><span class="line"></span><br><span class="line">    # convert class vectors to binary class matrices</span><br><span class="line">    y_train &#x3D; tf.keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">    y_test &#x3D; tf.keras.utils.to_categorical(y_test, num_classes)</span><br><span class="line">    return x_train,x_test,y_train,y_test</span><br></pre></td></tr></table></figure>
<h4 id="5-2-1-构建原始的MNIST分类模型"><a href="#5-2-1-构建原始的MNIST分类模型" class="headerlink" title="5.2.1 构建原始的MNIST分类模型"></a>5.2.1 构建原始的MNIST分类模型</h4><p>使用keras构建一个简单的keras模型如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def build_clean_model(input_shape):</span><br><span class="line">    l &#x3D; tf.keras.layers</span><br><span class="line">    model &#x3D; tf.keras.Sequential([</span><br><span class="line">        l.Conv2D(</span><br><span class="line">            32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;, input_shape&#x3D;input_shape),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.BatchNormalization(),</span><br><span class="line">        l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.Flatten(),</span><br><span class="line">        l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        l.Dropout(0.4),</span><br><span class="line">        l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;)</span><br><span class="line">    ])</span><br><span class="line">    model.compile(</span><br><span class="line">        loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">        optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">        metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">    model.summary()</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>
<p>训练模型代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def train_clean_model(x_train,x_test,y_train,y_test,epochs,ori_mnist_model_file):</span><br><span class="line">    callbacks &#x3D; [tf.keras.callbacks.TensorBoard(log_dir&#x3D;logdir, profile_batch&#x3D;0)]</span><br><span class="line">    input_shape &#x3D; (img_rows, img_cols, 1)</span><br><span class="line">    model &#x3D; build_clean_model(input_shape)</span><br><span class="line">    model.fit(x_train, y_train,</span><br><span class="line">              batch_size&#x3D;batch_size,</span><br><span class="line">              epochs&#x3D;epochs,</span><br><span class="line">              verbose&#x3D;1,</span><br><span class="line">              callbacks&#x3D;callbacks,</span><br><span class="line">              validation_data&#x3D;(x_test, y_test))</span><br><span class="line">    score &#x3D; model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">    print(&#39;Saving model to: &#39;,ori_mnist_model_file)</span><br><span class="line">    tf.keras.models.save_model(model,ori_mnist_model_file, include_optimizer&#x3D;False)</span><br><span class="line">    print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">    print(&#39;Test accuracy:&#39;, score[1])</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test &#x3D; prepare_trainval(img_rows, img_cols)</span><br><span class="line">ori_mnist_model_file &#x3D; &quot;.&#x2F;ori_mnist_classifier.h5&quot;</span><br><span class="line">train_clean_model(x_train,x_test,y_train,y_test,epochs,ori_mnist_model_file)</span><br></pre></td></tr></table></figure>
<p>模型训练结果输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">45568&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;........] - ETA: 0s - loss: 0.0119 - accuracy: 0.9962</span><br><span class="line">46720&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.......] - ETA: 0s - loss: 0.0120 - accuracy: 0.9962</span><br><span class="line">47872&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.......] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961</span><br><span class="line">49024&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;......] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">50176&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.....] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">51328&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.....] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961</span><br><span class="line">52480&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">53632&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0121 - accuracy: 0.9962</span><br><span class="line">54784&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0125 - accuracy: 0.9961</span><br><span class="line">56064&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">57216&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">58368&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9961</span><br><span class="line">59520&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9962</span><br><span class="line">60000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 3s 49us&#x2F;sample - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0297 - val_accuracy: 0.9919</span><br><span class="line">Saving model to: .&#x2F;ori_mnist_classifier.h5</span><br><span class="line">Test loss: 0.029679151664800906</span><br><span class="line">Test accuracy: 0.9919</span><br></pre></td></tr></table></figure>

<h4 id="5-2-2-构建剪枝的MNIST分类模型"><a href="#5-2-2-构建剪枝的MNIST分类模型" class="headerlink" title="5.2.2 构建剪枝的MNIST分类模型"></a>5.2.2 构建剪枝的MNIST分类模型</h4><p>注意和上面的5.2.1构建原始分类模型的代码对比</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">def build_prune_model(input_shape,end_step):</span><br><span class="line">    l &#x3D; tf.keras.layers</span><br><span class="line">    print(&#39;End step: &#39; + str(end_step))</span><br><span class="line">    pruning_params &#x3D; &#123;</span><br><span class="line">          &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                       final_sparsity&#x3D;0.90,</span><br><span class="line">                                                       begin_step&#x3D;2000,</span><br><span class="line">                                                       end_step&#x3D;end_step,</span><br><span class="line">                                                       frequency&#x3D;100)</span><br><span class="line">    &#125;</span><br><span class="line">    pruned_model &#x3D; tf.keras.Sequential([</span><br><span class="line">        sparsity.prune_low_magnitude(</span><br><span class="line">            l.Conv2D(32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">            input_shape&#x3D;input_shape,</span><br><span class="line">            **pruning_params),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.BatchNormalization(),</span><br><span class="line">        sparsity.prune_low_magnitude(</span><br><span class="line">            l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;), **pruning_params),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.Flatten(),</span><br><span class="line">        sparsity.prune_low_magnitude(l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">                                     **pruning_params),</span><br><span class="line">        l.Dropout(0.4),</span><br><span class="line">        sparsity.prune_low_magnitude(l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;),</span><br><span class="line">                                     **pruning_params)</span><br><span class="line">    ])</span><br><span class="line">    pruned_model.compile(</span><br><span class="line">        loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">        optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">        metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">    pruned_model.summary()</span><br><span class="line">    return pruned_model</span><br></pre></td></tr></table></figure>
<p>训练剪枝模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def train_prune_model(x_train,x_test,y_train,y_test,epochs,prune_model_file):</span><br><span class="line">    input_shape &#x3D; (img_rows, img_cols,1)</span><br><span class="line">    num_train_samples &#x3D; x_train.shape[0]</span><br><span class="line">    end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">    pruned_model &#x3D; build_prune_model(input_shape,end_step)</span><br><span class="line">    # Add a pruning step callback to peg the pruning step to the optimizer&#39;s</span><br><span class="line">    # step. Also add a callback to add pruning summaries to tensorboard</span><br><span class="line">    callbacks &#x3D; [</span><br><span class="line">        sparsity.UpdatePruningStep(),</span><br><span class="line">        sparsity.PruningSummaries(log_dir&#x3D;logdir, profile_batch&#x3D;0)</span><br><span class="line">    ]</span><br><span class="line">    pruned_model.fit(x_train, y_train,</span><br><span class="line">              batch_size&#x3D;batch_size,</span><br><span class="line">              epochs&#x3D;10,</span><br><span class="line">              verbose&#x3D;1,</span><br><span class="line">              callbacks&#x3D;callbacks,</span><br><span class="line">              validation_data&#x3D;(x_test, y_test))</span><br><span class="line">    score &#x3D; pruned_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">    print(&#39;Saving pruned model to: &#39;, prune_model_file)</span><br><span class="line">    # 保存模型时要设置 include_optimizer 为True by default.</span><br><span class="line">    tf.keras.models.save_model(pruned_model,prune_model_file, include_optimizer&#x3D;True)</span><br><span class="line">    print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">    print(&#39;Test accuracy:&#39;, score[1])</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test &#x3D; prepare_trainval(img_rows, img_cols)</span><br><span class="line">prune_model_file &#x3D; &quot;.&#x2F;prune_mnist_classifier.h5&quot;</span><br><span class="line">train_prune_model(x_train,x_test,y_train,y_test,epochs,prune_model_file)</span><br></pre></td></tr></table></figure>

<p>训练结果输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">52224&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0127 - accuracy: 0.9961</span><br><span class="line">53120&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">54016&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0125 - accuracy: 0.9962</span><br><span class="line">54912&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0124 - accuracy: 0.9962</span><br><span class="line">55808&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0124 - accuracy: 0.9962</span><br><span class="line">56704&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">57600&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">58368&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">59264&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">60000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 69us&#x2F;sample - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9920</span><br><span class="line">Saving pruned model to: .&#x2F;prune_mnist_classifier.h5</span><br><span class="line">Test loss: 0.022609539373161534</span><br><span class="line">Test accuracy: 0.992</span><br></pre></td></tr></table></figure>

<p>如果我们要载入剪枝的模型，我们得使用<strong>prune_scope()会话</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">with sparsity.prune_scope():</span><br><span class="line">  restored_model &#x3D; tf.keras.models.load_model(checkpoint_file)</span><br><span class="line"></span><br><span class="line">restored_model.fit(x_train, y_train,</span><br><span class="line">                   batch_size&#x3D;batch_size,</span><br><span class="line">                   epochs&#x3D;2,</span><br><span class="line">                   verbose&#x3D;1,</span><br><span class="line">                   callbacks&#x3D;callbacks,</span><br><span class="line">                   validation_data&#x3D;(x_test, y_test))</span><br><span class="line"></span><br><span class="line">score &#x3D; restored_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">print(&#39;Test accuracy:&#39;, score[1])</span><br></pre></td></tr></table></figure>

<p>在训练和载入剪枝模型时有两点需要注意</p>
<ol>
<li>保存模型时， <code>include_optimizer</code>必须设置为<code>True</code>。因为剪枝过程需要保存optimizer的状态。</li>
<li>载入剪枝模型时需要在<code>prune_scope()</code>会话中来解序列化。</li>
</ol>
<h4 id="5-2-3-对照：如何使用剪枝模型"><a href="#5-2-3-对照：如何使用剪枝模型" class="headerlink" title="5.2.3 对照：如何使用剪枝模型"></a>5.2.3 对照：如何使用剪枝模型</h4><p><strong>构建模型时</strong></p>
<p><img src="/images/blog/model_pruning_8.png" alt="模型剪枝和优化"></p>
<p>我们对比发现，只有需要计算梯度的网络层需要使用剪枝的包装。同时需要设定好剪枝的规划。</p>
<p><strong>训练模型时</strong></p>
<p><img src="/images/blog/model_pruning_9.png" alt="模型剪枝和优化"></p>
<p>没有太大的区别，除了以下两点</p>
<ol>
<li>需要新增关于剪枝的统计</li>
<li>保存模型时需要将optimizer也一起保存</li>
</ol>
<p>使用netron打开两个保存的模型，效果如下，可以看到裁剪的模型都被放在了<code>PruneLowMagnitude</code>中。</p>
<p><img src="/images/blog/model_pruning_10.png" alt="模型剪枝和优化"></p>
<h3 id="5-3-对整个模型剪枝"><a href="#5-3-对整个模型剪枝" class="headerlink" title="5.3 对整个模型剪枝"></a>5.3 对整个模型剪枝</h3><p>函数<code>prune_low_magnitude</code>可以应用于整个keras模型。此时算法会被应用于所有对权重剪枝<strong>友好</strong>(Keras api的知道的)的网络层，<strong>不友好</strong>的网络层会直接忽略掉，<strong>未知</strong>的网络层可能会报错。</p>
<p>如果模型的网络层是API不知道如何剪枝的，但是非常适合不剪枝，那么交给API来修剪每层的basis即可(即不修剪卷积核的权重，只修剪basis)。</p>
<p>除去剪枝配置参数，相同的配置可以应用于网络的所有的剪枝层。同时需要注意的是，剪枝不保留原模型的优化器optimizer，需要对剪枝的模型重新训练一个新的优化器optimizer。</p>
<p>开始之前，假设我们已经有一个已经序列化过的预训练的Keras模型，想对其权重剪枝。以前面的MNIST模型为例。先载入模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Load the serialized model</span><br><span class="line">loaded_model &#x3D; tf.keras.models.load_model(keras_file)</span><br></pre></td></tr></table></figure>
<p>然后可以剪枝模型然后编译剪枝之后的模型并训练。此时的训练将重新从第0步开始，鉴于模型此时已经达到了一定的准确率，我们可以直接开始剪枝。将开始步骤设置为0，然后只训练4个epochs。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">epochs &#x3D; 4</span><br><span class="line">end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">print(end_step)</span><br><span class="line"></span><br><span class="line">new_pruning_params &#x3D; &#123;</span><br><span class="line">      &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                   final_sparsity&#x3D;0.90,</span><br><span class="line">                                                   begin_step&#x3D;0,</span><br><span class="line">                                                   end_step&#x3D;end_step,</span><br><span class="line">                                                   frequency&#x3D;100)</span><br><span class="line">&#125;</span><br><span class="line">new_pruned_model &#x3D; sparsity.prune_low_magnitude(model, **new_pruning_params)</span><br><span class="line">new_pruned_model.summary()</span><br><span class="line">new_pruned_model.compile(</span><br><span class="line">    loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">    optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">    metrics&#x3D;[&#39;accuracy&#39;])</span><br></pre></td></tr></table></figure>
<p>再训练4个epochs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Add a pruning step callback to peg the pruning step to the optimizer&#39;s</span><br><span class="line"># step. Also add a callback to add pruning summaries to tensorboard</span><br><span class="line">callbacks &#x3D; [</span><br><span class="line">    sparsity.UpdatePruningStep(),</span><br><span class="line">    sparsity.PruningSummaries(log_dir&#x3D;logdir, profile_batch&#x3D;0)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">new_pruned_model.fit(x_train, y_train,</span><br><span class="line">          batch_size&#x3D;batch_size,</span><br><span class="line">          epochs&#x3D;epochs,</span><br><span class="line">          verbose&#x3D;1,</span><br><span class="line">          callbacks&#x3D;callbacks,</span><br><span class="line">          validation_data&#x3D;(x_test, y_test))</span><br><span class="line"></span><br><span class="line">score &#x3D; new_pruned_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">print(&#39;Test accuracy:&#39;, score[1])</span><br></pre></td></tr></table></figure>
<p>模型导出到serving</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">final_model &#x3D; sparsity.strip_pruning(pruned_model)</span><br><span class="line">final_model.summary()</span><br><span class="line"></span><br><span class="line">_, new_pruned_keras_file &#x3D; tempfile.mkstemp(&#39;.h5&#39;)</span><br><span class="line">print(&#39;Saving pruned model to: &#39;, new_pruned_keras_file)</span><br><span class="line">tf.keras.models.save_model(final_model, new_pruned_keras_file, </span><br><span class="line">                        include_optimizer&#x3D;False)</span><br><span class="line"></span><br><span class="line"># 压缩之后的模型大小与前面一层层剪枝的大小一样</span><br><span class="line">_, zip3 &#x3D; tempfile.mkstemp(&#39;.zip&#39;)</span><br><span class="line">with zipfile.ZipFile(zip3, &#39;w&#39;, compression&#x3D;zipfile.ZIP_DEFLATED) as f:</span><br><span class="line">  f.write(new_pruned_keras_file)</span><br><span class="line">print(&quot;Size of the pruned model before compression: %.2f Mb&quot; </span><br><span class="line">      % (os.path.getsize(new_pruned_keras_file) &#x2F; float(2**20)))</span><br><span class="line">print(&quot;Size of the pruned model after compression: %.2f Mb&quot; </span><br><span class="line">      % (os.path.getsize(zip3) &#x2F; float(2**20)))</span><br></pre></td></tr></table></figure>


<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505" target="_blank" rel="noopener">medium Pruning Deep Neural Networks</a></li>
<li><a href="https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb" target="_blank" rel="noopener">tensorflow mnist 剪枝</a></li>
<li><a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" target="_blank" rel="noopener">Pruning deep neural networks to make them fast and small</a></li>
<li><a href="https://stackoverflow.com/questions/43839431/tensorflow-how-to-replace-or-modify-gradient/43948872" target="_blank" rel="noopener">stackoverflow 如何在tensorflow计算梯度时更改计算方式</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient" target="_blank" rel="noopener">Tensorflow官方API 如何更改梯度计算方式</a></li>
</ol>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-11-26-model-pruning/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-11-26-model-pruning/" title="模型剪枝和优化-torch和Tensorflow为例">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-10-28--understand-pytorch/">
    		理解pytorch的计算逻辑
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.735Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="1-线性回归问题"><a href="#1-线性回归问题" class="headerlink" title="1 线性回归问题"></a>1 线性回归问题</h2><p>假定我们以一个线性回归问题来逐步解释pytorch过程中的一些操作和逻辑。线性回归公式如下<br>$$<br> y = a+bx+e\quad \quad 此处假定a=1,b=2的一个线性回归函数<br>$$</p>
<h3 id="1-1-先用普通的numpy来展示线性回归过程"><a href="#1-1-先用普通的numpy来展示线性回归过程" class="headerlink" title="1.1 先用普通的numpy来展示线性回归过程"></a>1.1 先用普通的numpy来展示线性回归过程</h3><p>我们随机生成100个数据，并以一定的随机概率扰动数据集，训练集和验证集八二分，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Data Generation</span><br><span class="line">np.random.seed(42)</span><br><span class="line">x &#x3D; np.random.rand(100, 1)</span><br><span class="line">y &#x3D; 1 + 2 * x + .1 * np.random.randn(100, 1)</span><br><span class="line"></span><br><span class="line"># Shuffles the indices</span><br><span class="line">idx &#x3D; np.arange(100)</span><br><span class="line">np.random.shuffle(idx)</span><br><span class="line"></span><br><span class="line"># Uses first 80 random indices for train</span><br><span class="line">train_idx &#x3D; idx[:80]</span><br><span class="line"># Uses the remaining indices for validation</span><br><span class="line">val_idx &#x3D; idx[80:]</span><br><span class="line"></span><br><span class="line"># Generates train and validation sets</span><br><span class="line">x_train, y_train &#x3D; x[train_idx], y[train_idx]</span><br><span class="line">x_val, y_val &#x3D; x[val_idx], y[val_idx]</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/understand_pytorch_1.png" alt=""></p>
<p>上面这是我们已经知道的是一个线性回归数据分布，并且回归的参数是$a=1,b=2$，如果我们只知道数据<code>x_train</code>和<code>y_train</code>，需要求这两个参数$a,b$呢，一般是使用梯度下降方法。</p>
<p>注意，下面的梯度下降方法是全量梯度，一次计算了所有的数据的梯度，只是在迭代了1000个epoch，通常训练时会把全量数据分成多个batch，每次都是小批量更新。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 初始化线性回归的参数 a 和 b</span><br><span class="line">np.random.seed(42)</span><br><span class="line">a &#x3D; np.random.randn(1)</span><br><span class="line">b &#x3D; np.random.randn(1)</span><br><span class="line">print(&quot;初始化的 a : %d 和 b : %d&quot;%(a,b))</span><br><span class="line">leraning_rate &#x3D; 1e-2</span><br><span class="line">epochs &#x3D; 1000</span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    pred &#x3D; a+ b*x_train</span><br><span class="line">    # 计算预测值和真实值之间的误差</span><br><span class="line">    error &#x3D; y_train-pred</span><br><span class="line">    # 使用MSE 来计算回归误差</span><br><span class="line">    loss &#x3D; (error**2).mean()</span><br><span class="line">    # 计算参数 a 和 b的梯度</span><br><span class="line">    a_grad &#x3D; -2*error.mean()</span><br><span class="line">    b_grad &#x3D; -2*(x_train*error).mean()</span><br><span class="line">    # 更新参数：用学习率和梯度</span><br><span class="line">    a &#x3D; a-leraning_rate*a_grad</span><br><span class="line">    b &#x3D; b -leraning_rate*b_grad</span><br><span class="line"></span><br><span class="line">print(&quot;最终获得参数为 a : %.2f, b :%.2f &quot;%(a,b))</span><br></pre></td></tr></table></figure>
<p>得到的输出如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初始化的 a : 0 和 b : 0</span><br><span class="line">最终获得参数为 a : 0.98, b :1.94</span><br></pre></td></tr></table></figure>
<p>再验证下是否与sklearn的LinearRegression回归算法得到的结果相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 检查下，我们获得结果是否与sklearn的结果一致</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">linr &#x3D; LinearRegression()</span><br><span class="line">linr.fit(x_train,y_train)</span><br><span class="line">print(linr.intercept_,linr.coef_[0])</span><br></pre></td></tr></table></figure>

<p>得到的参数如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.98312156] [1.94067463]</span><br></pre></td></tr></table></figure>

<h2 id="2-pytorhc-来解决回归问题"><a href="#2-pytorhc-来解决回归问题" class="headerlink" title="2 pytorhc 来解决回归问题"></a>2 pytorhc 来解决回归问题</h2><h3 id="2-1-pytorch的一些基础问题"><a href="#2-1-pytorch的一些基础问题" class="headerlink" title="2.1 pytorch的一些基础问题"></a>2.1 pytorch的一些基础问题</h3><ul>
<li>如果将numpy数组转化为pytorch的tensor呢？使用<code>torch.from_numpy(data)</code></li>
<li>如果想将计算的数据放入GPU计算：<code>data.to(device)</code>(其中的device就是GPU或cpu)</li>
<li>数据类型转换示例： <code>data.float()</code></li>
<li>如果确定数据位于CPU还是GPU:<code>data.type()</code>会得到类似于<code>torch.cuda.FloatTensor</code>的结果，表明在GPU中</li>
<li>从GPU中把数据转化成numpy：先取出到cpu中，再转化成numpy数组。<code>data.cpu().numpy()</code></li>
</ul>
<h3 id="2-2-使用pytorch构建参数"><a href="#2-2-使用pytorch构建参数" class="headerlink" title="2.2 使用pytorch构建参数"></a>2.2 使用pytorch构建参数</h3><p>如何区分普通数据和参数/权重呢？<strong>需要计算梯度的是参数，否则就是普通数据</strong>。参数需要用梯度来更新，我们需要选项<code>requires_grad=True</code>。使用了这个选项就是告诉pytorch，我们要计算此变量的梯度了。</p>
<p>我们可以使用如下三种方式来构建参数</p>
<ol>
<li>此方法构建出来的参数全部都在cpu中<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></li>
<li>此方法尝试把tensor参数传入到gpu<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float).to(device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float).to(device)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
此时如果查看输出，会发现两个tensor ，$a和b$的梯度选项没了（没了requires_grad=True）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.5158], device&#x3D;&#39;cuda:0&#39;, grad_fn&#x3D;&lt;CopyBackwards&gt;) tensor([0.0246], device&#x3D;&#39;cuda:0&#39;, grad_fn&#x3D;&lt;CopyBackwards&gt;)</span><br></pre></td></tr></table></figure></li>
<li>先将tensor传入gpu，然后再使用<code>requires_grad_()</code>选项来重构tensor的属性。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, dtype&#x3D;torch.float).to(device)</span><br><span class="line">b &#x3D; torch.randn(1, dtype&#x3D;torch.float).to(device)</span><br><span class="line"># and THEN set them as requiring gradients...</span><br><span class="line">a.requires_grad_()</span><br><span class="line">b.requires_grad_()</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></li>
<li>最佳策略当然是初始化的时候直接赋予<code>requires_grad=True</code>属性了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># We can specify the device at the moment of creation - RECOMMENDED!</span><br><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
查看tensor的属性<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.6226], device&#x3D;&#39;cuda:0&#39;, requires_grad&#x3D;True) tensor([1.4505], device&#x3D;&#39;cuda:0&#39;, requires_grad&#x3D;True)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="2-3-自动求导-Autograd"><a href="#2-3-自动求导-Autograd" class="headerlink" title="2.3 自动求导 Autograd"></a>2.3 自动求导 Autograd</h3><p>Autograd是Pytorch的自动求导包，有了它，我们就不必担忧偏导数和链式法则等一系列问题。Pytorch计算所有梯度的方法是<code>backward()</code>。计算梯度之前，我们需要先计算损失，那么需要调用对应(损失)变量的求导方法，如<code>loss.backward()</code>。</p>
<ul>
<li>计算所有变量的梯度(假设损失变量是loss): <code>loss.back()</code></li>
<li>获取某个变量的实际的梯度值(假设变量为att):<code>att.grad</code></li>
<li>由于梯度是累加的，每次用梯度更新参数之后，需要清零(假设梯度变量是att):<code>att.zero_()</code>,下划线是一种运算符，相当于直接作用于原变量上，等同于<code>att=0</code>(不要手动赋值，因为此过程可能涉及到GPU、CPU之间数据传输，容易出错)</li>
</ul>
<p>我们接下来尝试下手工更新参数和梯度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    error &#x3D; y_train_tensor - yhat</span><br><span class="line">    loss &#x3D; (error ** 2).mean()</span><br><span class="line"></span><br><span class="line">    # 这个是numpy的计算梯度的方式</span><br><span class="line">    # a_grad &#x3D; -2 * error.mean()</span><br><span class="line">    # b_grad &#x3D; -2 * (x_tensor * error).mean()</span><br><span class="line">    </span><br><span class="line">    # 告诉pytorch计算损失loss，计算所有变量的梯度</span><br><span class="line">    loss.backward()</span><br><span class="line">    # Let&#39;s check the computed gradients...</span><br><span class="line">    print(a.grad)</span><br><span class="line">    print(b.grad)  </span><br><span class="line">    </span><br><span class="line">    # 1. 手动更新参数，会出错 AttributeError: &#39;NoneType&#39; object has no attribute &#39;zero_&#39;</span><br><span class="line">    # 错误的原因是，我们重新赋值时会丢掉变量的 梯度属性</span><br><span class="line">    # a &#x3D; a - lr * a.grad</span><br><span class="line">    # b &#x3D; b - lr * b.grad</span><br><span class="line">    # print(a)</span><br><span class="line">    # 2. 再次手动更新参数，这次我们没有重新赋值，而是使用in-place的方式赋值  RuntimeError: a leaf Variable that requires grad has been used in an in- place operation.</span><br><span class="line">    # 这是因为 pytorch 给所有需要计算梯度的python操作以及依赖都纳入了动态计算图，稍后会解释</span><br><span class="line">    # a -&#x3D; lr * a.grad</span><br><span class="line">    # b -&#x3D; lr * b.grad        </span><br><span class="line"></span><br><span class="line">    # 3. 如果我们真想手动更新，不使用pytorch的计算图呢，必须使用no_grad来将此参数移除自动计算梯度变量之外。</span><br><span class="line">    # 这是源于pytorch的动态计算图DYNAMIC GRAPH，后面会有详细的解释</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        a -&#x3D; lr * a.grad</span><br><span class="line">        b -&#x3D; lr * b.grad</span><br><span class="line">    </span><br><span class="line">    # PyTorch is &quot;clingy&quot; to its computed gradients, we need to tell it to let it go...</span><br><span class="line">    a.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-动态计算图"><a href="#2-4-动态计算图" class="headerlink" title="2.4 动态计算图"></a>2.4 动态计算图</h3><p>如果想可视化计算图，可以使用辅助包<a href="https://github.com/szagoruyko/pytorchviz" target="_blank" rel="noopener">torchviz</a>，需要自己安装。使用其<code>make_dot(变量)</code>方法来可视化与当前给定变量相关的计算图。示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line"></span><br><span class="line">yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">error &#x3D; y_train_tensor - yhat</span><br><span class="line">loss &#x3D; (error ** 2).mean()</span><br><span class="line">make_dot(yhat)</span><br></pre></td></tr></table></figure>
<p>使用<code>make_dot(yhat)</code>会得到相关的三个计算图如下</p>
<p><img src="/images/blog/understand_pytorch_2.png" alt=""></p>
<p>各个组件，解释如下</p>
<ul>
<li><strong>蓝色盒子</strong>：作为参数的tensor，需要pytorch计算梯度的</li>
<li><strong>灰色盒子</strong>：与计算梯度相关的或者计算梯度依赖的，python操作</li>
<li><strong>绿色盒子</strong>：与灰色盒子一样，区别是，它是计算梯度的起始点（假设<code>backward()</code>方法是需要可视化图的变量调用的）-计算图自底向上构建。</li>
</ul>
<p>上图的<code>error</code>(图中)和<code>loss</code>(图右)，与左图的唯一区别就是中间步骤(灰色盒子)的数目。看左边的绿色盒子，有两个箭头指向该绿色盒子，代表两个变量相加。<code>a</code>和<code>b*x</code>。再看该图中的灰色盒子，它执行的是乘法计算，即<code>b*x</code>，但是为啥只有一个箭头指向呢，只有来自蓝色盒子的参数<code>b</code>，为啥没有数据<code>x</code>?因为我们不需要为数据<code>x</code>计算梯度（<strong>不计算梯度的变量不会出现在计算图中</strong>）。那么，如果我们去掉变量的<code>requires_grad</code>属性(设置为False)会怎样？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a_nongrad &#x3D; torch.randn(1,requires_grad&#x3D;False,dtype&#x3D;torch.float,device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1,requires_grad&#x3D;True,dtype&#x3D;torch.float,device&#x3D;device)</span><br><span class="line">yhat &#x3D; a_nongrad+b*x_train_tensor</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/understand_pytorch_3.png" alt=""></p>
<p>可以看到，对应参数<code>a</code>的蓝色盒子没有了，所以很简单明了，<strong>不计算梯度，就不出现在计算图中</strong>。</p>
<h2 id="3-优化器-Optimizer"><a href="#3-优化器-Optimizer" class="headerlink" title="3 优化器 Optimizer"></a>3 优化器 Optimizer</h2><p>到目前为止，我们都是手动计算梯度并更新参数的，如果有非常多的变量。我们可以使用pytorch的优化器，像<code>SGD</code>或者<code>Adam</code>。</p>
<p>优化器需要指定需要优化的参数，以及学习率，然后使用<code>step()</code>方法来更新，此外，<strong>我们不必再一个个的去将梯度赋值为0了，只需要使用优化器的<code>zero_grad()</code>方法即可。</strong>。</p>
<p>代码示例，使用SGD优化器更新参数<code>a</code>和<code>b</code>的梯度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Defines a SGD optimizer to update the parameters</span><br><span class="line">optimizer &#x3D; optim.SGD([a, b], lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    # 第一步，计算损失</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    error &#x3D; y_train_tensor - yhat</span><br><span class="line">    loss &#x3D; (error ** 2).mean()</span><br><span class="line">    # 第二步，后传损失</span><br><span class="line">    loss.backward()    </span><br><span class="line">    </span><br><span class="line">    # 不用再手动更新参数了</span><br><span class="line">    # with torch.no_grad():</span><br><span class="line">    # a -&#x3D; lr * a.grad</span><br><span class="line">    # b -&#x3D; lr * b.grad</span><br><span class="line">    # 使用优化器的step方法一步到位</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    # 也不用告诉pytorch需要对哪些梯度清零操作了，优化器的zero_grad()一步到位</span><br><span class="line">    # a.grad.zero_()</span><br><span class="line">    # b.grad.zero_()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>

<h2 id="4-计算损失loss"><a href="#4-计算损失loss" class="headerlink" title="4  计算损失loss"></a>4  计算损失loss</h2><p>pytorch提供了很多损失函数，可以直接调用。简单使用如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># 此处定义了损失函数为MSE</span><br><span class="line">loss_fn &#x3D; nn.MSELoss(reduction&#x3D;&#39;mean&#39;)</span><br><span class="line"></span><br><span class="line">optimizer &#x3D; optim.SGD([a, b], lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    </span><br><span class="line">    # 不用再手动计算损失了</span><br><span class="line">    # error &#x3D; y_tensor - yhat</span><br><span class="line">    # loss &#x3D; (error ** 2).mean()</span><br><span class="line">    # 直接调用定义好的损失函数即可</span><br><span class="line">    loss &#x3D; loss_fn(y_train_tensor, yhat)</span><br><span class="line"></span><br><span class="line">    loss.backward()    </span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>

<h2 id="5-模型"><a href="#5-模型" class="headerlink" title="5 模型"></a>5 模型</h2><p>pytorch中模型由一个继承自<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" target="_blank" rel="noopener">Module</a>的Python类来定义。需要实现两个最基本的方法</p>
<ol>
<li><code>__init__(self)</code>:定义了模型由哪几部分组成，当前模型只有两个变量<code>a</code>和<code>b</code>。模型可以定义更多的参数，并且可以将其他模型或者网络层定义为其参数</li>
<li><code>forwad(self,x)</code>:真实执行计算的方法，它对给定输入<code>x</code>输出模型预测值。不要显示调用此<code>forward(x)</code>方法，而是直接调用模型本身，即<code>model(x)</code>。</li>
</ol>
<p>简单的回归模型如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class ManualLinearRegression(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # To make &quot;a&quot; and &quot;b&quot; real parameters of the model, we need to wrap them with nn.Parameter</span><br><span class="line">        self.a &#x3D; nn.Parameter(torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float))</span><br><span class="line">        self.b &#x3D; nn.Parameter(torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float))</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Computes the outputs &#x2F; predictions</span><br><span class="line">        return self.a + self.b * x</span><br></pre></td></tr></table></figure>
<p>在<code>__init__(self)</code>方法中，我们使用<code>Parameters()</code>类定义了两个参数<code>a</code>和<code>b</code>，告诉Pytorch，这两个tensor要被作为模型的参数的属性。这样，我们就可以使用模型的<code>parameters()</code>方法来找到模型每次迭代时的所有参数值了，即便模型是嵌套模型都可以找得到，这样就能将参数喂入优化器optimizer来计算了(而非手动维护一张参数表)。并且，我们可以使用模型的<code>state_dict()</code>方法来获取所有参数的当前值。</p>
<p><strong>注意：模型应当与数据出于相同位置(GPU/CPU)，如果数据时GPU tensor，我们的模型也必须在GPU中</strong></p>
<p>代码示例如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line"></span><br><span class="line"># Now we can create a model and send it at once to the device</span><br><span class="line">model &#x3D; ManualLinearRegression().to(device)</span><br><span class="line"># We can also inspect its parameters using its state_dict</span><br><span class="line">print(model.state_dict())</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line">loss_fn &#x3D; nn.MSELoss(reduction&#x3D;&#39;mean&#39;)</span><br><span class="line">optimizer &#x3D; optim.SGD(model.parameters(), lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    #  注意，模型一般都有个train()方法，但是不要手动调用，此处只是为了说明此时是在训练，防止有些模型在训练模型和验证模型时操作不一致，训练时有dropout之类的</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    # No more manual prediction!</span><br><span class="line">    # yhat &#x3D; a + b * x_tensor</span><br><span class="line">    yhat &#x3D; model(x_train_tensor)</span><br><span class="line">    </span><br><span class="line">    loss &#x3D; loss_fn(y_train_tensor, yhat)</span><br><span class="line">    loss.backward()    </span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(model.state_dict())</span><br></pre></td></tr></table></figure>

<h2 id="6-训练步"><a href="#6-训练步" class="headerlink" title="6 训练步"></a>6 训练步</h2><p>我们定义了<code>optimizer</code>,<code>loss function</code>,<code>model</code>为模型三要素，同时需要提供训练时用的特征(<code>feature</code>)和对应的标签(<code>label</code>)数据。一个完整的模型训练有以下组成</p>
<ul>
<li>模型三要素<ul>
<li>优化器optimizer</li>
<li>损失函数loss</li>
<li>模型 model</li>
</ul>
</li>
<li>数据<ul>
<li>特征数据feature</li>
<li>数据标签label</li>
</ul>
</li>
</ul>
<p>我们可以写一个包含模型三要素的通用的训练函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def make_train_step(model, loss_fn, optimizer):</span><br><span class="line">    # Builds function that performs a step in the train loop</span><br><span class="line">    def train_step(x, y):</span><br><span class="line">        # Sets model to TRAIN mode</span><br><span class="line">        model.train()</span><br><span class="line">        # Makes predictions</span><br><span class="line">        yhat &#x3D; model(x)</span><br><span class="line">        # Computes loss</span><br><span class="line">        loss &#x3D; loss_fn(y, yhat)</span><br><span class="line">        # Computes gradients</span><br><span class="line">        loss.backward()</span><br><span class="line">        # Updates parameters and zeroes gradients</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        # Returns the loss</span><br><span class="line">        return loss.item()</span><br><span class="line">    </span><br><span class="line">    # Returns the function that will be called inside the train loop</span><br><span class="line">    return train_step</span><br></pre></td></tr></table></figure>
<p>然后在每个epoch时迭代模型训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Creates the train_step function for our model, loss function and optimizer</span><br><span class="line">train_step &#x3D; make_train_step(model, loss_fn, optimizer)</span><br><span class="line">losses &#x3D; []</span><br><span class="line"></span><br><span class="line"># For each epoch...</span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    # Performs one train step and returns the corresponding loss</span><br><span class="line">    loss &#x3D; train_step(x_train_tensor, y_train_tensor)</span><br><span class="line">    losses.append(loss)</span><br><span class="line">    </span><br><span class="line"># Checks model&#39;s parameters</span><br><span class="line">print(model.state_dict())</span><br></pre></td></tr></table></figure>

<ul>
<li><a href="https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e" target="_blank" rel="noopener">medium understand pytorch</a></li>
</ul>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-10-28--understand-pytorch/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-10-28--understand-pytorch/" title="理解pytorch的计算逻辑">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-09-24-outlier-detection/">
    		使用pyod做离群点检测
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.734Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p><a href="https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/</a></p>
<h3 id="1-什么是离群点"><a href="#1-什么是离群点" class="headerlink" title="1 什么是离群点"></a>1 什么是离群点</h3><p>一个极大的偏离正常值的数据点。下面是一些常见的离群点</p>
<ul>
<li>一个学生的平均得分超过剩下90%的得分，而其他人的得分平均仅为70%。显然的离群点</li>
<li>分析某个顾客的购买行为，大部分集中在100-块，突然出现1000块的消费。</li>
</ul>
<p>有多种类型的离群点</p>
<ul>
<li><strong>单变量的</strong>： 只有一个变量的值会出现极端值</li>
<li><strong>多变量的</strong>： 至少两个以上的变量值的综合得分极端。</li>
</ul>
<h3 id="2-为什么需要检测离群点"><a href="#2-为什么需要检测离群点" class="headerlink" title="2 为什么需要检测离群点"></a>2 为什么需要检测离群点</h3><p>离群点会影响我们的正常的数据分析和建模，如下图左边是包含离群点的模型，右边是处理掉离群点之后的模型结构。</p>
<p><img src="/images/blog/outlier_sample.png" alt=""></p>
<p>但是，<strong>离群点并非一直都是不好的</strong>。简单的移除离群点并非明智之举，我们需要去理解离群点。</p>
<p>现在的趋势是使用直接的方式如盒图、直方图和散点图来检测离群点。但是<strong>在处理大规模数据集和需要在更大数据集中识别某种模式时，专用的离群点检测算法是非常有价值的</strong>。</p>
<p>某些应用，如金融欺诈识别和网络安全里面的入侵检测需要及时响应的以及精确的技术来识别离群点。</p>
<h3 id="3-为什么要使用PyOD-来做离群点检测"><a href="#3-为什么要使用PyOD-来做离群点检测" class="headerlink" title="3 为什么要使用PyOD 来做离群点检测"></a>3 为什么要使用PyOD 来做离群点检测</h3><p>现有的一些实现，比如PyNomaly，并非为了做离群点而设计的（尽管依然值得一试）。PyOD是一个可拓展的Python工具包用来检测多变量数据中的离群点。提供了接近20种离群点检测算法。</p>
<h3 id="4-PyOD的特征"><a href="#4-PyOD的特征" class="headerlink" title="4 PyOD的特征"></a>4 PyOD的特征</h3><ul>
<li>开源、并附有详细的说明文档和实例。</li>
<li>支持先进的模型，包括神经网络，深度学习和离群点检测集成学习方法</li>
<li>使用JIT优化加速，以及使用numba和joblib并行化</li>
<li>python2 和3 都可以用</li>
</ul>
<h3 id="5-安装使用PyOD"><a href="#5-安装使用PyOD" class="headerlink" title="5 安装使用PyOD"></a>5 安装使用PyOD</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pyod</span><br><span class="line">pip install --upgrade pyod # to make sure that the latest version is installed!</span><br></pre></td></tr></table></figure>
<p>注意，PyOD包含了一些神经网络模型，基于keras。但是它不会自动安装Keras或者Tensorflow。需要手动安装这两个库，才能使用其神经网络模型。安装过程的依赖有点多。</p>
<h3 id="6-使用PyOD来做离群点检测"><a href="#6-使用PyOD来做离群点检测" class="headerlink" title="6 使用PyOD来做离群点检测"></a>6 使用PyOD来做离群点检测</h3><p>注意，我们使用的是<strong>离群得分</strong>，即每个模型都会给每个数据点打分而非直接根据一个阈值判定某个点是否为离群点。</p>
<p><strong>Angle_Based Outlier Detection (ABOD)</strong></p>
<ul>
<li>它考虑了每个数据点和其邻居的关系，但是不考虑邻居之间的关系。<ul>
<li>ABOD在多维度数据上表现较好</li>
<li>PyOD提供了两种不同版本的ABOD</li>
</ul>
</li>
<li>Fast ABOD：使用KNN来近似</li>
<li>Original ABOD：以高时间复杂度来考虑所有训练数据点</li>
</ul>
<p><strong>KNN 检测器</strong></p>
<ul>
<li>对于任意数据点，其到第k个邻居的距离可以作为其离群得分</li>
<li>PyOD提供三种不同的KNN检测器<ul>
<li><code>Largest</code>： 使用第k个邻居的距离来作为离群得分</li>
<li><code>Mean</code>: 使用全部k个邻居的平均距离作为离群得分</li>
<li><code>Median</code>:使用k个邻居的距离的中位数作为离群得分</li>
</ul>
</li>
</ul>
<p><strong>Isolation Forest</strong></p>
<ul>
<li>内部使用sklearn，此方法中，使用一个集合的树来完成数据分区。孤立森林提供农一个离群得分来判定一个数据点在结构中有多孤立。其离群得分用来将它与正常观测数据区分开来。</li>
<li>孤立森林在多维数据上表现很好</li>
</ul>
<p><strong>Histogram-based Outiler Detection</strong></p>
<ul>
<li>一种高效的无监督方法，它假设特征之间独立，然后通过构建直方图来计算离群得分</li>
<li>比多变量方法快得多，但是要损失一些精度</li>
</ul>
<p><strong>Local Correlation Integral(LOCI)</strong></p>
<ul>
<li>LOCI在离群检测和离群点分组上十分高效。它为每个数据点提供一个LOCI plot，该plot反映了数据点在附近数据点的诸多信息，确定集群、微集群、它们的半径以及它们的内部集群距离</li>
<li>现存的所有离群检测算法都无法超越此特性，因为他们的输出仅仅是给每个数据点的输出一个单一值。</li>
</ul>
<p><strong>Feature Bagging</strong></p>
<ul>
<li>一个特征集合检测器，它在数据集的一系列子集上拟合了大量的基准检测器。它使用平均或者其他结合方法来提高预测准确率</li>
<li>默认使用LOF(Local Outiler Factor)作为基准评估器。但是其他检测器，如KNN，ABOD都可以作为基准检测器</li>
<li>Feature Bagging首先通过随机选取特征子集来构建n个子样本。这带来了基准评估器的多样性。最终，通过取所有基准评估器的平均或者最大值来预测得分。</li>
</ul>
<p><strong>*Clustering Based  Local Outiler Factor</strong></p>
<ul>
<li>它将数据分为小聚类簇和大聚类簇。离群得分基于数据点所属的聚类簇的大小来计算，距离计算方式为到最近大聚类簇的距离。</li>
</ul>
<h3 id="7-PyOD在-Big-Mart-Sales-问题上的表现"><a href="#7-PyOD在-Big-Mart-Sales-问题上的表现" class="headerlink" title="7 PyOD在 Big Mart Sales 问题上的表现"></a>7 PyOD在 Big Mart Sales 问题上的表现</h3><p><a href="https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/?utm_source=outlierdetectionpyod&utm_medium=blog" target="_blank" rel="noopener">Big Mart Sales Problem</a>。需要注册然后下载数据集，附件中有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy import stats</span><br><span class="line">from pyod.models.abod import ABOD</span><br><span class="line">from pyod.models.cblof import CBLOF</span><br><span class="line">from pyod.models.feature_bagging import FeatureBagging</span><br><span class="line">from pyod.models.hbos import HBOS</span><br><span class="line">from pyod.models.iforest import IForest</span><br><span class="line">from pyod.models.knn import KNN</span><br><span class="line">from pyod.models.lof import LOF</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.font_manager as mfm</span><br><span class="line"></span><br><span class="line">df &#x3D; pd.read_csv(&quot;train.csv&quot;)</span><br><span class="line">print(df.describe())</span><br><span class="line">show &#x3D; False</span><br><span class="line">if show:</span><br><span class="line">    plt.figure(figsize&#x3D;(10,10))</span><br><span class="line">    plt.scatter(df[&#39;Item_MRP&#39;],df[&#39;Item_Outlet_Sales&#39;])</span><br><span class="line">    plt.xlabel(&quot;Item_MRF&quot;)</span><br><span class="line">    plt.ylabel(&quot;Item_Outlet_Sales&quot;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">scaler &#x3D; MinMaxScaler(feature_range&#x3D;(0,1))</span><br><span class="line">df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]] &#x3D; scaler.fit_transform(df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]])</span><br><span class="line">print(df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]].head())</span><br><span class="line"></span><br><span class="line">x1 &#x3D; df[&#39;Item_MRP&#39;].values.reshape(-1,1)</span><br><span class="line">x2 &#x3D; df[&#39;Item_Outlet_Sales&#39;].values.reshape(-1,1)</span><br><span class="line">x &#x3D; np.concatenate((x1,x2),axis&#x3D;1)</span><br><span class="line"># 设置 5%的离群点数据</span><br><span class="line">random_state &#x3D; np.random.RandomState(42)</span><br><span class="line">outliers_fraction &#x3D; 0.05</span><br><span class="line"># 定义7个后续会使用的离群点检测模型</span><br><span class="line">classifiers &#x3D; &#123;</span><br><span class="line">    &quot;Angle-based Outlier Detector(ABOD)&quot; : ABOD(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Cluster-based Local Outiler Factor (CBLOF)&quot;: CBLOF(contamination &#x3D; outliers_fraction,check_estimator&#x3D;False,random_state &#x3D; random_state),</span><br><span class="line">    &quot;Feature Bagging&quot; : FeatureBagging(LOF(n_neighbors&#x3D;35),contamination&#x3D;outliers_fraction,check_estimator&#x3D;False,random_state &#x3D; random_state),</span><br><span class="line">    &quot;Histogram-base Outlier Detection(HBOS)&quot; : HBOS(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Isolation Forest&quot; :IForest(contamination&#x3D;outliers_fraction,random_state &#x3D; random_state),</span><br><span class="line">    &quot;KNN&quot; : KNN(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Average KNN&quot; :KNN(method&#x3D;&#39;mean&#39;,contamination&#x3D;outliers_fraction)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#逐一 比较模型</span><br><span class="line">xx,yy &#x3D; np.meshgrid(np.linspace(0,1,200),np.linspace(0,1,200))</span><br><span class="line">for i ,(clf_name,clf) in enumerate(classifiers.items()):</span><br><span class="line">    clf.fit(x)</span><br><span class="line">    # 预测利群得分</span><br><span class="line">    scores_pred &#x3D; clf.decision_function(x)*-1</span><br><span class="line">    # 预测数据点是否为 离群点</span><br><span class="line">    y_pred &#x3D; clf.predict(x)</span><br><span class="line">    n_inliers &#x3D; len(y_pred)-np.count_nonzero(y_pred)</span><br><span class="line">    n_outliers &#x3D; np.count_nonzero(y_pred&#x3D;&#x3D;1)</span><br><span class="line">    plt.figure(figsize&#x3D;(10,10))</span><br><span class="line"></span><br><span class="line">    # 复制一份数据</span><br><span class="line">    dfx &#x3D; df</span><br><span class="line">    dfx[&#39;outlier&#39;] &#x3D; y_pred.tolist()</span><br><span class="line">    # IX1 非离群点的特征1，IX2 非利群点的特征2</span><br><span class="line">    IX1 &#x3D; np.array(dfx[&#39;Item_MRP&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;0]).reshape(-1,1)</span><br><span class="line">    IX2 &#x3D; np.array(dfx[&#39;Item_Outlet_Sales&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;0]).reshape(-1,1)</span><br><span class="line">    # OX1 离群点的特征1，OX2离群点特征2</span><br><span class="line">    OX1 &#x3D; np.array(dfx[&#39;Item_MRP&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;1]).reshape(-1,1)</span><br><span class="line">    OX2 &#x3D; np.array(dfx[&#39;Item_Outlet_Sales&#39;][dfx[&#39;outlier&#39;] &#x3D;&#x3D; 1]).reshape(-1, 1)</span><br><span class="line">    print(&quot;模型 %s 检测到的&quot;%clf_name,&quot;离群点有 &quot;,n_outliers,&quot;非离群点有&quot;,n_inliers)</span><br><span class="line"></span><br><span class="line">    # 判定数据点是否为离群点的 阈值</span><br><span class="line">    threshold &#x3D; stats.scoreatpercentile(scores_pred,100*outliers_fraction)</span><br><span class="line">    # 决策函数来计算原始的每个数据点的离群点得分</span><br><span class="line">    z &#x3D; clf.decision_function(np.c_[xx.ravel(),yy.ravel()]) * -1</span><br><span class="line">    z &#x3D; z.reshape(xx.shape)</span><br><span class="line">    # 最小离群得分和阈值之间的点 使用蓝色填充</span><br><span class="line">    plt.contourf(xx,yy,z,levels&#x3D;np.linspace(z.min(),threshold,7),cmap&#x3D;plt.cm.Blues_r)</span><br><span class="line">    # 离群得分等于阈值的数据点 使用红色填充</span><br><span class="line">    a &#x3D; plt.contour(xx,yy,z,levels&#x3D;[threshold],linewidths &#x3D;2,colors&#x3D;&#39;red&#39;)</span><br><span class="line">    # 离群得分在阈值和最大离群得分之间的数据 使用橘色填充</span><br><span class="line">    plt.contourf(xx,yy,z,levels&#x3D;[threshold,z.max()],colors&#x3D;&#39;orange&#39;)</span><br><span class="line">    b &#x3D; plt.scatter(IX1,IX2,c&#x3D;&#39;white&#39;,s&#x3D;20,edgecolor &#x3D;&#39;k&#39;)</span><br><span class="line">    c &#x3D; plt.scatter(OX1,OX2,c&#x3D;&#39;black&#39;,s&#x3D;20,edgecolor &#x3D; &#39;k&#39;)</span><br><span class="line">    plt.axis(&#39;tight&#39;)</span><br><span class="line">    # loc &#x3D; 2 用来左上角</span><br><span class="line">    plt.legend(</span><br><span class="line">        [a.collections[0],b,c],</span><br><span class="line">        [&#39;learned decision function&#39;,&#39;inliers&#39;,&#39;outliers&#39;],</span><br><span class="line">        prop&#x3D;mfm.FontProperties(size&#x3D;20),</span><br><span class="line">        loc&#x3D;2</span><br><span class="line">    )</span><br><span class="line">    plt.xlim((0,1))</span><br><span class="line">    plt.ylim((0,1))</span><br><span class="line">    plt.title(clf_name)</span><br><span class="line">    plt.savefig(&quot;%s.png&quot;%clf_name)</span><br><span class="line">    #plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下；</p>
<p><img src="/images/blog/outlier_detection_ABOD.png" alt=""><br><img src="/images/blog/outlier_detection_avg_knn.png" alt=""><br><img src="/images/blog/outlier_detection_CBLOF.png" alt=""><br><img src="/images/blog/outlier_detection_feature_bagging.png" alt=""><br><img src="/images/blog/outlier_detection_HBOS.png" alt=""><br><img src="/images/blog/outlier_detection_Isolation_Forest.png" alt=""><br><img src="/images/blog/outlier_detection_KNN.png" alt=""></p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-09-24-outlier-detection/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-09-24-outlier-detection/" title="使用pyod做离群点检测">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-05-12-edit-stylegan-humanface/">
    		使用StyleGAN训练自己的数据集.md
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.724Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>参考： <a href="https://www.gwern.net/Faces#compute" target="_blank" rel="noopener">https://www.gwern.net/Faces#compute</a></p>
<h2 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1 数据准备"></a>1 数据准备</h2><p>执行StyleGAN的最大难点在于准备数据集，不像其他的GAN可以接受文件夹输入，它只能接收<code>.tfrecords</code>作为输入，它将每张图片不同分辨率存储为数组。因此，输入文件必须是完美正态分布的，通过特定的dataset_tools.py工具将图片转成.tfrecords，这会导致实际存储尺寸达到原图的19倍。</p>
<p>注意：</p>
<ul>
<li>StyleGAN的数据集必须由相同的方式组成，$512\times 512$ 或 $1024\times 1024$( $513\times 513$就不行)</li>
<li>必须是相同的颜色空间，不能既有sRGB又有灰度图JPGs。</li>
<li>文件类型必须是与你要重新训练的模型所使用的图像格式相同的，比如，你不能用PNG图片来重新训练一个用JPG格式图像的模型。</li>
<li>不可以有细微的错误，比如CRC校验失败。</li>
</ul>
<h2 id="2-准备脸部数据"><a href="#2-准备脸部数据" class="headerlink" title="2 准备脸部数据"></a>2 准备脸部数据</h2><ol>
<li>下载原始数据集 <a href="https://www.gwern.net/Danbooru2018#download" target="_blank" rel="noopener">Danbooru2018</a></li>
<li>从Danbooru2018的metadata的JSON文件中抽取所有的图像子集的ID，如果需要指定某个特定的Danbooru标签,使用<code>jq</code>以及shell脚本</li>
<li>将原图裁剪。可以使用<a href="https://github.com/nagadomi/lbpcascade_animeface" target="_blank" rel="noopener">nagadomi</a>的人脸裁剪算法，普通的人脸检测算法无法适用于这个卡通人脸。</li>
<li>删除空文件，单色图，灰度图，删掉重名文件</li>
<li>转换成JPG格式</li>
<li>将所有图片上采样到目标分辨率即$512\times 512$，可以使用 <a href="https://github.com/nagadomi/waifu2x" target="_blank" rel="noopener">waifu2x</a></li>
<li>将所有图像转换成 $512\times 512$的sRGB JPG格式图像</li>
<li>可以人工筛选出质量高的图像，使用<code>findimagedupes</code>删除近似的图像，并用预训练的GAN Discriminator过滤掉部分。</li>
<li>使用StyleGAN的<code>data_tools.py</code>将图片转换成tfrecords</li>
</ol>
<p>目标是将此图</p>
<p><img src="/images/blog/stylegan_owndata_1.png" alt=""></p>
<p>转换成</p>
<p><img src="/images/blog/stylegan_owndata_2.png" alt=""></p>
<p>下面使用了一些脚本进行数据处理，可以使用<a href="https://github.com/reidsanders/danbooru-utility" target="_blank" rel="noopener">danbooru-utility</a>协助。</p>
<h3 id="2-1-裁剪"><a href="#2-1-裁剪" class="headerlink" title="2.1 裁剪"></a>2.1 裁剪</h3><p>原始的<a href="https://www.gwern.net/Danbooru2018#download" target="_blank" rel="noopener">Danbooru2018</a>可以使用磁链下载，提供了JSON的metadata，被压缩到<code>metadata/2*</code>和目录结构为<code>{original,512px}/{0-999}/$ID.{png,jpg}</code>。可以使用Danbooru2018<code>512像素</code>版本在整个SFW图像集上的训练，但是将所有图像缩放到512像素并非明智之举，因为会丢失大量面部信息，而保留高质量面部图像是个挑战。可以从<code>512px/</code>目录下的文件名中直接抽取SFW IDs，或者从metadata中抽取<code>id</code>和<code>rating</code>字段并存入某个文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">find .&#x2F;512px&#x2F; -type f | sed -e &#39;s&#x2F;.*\&#x2F;\([[:digit:]]*\)\.jpg&#x2F;\1&#x2F;&#39;</span><br><span class="line"># 967769</span><br><span class="line"># 1853769</span><br><span class="line"># 2729769</span><br><span class="line"># 704769</span><br><span class="line"># 1799769</span><br><span class="line"># ...</span><br><span class="line">tar xf metadata.json.tar.xz</span><br><span class="line">cat metadata&#x2F;* | jq &#39;[.id, .rating]&#39; -c | fgrep &#39;&quot;s&quot;&#39; | cut -d &#39;&quot;&#39; -f 2 # &quot;</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure>
<p>可以安装和使用<a href="https://github.com/nagadomi/lbpcascade_animeface" target="_blank" rel="noopener">lbpcascade_animeface</a>以及opencv，使用简单的一个脚本<a href="https://github.com/nagadomi/lbpcascade_animeface/issues/1#issue-205363706" target="_blank" rel="noopener">lbpcascade_animeface issue</a>来裁剪图像。在Danbooru图像上表现惊人，大概有90%的高质量面部图像，5%低质量的，以及5%的错误图像(没有脸部)。也可以通过给脚本更多的限制，比如要求$256\times 256px$区域，可以消除大部分低质量的面部和错误。以下是<code>crop.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import sys</span><br><span class="line">import os.path</span><br><span class="line"></span><br><span class="line">def detect(cascade_file, filename, outputname):</span><br><span class="line">    if not os.path.isfile(cascade_file):</span><br><span class="line">        raise RuntimeError(&quot;%s: not found&quot; % cascade_file)</span><br><span class="line"></span><br><span class="line">    cascade &#x3D; cv2.CascadeClassifier(cascade_file)</span><br><span class="line">    image &#x3D; cv2.imread(filename)</span><br><span class="line">    gray &#x3D; cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    gray &#x3D; cv2.equalizeHist(gray)</span><br><span class="line"></span><br><span class="line">    ## Suggested modification: increase minSize to &#39;(250,250)&#39; px,</span><br><span class="line">    ## increasing proportion of high-quality faces &amp; reducing</span><br><span class="line">    ## false positives. Faces which are only 50x50px are useless</span><br><span class="line">    ## and often not faces at all.</span><br><span class="line"></span><br><span class="line">    faces &#x3D; cascade.detectMultiScale(gray,</span><br><span class="line">                                     # detector options</span><br><span class="line">                                     scaleFactor &#x3D; 1.1,</span><br><span class="line">                                     minNeighbors &#x3D; 5,</span><br><span class="line">                                     minSize &#x3D; (50, 50))</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    for (x, y, w, h) in faces:</span><br><span class="line">        cropped &#x3D; image[y: y + h, x: x + w]</span><br><span class="line">        cv2.imwrite(outputname+str(i)+&quot;.png&quot;, cropped)</span><br><span class="line">        i&#x3D;i+1</span><br><span class="line"></span><br><span class="line">if len(sys.argv) !&#x3D; 4:</span><br><span class="line">    sys.stderr.write(&quot;usage: detect.py &lt;animeface.xml file&gt; &lt;input&gt; &lt;output prefix&gt;\n&quot;)</span><br><span class="line">    sys.exit(-1)</span><br><span class="line"></span><br><span class="line">detect(sys.argv[1], sys.argv[2], sys.argv[3])</span><br></pre></td></tr></table></figure>

<p>IDs可以和提供的<code>lbpcascade_animeface</code>脚本使用<code>xargs</code>结合起来，但是这样还是太慢，使用并行策略<code>xargs --max-args=1 --max-procs=16</code>或者参数<code>parallel</code>更有效。<code>lbpcascade_animeface</code>脚本似乎使用了所有的GPU显存，但是没有可见的提升，我发现可以通过设置<code>CUDA_VISIBLE_DEVICES=&quot;&quot;</code>来禁用GPU（此步骤还是使用多核CPU更有效）。</p>
<p>一切就绪之后，可以按照如下方式在整个Danbooru2018数据子集上使用并行的面部图像切割</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cropFaces() &#123;</span><br><span class="line">    BUCKET&#x3D;$(printf &quot;%04d&quot; $(( $@ % 1000 )) )</span><br><span class="line">    ID&#x3D;&quot;$@&quot;</span><br><span class="line">    CUDA_VISIBLE_DEVICES&#x3D;&quot;&quot; nice python ~&#x2F;src&#x2F;lbpcascade_animeface&#x2F;examples&#x2F;crop.py \</span><br><span class="line">     ~&#x2F;src&#x2F;lbpcascade_animeface&#x2F;lbpcascade_animeface.xml \</span><br><span class="line">     .&#x2F;original&#x2F;$BUCKET&#x2F;$ID.* &quot;.&#x2F;faces&#x2F;$ID&quot;</span><br><span class="line">&#125;</span><br><span class="line">export -f cropFaces</span><br><span class="line"></span><br><span class="line">mkdir .&#x2F;faces&#x2F;</span><br><span class="line">cat sfw-ids.txt | parallel --progress cropFaces</span><br></pre></td></tr></table></figure>

<h3 id="2-2-上采样和使用GAN的Discriminator进行数据清洗"><a href="#2-2-上采样和使用GAN的Discriminator进行数据清洗" class="headerlink" title="2.2 上采样和使用GAN的Discriminator进行数据清洗"></a>2.2 上采样和使用GAN的Discriminator进行数据清洗</h3><p>在训练GAN一段时间之后，重新用Disciminator对真实的数据点进行排序。通常情况下，被Disciminator判定最低得分的图片通常也是质量较差的，可以移除，这样也有助于提升GAN。然后GAN可以在新的干净数据集上重新训练，得以提升GAN。</p>
<p>由于对图像排序是Disciminator默认会做的事，所有不需要额外的训练或算法。下面是一个简单的ranker.py脚本，载入StyleGAN的<code>.pkl</code>模型，然后运行图片名列表，并打印D得分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    tflib.init_tf()</span><br><span class="line">    _G, D, _Gs &#x3D; pickle.load(open(sys.argv[1], &quot;rb&quot;))</span><br><span class="line">    image_filenames &#x3D; sys.argv[2:]</span><br><span class="line"></span><br><span class="line">    for i in range(0, len(image_filenames)):</span><br><span class="line">        img &#x3D; np.asarray(PIL.Image.open(image_filenames[i]))</span><br><span class="line">        img &#x3D; img.reshape(1, 3,512,512)</span><br><span class="line">        score &#x3D; D.run(img, None)</span><br><span class="line">        print(image_filenames[i], score[0][0])</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>使用示例如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">find &#x2F;media&#x2F;gwern&#x2F;Data&#x2F;danbooru2018&#x2F;characters-1k-faces&#x2F; -type f | xargs -n 9000 --max-procs&#x3D;1 \</span><br><span class="line">    python ranker.py results&#x2F;02086-sgan-portraits-2gpu&#x2F;network-snapshot-058662.pkl \</span><br><span class="line">    | tee portraitfaces-rank.txt</span><br><span class="line">fgrep &#x2F;media&#x2F;gwern&#x2F; 2019-04-22-portraitfaces-rank.txt | \</span><br><span class="line">    sort --field-separator &#39; &#39; --key 2 --numeric-sort | head -100</span><br><span class="line"># ...&#x2F;megurine.luka&#x2F;7853120.jpg -708.6835</span><br><span class="line"># ...&#x2F;remilia.scarlet&#x2F;26352470.jpg -707.39856</span><br><span class="line"># ...&#x2F;z1.leberecht.maass..kantai.collection.&#x2F;26703440.jpg -702.76904</span><br><span class="line"># ...&#x2F;suzukaze.aoba&#x2F;27957490.jpg -700.5606</span><br><span class="line"># ...&#x2F;jack.the.ripper..fate.apocrypha.&#x2F;31991880.jpg -700.0554</span><br><span class="line"># ...&#x2F;senjougahara.hitagi&#x2F;4947410.jpg -699.0976</span><br><span class="line"># ...&#x2F;ayase.eli&#x2F;28374650.jpg -698.7358</span><br><span class="line"># ...&#x2F;ayase.eli&#x2F;16185520.jpg -696.97845</span><br><span class="line"># ...&#x2F;illustrious..azur.lane.&#x2F;31053930.jpg -696.8634</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure>

<p>你可以选择删除一定数量，或者最靠近末尾的TOP N%的图片。同时也应该检查最靠前的TOP的图像，有些十分异常的也需要删除。可以使用ranker.py提高生成的样本质量，简单示例。</p>
<h3 id="2-3-质量检测和数据增强"><a href="#2-3-质量检测和数据增强" class="headerlink" title="2.3 质量检测和数据增强"></a>2.3 质量检测和数据增强</h3><p>我们可以对图像质量进行人工校验，逐个浏览成百上千的图片，使用<code>findimagedupes -t 99%</code>来寻找近似相近的面部。在Danbooru2018中，可以有600-700000张脸，这已足够训练StyleGAN并且最终数据集有点大，会增加19倍。</p>
<p>但是如果我们需要在单一特征的小数据集上做，数据增强就比较有必要了。不需要做上下/左右翻转了，StyleGAN内部有做。我们可以做的是，颜色变换，锐化，模糊，增加/减小对比度，裁剪等操作。</p>
<h3 id="2-4-上采样和转换"><a href="#2-4-上采样和转换" class="headerlink" title="2.4 上采样和转换"></a>2.4 上采样和转换</h3><p>将图像转换成JPG可以大概节省33%的存储空间。但是切记，StyleGAN模型只接收在与其训练时所使用的相同的图片格式，像FFHQ数据集所使用的是PNG.</p>
<p>鉴于<code>dataset_tool.py</code>脚本在转换图片到tfrecords时太诡异，最好是打印每个处理完的图片，一旦程序崩溃，可以排错。对<code>dataset_tool.py</code>的简单修改如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:</span><br><span class="line">         order &#x3D; tfr.choose_shuffled_order() if shuffle else np.arange(len(image_filenames))</span><br><span class="line">         for idx in range(order.size):</span><br><span class="line">  print(image_filenames[order[idx]])</span><br><span class="line">             img &#x3D; np.asarray(PIL.Image.open(image_filenames[order[idx]]))</span><br><span class="line">             if channels &#x3D;&#x3D; 1:</span><br><span class="line">                 img &#x3D; img[np.newaxis, :, :] # HW &#x3D;&gt; CHW</span><br></pre></td></tr></table></figure>

<h2 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3 训练模型"></a>3 训练模型</h2><p><strong>参数配置</strong></p>
<ol>
<li><p><code>train/training_loop.py</code>:关键配置参数是training_loop.py的112行起。关键参数</p>
<ul>
<li><code>G_smoothing_kimg</code> 和<code>D_repeats</code>(影响学习的动态learning dynamics),</li>
<li><code>network_snapshot_ticks</code>(多久存储一次中间模型)</li>
<li><code>resume_run_id</code>: 设置为<code>latest</code></li>
<li><code>resume_kimg</code>.注意，它决定了模型训练的阶段，如果设置为0，模型会从头开始训练而无视之前的训练结果，即从最低分辨率开始。如果要做迁移学习，需要将其设置为一个足够高的数目，如10000，这样一来，模型就可以在最高分辨率，如$512\times 512$的阶段开始训练。</li>
<li>建议将<code>minibatch_repeats = 5</code>改为<code>minibatch_repeats = 1</code>。此处我怀疑ProGAN/StyleGAN中的梯度累加的实现，这样会使得训练过程更加稳定、更快。</li>
<li>注意，一些参数如学习率，会在<code>train.py</code>中被覆盖。最好是在覆盖的地方修改，</li>
</ul>
</li>
<li><p><code>train.py</code> (以前是<code>config.py</code>):设置GPU的数目，图像分辨率，数据集，学习率，水平翻转/镜像数据增强，以及minibatch-size。(此文件包含了ProGAN的一些配置参数，你并不是突然开启了ProGAN)。学习率和minbatch通常不用管（除非你想在训练的末尾阶段降低学习率以提升算法能力）。图像分辨率/dataset/mirroring需要设置，如</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc +&#x3D; &#39;-faces&#39;; dataset &#x3D; EasyDict(tfrecord_dir&#x3D;&#39;faces&#39;, resolution&#x3D;512); train.mirror_augment &#x3D; True</span><br></pre></td></tr></table></figure>
<p>此处设置了$512\times 512$的脸部数据集，我们前面创建的<code>datasets/faces</code>，启用mirror。假如没有8个GPU，必须修改<code>-preset</code>以匹配你的GPU数量，StyleGAN不会自动修改的。对于两块 2080ti，设置如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">desc +&#x3D; &#39;-preset-v2-2gpus&#39;; submit_config.num_gpus &#x3D; 2; sched.minibatch_base &#x3D; 8; sched.minibatch_dict &#x3D; \</span><br><span class="line">    &#123;4: 256, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16, 256: 8&#125;; sched.G_lrate_dict &#x3D; &#123;512: 0.0015, 1024: 0.002&#125;; \</span><br><span class="line">    sched.D_lrate_dict &#x3D; EasyDict(sched.G_lrate_dict); train.total_kimg &#x3D; 99000</span><br></pre></td></tr></table></figure>
<p>最后的结果会被保存到<code>results/00001-sgan-faces-2gpu</code>（<code>00001</code>代表递增ID,<code>sgan</code>因为使用的是StyleGAN而非ProGAN,<code>-faces</code>是训练的数据集,<code>-2gpu</code>即我们使用的多GPU）。</p>
<h2 id="4-运行过程"><a href="#4-运行过程" class="headerlink" title="4 运行过程"></a>4 运行过程</h2><p>相比于训练其他GAN，StyleGAN更稳定更好训练，但是也容易出问题。</p>
<h3 id="4-1-Crashproofing"><a href="#4-1-Crashproofing" class="headerlink" title="4.1 Crashproofing"></a>4.1 Crashproofing</h3><p>StyleGAN容易在混合GPU(1080ti+Titan V)上训练时崩溃，低版本的Tensorflow上也是，可以升级解决。如果崩溃了，代码无法自动继续上一次的训练迭代次数，需要手工在<code>training_loop.py</code>中修改<code>resume_run_id</code>为最后崩溃时的迭代次数。建议将此处的<code>resume_run_id</code>参数修改为<code>resume_run_id=latest</code>。</p>
<h3 id="4-2-调节学习率"><a href="#4-2-调节学习率" class="headerlink" title="4.2 调节学习率"></a>4.2 调节学习率</h3><p>学习率这个是最重要的超参数之一：在小batch size数据过大的更新会极大破坏GAN的稳定性和最终结果。论文在FFHQ数据集上，8个GPU，32的batch size时使用的学习率是0.003，但是在我们的动画数据集上，batch size=8更低的学习率效果更好。学习率与batch size非常相关，越难的数据集学习率应该更小。</p>
<h3 id="4-3-G-D的均衡"><a href="#4-3-G-D的均衡" class="headerlink" title="4.3 G/D的均衡"></a>4.3 G/D的均衡</h3><p>在后续的训练中，如果G没有产生很好的进步，没有朝着0.5的损失前进（而对应的D的损失朝着0.5大幅度缩减），并且在-1.0左右卡住或者其他的问题。此时，有必要调节G/D的均衡了。有几种方法可以完成此事，最简单的办法是在<code>train.py</code>中调节sched.G_lrate_dict的学习率参数。</p>
<p><img src="/images/blog/stylegan_owndata_3.png" alt=""></p>
<p>需要时刻关注G/D的损失，以及面部图像的perceptual质量，同时需要基于面部图像以及G/D的损失是否在爆炸或者严重不均衡而减小G和D的学习率（或者只减小D的学习率）。我们设想的是G/D的损失在一个确定的绝对损失值，同时质量有肉眼可见的提高，减小D的学习率有助于保持与G的均衡。当然如果超出你的耐心，或者时间不够，可以考虑同时减小D/G的学习率达到一个局部最优。</p>
<p>默认的0.003的学习率可能在达到高质量的面部和肖像图像时变得太高，可以将其减小三分之一或十分之一。如果任然不能收敛，D可能太强，可以单独的将其能力降低。由于训练的随机性和损失的相对性，可能需要在修改参数之后的很多小时或者很多天之后才能看到效果。</p>
<h3 id="4-4-跳过FID指标"><a href="#4-4-跳过FID指标" class="headerlink" title="4.4 跳过FID指标"></a>4.4 跳过FID指标</h3><p>一些指标用来计算日志。FID指标是ImageNet CNN的计算指标，可能在ImageNet中重要的特性在你的特定领域中其实是不相关的，并且一个大的FID如100是可以考虑的，FIDs为20或者增大都不太是个问题或者是个有用的指导，还不如直接看生成的样本呢。建议直接禁用FIDs指标（训练阶段并没有，所以直接禁用是安全的）。</p>
<p>可以直接通过注释<code>metrics.run</code>的调用来禁用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@@ -261,7 +265,7 @@ def training_loop()</span><br><span class="line">        if cur_tick % network_snapshot_ticks &#x3D;&#x3D; 0 or done or cur_tick &#x3D;&#x3D; 1:</span><br><span class="line">            pkl &#x3D; os.path.join(submit_config.run_dir, &#39;network-snapshot-%06d.pkl&#39; % (cur_nimg &#x2F;&#x2F; 1000))</span><br><span class="line">            misc.save_pkl((G, D, Gs), pkl)</span><br><span class="line">            # metrics.run(pkl, run_dir&#x3D;submit_config.run_dir, num_gpus&#x3D;submit_config.num_gpus, tf_config&#x3D;tf_config)</span><br></pre></td></tr></table></figure>
<h3 id="4-5-BLOB-斑块-和CRACK-裂缝-缺陷"><a href="#4-5-BLOB-斑块-和CRACK-裂缝-缺陷" class="headerlink" title="4.5 BLOB(斑块)和CRACK(裂缝)缺陷"></a>4.5 BLOB(斑块)和CRACK(裂缝)缺陷</h3><p>训练过程中，<code>blobs</code>(可以理解为斑块)时不时出现。这些blobs甚至出现在训练的后续阶段，在一些已经生成的高质量图像上，并且这些blob可能是与StyleGAN独有的(至少没有在其他GAN上出现过这个blob)。这些blob如此大并且刺眼。这些斑块出现的原因未知，据推测可能是$3\times 3$的卷积层导致的；可能使用额外的$1\times 1$卷积或者自相关层可以消除这个问题。</p>
<p>如果斑块出现得太频繁或者想完全消除，降低学习率达到一个局部最优可能有用。</p>
<p>训练动漫人物面部时，我看到了其他的缺陷，看起来像裂缝或者波浪或者皮肤上的皱纹，它们会一直伴随着训练直至最终。在小数据集做迁移学习时 会经常出现。与blob斑块相反，我目前怀疑裂缝的出现是过拟合的标识，而非StyleGAN的一种特质。当G开始记住最终的线条或像素上的精细细节的噪音时，目前的仅有的解决方案是要么停止训练要么增加数据。</p>
<h3 id="4-6-梯度累加"><a href="#4-6-梯度累加" class="headerlink" title="4.6 梯度累加"></a>4.6 梯度累加</h3><p>ProGAN/StyleGAN的代码宣称支持梯度累加，这是一种形似大的minibatch训练(batch_size=2048)的技巧，它通过不向后传播每个minibatch，但是累加多个minibatch，然后一次执行的方式实现。这是一种保持训练稳定的有效策略，增加minibatch尺寸有助于提高生成图像的质量。</p>
<p>但是ProGAN/StyleGAN的梯度累加的实现在Tensorflow或Pytorch中并没有类似的，<strong>以我个人的经验来看，最大可以加到4096，但是并没有看到什么区别，所以我怀疑这个实现是错误的。</strong></p>
<p>下面是我训练的动漫人脸的模型，训练了21980步，在2100万张图像上，38个GPU一天，尽管还没完全收敛，但是效果很好。<br><a href="https://www.gwern.net/images/gan/2019-03-16-stylegan-facestraining.mp4" target="_blank" rel="noopener">训练效果</a></p>
<h2 id="5-采样"><a href="#5-采样" class="headerlink" title="5 采样"></a>5 采样</h2><h3 id="5-1-PSI-Truncation-Trick"><a href="#5-1-PSI-Truncation-Trick" class="headerlink" title="5.1 PSI/Truncation Trick"></a>5.1 PSI/Truncation Trick</h3><p>截断技巧$\phi$  是所有StyleGAN生成器的最重要的超参数。它用在样本生成阶段，而非训练时。思路是，编辑latent 向量z，一个服从N(0,1)分布的向量，会自动删除所有大于特定值，比如0.5或1.0的变量。这看起来会避免极端的latent值，或者删除那些与G组合不太好的latent值。G不会生成与每个latent值在+1.5SD的点生成很多数据点。<br>代价便是这些依然是全部latent变量的何方区域，并且可以在训练期间被用来覆盖部分数据分布。因而，尽管latent变量接近0的均值才是最准确的模型，它们仅仅是全部可能的产生图像的数据空间上的一小部分。因而，我们可以从全部的无限制的正态分布$N(0,1)$上生成latent变量，也既可以截断如$+1SD或者+0.7SD$。</p>
<p>$\omega =0$时，多样性为0，并且所有生成的脸都是同一个角度(棕色眼睛，棕色头发的校园女孩，毫无例外的)，在$\omega \pm 0.5$时有更多区间的脸，在$\omega \pm 1.2$时会看到大量的多样性的脸/发型/一致性,但是也能看到大量的伪造像/失真像.参数$\omega$会极大地影响原始的输出。$\omega =1.2$时，得到的是异常原始但是极度真实或者失真。$\omega =0.5$时，具备一致连贯性，但是也很无聊。我的大部分采样，设置$\omega =0.7$可以得到最好的均衡。(就个人来说$\omega =1.2$时，采样最有趣)</p>
<h3 id="5-2-随机采样"><a href="#5-2-随机采样" class="headerlink" title="5.2 随机采样"></a>5.2 随机采样</h3><p>StyleGAN有个简单的脚本<code>prtrained_example.py</code>下载和生成单张人脸，为了复现效果，它在模型中指定了RNG随机数的种子，这样它会生成特定的人脸。然而，可以轻易地引入使用本地模型并生成，比如说1000张图像，指定参数$\omega =0.6$（此时会产生高质量图像，但是图像多样性较差）并保存结果到<code>results/example-{0-999}.png</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    tflib.init_tf()</span><br><span class="line">    _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02051-sgan-faces-2gpu&#x2F;network-snapshot-021980.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">    Gs.print_layers()</span><br><span class="line"></span><br><span class="line">    for i in range(0,1000):</span><br><span class="line">        rnd &#x3D; np.random.RandomState(None)</span><br><span class="line">        latents &#x3D; rnd.randn(1, Gs.input_shape[1])</span><br><span class="line">        fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">        images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;0.6, randomize_noise&#x3D;True, output_transform&#x3D;fmt)</span><br><span class="line">        os.makedirs(config.result_dir, exist_ok&#x3D;True)</span><br><span class="line">        png_filename &#x3D; os.path.join(config.result_dir, &#39;example-&#39;+str(i)+&#39;.png&#39;)</span><br><span class="line">        PIL.Image.fromarray(images[0], &#39;RGB&#39;).save(png_filename)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h3 id="5-3-Karras-et-al-2018图像"><a href="#5-3-Karras-et-al-2018图像" class="headerlink" title="5.3 Karras et al 2018图像"></a>5.3 Karras et al 2018图像</h3><p>此图像展示了使用1024像素的FFHQ 脸部模型(以及其他)，使用脚本<code>generate_figure.py</code>生成随机样本以及style noise的方面影响。此脚本需要大量修改来运行我的512像素的动漫人像。</p>
<ul>
<li><p>代码使用$\omega=1.0$截断，但是面部在$\omega=0.7$的时候看起来更好(好几个脚本都是用了<code>truncation_psi=</code>,但是严格来说，图3的<code>draw_style_mixiing_figure</code>将参数$\omega$隐藏在全局变量<code>sythesis_kwargs</code>中)</p>
</li>
<li><p>载入模型需要被换到动漫面部模型</p>
</li>
<li><p>需要将维度$1024\rightarrow 512$，其他被硬编码(hardcoded)的区间(ranges)必须被减小到521像素的图像。</p>
</li>
<li><p>截断技巧图8并没有足够的足够的面部来展示latent空间的用处，所以它需要被扩充来展示随机种子和面部图像，以及更多的$\omega$值。</p>
</li>
<li><p><code>bedroom/car/cat</code>样本应该被禁用</p>
</li>
</ul>
<p>代码改动如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"> url_cars &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MJ6iCfNtMIRicihwRorsM3b7mmtmK9c3&#39; # karras2019stylegan-cars-512x384.pkl</span><br><span class="line"> url_cats &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MQywl0FNt6lHu8E_EUqnRbviagS7fbiJ&#39; # karras2019stylegan-cats-256x256.pkl</span><br><span class="line"></span><br><span class="line">-synthesis_kwargs &#x3D; dict(output_transform&#x3D;dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True), minibatch_size&#x3D;8)</span><br><span class="line">+synthesis_kwargs &#x3D; dict(output_transform&#x3D;dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True), minibatch_size&#x3D;8, truncation_psi&#x3D;0.7)</span><br><span class="line"></span><br><span class="line"> _Gs_cache &#x3D; dict()</span><br><span class="line"></span><br><span class="line"> def load_Gs(url):</span><br><span class="line">- if url not in _Gs_cache:</span><br><span class="line">- with dnnlib.util.open_url(url, cache_dir&#x3D;config.cache_dir) as f:</span><br><span class="line">- _G, _D, Gs &#x3D; pickle.load(f)</span><br><span class="line">- _Gs_cache[url] &#x3D; Gs</span><br><span class="line">- return _Gs_cache[url]</span><br><span class="line">+ _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02051-sgan-faces-2gpu&#x2F;network-snapshot-021980.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">+ return Gs</span><br><span class="line"></span><br><span class="line"> #----------------------------------------------------------------------------</span><br><span class="line"> # Figures 2, 3, 10, 11, 12: Multi-resolution grid of uncurated result images.</span><br><span class="line">@@ -85,7 +82,7 @@ def draw_noise_detail_figure(png, Gs, w, h, num_samples, seeds):</span><br><span class="line">     canvas &#x3D; PIL.Image.new(&#39;RGB&#39;, (w * 3, h * len(seeds)), &#39;white&#39;)</span><br><span class="line">     for row, seed in enumerate(seeds):</span><br><span class="line">         latents &#x3D; np.stack([np.random.RandomState(seed).randn(Gs.input_shape[1])] * num_samples)</span><br><span class="line">- images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;1, **synthesis_kwargs)</span><br><span class="line">+ images &#x3D; Gs.run(latents, None, **synthesis_kwargs)</span><br><span class="line">         canvas.paste(PIL.Image.fromarray(images[0], &#39;RGB&#39;), (0, row * h))</span><br><span class="line">         for i in range(4):</span><br><span class="line">             crop &#x3D; PIL.Image.fromarray(images[i + 1], &#39;RGB&#39;)</span><br><span class="line">@@ -109,7 +106,7 @@ def draw_noise_components_figure(png, Gs, w, h, seeds, noise_ranges, flips):</span><br><span class="line">     all_images &#x3D; []</span><br><span class="line">     for noise_range in noise_ranges:</span><br><span class="line">         tflib.set_vars(&#123;var: val * (1 if i in noise_range else 0) for i, (var, val) in enumerate(noise_pairs)&#125;)</span><br><span class="line">- range_images &#x3D; Gsc.run(latents, None, truncation_psi&#x3D;1, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">+ range_images &#x3D; Gsc.run(latents, None, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">         range_images[flips, :, :] &#x3D; range_images[flips, :, ::-1]</span><br><span class="line">         all_images.append(list(range_images))</span><br><span class="line"></span><br><span class="line">@@ -144,14 +141,11 @@ def draw_truncation_trick_figure(png, Gs, w, h, seeds, psis):</span><br><span class="line"> def main():</span><br><span class="line">     tflib.init_tf()</span><br><span class="line">     os.makedirs(config.result_dir, exist_ok&#x3D;True)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure02-uncurated-ffhq.png&#39;), load_Gs(url_ffhq), cx&#x3D;0, cy&#x3D;0, cw&#x3D;1024, ch&#x3D;1024, rows&#x3D;3, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;5)</span><br><span class="line">- draw_style_mixing_figure(os.path.join(config.result_dir, &#39;figure03-style-mixing.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, src_seeds&#x3D;[639,701,687,615,2268], dst_seeds&#x3D;[888,829,1898,1733,1614,845], style_ranges&#x3D;[range(0,4)]*3+[range(4,8)]*2+[range(8,18)])</span><br><span class="line">- draw_noise_detail_figure(os.path.join(config.result_dir, &#39;figure04-noise-detail.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, num_samples&#x3D;100, seeds&#x3D;[1157,1012])</span><br><span class="line">- draw_noise_components_figure(os.path.join(config.result_dir, &#39;figure05-noise-components.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, seeds&#x3D;[1967,1555], noise_ranges&#x3D;[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips&#x3D;[1])</span><br><span class="line">- draw_truncation_trick_figure(os.path.join(config.result_dir, &#39;figure08-truncation-trick.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, seeds&#x3D;[91,388], psis&#x3D;[1, 0.7, 0.5, 0, -0.5, -1])</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure10-uncurated-bedrooms.png&#39;), load_Gs(url_bedrooms), cx&#x3D;0, cy&#x3D;0, cw&#x3D;256, ch&#x3D;256, rows&#x3D;5, lods&#x3D;[0,0,1,1,2,2,2], seed&#x3D;0)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure11-uncurated-cars.png&#39;), load_Gs(url_cars), cx&#x3D;0, cy&#x3D;64, cw&#x3D;512, ch&#x3D;384, rows&#x3D;4, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;2)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure12-uncurated-cats.png&#39;), load_Gs(url_cats), cx&#x3D;0, cy&#x3D;0, cw&#x3D;256, ch&#x3D;256, rows&#x3D;5, lods&#x3D;[0,0,1,1,2,2,2], seed&#x3D;1)</span><br><span class="line">+ draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure02-uncurated-ffhq.png&#39;), load_Gs(url_ffhq), cx&#x3D;0, cy&#x3D;0, cw&#x3D;512, ch&#x3D;512, rows&#x3D;3, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;5)</span><br><span class="line">+ draw_style_mixing_figure(os.path.join(config.result_dir, &#39;figure03-style-mixing.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, src_seeds&#x3D;[639,701,687,615,2268], dst_seeds&#x3D;[888,829,1898,1733,1614,845], style_ranges&#x3D;[range(0,4)]*3+[range(4,8)]*2+[range(8,16)])</span><br><span class="line">+ draw_noise_detail_figure(os.path.join(config.result_dir, &#39;figure04-noise-detail.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, num_samples&#x3D;100, seeds&#x3D;[1157,1012])</span><br><span class="line">+ draw_noise_components_figure(os.path.join(config.result_dir, &#39;figure05-noise-components.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, seeds&#x3D;[1967,1555], noise_ranges&#x3D;[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips&#x3D;[1])</span><br><span class="line">+ draw_truncation_trick_figure(os.path.join(config.result_dir, &#39;figure08-truncation-trick.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, seeds&#x3D;[91,388, 389, 390, 391, 392, 393, 394, 395, 396], psis&#x3D;[1, 0.7, 0.5, 0.25, 0, -0.25, -0.5, -1])</span><br></pre></td></tr></table></figure>
<p>修改完之后，可以得到一些有趣的动漫人脸样本。</p>
<p><img src="/images/blog/stylegan_owndata_4.png" alt=""></p>
<p>上图是随机样本</p>
<p><img src="/images/blog/stylegan_owndata_5.png" alt=""></p>
<p>上图是使用风格混合样本。展示了编辑和差值(第一行是风格，左边列代表了要转变风格的图像)</p>
<p><img src="/images/blog/stylegan_owndata_6.png" alt=""></p>
<p>上图展示了使用阶段技巧的。10张随机面部，$\omega$区间为$[1,0.7,0.5,0.25,-0.25,-0.5,-1]$展示了在多样性/质量/平均脸之间的妥协。</p>
<h2 id="6-视频"><a href="#6-视频" class="headerlink" title="6 视频"></a>6 视频</h2><h3 id="6-1-训练剪辑"><a href="#6-1-训练剪辑" class="headerlink" title="6.1 训练剪辑"></a>6.1 训练剪辑</h3><p>最简单的样本时在训练过程中产生的中间结果，训练过程中由于分辨率递增和更精细细节的生成，样本尺寸也会增加，最后视频可能会很大(动漫人脸大概会有14MB)，所以有必要做一些压缩。使用工具<code>pngnq+advpng</code>或者将它们转成JPG格式(图像质量会降低)，在PNG图像上使用FFmpeg将训练过程中的图像转成视频剪辑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat $(ls .&#x2F;results&#x2F;*faces*&#x2F;fakes*.png | sort --numeric-sort) | ffmpeg -framerate 10 \ # show 10 inputs per second</span><br><span class="line">    -i - # stdin</span><br><span class="line">    -r 25 # output frame-rate; frames will be duplicated to pad out to 25FPS</span><br><span class="line">    -c:v libx264 # x264 for compatibility</span><br><span class="line">    -pix_fmt yuv420p # force ffmpeg to use a standard colorspace - otherwise PNG colorspace is kept, breaking browsers (!)</span><br><span class="line">    -crf 33 # adequate high quality</span><br><span class="line">    -vf &quot;scale&#x3D;iw&#x2F;2:ih&#x2F;2&quot; \ # shrink the image by 2x, the full detail is not necessary &amp; saves space</span><br><span class="line">    -preset veryslow -tune animation \ # aim for smallest binary possible with animation-tuned settings</span><br><span class="line">    .&#x2F;stylegan-facestraining.mp4</span><br></pre></td></tr></table></figure>

<h3 id="6-2-差值"><a href="#6-2-差值" class="headerlink" title="6.2 差值"></a>6.2 差值</h3><p>原始的ProGAN仓库代码提供了配置文件来生成差值视频的，但是在StyleGAN中被移除了，<a href="https://colab.research.google.com/gist/kikko/d48c1871206fc325fa6f7372cf58db87/stylegan-experiments.ipynb" target="_blank" rel="noopener">Cyril Diagne的替代实现</a>(已经没法打开了)提供了三种视频</p>
<ol>
<li><p><code>random_grid_404.mp4</code>:标准差值视频，在latent空间中简单的随机游走。修改这些所有变量变量并做成动画，默认会作出$2\times 2$一共4个视频。几个差值视频可以从<a href="https://www.gwern.net/Faces#examples" target="_blank" rel="noopener">这里</a>看到 </p>
</li>
<li><p><code>interpolate.mp4</code>:粗糙的风格混合视频。生成单一的<code>源</code>面部图，一个二流的差值视频，在生成之前在latent空间中随机游走，每个随机步，其<code>粗糙(coarse)/高级(high-level)风格</code>噪音都会从随机步复制到<code>源</code>面部风格噪音数据中。对于面部来说，<code>源</code>面部会被各式各样地修改，比如方向、面部表情，但是基本面部可以被识别。</p>
</li>
</ol>
<p>下面是<code>video.py</code>代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line">import scipy</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    tflib.init_tf()</span><br><span class="line"></span><br><span class="line">    # Load pre-trained network.</span><br><span class="line">    # url &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ&#39;</span><br><span class="line">    # with dnnlib.util.open_url(url, cache_dir&#x3D;config.cache_dir) as f:</span><br><span class="line">    ## NOTE: insert model here:</span><br><span class="line">    _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02047-sgan-faces-2gpu&#x2F;network-snapshot-013221.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">    # _G &#x3D; Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.</span><br><span class="line">    # _D &#x3D; Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.</span><br><span class="line">    # Gs &#x3D; Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.</span><br><span class="line"></span><br><span class="line">    grid_size &#x3D; [2,2]</span><br><span class="line">    image_shrink &#x3D; 1</span><br><span class="line">    image_zoom &#x3D; 1</span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line">    random_seed &#x3D; 404</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;random_grid_%s.mp4&#39; % random_seed</span><br><span class="line">    minibatch_size &#x3D; 8</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line">    # Generate latent vectors</span><br><span class="line">    shape &#x3D; [num_frames, np.prod(grid_size)] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    all_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    import scipy</span><br><span class="line">    all_latents &#x3D; scipy.ndimage.gaussian_filter(all_latents, [smoothing_sec * mp4_fps] + [0] * len(Gs.input_shape), mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    all_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(all_latents)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def create_image_grid(images, grid_size&#x3D;None):</span><br><span class="line">        assert images.ndim &#x3D;&#x3D; 3 or images.ndim &#x3D;&#x3D; 4</span><br><span class="line">        num, img_h, img_w, channels &#x3D; images.shape</span><br><span class="line"></span><br><span class="line">        if grid_size is not None:</span><br><span class="line">            grid_w, grid_h &#x3D; tuple(grid_size)</span><br><span class="line">        else:</span><br><span class="line">            grid_w &#x3D; max(int(np.ceil(np.sqrt(num))), 1)</span><br><span class="line">            grid_h &#x3D; max((num - 1) &#x2F;&#x2F; grid_w + 1, 1)</span><br><span class="line"></span><br><span class="line">        grid &#x3D; np.zeros([grid_h * img_h, grid_w * img_w, channels], dtype&#x3D;images.dtype)</span><br><span class="line">        for idx in range(num):</span><br><span class="line">            x &#x3D; (idx % grid_w) * img_w</span><br><span class="line">            y &#x3D; (idx &#x2F;&#x2F; grid_w) * img_h</span><br><span class="line">            grid[y : y + img_h, x : x + img_w] &#x3D; images[idx]</span><br><span class="line">        return grid</span><br><span class="line"></span><br><span class="line">    # Frame generation func for moviepy.</span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        latents &#x3D; all_latents[frame_idx]</span><br><span class="line">        fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">        images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;0.7,</span><br><span class="line">                              randomize_noise&#x3D;False, output_transform&#x3D;fmt)</span><br><span class="line"></span><br><span class="line">        grid &#x3D; create_image_grid(images, grid_size)</span><br><span class="line">        if image_zoom &gt; 1:</span><br><span class="line">            grid &#x3D; scipy.ndimage.zoom(grid, [image_zoom, image_zoom, 1], order&#x3D;0)</span><br><span class="line">        if grid.shape[2] &#x3D;&#x3D; 1:</span><br><span class="line">            grid &#x3D; grid.repeat(3, 2) # grayscale &#x3D;&gt; RGB</span><br><span class="line">        return grid</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">    # import scipy</span><br><span class="line">    # coarse</span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_seed &#x3D; 500</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w &#x3D; 512</span><br><span class="line">    h &#x3D; 512</span><br><span class="line">    #src_seeds &#x3D; [601]</span><br><span class="line">    dst_seeds &#x3D; [700]</span><br><span class="line">    style_ranges &#x3D; ([0] * 7 + [range(8,16)]) * len(dst_seeds)</span><br><span class="line"></span><br><span class="line">    fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">    synthesis_kwargs &#x3D; dict(output_transform&#x3D;fmt, truncation_psi&#x3D;0.7, minibatch_size&#x3D;8)</span><br><span class="line"></span><br><span class="line">    shape &#x3D; [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    src_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    src_latents &#x3D; scipy.ndimage.gaussian_filter(src_latents,</span><br><span class="line">                                                smoothing_sec * mp4_fps,</span><br><span class="line">                                                mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    src_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(src_latents)))</span><br><span class="line"></span><br><span class="line">    dst_latents &#x3D; np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in dst_seeds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    src_dlatents &#x3D; Gs.components.mapping.run(src_latents, None) # [seed, layer, component]</span><br><span class="line">    dst_dlatents &#x3D; Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]</span><br><span class="line">    src_images &#x3D; Gs.components.synthesis.run(src_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">    dst_images &#x3D; Gs.components.synthesis.run(dst_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    canvas &#x3D; PIL.Image.new(&#39;RGB&#39;, (w * (len(dst_seeds) + 1), h * 2), &#39;white&#39;)</span><br><span class="line"></span><br><span class="line">    for col, dst_image in enumerate(list(dst_images)):</span><br><span class="line">        canvas.paste(PIL.Image.fromarray(dst_image, &#39;RGB&#39;), ((col + 1) * h, 0))</span><br><span class="line"></span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        src_image &#x3D; src_images[frame_idx]</span><br><span class="line">        canvas.paste(PIL.Image.fromarray(src_image, &#39;RGB&#39;), (0, h))</span><br><span class="line"></span><br><span class="line">        for col, dst_image in enumerate(list(dst_images)):</span><br><span class="line">            col_dlatents &#x3D; np.stack([dst_dlatents[col]])</span><br><span class="line">            col_dlatents[:, style_ranges[col]] &#x3D; src_dlatents[frame_idx, style_ranges[col]]</span><br><span class="line">            col_images &#x3D; Gs.components.synthesis.run(col_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">            for row, image in enumerate(list(col_images)):</span><br><span class="line">                canvas.paste(PIL.Image.fromarray(image, &#39;RGB&#39;), ((col + 1) * h, (row + 1) * w))</span><br><span class="line">        return np.array(canvas)</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;interpolate.mp4&#39;</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line"></span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">    import scipy</span><br><span class="line"></span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_seed &#x3D; 503</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w &#x3D; 512</span><br><span class="line">    h &#x3D; 512</span><br><span class="line">    style_ranges &#x3D; [range(6,16)]</span><br><span class="line"></span><br><span class="line">    fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">    synthesis_kwargs &#x3D; dict(output_transform&#x3D;fmt, truncation_psi&#x3D;0.7, minibatch_size&#x3D;8)</span><br><span class="line"></span><br><span class="line">    shape &#x3D; [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    src_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    src_latents &#x3D; scipy.ndimage.gaussian_filter(src_latents,</span><br><span class="line">                                                smoothing_sec * mp4_fps,</span><br><span class="line">                                                mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    src_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(src_latents)))</span><br><span class="line"></span><br><span class="line">    dst_latents &#x3D; np.stack([random_state.randn(Gs.input_shape[1])])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    src_dlatents &#x3D; Gs.components.mapping.run(src_latents, None) # [seed, layer, component]</span><br><span class="line">    dst_dlatents &#x3D; Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        col_dlatents &#x3D; np.stack([dst_dlatents[0]])</span><br><span class="line">        col_dlatents[:, style_ranges[0]] &#x3D; src_dlatents[frame_idx, style_ranges[0]]</span><br><span class="line">        col_images &#x3D; Gs.components.synthesis.run(col_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">        return col_images[0]</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;fine_%s.mp4&#39; % (random_seed)</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line"></span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><code>fine_503.mp4</code>：一个精细风格混合视频。</li>
</ol>
<h2 id="7-模型"><a href="#7-模型" class="headerlink" title="7 模型"></a>7 模型</h2><h3 id="7-1-动漫人脸"><a href="#7-1-动漫人脸" class="headerlink" title="7.1  动漫人脸"></a>7.1  动漫人脸</h3><p>训练的基准模型的数据来源是上面的数据预处理和训练阶段介绍过。是一个在218794张动漫人脸上，使用512像素的StyleGAN训练出来的，数据时所有Danboru2017数据集上裁剪的，清洗、上采样，并训练了21980次迭代，38个GPU天。</p>
<p>下载（推荐使用最近的<a href="https://www.gwern.net/Faces#portrait-results" target="_blank" rel="noopener">portrait StyleGAN</a>,除非需要特别剪切的脸部）</p>
<ul>
<li><p><a href="https://mega.nz/#!2DRDQIjJ!JKQ_DhEXCzeYJXjliUSWRvE-_rfrvWv_cq3pgRuFadw" target="_blank" rel="noopener">随机样本</a> 在2019年2月14日随机生成的，使用了一个极大的$\omega=1.2$(165MB,JPG)</p>
</li>
<li><p><a href="https://mega.nz/#!aPRFDKaC!FDpQi_FEPK443JoRBEOEDOmlLmJSblKFlqZ1A1XPt2Y" target="_blank" rel="noopener">StyleGAN 模型 This Waifu Does Not Exist</a>(294MBm<code>.pkl</code>)</p>
</li>
<li><p><a href="https://mega.nz/#!vawjXISI!F7s13yRicxDA3QYqYDL2kjnc2K7Zk3DwCIYETREmBP4" target="_blank" rel="noopener">动漫人脸StyleGAN模型</a>最近训练的。</p>
</li>
</ul>
<h2 id="8-迁移学习"><a href="#8-迁移学习" class="headerlink" title="8 迁移学习"></a>8 迁移学习</h2><p>特定的动漫人脸模型迁移学习到特定角色是很简单的：角色的图像太少，无法训练一个好的StyleGAN模型，同样的，采样不充分的StyleGAN的数据增强也不行，但是由于StyleGAN在所有类型的动漫人脸训练得到，StyleGAN学习到足够充分的特征空间，可以轻易地拟合到特定角色而不会出现过拟合。</p>
<p>制作特定脸部模型时，图像数量越多越好，但是一般n=500-5000足矣，甚至n=50都可以。论文中的结论</p>
<p><strong>尽管StyleGAN的 generator是在人脸数据集上训练得到的，但是其embeding算法足以表征更大的空间。论文中的图表示，虽然比不上生成人脸的效果，但是依然能获得不错的高质量的猫、狗甚至油画和车辆的表征</strong>如果说连如此不同的车辆都可以被成功编码进人脸的StyleGAN，那么很显然latent空间可以轻易地对一个新的人脸建模。因此，我们可以判断训练过程可能与学习新面孔不太相关，这样任务就简单许多。</p>
<p>由于StyleGAN目前是非条件生成网络也没有在限定领域文本或元数据上编码，只使用了海量图片，所有需要做的就是将新数据集编码，然后简单地在已有模型基础上开始训练就可以了。</p>
<ol>
<li>准备新数据集</li>
<li>编辑<code>train.py</code>,给<code>-desc</code>行重新赋值</li>
<li>正确地给<code>resume_kimg</code>赋值，<code>resume_run_id=&quot;latest&quot;</code></li>
<li>开始运行<code>python train.py</code>，就可以迁移学习了</li>
</ol>
<p>主要问题是，没法从头开始(第0次迭代)，我尝试过这么做，但是效果不好并且StyleGAN看起来可能直接忽视了预训练模型。我个人假设是，作为ProGAN的一部分，在额外的分辨率或网络层上增长或消退，StyleGAN简单的随机或擦除新的网络层并覆盖它们，这使得这么做没有意义。这很好避免，简单地跳过训练进程，直接到期望的分辨率。例如，开始一个512像素的数据集训练时，可以在<code>training_loop.py</code>中设置<code>resume_king=7000</code>。这会强行让StyleGAN跳过所有的progressing growing步骤，并载入全部的模型。如何校验呢？检查第一幅吐下你给(<code>fakes07000.png</code>或者其他的)，从之前的任何的迁移学习训练完成，它应当看起来像是原始模型在训练结束时的效果。接下来的训练样本应该表现出原始图像快速适应(变形到)新数据集（应该不会出现类似<code>fakes0000.png</code>的图像，因为这表明是从头开始训练）</p>
<h3 id="8-1-动漫人脸模型迁移到特定角色人脸"><a href="#8-1-动漫人脸模型迁移到特定角色人脸" class="headerlink" title="8.1 动漫人脸模型迁移到特定角色人脸"></a>8.1 动漫人脸模型迁移到特定角色人脸</h3><p>第一个迁移的角色是 Holo，使用了从Danboru2017的数据集中筛选出来的Holo面部图像，使用<code>waifu2x</code>缩放到512像素，手工清理，并做数据增强，从3900张增强到12600张图像，同时使用了镜像翻转，因为Holo面部是对称的。使用的预训练模型是2019年2月9号的一个动漫人脸模型，尚未完全收敛。</p>
<p>值得一提的是，这个数据集之前用ProGAN来训练的，但是几周的训练之后，ProGAN严重过拟合，并产生崩坏。<br>训练过程相当快，只有几百次迭代之后就可以看到肉眼可见的Holo的脸部图了。</p>
<p>StyleGAN要成功得多，尽管有几个失败的点出现在动漫人脸上。事实上，几百次迭代之后，它开始过拟合这些裂缝/伪影/脏点。最终使用的是迭代次数为11370的模型，而且依然有些过拟合。我个人认为总数n(数据增强之后)，Holo应该训练训练更长时间(FFHQ数据集的1/7)，但是显然不是。可能数据增强并没有太大价值，又或者要么多样性编码并没那么有用，要么这些操作有用，但是StyleGAN已经从之前的训练中学习到，并且需要更多真实数据来理解Holo的面部。</p>
<p>11370次迭代的<a href="https://mega.nz/#!afIjAAoJ!ATuVaw-9k5I5cL_URTuK2zI9mybdgFGYMJKUUHUfbk8" target="_blank" rel="noopener">模型下载</a></p>
<h3 id="8-2-动漫人脸迁移到FFHQ人脸"><a href="#8-2-动漫人脸迁移到FFHQ人脸" class="headerlink" title="8.2 动漫人脸迁移到FFHQ人脸"></a>8.2 动漫人脸迁移到FFHQ人脸</h3><p>如果StyleGAN可以平滑地表征动漫人脸，并使用参数$\omega$承载了全局的如头发长度+颜色属性转换，参数$\omega$可能一种快速的方式来空值单一角色的大尺度变化。例如，性别变换，或者动漫到真人的变换？（给定图像/latent向量，可以简单地改变正负号来将其变成相反的属性，这可以每个随机脸相反的版本，而且如果有人有编码器，就可以自动地转换了）。</p>
<p>数据来源：可以方便的使用FFHQ下载脚本，然后将图像下采样到512像素，甚至构建一个FFHQ+动漫头像的数据集。<br>最快最先要做的是，从动漫人脸到FFHQ真人脸的迁移学习。可能模型无法得到足够的动漫知识，然后去拟合，但是值得一试。早期的训练结果如下，有点像僵尸</p>
<p><img src="/images/blog/stylegan_owndata_7.png" alt=""></p>
<p>97次迭代(ticks)之后，模型收敛到一个正常的面孔，唯一可能保留的线索是一些训练样本中的过度美化的发型。</p>
<p><img src="/images/blog/stylegan_owndata_8.png" alt=""></p>
<h3 id="8-3-动漫脸–-gt-动漫脸-FFHQ脸"><a href="#8-3-动漫脸–-gt-动漫脸-FFHQ脸" class="headerlink" title="8.3 动漫脸–&gt;动漫脸+FFHQ脸"></a>8.3 动漫脸–&gt;动漫脸+FFHQ脸</h3><p>下一步是同时训练动漫脸和FFHQ脸模型，尽管开始时数据集的鲜明的不同，将会是正的VS负的$\omega$最终导致划分为真实VS动漫，并提供一个便宜并且简单的方法来转换任意脸部图像。</p>
<p>简单的合并512像素的FFHQ脸部图像和521像素的动漫脸部，并从之前的FFHQ模型基础上训练（我怀疑，一些动漫图像数据仍然在模型中，因此这将会比从原始的动漫脸部模型中训练要快一点）。我训练了812次迭代，11359-12171张图像，超过2个GPU天。</p>
<p>它确实能够较好地学习两种类型的面孔，清晰地分离样本如下</p>
<p><img src="/images/blog/stylegan_owndata_9.png" alt=""></p>
<p>但是，迁移学习和$\omega$采样的结果是不如意的，修改不同领域的风格混合，或者不同领域之间的转换的能力有限。截断技巧无法清晰地解耦期望的特征（事实上，多种$\omega$ 没法清晰对应什么）。</p>
<p><img src="/images/blog/stylegan_owndata_10.png" alt=""></p>
<p>StyleGAN的动漫+FFHQ的风格混合结果。</p>
<h2 id="9-逆转StyleGAN来控制和修改图像"><a href="#9-逆转StyleGAN来控制和修改图像" class="headerlink" title="9 逆转StyleGAN来控制和修改图像"></a>9 逆转StyleGAN来控制和修改图像</h2><p>一个非条件GAN架构，默认是单向的：latent向量z从众多$N(0,1)$变量中随机生成得到的，喂入GAN，并输出图像。没有办法让非条件GAN逆向，即喂入图像输出其latent。</p>
<p>最直接的方法是转向条件GAN架构，基于文本或者标签embeding。然后生成特定特征，戴眼镜，微笑。当前无法操作，因为生成一个带标签或者embedding并且训练的StyleGAN需要的不是一点半点的修改。这也不是一个完整的解决方案，因为它无法在现存的图像进行编辑。</p>
<p>对于非条件GAN，有两种实现方式来逆转G。</p>
<ol>
<li>神经网络可以做什么，另外一个神经网络就可以学到逆操作。<a href="https://arxiv.org/abs/1907.02544" target="_blank" rel="noopener">Donahue 2016</a>,<a href="https://arxiv.org/abs/1907.02544" target="_blank" rel="noopener">Donahue Simonyan 2019</a>.如果StyleGAN学习到了$z$到图像的映射，那么训练第二个神经网络来监督学习从图像到$z$的映射，</li>
</ol>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-05-12-edit-stylegan-humanface/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-05-12-edit-stylegan-humanface/" title="使用StyleGAN训练自己的数据集.md">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-04-28-cv-switch-face/">
    		使用传统方法换脸算法
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.722Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>参考 <a href="https://matthewearl.github.io/2015/07/28/switching-eds-with-python/" target="_blank" rel="noopener">switch face with python</a></p>
<h3 id="1-使用dlib抽取面部关键点"><a href="#1-使用dlib抽取面部关键点" class="headerlink" title="1 使用dlib抽取面部关键点"></a>1 使用dlib抽取面部关键点</h3><p><img src="/images/blog/cv_switch_face_1.png" alt=""></p>
<p>关键代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">PREDICTOR_PATH &#x3D; &quot;&#x2F;home&#x2F;matt&#x2F;dlib-18.16&#x2F;shape_predictor_68_face_landmarks.dat&quot;</span><br><span class="line"></span><br><span class="line">detector &#x3D; dlib.get_frontal_face_detector()</span><br><span class="line">predictor &#x3D; dlib.shape_predictor(PREDICTOR_PATH)</span><br><span class="line"></span><br><span class="line">def get_landmarks(im):</span><br><span class="line">    rects &#x3D; detector(im, 1)</span><br><span class="line">    </span><br><span class="line">    if len(rects) &gt; 1:</span><br><span class="line">        raise TooManyFaces</span><br><span class="line">    if len(rects) &#x3D;&#x3D; 0:</span><br><span class="line">        raise NoFaces</span><br><span class="line"></span><br><span class="line">    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])</span><br></pre></td></tr></table></figure>

<p>特征抽取器<code>predictor</code>传入一个矩形的人脸部分，预测内部的人脸的68个关键点坐标，即$68\times 2$个值。</p>
<h3 id="2-使用procrustes分析进行人脸对齐"><a href="#2-使用procrustes分析进行人脸对齐" class="headerlink" title="2 使用procrustes分析进行人脸对齐"></a>2 使用procrustes分析进行人脸对齐</h3><p>检测两张人脸的关键点之后，每个点的特定属性我们是知道的，比如第30个点代表的是鼻尖的坐标。我们接下来要做的是，如何扭曲、转换、以及缩放第一个人脸的点，使得它与目标关键点尽可能接近。这个相同的转换步骤可以用，第二个人脸图像来覆盖第一个人脸图像来实现。</p>
<p>数学形式的解法为，我们寻找$T,s,R$最小化下面的等式:</p>
<p>$$<br>\sum _{i=1} ^{68}|sRp_i ^T+T-q_i ^T|^2<br>$$</p>
<p>其中$R$是一个$2\times 2$的正交矩阵，$s$是个标量，$T$是一个2向量，$p_ihe q_i$是上面计算得到的68个关键点。此问题等价于求解一个<strong>正交procrustes分析</strong>问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def transformation_from_points(points1, points2):</span><br><span class="line">    points1 &#x3D; points1.astype(numpy.float64)</span><br><span class="line">    points2 &#x3D; points2.astype(numpy.float64)</span><br><span class="line"></span><br><span class="line">    c1 &#x3D; numpy.mean(points1, axis&#x3D;0)</span><br><span class="line">    c2 &#x3D; numpy.mean(points2, axis&#x3D;0)</span><br><span class="line">    points1 -&#x3D; c1</span><br><span class="line">    points2 -&#x3D; c2</span><br><span class="line"></span><br><span class="line">    s1 &#x3D; numpy.std(points1)</span><br><span class="line">    s2 &#x3D; numpy.std(points2)</span><br><span class="line">    points1 &#x2F;&#x3D; s1</span><br><span class="line">    points2 &#x2F;&#x3D; s2</span><br><span class="line"></span><br><span class="line">    U, S, Vt &#x3D; numpy.linalg.svd(points1.T * points2)</span><br><span class="line">    R &#x3D; (U * Vt).T</span><br><span class="line"></span><br><span class="line">    return numpy.vstack([numpy.hstack(((s2 &#x2F; s1) * R,</span><br><span class="line">                                       c2.T - (s2 &#x2F; s1) * R * c1.T)),</span><br><span class="line">                         numpy.matrix([0., 0., 1.])])</span><br></pre></td></tr></table></figure>

<p>以上代码执行了如下步骤</p>
<ol>
<li>将所有输入转换为浮点型，便于后续的计算</li>
<li>减去每个点集合的中心坐标(即减去均值)。一旦结果点集合的最优变换和扭曲解找到，中心的<code>c1</code>和<code>c2</code>可以用来求解全局解。</li>
<li>类似的，每个点除以标准差。消除尺度影响</li>
<li>使用SVD计算扭曲比率，需要去查看<strong>正交Procrustes问题</strong>的求解过程才能了解。</li>
<li>返回完整的转换为放射变换矩阵。</li>
</ol>
<p>结果可以用Opencv的<code>cv2.wrapAffine</code>函数来映射第二张图到第一张图。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def warp_im(im, M, dshape):</span><br><span class="line">    output_im &#x3D; numpy.zeros(dshape, dtype&#x3D;im.dtype)</span><br><span class="line">    cv2.warpAffine(im,</span><br><span class="line">                   M[:2],</span><br><span class="line">                   (dshape[1], dshape[0]),</span><br><span class="line">                   dst&#x3D;output_im,</span><br><span class="line">                   borderMode&#x3D;cv2.BORDER_TRANSPARENT,</span><br><span class="line">                   flags&#x3D;cv2.WARP_INVERSE_MAP)</span><br><span class="line">    return output_im</span><br></pre></td></tr></table></figure>
<p>其实就是为了让两张图的点位能对得上，将某张图进行旋转，缩放，使得两张图的人脸的关键点能处于相同的坐标位置。</p>
<p><img src="/images/blog/cv_switch_face_2.gif" alt=""></p>
<h3 id="3-目标图的轮廓纠正"><a href="#3-目标图的轮廓纠正" class="headerlink" title="3 目标图的轮廓纠正"></a>3 目标图的轮廓纠正</h3><p><img src="/images/blog/cv_switch_face_3.png" alt=""></p>
<p>Non colour-corrected overlay<br>接下来需要解决的问题是，两张图像的不同肤色和光照差异，会导致连接处边缘的突兀。下面的方法是尝试纠正这个问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">COLOUR_CORRECT_BLUR_FRAC &#x3D; 0.6</span><br><span class="line">LEFT_EYE_POINTS &#x3D; list(range(42, 48))</span><br><span class="line">RIGHT_EYE_POINTS &#x3D; list(range(36, 42))</span><br><span class="line"></span><br><span class="line">def correct_colours(im1, im2, landmarks1):</span><br><span class="line">    blur_amount &#x3D; COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(</span><br><span class="line">                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis&#x3D;0) -</span><br><span class="line">                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis&#x3D;0))</span><br><span class="line">    blur_amount &#x3D; int(blur_amount)</span><br><span class="line">    if blur_amount % 2 &#x3D;&#x3D; 0:</span><br><span class="line">        blur_amount +&#x3D; 1</span><br><span class="line">    im1_blur &#x3D; cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)</span><br><span class="line">    im2_blur &#x3D; cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)</span><br><span class="line"></span><br><span class="line">    # Avoid divide-by-zero errors.</span><br><span class="line">    im2_blur +&#x3D; 128 * (im2_blur &lt;&#x3D; 1.0)</span><br><span class="line"></span><br><span class="line">    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) &#x2F;</span><br><span class="line">                                                im2_blur.astype(numpy.float64))</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/cv_switch_face_4.png" alt=""></p>
<p>此方法尝试改变第二张图像的轮廓去匹配第一张图的。做法是：<strong>第二张图除以其高斯模糊，然后乘以第一张图的高斯模糊</strong>。此算法源于<a href="https://en.wikipedia.org/wiki/Color_balance#Scaling_monitor_R.2C_G.2C_and_B" target="_blank" rel="noopener">RGB的缩放轮廓纠正</a>,但是对所有图像使用了一个常量的缩放因子，每个像素有其局部缩放因子。</p>
<p>由此方法，两张图像的光照差异可以在某种程度上累加。例如，如果第一张图某一边在发光二第二张图有均衡的光照，那么轮廓纠正之后图二会出现出现某些暗处。也就是说这是个比较粗暴的方案，合适大小的高斯核是问题的关键。太小的话会导致图一种某些面部特征会出现在图二中，太大的话会导致核外面某些像素重叠，并出现变色。此处使用的是$0.6\times 瞳孔距离$。</p>
<h3 id="4-将图二特征渲染回图一"><a href="#4-将图二特征渲染回图一" class="headerlink" title="4 将图二特征渲染回图一"></a>4 将图二特征渲染回图一</h3><p>使用一个mask从图二中抽取部分，渲染到图一中。</p>
<p><img src="/images/blog/cv_switch_face_5.png" alt=""></p>
<ul>
<li>值为1的区域(上图中的白色区域)对应的是图二中人脸特征选取的部分</li>
<li>值为0的区域(上图中黑色区域)对应的是图一该出现的部分。0到1之间的区域是两张图的混合。</li>
</ul>
<p>下面代码是生成如上区域的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">LEFT_EYE_POINTS &#x3D; list(range(42, 48))</span><br><span class="line">RIGHT_EYE_POINTS &#x3D; list(range(36, 42))</span><br><span class="line">LEFT_BROW_POINTS &#x3D; list(range(22, 27))</span><br><span class="line">RIGHT_BROW_POINTS &#x3D; list(range(17, 22))</span><br><span class="line">NOSE_POINTS &#x3D; list(range(27, 35))</span><br><span class="line">MOUTH_POINTS &#x3D; list(range(48, 61))</span><br><span class="line">OVERLAY_POINTS &#x3D; [</span><br><span class="line">    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,</span><br><span class="line">    NOSE_POINTS + MOUTH_POINTS,</span><br><span class="line">]</span><br><span class="line">FEATHER_AMOUNT &#x3D; 11</span><br><span class="line"></span><br><span class="line">def draw_convex_hull(im, points, color):</span><br><span class="line">    points &#x3D; cv2.convexHull(points)</span><br><span class="line">    cv2.fillConvexPoly(im, points, color&#x3D;color)</span><br><span class="line"></span><br><span class="line">def get_face_mask(im, landmarks):</span><br><span class="line">    im &#x3D; numpy.zeros(im.shape[:2], dtype&#x3D;numpy.float64)</span><br><span class="line"></span><br><span class="line">    for group in OVERLAY_POINTS:</span><br><span class="line">        draw_convex_hull(im,</span><br><span class="line">                         landmarks[group],</span><br><span class="line">                         color&#x3D;1)</span><br><span class="line"></span><br><span class="line">    im &#x3D; numpy.array([im, im, im]).transpose((1, 2, 0))</span><br><span class="line"></span><br><span class="line">    im &#x3D; (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) &gt; 0) * 1.0</span><br><span class="line">    im &#x3D; cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)</span><br><span class="line"></span><br><span class="line">    return im</span><br><span class="line"></span><br><span class="line">mask &#x3D; get_face_mask(im2, landmarks2)</span><br><span class="line">warped_mask &#x3D; warp_im(mask, M, im1.shape)</span><br><span class="line">combined_mask &#x3D; numpy.max([get_face_mask(im1, landmarks1), warped_mask],</span><br><span class="line">                          axis&#x3D;0)</span><br></pre></td></tr></table></figure>

<p>上面代码分解如下</p>
<ol>
<li><p><code>get_face_mask()</code>方法是用来定义生成图像的一个mask和一个面部关键点的矩阵。它画了两个白色凸多边形:一个环绕着眼睛区域，一个环绕着鼻子和嘴巴区域。接着它会羽化mask的边缘11个像素。羽化有助于隐藏遗留的颜色不连续问题</p>
</li>
<li><p>此类面部的mask会给两张图都生成。图二的mask会被转换进图一的坐标空间，使用步骤2相同的转换。</p>
</li>
<li><p>此mask接下来会逐像素取最大值。结合两个mask可以保证图一中的特征会被覆盖，同时图二中的特征也会被展示。</p>
</li>
</ol>
<p>最后，mask用于得到最终的合成结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output_im &#x3D; im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/cv_switch_face_6.png" alt=""></p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-04-28-cv-switch-face/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-04-28-cv-switch-face/" title="使用传统方法换脸算法">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-04-22-Mask%20Embedding%20in%20conditional%20GAN/">
    		Mask Embedding in conditional GAN for Guided Synthesis of High Resolution Images
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.720Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p><a href="https://arxiv.org/pdf/1907.01710.pdf" target="_blank" rel="noopener">论文来源</a></p>
<h2 id="1-引入"><a href="#1-引入" class="headerlink" title="1 引入"></a>1 引入</h2><p>目前大多数从语义分割mask生成照片的方法要么是使用从粗糙到精细的级联网络，要么是设计特定的损失函数来增加模型稳定性。目前，使用语义分割mask为指引合成包含丰富局部特征高分辨率的图像依然有难度。</p>
<p>本文就提出了一种关联提供的语义分割mask，同时保留了丰富的局部细节的合成图像方法。</p>
<p>图像翻译模型如Pix2Pix使用Unet风格的生成器直接映射图像的抽象表征，但是没有合理的机制来随机特征的实现。这通常会导致模型的输出是特征空间不同分辨率之间的模棱两可的特征，如下图。通常这会导致模糊的图像和糟糕的纹理细节。解决方案之一是Pix2PixHD论文中，使用了一个粗糙到精细的方法，以及风格损失函数来修正图像的输出质量。但是这个方法需要很大的模型，但仍然没解决特征映射的基础问题。一个更加合理的解决方案是Tub-GAN，使用了latent向量z(选中的分布的噪音)和一个语义分割的mask作为条件输入，允许模型学习其联合分布。虽然他们提出了一个融合策略:投影latent特征以及投影mask特征并不能保证是天然关联的，因而此模型也是受限的仅仅能生成背景空间的低频信息。</p>
<p><img src="/images/blog/mask_embedding_gan_1.jpg" alt=""></p>
<p>图说明：生成的样本图像和卡通示例了：训练期间一个mask引导的面部生成会遇到的采样空间映射的挑战：图像翻译模型会被训练着映射相同的圆形模式到各种各样的球形模式上。<strong>模可能学习到了训练数据集的平均的篮球模式而不能完整的独立地映射它们。相同的问题存在于训练一个生成器使用相似的mask表征来复制完全不同的脸</strong>。</p>
<p>本文提出了当前可用的以mask为指引像素级语义输入的生成模型的两种主要问题</p>
<ol>
<li>合成的结果中的精细的纹理细节缺乏多样性，源于mask到图像域的不充分的映射。</li>
<li>当前的多条件输入架构设计中的低参数效率</li>
</ol>
<p>对于第一个问题，我们认为带语义分割map输入的多个latent向量会取得更好的样本空间映射，这也会使得合成结果里纹理细节的多样性。对于第二个问题，我们的解决方案是<strong>在初始的特征投影之前将mask embeding反射回latent向量输入中，此操作显著提升了合成结果中的纹理细节</strong>。将mask embeding 向量和latent向量结合起来是一个有效的方法来添加mask限制，因为它允许初始的特征投影可匹敌于像素级别mask限制。与Tube-GAN相反，我们使用的是投影的mask特征作为latent特征的主要限制，使得网络的上采样路径能够保留其大部分能力来实现精细的局部纹理特征。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><h3 id="2-1-Conditional-条件GAN"><a href="#2-1-Conditional-条件GAN" class="headerlink" title="2.1 (Conditional)条件GAN"></a>2.1 (Conditional)条件GAN</h3><p>条件GAN可以通过联合latent向量和条件输入来控制生成器的输出。许多研究使用了cGAN，使用的是向量形式(比如标签)的图像属性来控制图像合成。Pix2Pix和Pix2PixHD首次提出在编码器-解码器风格的图像到图像翻译的结构中使用语义分割为输入。有些研究应用输入embeding来转换高维属性，如将语义分割mask转换成压缩的低维形式。CCGAN提出使用包含图像特色的句子embeding作为特征来进行cycle-GAN训练。他们的研究表明，凝练的文本信息可以被用来与生成器latent特征融合作为条件来做图像合成。Disentangling Multiple Conditional Inputs in GANs使用binary(二值)mask embeding作为条件输入的部分来控制生成的服饰的形状。但是这个论文表明，mask embedding向量不足以完成像素级以mask为指引的受限图像合成。它们的结构中输出的形状并不总是与输入的mask一致的。</p>
<h3 id="2-2-State-of-art的Pix2Pix的风格生成器"><a href="#2-2-State-of-art的Pix2Pix的风格生成器" class="headerlink" title="2.2 State-of-art的Pix2Pix的风格生成器"></a>2.2 State-of-art的Pix2Pix的风格生成器</h3><p>Pix2Pix在图像翻译时，无法生成稳定多样的精细纹理特征。Pix2Pix-HD模型提出从粗糙到精细的级联结构，加上风格损失和多尺度的判别器。主要思想是使用额外的损失项来正则惩罚膨胀的模型能力，尤其是拼接的精修网络(refinedment networks)。尽管此模型有通过样例级别的特征embedding来随机纹理的机制，局部纹理细节的多样性依然取决于实例标签映射的镜像扰动。某种程度上来说，此机制允许随机纹理生成，但是纹理映射仅与物体的形状扰动相关。换句话说，此图像翻译模型仍然受限于一对一的映射(如上图)，由于这种原因，图像质量和多样性依然相当低。</p>
<h3 id="2-3-ProGAN-Progressive-Growing"><a href="#2-3-ProGAN-Progressive-Growing" class="headerlink" title="2.3 ProGAN(Progressive Growing )"></a>2.3 ProGAN(Progressive Growing )</h3><p>ProGAN是一种训练方法：它渐进式地给生成器和判别器增加卷积层来获得更好的稳定性和更快的收敛。此技术使得使用轻量级的修改的DCGAN来合成高清图像成为可能。</p>
<h2 id="3-生成器中的Mask-Embedding"><a href="#3-生成器中的Mask-Embedding" class="headerlink" title="3 生成器中的Mask Embedding"></a>3 生成器中的Mask Embedding</h2><p>为了控制生成器的输出的形状，mask经常被用来作为编码器-解码器形式网络的中生成器的仅有输入来增强像素级限制。此类图像翻译模型的主要定律是构建一种翻译$G(v)\rightarrow \lbrace r \rbrace$，其中一对一的翻译由输入$v$限定的。使用诸如dropout或者噪音z覆盖作为输入$v$的机制，一对多的关联$G(v,z)\rightarrow \lbrace r_1,r_2,…r_m \rbrace$ 原理上就称为可能。然而，受限于卷积操作和目标函数，Pix2Pix表明，覆盖的噪音常常会被模型忽视。模型输出严重依赖于语义分割输入的mask和dropout，使得高频纹理特征的多样性也是受限的。话句话说，给定一个局部最优的图像到图像的翻译网络$G`$，其采样形式在实际中可能变成了$G’(v,z)\rightarrow \lbrace r_1,r_2,..r_n \rbrace,其中n&lt;&lt;m$。映射的样本空间就变得非常稀疏了。如下图(不同映射机制下，可达的采样空间)所示，我们复制了不同的策略(Pix2Pix,Pix2Pix-HD)以及我们的策略，允许模型的采样空间增加到更大的子集，直至整个域空间，反过来证明了越大的生成器在多样性、分辨率和实现上更好。</p>
<p><img src="/images/blog/mask_embedding_gan_2.jpg" alt=""></p>
<h3 id="3-1-像素级的Mask限制和模型设计"><a href="#3-1-像素级的Mask限制和模型设计" class="headerlink" title="3.1 像素级的Mask限制和模型设计"></a>3.1 像素级的Mask限制和模型设计</h3><p>我们提出的生成器架构如下图所示，借用了PGAN生成器的架构，其中的生成器投影了一个latent向量到latent空间，然后接着几个上采样和卷积层来生成输出图像。为了插入语义分割信息，我们构建了一系列的mask特征并拼接二者到对应的latent特征中。这种形式的UNet架构与Pix2Pix的实现很像，但是没有Pix2Pix没有latent向量的输入。</p>
<p><img src="/images/blog/mask_embedding_gan_3.jpg" alt=""></p>
<p>然而，我们观察到此方案的初始的实现，与原始的PGAN架构相比，带有严重的质量下滑。我们将此问题看做是一个空间采样问题，即mask引入了特征投影路径上的限制，从早期层到后续层的特征映射变得越来越<strong>不依赖于</strong>带mask为输入所引入的在空间和形态学上的限制了，这也导致了模型的训练过程的不稳定和模型能力的衰退。解决方案是，实现一种机制，允许初始的latent特征映射基本与mask限制吻合。然后模型可以使用mask特征的短连接(上图中的水平的箭头)仅作为一种办法来增强像素级限制，而不必抵消过多的模型能力来完成全局的图像结构。我们通过构建一个mask embedding向量并将其嵌入到latent输入向量中来完成此操作(上图左下方)。</p>
<h3 id="3-2-公式"><a href="#3-2-公式" class="headerlink" title="3.2 公式"></a>3.2 公式</h3><p>尽管在空间采样问题中同时出现了mask限制和局部精修的纹理细节问题，此条件下上采样大部分由卷积层督导完成。mask输入并不能识别真实图像数据集中某一张图像，但是与一簇真实图片相关。因此，从数据集中收集的mask定义了一部分的真实图像的manifolds(副本)。下图中左上的椭圆说明了一个两部分的猫和狗的mask。我们也展示了由更小的椭圆设定的低分辨率特征。由一系列的卷积层链接，是一个限定接收域的局部操作，扇区是层次继承的并且每个manifold获得了类似的几何结构。我们的结构首先分两步正确地在最低分辨率的manifold上采样了一个mask限制点(1)通过mask embedding定位准确的扇区。(2)通过一个latent 特征向量在扇区内采样一个点。然后一个上采样步骤精修了细节并通过垂直的mask信息注射增强了mask限制。<strong>两部分，latent特征向量和mask embedding，是我们的方法与其他方法的根本区别</strong>。</p>
<p><img src="/images/blog/mask_embedding_gan_4.jpg" alt=""></p>
<p>图示说明：展示了使用一个狗的mask为指引的图像生成过程。左边：展示了图像生成过程中使用一系列卷积层处理的特征空间。右边：使用和未使用mask embeding的两个生成狗照片的示例。在inference时，带mask embeding的完美的模型投影基础特征到正确的manifold并使用卷积层执行合适的上采样。然而，没有mask embeding的模型学习到的是(1)只投影平均的基础图像(2)即便按照mask 限制，也只能低效地映射基础图像到狗。</p>
<p>现在着重说说latent特征向量和mask embedding。<strong>如果没有latent 特征向量</strong>，Pix2Pix或Pix2Pix-HD这类模型只有mask输入，而没有足够的随机性。因此，其生成的图像几乎完全没mask限定到唯一了。没有latent特征向量的模型生成的图像的多样性是十分有限的。相反，我们的模型有latent向量，它编码了一个大的多样性的细节。给定一个mask，我们可以生成十分不同的图像，并保有精良的细节。</p>
<p><strong>如果没有mask embeding</strong>，比如Tube-GAN,其限制在低维特征得不到重视，以及后面层中的参数可能需要纠正分辨率图像的一些错误，这限制了它在表达细节的能力。而我们的模型使用mask embedding潜在地找到了正确的扇区，big生成正确的latent空间表征。因此，所有的后续层可以更集中在生成细节上。上图就用卡通图展示了这种对比。蓝色点线，是我们模型的处理过程并从第二列到最优列生成以不同分辨率生成狗的图像。后续的网络层，卷积层和mask嵌入将图像从猫纠正到狗。不幸的是，最后的图像看起来像狗，但是分辨率太低。上图并不是真实的，但是在现实场景中我们观察到类似的场景。这些观测说明，嵌入mask embedding显著提升了特征映射效率。</p>
<h3 id="3-3-架构"><a href="#3-3-架构" class="headerlink" title="3.3 架构"></a>3.3 架构</h3><p>我们提出的模型由mask 投影路径和latent投影路径分别对应了UNet的contracting和expanding路径。输入到mask投影路径是一个二值面部边缘map。mask经过一系列blocks之后，每个block由2个卷积层，strides分别为1和2.每个block输出一个递增数目的特征到接下来的网络层，并仅连接前面的8个特征到latent投影路径来形成mask限制。</p>
<p>mask投影路径有两个主要的函数。第一个给latent投影路径上的特征上采样过程提供空间限制。第二个，输出mask embedding告知latent投影层其特征特征聚类簇，极可能与特定mask关联。为了反映事实：左边的contracting路径上的mask特征主要扮演着限制者角色，只有8个mask特征被拼接到网络的latent投影路径上。此设计基于两点</p>
<ul>
<li>更多的特征需要更大的模型能力来将它们合适的融合到投影的latent特征上</li>
<li>我们之前的实验表明一个训练过的模型会基本上将mask特征相差无几地投影到特定模式上。</li>
</ul>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4 训练"></a>4 训练</h2><p>三个模型都是用WGAN-GP的损失函数等式。Pix2Pix的baseline直接以目标分辨率训练了25个epochs。我们的带mask embedding和不带mask embedding模型都是用了渐进式progressive growing训练策略(源于pGAN)。我们从第一个输出分辨率$8\times 8$开始，训练了45k步，然后在新卷积blocks中fade加倍输入和输出分辨率。鉴于mask投影路径的轻量的权重，没有实现其fading连接。</p>
<p>为了对比mask embedding机制的效果，训练过程的超参数batch size，learing rate和每个生成器的判别器的优化数目都是一样的。是用了$batch size=256$,输出分辨率$8\times 8$。每次加倍输出分辨率的时候缩减一半的batch size。学习率初始设定为0.001，并且在模型的输出分辨率达到256时，增加到0.002.使用Tensorflow写的代码，每个模型在4块Nvidia V100上训练2天达到最终的分辨率512.</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p>我们对比了Pix2Pix baseline(23.23M)的生成器，和我们的不带embedding的baseline(23.07M)，和我们带embedding的模型，在图像合成任务中，使用Celeba-HQ数据集。没有对比Pix2Pix-HD，因为其个体实例级的特征embedding机制依赖于mask的perturbation来生成多样图像。而我们的模型在latent’投影路径上(对Pix2Pix来说是上采样路径)来说是保持相似的。我们的两个模型的baseline的判别器和模型相同都包含了23.07M个参数。生成器的性能使用切片的Wassertein 距离来衡量(SWD)。</p>
<h3 id="5-1-Celeba-HQ数据集"><a href="#5-1-Celeba-HQ数据集" class="headerlink" title="5.1 Celeba-HQ数据集"></a>5.1 Celeba-HQ数据集</h3><p>使用此数据集，我们使用python的Dlib包抽取了每个面部的68个关键点，关键点的检测是在分辨率为1024的图像上做的。<strong>面部关键点与原始标注的属性严重不符的图像被删掉，最后总共有27000张图像被选做训练集</strong>。</p>
<h3 id="5-2-量化评估"><a href="#5-2-量化评估" class="headerlink" title="5.2 量化评估"></a>5.2 量化评估</h3><p>我们使用切片的SWD距离来评估模型的效果，下表展示了之前的一些参数设置。由于内存限制，SWD在批量的每一对合成图和真实上计算平均值。玩魔兽先计算240对(真图-合成图)图像，然后重复直至覆盖了8192对。我们首先生成图像从$512\times 512$到$16\times 16$分辨率的拉普拉斯金字塔。每个层级的金字塔我们抽取128个$7\times 7$的patches，正则化然后计算与每一层的真图的平均距离。下图，SWD指标集中于我们的方法和其他方法的不同表现。</p>
<table>
<thead>
<tr>
<th>Configurations</th>
<th>512</th>
<th>256</th>
<th>128</th>
<th>64</th>
<th>32</th>
<th>16</th>
<th>avg</th>
</tr>
</thead>
<tbody><tr>
<td>Real</td>
<td>10.82</td>
<td>9.98</td>
<td>10.14</td>
<td>9.75</td>
<td>9.83</td>
<td>7.52</td>
<td>9.67</td>
</tr>
<tr>
<td>Pix2Pix</td>
<td>67.74</td>
<td>27.72</td>
<td>25.08</td>
<td>20.46</td>
<td>19.05</td>
<td>151.78</td>
<td>65.52</td>
</tr>
<tr>
<td>不带embedding</td>
<td>58.20</td>
<td>27.7722.19</td>
<td>18.25</td>
<td>17.58</td>
<td>70.49</td>
<td>35.57</td>
<td></td>
</tr>
<tr>
<td>带embedding</td>
<td>43.74</td>
<td>22.46</td>
<td>17.48</td>
<td>14.83</td>
<td>13.65</td>
<td>37.57</td>
<td>24.96</td>
</tr>
</tbody></table>
<p>可以看到使用mask embedding可以取得更好的合成图像质量。</p>
<h3 id="5-3-量化对比"><a href="#5-3-量化对比" class="headerlink" title="5.3 量化对比"></a>5.3 量化对比</h3><p><img src="/images/blog/mask_embedding_gan_5.jpg" alt=""></p>
<ul>
<li>a: 输入的mask</li>
<li>b: 使用Pix2Pix合成的图。【只能生成十分相似的粗糙的风格】</li>
<li>c: 使用我们的不带mask embedding baseline模型合成的效果【无法生成高保真的纹理特征】</li>
<li>d: 使用我们的带mask embedding模型合成的效果</li>
</ul>
<p>不带embedding的模型无法生成高稳定性的纹理特征。生成的图像包含了主要的噪音实现和上采样的人工痕迹或多或少地增加了模型能力。观测结果也符合我们的假设，即不带embedding的模型会被强制投影出示的特征到样本分布的重叠空间，这就导致了模糊的特征纹理以及模棱两可的结构。生成器能力不充分的结果便是模型会生成明显的人工痕迹，比如对角直线和棋盘纹理模式。</p>
<h3 id="5-4-改变Latent输入"><a href="#5-4-改变Latent输入" class="headerlink" title="5.4 改变Latent输入"></a>5.4 改变Latent输入</h3><p><img src="/images/blog/mask_embedding_gan_6.jpg" alt=""></p>
<ul>
<li>a: 输入的mask</li>
<li>b: 原图</li>
<li>c,d,e: 使用相同mask，但是不同latent 向量合成的图</li>
</ul>
<p>上图展示了，相同的mask输入可以与不同的latent向量并生成不同的人脸。我们也注意到latent向量和mask embedding并不完全是不关联的。latent向量更多的是侧重图像风格，即头发、皮肤、面部头发。另一方面，面部关键点标记应该是由提供的mask图决定的。不足之处是，与各种各样的不同的面部相比图像数量过少。我们观察到一些面部mask与某些特性粘合，比如性别、皮肤颜色等与我们给定的面部mask不是明显相关的。后续可以增加更多的抽象mask和对应的数据集可能可以克服这个问题。并且，实现随机模糊和mask特征的dropout可以帮助增加输出的多样性。</p>
<h2 id="5-代码逻辑"><a href="#5-代码逻辑" class="headerlink" title="5 代码逻辑"></a>5 代码逻辑</h2><p><img src="/images/blog/mask_embedding_gan_7.jpg" alt=""></p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-04-22-Mask%20Embedding%20in%20conditional%20GAN/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-04-22-Mask%20Embedding%20in%20conditional%20GAN/" title="Mask Embedding in conditional GAN for Guided Synthesis of High Resolution Images">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-04-21-image-quality-assessment/">
    		图像质量评估
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.718Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <p>参考 <a href="https://github.com/ocampor/notebooks/blob/master/notebooks/image/quality/brisque.ipynb?source=post_page-----391a6be52c11----------------------" target="_blank" rel="noopener">Image Quality Assessment</a></p>
<h2 id="1-说明"><a href="#1-说明" class="headerlink" title="1 说明"></a>1 说明</h2><p>图像质量评估(Image Quality Assessment(IQA))看起来是个非常主观的事，但是可以借鉴一些方法，目前主流方法分两类</p>
<ol>
<li>基于引用比较的评估。</li>
<li>无引用的评估</li>
</ol>
<p>主要区别是，基于引用的评估方法需要依赖一张高质量的图片作为评估源，来比较两张图之间的区别。常用的基于引用的评估方法是结构相似性索引(Structural Similarity Index(SSIM))。</p>
<h2 id="2-无引用图片质量评估"><a href="#2-无引用图片质量评估" class="headerlink" title="2 无引用图片质量评估"></a>2 无引用图片质量评估</h2><p>它不需要引用图片，仅仅依赖于接收到的图片信息，称为盲测方法。分为两步(1)计算能够描述图片结构的特征(2)计算与人类对于图片质量观点相关的特征。TID2008是一个基于方法论的数据集，它描述了如何从引用图片中评估人类的观点，被广泛用于对比IQA算法的性能。</p>
<h3 id="2-1-Blind-referenceless-image-spatial-quality-evaluator-BRISQUE"><a href="#2-1-Blind-referenceless-image-spatial-quality-evaluator-BRISQUE" class="headerlink" title="2.1 Blind/referenceless image spatial quality evaluator (BRISQUE)"></a>2.1 Blind/referenceless image spatial quality evaluator (BRISQUE)</h3><p>BRISQUE是一种仅使用图像像素来计算特征(其他方法都是基于图像转换到其他空间，比如wavelet 或者DCT)。非常高效，因为它不需要其他任何信息来计算其特征。</p>
<p>它依赖于空间域中局部正规化的亮度系数的空间自然场景统计(Spatial Natural Scene Statistics(NSS))模型，以及这些系数的点与点之间的内积的模型。</p>
<h3 id="2-2-方法论"><a href="#2-2-方法论" class="headerlink" title="2.2 方法论"></a>2.2 方法论</h3><h4 id="2-2-1-Natural-Scene-Statistics-in-the-Spatial-Domain"><a href="#2-2-1-Natural-Scene-Statistics-in-the-Spatial-Domain" class="headerlink" title="2.2.1 Natural Scene Statistics in the Spatial Domain"></a>2.2.1 Natural Scene Statistics in the Spatial Domain</h4><p>给定图像$I(i,j)$，首先通过减去局部均值$\mu (i,j)$，然后除以局部方差$\delta(i,j)$ 来计算局部亮度系数$\hat I(i,j)$。加上$C$是为了避免除以0.</p>
<p>$$<br>\hat I(i,j) = \frac{I(i,j)-\mu(i,j)}{\delta(i,j)+c}\<br>其中如果I(i,j)\in [0,255]，则C=1,如果 \in [0,1]，那么C=1/255<br>$$<br>为了计算局部归一化的亮度，即平均减去的对比度归一化(MSCN)系数，首先，我们需要计算局部均值。<br>$$<br>\mu (i,j) = \sum <em>{k=-K} ^{K} \sum _{I=-k} ^L w</em>{k,l}I_{k,l}(i,j) \<br>其中w是尺寸为(K,L)的高斯核<br>$$<br>计算代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def normalize_kernel(kernel):</span><br><span class="line">    return kernel &#x2F; np.sum(kernel)</span><br><span class="line"></span><br><span class="line">def gaussian_kernel2d(n, sigma):</span><br><span class="line">    Y, X &#x3D; np.indices((n, n)) - int(n&#x2F;2)</span><br><span class="line">    gaussian_kernel &#x3D; 1 &#x2F; (2 * np.pi * sigma ** 2) * np.exp(-(X ** 2 + Y ** 2) &#x2F; (2 * sigma ** 2)) </span><br><span class="line">    return normalize_kernel(gaussian_kernel)</span><br><span class="line"></span><br><span class="line">def local_mean(image, kernel):</span><br><span class="line">    return signal.convolve2d(image, kernel, &#39;same&#39;)</span><br></pre></td></tr></table></figure>
<p>然后计算局部偏差<br>$$<br>\sigma(i,j)\sqrt{\sum_{k=-K} ^K \sum <em>{l=-L} ^L w</em>{k,l}(I_{k,l}(i,j)-\mu(i,j))^2 }<br>$$<br>代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def local_deviation(image, local_mean, kernel):</span><br><span class="line">    &quot;Vectorized approximation of local deviation&quot;</span><br><span class="line">    sigma &#x3D; image ** 2</span><br><span class="line">    sigma &#x3D; signal.convolve2d(sigma, kernel, &#39;same&#39;)</span><br><span class="line">    return np.sqrt(np.abs(local_mean ** 2 - sigma)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算得到MSCN系数<br>$$<br>\hat I(i,j) = \frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def calculate_mscn_coefficients(image, kernel_size&#x3D;6, sigma&#x3D;7&#x2F;6):</span><br><span class="line">    C &#x3D; 1&#x2F;255</span><br><span class="line">    kernel &#x3D; gaussian_kernel2d(kernel_size, sigma&#x3D;sigma)</span><br><span class="line">    local_mean &#x3D; signal.convolve2d(image, kernel, &#39;same&#39;)</span><br><span class="line">    local_var &#x3D; local_deviation(image, local_mean, kernel)</span><br><span class="line">    </span><br><span class="line">    return (image - local_mean) &#x2F; (local_var + C)</span><br></pre></td></tr></table></figure>
<p>作者发现一个扭曲的图片的MSCN系数服从一个广义高斯分布(GGD)<br>$$<br>f(x;\alpha,\sigma ^2)=\frac{\alpha}{2\beta T(1/\alpha)}e^{-(\frac{|x|}{\beta})^{\alpha}} \<br>其中 \beta = \sigma\sqrt{\frac{T(\frac{1}{\alpha})}{T(\frac{3}{\alpha})}},T是伽马函数，\alpha的形状控制形状以及\sigma ^2的方差<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def generalized_gaussian_dist(x, alpha, sigma):</span><br><span class="line">    beta &#x3D; sigma * np.sqrt(special.gamma(1 &#x2F; alpha) &#x2F; special.gamma(3 &#x2F; alpha))</span><br><span class="line">    coefficient &#x3D; alpha &#x2F; (2 * beta() * special.gamma(1 &#x2F; alpha))</span><br><span class="line">    return coefficient * np.exp(-(np.abs(x) &#x2F; beta) ** alpha)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-相邻MSCN系数的点对内积"><a href="#2-2-2-相邻MSCN系数的点对内积" class="headerlink" title="2.2.2 相邻MSCN系数的点对内积"></a>2.2.2 相邻MSCN系数的点对内积</h4><p>邻接的系数的符号也代表了某种特定结构，可能是某种扭曲的分布。邻接MSCN系数点对内积的沿着四个方向</p>
<ol>
<li>水平方向</li>
<li>垂直方向</li>
<li>主对角(main-diagonal)D1</li>
<li>次对角(secondary-diagonal)D2</li>
</ol>
<p>$$<br>D2(i,j) = \hat I(i,j)\hat I(i+1,j+1)<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def calculate_pair_product_coefficients(mscn_coefficients):</span><br><span class="line">    return collections.OrderedDict(&#123;</span><br><span class="line">        &#39;mscn&#39;: mscn_coefficients,</span><br><span class="line">        &#39;horizontal&#39;: mscn_coefficients[:, :-1] * mscn_coefficients[:, 1:],</span><br><span class="line">        &#39;vertical&#39;: mscn_coefficients[:-1, :] * mscn_coefficients[1:, :],</span><br><span class="line">        &#39;main_diagonal&#39;: mscn_coefficients[:-1, :-1] * mscn_coefficients[1:, 1:],</span><br><span class="line">        &#39;secondary_diagonal&#39;: mscn_coefficients[1:, :-1] * mscn_coefficients[:-1, 1:]</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>广义高斯分布不能很好的拟合系数内积的经验直方图。因此，又提出了 Asymmetric Generalized Gaussian Distribution (AGGD)[非对称广义高斯分布模型](Multiscale skewed heavy-tailed model for texture analysis. Proceedings - International Conference on Image Processing)</p>
<p>$$<br>f(x;v,\sigma _l ^2,\sigma _r ^2) = \frac{v}{(\beta _l+\beta _r)T(\frac{1}{v})}e^{(-(\frac{-x}{\beta _l})^v)} \quad\quad x&lt;0 \<br>f(x;v,\sigma _l ^2,\sigma _r ^2) = \frac{v}{(\beta _l+\beta _r)T(\frac{1}{v})}e^{(-(\frac{-x}{\beta _r})^v)} \quad\quad x\gt 0 \<br>其中 \beta _{side} = \sigma _{side}\sqrt{\frac{T(\frac{1}{v})}{T(\frac{3}{v})}} ,side 可以是 r 或 l \<br>前面没有提到的参数是均值 m = (\beta _r-\beta_l)\frac{T(\frac{2}{v})}{T(frac{1}{v})}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def asymmetric_generalized_gaussian(x, nu, sigma_l, sigma_r):</span><br><span class="line">    def beta(sigma):</span><br><span class="line">        return sigma * np.sqrt(special.gamma(1 &#x2F; nu) &#x2F; special.gamma(3 &#x2F; nu))</span><br><span class="line">    </span><br><span class="line">    coefficient &#x3D; nu &#x2F; ((beta(sigma_l) + beta(sigma_r)) * special.gamma(1 &#x2F; nu))</span><br><span class="line">    f &#x3D; lambda x, sigma: coefficient * np.exp(-(x &#x2F; beta(sigma)) ** nu)</span><br><span class="line">        </span><br><span class="line">    return np.where(x &lt; 0, f(-x, sigma_l), f(x, sigma_r))</span><br></pre></td></tr></table></figure>

<h4 id="2-2-3-拟合AGGD"><a href="#2-2-3-拟合AGGD" class="headerlink" title="2.2.3 拟合AGGD"></a>2.2.3 拟合AGGD</h4><ol>
<li><p>计算$\hat \gamma，其中N_l$是负样本数量，而$N_r$是正样本数量.<br>$$<br>\hat \gamma = \frac{\sqrt{\frac{1}{N_l}\sum_{k=1,x_k&lt;0} ^{N_l}x_k ^2}}{\sqrt{\frac{1}{N_r}\sum_{k=1,x_k&lt;0} ^{N_r}x_k ^2}}<br>$$</p>
</li>
<li><p>计算$\hat r$<br>$$<br>\hat r = \frac{(\frac{\sum|x_k|}{N_l+N_r})^2}{\frac{\sum x_k ^2}{N_l+N_r}}<br>$$</p>
</li>
<li><p>使用$\hat \gamma ,\hat r$计算$\hat R$<br>$$<br>\hat R = \hat r\frac{(\hat \gamma ^3+1)(\hat \gamma +1)}{(\hat \gamma ^2+1)^2}<br>$$</p>
</li>
<li><p>使用反广义高斯比率计算$\alpha$<br>$$<br>\rho (\alpha) =\frac{T(2/\alpha)^2}{T(1/\alpha)T(3/\alpha)}<br>$$</p>
</li>
<li><p>评估左右scale参数<br>$$<br>\sigma _l = sqrt{\frac{1}{N_l-1}\sum _{k=l,x_k&lt;0} ^{N_l} x_k ^2} \<br>\sigma _r = sqrt{\frac{1}{N_r-1}\sum _{k=r,x_k\gt 0} ^{N_r} x_k ^2}<br>$$</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def asymmetric_generalized_gaussian_fit(x):</span><br><span class="line">    def estimate_phi(alpha):</span><br><span class="line">        numerator &#x3D; special.gamma(2 &#x2F; alpha) ** 2</span><br><span class="line">        denominator &#x3D; special.gamma(1 &#x2F; alpha) * special.gamma(3 &#x2F; alpha)</span><br><span class="line">        return numerator &#x2F; denominator</span><br><span class="line"></span><br><span class="line">    def estimate_r_hat(x):</span><br><span class="line">        size &#x3D; np.prod(x.shape)</span><br><span class="line">        return (np.sum(np.abs(x)) &#x2F; size) ** 2 &#x2F; (np.sum(x ** 2) &#x2F; size)</span><br><span class="line"></span><br><span class="line">    def estimate_R_hat(r_hat, gamma):</span><br><span class="line">        numerator &#x3D; (gamma ** 3 + 1) * (gamma + 1)</span><br><span class="line">        denominator &#x3D; (gamma ** 2 + 1) ** 2</span><br><span class="line">        return r_hat * numerator &#x2F; denominator</span><br><span class="line"></span><br><span class="line">    def mean_squares_sum(x, filter &#x3D; lambda z: z &#x3D;&#x3D; z):</span><br><span class="line">        filtered_values &#x3D; x[filter(x)]</span><br><span class="line">        squares_sum &#x3D; np.sum(filtered_values ** 2)</span><br><span class="line">        return squares_sum &#x2F; ((filtered_values.shape))</span><br><span class="line"></span><br><span class="line">    def estimate_gamma(x):</span><br><span class="line">        left_squares &#x3D; mean_squares_sum(x, lambda z: z &lt; 0)</span><br><span class="line">        right_squares &#x3D; mean_squares_sum(x, lambda z: z &gt;&#x3D; 0)</span><br><span class="line"></span><br><span class="line">        return np.sqrt(left_squares) &#x2F; np.sqrt(right_squares)</span><br><span class="line"></span><br><span class="line">    def estimate_alpha(x):</span><br><span class="line">        r_hat &#x3D; estimate_r_hat(x)</span><br><span class="line">        gamma &#x3D; estimate_gamma(x)</span><br><span class="line">        R_hat &#x3D; estimate_R_hat(r_hat, gamma)</span><br><span class="line"></span><br><span class="line">        solution &#x3D; optimize.root(lambda z: estimate_phi(z) - R_hat, [0.2]).x</span><br><span class="line"></span><br><span class="line">        return solution[0]</span><br><span class="line"></span><br><span class="line">    def estimate_sigma(x, alpha, filter &#x3D; lambda z: z &lt; 0):</span><br><span class="line">        return np.sqrt(mean_squares_sum(x, filter))</span><br><span class="line">    </span><br><span class="line">    def estimate_mean(alpha, sigma_l, sigma_r):</span><br><span class="line">        return (sigma_r - sigma_l) * constant * (special.gamma(2 &#x2F; alpha) &#x2F; special.gamma(1 &#x2F; alpha))</span><br><span class="line">    </span><br><span class="line">    alpha &#x3D; estimate_alpha(x)</span><br><span class="line">    sigma_l &#x3D; estimate_sigma(x, alpha, lambda z: z &lt; 0)</span><br><span class="line">    sigma_r &#x3D; estimate_sigma(x, alpha, lambda z: z &gt;&#x3D; 0)</span><br><span class="line">    </span><br><span class="line">    constant &#x3D; np.sqrt(special.gamma(1 &#x2F; alpha) &#x2F; special.gamma(3 &#x2F; alpha))</span><br><span class="line">    mean &#x3D; estimate_mean(alpha, sigma_l, sigma_r)</span><br><span class="line">    </span><br><span class="line">    return alpha, mean, sigma_l, sigma_r</span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-计算BRISQUE特征"><a href="#2-2-4-计算BRISQUE特征" class="headerlink" title="2.2.4 计算BRISQUE特征"></a>2.2.4 计算BRISQUE特征</h4><p>计算图像质量的特征即拟合MSCN系数的结果并移动shifted内积到广义高斯分布。首先，我们需要拟合MSCN系数到GDD，然后点对内积到AGGD。特征概要如下</p>
<table>
<thead>
<tr>
<th>FeatureID</th>
<th>Feature Description</th>
<th>Computation Procedure</th>
</tr>
</thead>
<tbody><tr>
<td>$f_1-f_2 $</td>
<td>Shape and variance</td>
<td>Fit GGD to MSCN coefficients</td>
</tr>
<tr>
<td>$f_3-f_6$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>H</strong> pairwise products</td>
</tr>
<tr>
<td>$f_7-f_{10}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>V</strong> pairwise products</td>
</tr>
<tr>
<td>$f_{11}-f_{14}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>D1</strong> pairwise products</td>
</tr>
<tr>
<td>$f_{15}-f_{18}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>D2</strong> pairwise products</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def calculate_brisque_features(image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6):</span><br><span class="line">    def calculate_features(coefficients_name, coefficients, accum&#x3D;np.array([])):</span><br><span class="line">        alpha, mean, sigma_l, sigma_r &#x3D; asymmetric_generalized_gaussian_fit(coefficients)</span><br><span class="line"></span><br><span class="line">        if coefficients_name &#x3D;&#x3D; &#39;mscn&#39;:</span><br><span class="line">            var &#x3D; (sigma_l ** 2 + sigma_r ** 2) &#x2F; 2</span><br><span class="line">            return [alpha, var]</span><br><span class="line">        </span><br><span class="line">        return [alpha, mean, sigma_l ** 2, sigma_r ** 2]</span><br><span class="line">    </span><br><span class="line">    mscn_coefficients &#x3D; calculate_mscn_coefficients(image, kernel_size, sigma)</span><br><span class="line">    coefficients &#x3D; calculate_pair_product_coefficients(mscn_coefficients)</span><br><span class="line">    </span><br><span class="line">    features &#x3D; [calculate_features(name, coeff) for name, coeff in coefficients.items()]</span><br><span class="line">    flatten_features &#x3D; list(chain.from_iterable(features))</span><br><span class="line">    return np.array(flatten_features)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-计算图像质量"><a href="#2-3-计算图像质量" class="headerlink" title="2.3 计算图像质量"></a>2.3 计算图像质量</h3><p>首先，我们需要两个辅助函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def plot_histogram(x, label):</span><br><span class="line">    n, bins &#x3D; np.histogram(x.ravel(), bins&#x3D;50)</span><br><span class="line">    n &#x3D; n &#x2F; np.max(n)</span><br><span class="line">    plt.plot(bins[:-1], n, label&#x3D;label, marker&#x3D;&#39;o&#39;)</span><br></pre></td></tr></table></figure>

<ol>
<li>载入图像</li>
<li>计算系数.计算完MSCN系数和点对的内积之后，我们可以确定其分布实际上是不同的。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mscn_coefficients &#x3D; calculate_mscn_coefficients(gray_image, 7, 7&#x2F;6)</span><br><span class="line">coefficients &#x3D; calculate_pair_product_coefficients(mscn_coefficients)</span><br><span class="line">for name, coeff in coefficients.items():</span><br><span class="line">    plot_histogram(coeff.ravel(), name)</span><br><span class="line">plt.axis([-2.5, 2.5, 0, 1.05])</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>拟合系数到广义高斯分布</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brisque_features &#x3D; calculate_brisque_features(gray_image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>resize图像并计算BRISQUE特征</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ownscaled_image &#x3D; cv2.resize(gray_image, None, fx&#x3D;1&#x2F;2, fy&#x3D;1&#x2F;2, interpolation &#x3D; cv2.INTER_CUBIC)</span><br><span class="line">downscale_brisque_features &#x3D; calculate_brisque_features(downscaled_image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6)</span><br><span class="line"></span><br><span class="line">brisque_features &#x3D; np.concatenate((brisque_features, downscale_brisque_features))</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>缩放特征并喂入SVR.作者提供了一个与训练的SVR模型来计算质量评估。但是，为了有个好的结果，我们需要将特征缩放到[-1,1]。对于后者，我们需要用预缩放特征向量相同的参数。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def scale_features(features):</span><br><span class="line">    with open(&#39;normalize.pickle&#39;, &#39;rb&#39;) as handle:</span><br><span class="line">        scale_params &#x3D; pickle.load(handle)</span><br><span class="line">    </span><br><span class="line">    min_ &#x3D; np.array(scale_params[&#39;min_&#39;])</span><br><span class="line">    max_ &#x3D; np.array(scale_params[&#39;max_&#39;])</span><br><span class="line">    </span><br><span class="line">    return -1 + (2.0 &#x2F; (max_ - min_) * (features - min_))</span><br><span class="line"></span><br><span class="line">def calculate_image_quality_score(brisque_features):</span><br><span class="line">    model &#x3D; svmutil.svm_load_model(&#39;brisque_svm.txt&#39;)</span><br><span class="line">    scaled_brisque_features &#x3D; scale_features(brisque_features)</span><br><span class="line">    </span><br><span class="line">    x, idx &#x3D; svmutil.gen_svm_nodearray(</span><br><span class="line">        scaled_brisque_features,</span><br><span class="line">        isKernel&#x3D;(model.param.kernel_type &#x3D;&#x3D; svmutil.PRECOMPUTED))</span><br><span class="line">    </span><br><span class="line">    nr_classifier &#x3D; 1</span><br><span class="line">    prob_estimates &#x3D; (svmutil.c_double * nr_classifier)()</span><br><span class="line">    </span><br><span class="line">    return svmutil.libsvm.svm_predict_probability(model, x, prob_estimates)</span><br><span class="line">calculate_image_quality_score(brisque_features)</span><br></pre></td></tr></table></figure>

<h2 id="3-结论"><a href="#3-结论" class="headerlink" title="3 结论"></a>3 结论</h2><p>方法在TID2008数据集上测试，并且效果不错，即便与引用IQA方法比起来。后续可以用XGBoost,LightGBM方法来训练识别步骤来提高效率。</p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-04-21-image-quality-assessment/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-04-21-image-quality-assessment/" title="图像质量评估">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-03-12-yolo-v123/">
    		从yolov1到yolov3
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.717Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="1-YOLOv1"><a href="#1-YOLOv1" class="headerlink" title="1 YOLOv1"></a>1 YOLOv1</h2><p>我们先看yolov1的检测效果</p>
<h3 id="1-1-Grid-Cell"><a href="#1-1-Grid-Cell" class="headerlink" title="1.1 Grid Cell"></a>1.1 Grid Cell</h3><p>YOLOv1把输入图片切分成$s\times s$个grid cell，<strong>每个grid cell只预测一个物体</strong>。比如下图中，黄色grid cell会预测中心坐标点落入其中的这个<code>person</code>物体。</p>
<p>注意这里的grid cell只是在图像上看起来是一个方格，实际是原图在经过yolo网络之后会变成$s\times s$个feature map，下图中的一个grid cell经过网络变换之后到最后的特征层变成了一个坐标点。比如原图为$448\times 448$经过yolo最后抽取得到的feature map为$7\times 7\times 30$</p>
<p><img src="/images/blog/yolov123_1.png" alt="yolov123"></p>
<p>每个grid cell只预测固定数目的bbox，当前示例中黄色grid cell做了两个bbox(2个蓝色框)预测来定位person这个物体的位置</p>
<p><img src="/images/blog/yolov123_2.png" alt="yolov123"></p>
<p>然而，一个grid cell只预测一个物体的规则限制了YOLO的预测能力。如果几个物体密集紧邻(多个物体的中心坐标可能会落在一个grid cell里面)，yolo就会遗漏一些物体。如下图，左下方有9个圣诞老人，但是yolo只能检测出来5个。</p>
<p><img src="/images/blog/yolov123_3.png" alt="yolov123"></p>
<p>对于每个grid cell </p>
<ul>
<li>预测B个bbox，并且每个bbox有一个box置信度。</li>
<li>即便上面预测了B个box，但实际<strong>只会检测一个物体</strong>(只保留一个)</li>
<li>预测C个条件分类概率(每个目标分类的概率)</li>
</ul>
<p>比如，对<strong>VOC数据集，YOLO使用了$7\times 7$(S=7)的grids，每个grid cell预测2(B=2)个bbox，以及20个分类(C=20)。</strong></p>
<p>每个bbox有个5元组$(x,y,w,h,conf)$，分别为bbox的坐标位置和置信度。这个置信度反映的是这个bbox包含目标(物体)的概率，以及此bbox的精确度。我们会对坐标位置进行归一化，变成0-1之间的比率（这一点，在准备yolo的训练数据时得到体现）。注意，<strong>其中的x,y是相对于当前cell的偏移量</strong>。每个grid cell有20个条件分类概率，此条件分类概率是检测到的物体属于20个分类里某一分类的概率(每个grid cell为会所有的分类都有一个概率,此处20个分类，会有20个概率)。因此，yolo最终的预测矩阵为</p>
<p>$$<br>(S,S,B\times 5+c) = (7,7,2\times 5+20)=(7,7,30)<br>$$</p>
<p>这样会产生非常多的bbox（下图中间），但是只保留置信度高于一定阈值(0.25)的bbox作为最终预测（下图右边）。</p>
<p><img src="/images/blog/yolov123_4.png" alt="yolov123"></p>
<p>每个预测bbox的分类置信度计算公式如下<br>$$<br>分类置信度= box置信度 \times 条件分类概率<br>$$<br><strong>它同时在分类和定位上衡量预测的bbox的置信度</strong></p>
<p>上面的写法很容易混淆，一个详细的定义如下：</p>
<ul>
<li>$bbox置信度= P_r(object)\times IOU$</li>
<li>$条件分类概率= P_r(class_i|object)$</li>
<li>$分类置信度= P_r(class_i)\times IOU=bbox置信度\times 条件分类概率$</li>
</ul>
<p>其中</p>
<ul>
<li>$P_r(object)$是bbox包含物体的概率</li>
<li>$IOU$是预测bbox和真实bbox之间的IOU</li>
<li>$P_r(class_i|object)$为给定当前物体，预测其属于分类$class_i$的概率</li>
<li>$P_r(class_i)$是物体属于分类$class_i$的概率<h3 id="1-2-网络架构设计"><a href="#1-2-网络架构设计" class="headerlink" title="1.2 网络架构设计"></a>1.2 网络架构设计</h3></li>
</ul>
<p><img src="/images/blog/yolov123_5.png" alt="yolov123"></p>
<p>网络使用了24组卷积网络+2个全连接层。一些卷积层使用$1\times 1$以减小特征图深度。最后一个卷积层输出$7\times 7\times 1024$,<strong>再接2个全连接层实现一种线性回归</strong>，最终输出$(7,7,30)$。</p>
<h3 id="1-3-损失函数"><a href="#1-3-损失函数" class="headerlink" title="1.3 损失函数"></a>1.3 损失函数</h3><p>YOLO的每个grid cell预测多（VOC中是2个）个bbox。为了计算正阳性样本的损失，我们只要求它们之中的一个bbox对物体负责。因此，我们选取与真实标注(ground truth)有最高IOU的一个。这种策略导致bbox的的预测的特殊性，即每个预测在物体尺寸和比率上更准。</p>
<p>YOLO使用预测bbox和真实bbox的二次方差和来作为损失函数。损失函数由以下项组成</p>
<ul>
<li>分类损失</li>
<li>定位损失：预测bbox和真实bbox之间的误差</li>
<li>置信度损失：box包含物体的置信度</li>
</ul>
<h3 id="1-3-1-分类损失"><a href="#1-3-1-分类损失" class="headerlink" title="1.3.1 分类损失"></a>1.3.1 分类损失</h3><p>如果检测到物体，每个grid cell的分类损失是每个分类的条件概率的平方误差和。<br>$$<br>\sum _{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 \<br>其中如果在grid \quad cell\quad  i中出现物体，则1 _i ^{obj}=1，否则1 _i ^{obj}=0  \<br>\hat p_i(c) 代表grid\quad  cell\quad  i中分类c的条件分类概率<br>$$</p>
<p>所以这个误差项可以这么理解：</p>
<ol>
<li>首先，当前这个grid cell–&gt;一共有 $7\times 7$个，中有物体的概率。此项有$49\times 2(每个grid \quad cell预测2个)=98$个</li>
<li>其次，这个物体分别属于20个分类的概率</li>
</ol>
<h4 id="1-3-2-回归损失"><a href="#1-3-2-回归损失" class="headerlink" title="1.3.2 回归损失"></a>1.3.2 回归损失</h4><p>回归损失衡量的是，预测bbox的位置和尺寸的误差。YOLOv1只计算负责检测物体那个bbox的误差</p>
<p>$$<br>\lambda <em>{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] \</p>
<ul>
<li>\lambda <em>{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] \<br>其中 如果第i个grid \quad cell中的第j个bbox是负责检测物体的，则1_{ij} ^{obj}=1，否则1_{ij} ^{obj}=0. \<br>\lambda _{coord}增加bbox坐标的损失权重，默认为5<br>$$<br>在YOLO看来大的bbox和小的bbox的2个像素的误差是相等的，为刻意强调这一点，YOLO没有直接预测bbox的宽和高，而是宽的二次方根和高的二次方根。除此之外，对此loss乘以一个权重$\lambda _{coord}$以增加bbox的准确率。</li>
</ul>
<h4 id="1-3-3-置信度损失"><a href="#1-3-3-置信度损失" class="headerlink" title="1.3.3 置信度损失"></a>1.3.3 置信度损失</h4><p>如果某个物体在box中被检测到，其置信度损失（衡量的是box中的物体）为<br>$$<br>\sum <em>{i=0} ^{S^2}\sum</em>{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2 \<br>其中</p>
<ul>
<li>\hat C_i是grid\quad cell\quad i中第j个box的置信度</li>
<li>1_{ij} ^{obj}=1 如果第i个grid\quad cell的第j个box负责检测物体，否则为0<br>$$</li>
</ul>
<p>如果某个物体不在box中，其置信度损失为</p>
<p>$$<br>\lambda <em>{noobj}\sum</em>{i=0} ^{S^2}\sum <em>{j=0} ^B1</em>{ij} ^{noobj}(C_i-\hat C_i)^2 \<br>其中</p>
<ul>
<li>1_{ij} ^{noobj}是对1_{ij} ^{noobj}的一种补充</li>
<li>\hat C_i是第i个grid \quad cell的第j个box的置信度</li>
<li>\lambda _{noobj}降低背景检测损失的权重(noobj即背景)<br>$$<br>由于大部分box不包含任何物体，这会导致分类(正负样本)的不均衡，因此，我们降低了背景检测损失的权重，即$\lambda _{noobj}$默认值为0.5</li>
</ul>
<h4 id="1-3-4-最终损失"><a href="#1-3-4-最终损失" class="headerlink" title="1.3.4 最终损失"></a>1.3.4 最终损失</h4><p>最终损失为前面三种损失之和<br>$$<br> Loss= \sum <em>{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 +\<br>\lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] +\<br> \lambda <em>{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] +\<br>\sum <em>{i=0} ^{S^2}\sum</em>{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \<br>\lambda <em>{noobj}\sum</em>{i=0} ^{S^2}\sum <em>{j=0} ^B1</em>{ij} ^{noobj}(C_i-\hat C_i)^2<br>$$</p>
<h3 id="1-4-yolo评价"><a href="#1-4-yolo评价" class="headerlink" title="1.4 yolo评价"></a>1.4 yolo评价</h3><ul>
<li>优点 <ul>
<li>快速，实时检测</li>
<li>使用单一结构网络，可以端到端地训练</li>
<li>通用。用自然图片训练得到的网络也可用于艺术图片的检测</li>
<li>区域候选的方法限制了分类器识别特定区域。YOLO在预测边框时可以遍历整个图像。使用相关信息时，YOLO在背景区域检测更少的假阳性。</li>
</ul>
</li>
</ul>
<ul>
<li>YOLO每个grid cell只检测一个物体，这使得它在做预测时增加了空间多样性。</li>
</ul>
<h2 id="2-YOLOv2"><a href="#2-YOLOv2" class="headerlink" title="2 YOLOv2"></a>2 YOLOv2</h2><p>与基于区域候选的方法相比，YOLO有更高的定位损失以及更低的召回率。</p>
<h3 id="2-1-准确率提升"><a href="#2-1-准确率提升" class="headerlink" title="2.1 准确率提升"></a>2.1 准确率提升</h3><h4 id="2-1-1-BatchNorm"><a href="#2-1-1-BatchNorm" class="headerlink" title="2.1.1 BatchNorm"></a>2.1.1 BatchNorm</h4><p>每个卷积层之后添加一层BN,可以去掉dropout，同时提升2%的map</p>
<h4 id="2-1-2-高分辨率"><a href="#2-1-2-高分辨率" class="headerlink" title="2.1.2 高分辨率"></a>2.1.2 高分辨率</h4><p>我们先看看yolov1的训练步骤</p>
<ol>
<li>首先，训练一个类似VGG的分类网络【用的是$224\times 224$图片】</li>
<li>然后，使用全连接层替换卷积层</li>
<li>再端到端地训练目标检测【用的是$448\times 448$图片】</li>
</ol>
<p>YOLOv2的训练步骤</p>
<ol>
<li>训练一个分类器：先用$224\times 224$的图片训练，再用$448\times 448$图片接着训练（更少的epoch）<br>后续步骤一致。这使得yolov2的检测器更容易 训练，map也提升了4%。</li>
</ol>
<h4 id="2-1-3-anchor-box"><a href="#2-1-3-anchor-box" class="headerlink" title="2.1.3 anchor box"></a>2.1.3 anchor box</h4><p>从yolo的论文中，我们得知，训练阶段的早期会出现不稳定的梯度，因为yolo会随意预测物体尺度和位置。而实际中，一些物体的尺寸是有规律的。比如汽车的长宽比一般是0.41</p>
<p><img src="/images/blog/yolov123_6.png" alt="yolov123"><br>由于我们只需要一个bbox猜测值是对的就可以，因而如果我们按照实际物体比率来初始化这些预测的初始值，训练阶段就会稳定得多。例如，我们可以按照如下步骤构建5个anchor box</p>
<p><img src="/images/blog/yolov123_7.png" alt="yolov123"></p>
<p><strong>YOLOv2不直接预测bbox，而是预测其相对于上面5个bbox的偏移量。</strong>如果限制偏移量的值，我们可以保持预测的多样性，并使得每个预测集中于特定尺度，这样一来初始训练阶段就会稳定许多。</p>
<p>以下是对yolov1网络的改动</p>
<ol>
<li>移除了最后两个负责预测bbox的个全连接层</li>
</ol>
<p><img src="/images/blog/yolov123_8.png" alt="yolov123"></p>
<ol start="2">
<li>将分类预测从cell级转向bbox级。这样一来，每个预测包含了预测bbox的4个位置参数，一个box置信度（是否包含物体）以及20个分类概率。每个grid cell5个bbox，每个bbox25个参数，共125个参数。与yolov1一样，物体预测依旧是预测预测的bbox与真实bbox之间的IOU。</li>
</ol>
<p><img src="/images/blog/yolov123_9.png" alt="yolov123"><br>yolov1是如何做预测的</p>
<p><img src="/images/blog/yolov123_10.png" alt="yolov123"></p>
<p>yolov2是如何做预测的.注意与yolov1区别</p>
<ol start="3">
<li>为了生成$7\times7\times125$的预测，将网络的最后两层替换$3\times3$的卷积层，每个卷积层输出1024个通道，然后再用$1\times1$卷积将$7\times7\times1024$转换成$7\times7\times125$。</li>
<li>将网络输入图像尺寸从$448\times 448$改为$416\times 416$.由于网络中心一般是大物体坐标中心，之前的$8\times8$会出现不确定性预测。输入图像尺寸从$448\times448$改为$416\times416$之后，输出的特征空间尺度从$8\times8$变成$7\times7$。</li>
</ol>
<p><img src="/images/blog/yolov123_11.png" alt="yolov123"><br>使用anchor box使得map从69.5下降到69.2%，但是召回率从81%提升到88%。</p>
<h3 id="2-2-维度聚类"><a href="#2-2-维度聚类" class="headerlink" title="2.2 维度聚类"></a>2.2 维度聚类</h3><p>在诸多问题领域，bbox其实有很强的可识别模式。比如，自动驾驶领域，2个最常见的模式是汽车和行人。YOLOv2使用kmeans聚类在训练集上得到模式最多的K个bbox。</p>
<p>由于需要处理的是bbox，而非点，其距离衡量指标使用的是IOU。</p>
<p><img src="/images/blog/yolov123_12.png" alt="yolov123"></p>
<p>上图左边是在真实bbox中使用不同聚类中心数目得到的平均IOU，增加聚类中心数目，准确率提升。使用5个聚类中心得到比较均衡的结果，右边是5个聚类中心时的bbox分布。</p>
<h3 id="2-3-直接的位置预测"><a href="#2-3-直接的位置预测" class="headerlink" title="2.3 直接的位置预测"></a>2.3 直接的位置预测</h3><p><strong>预测的是位置相对于anchor左上角的偏移量。</strong>如果直接，无约束的预测会导致随机结果，YOLO预测的是5个参数$t_x,t_y,t_w,t_h$并使用sigma函数加以约束其取值范围。下图中蓝色框是预测的bbox，点线矩形框是anchor。左上角的$C_x,C_y$也是anchor的左上角位置，预测的是相对于此处的偏移。</p>
<p><img src="/images/blog/yolov123_13.png" alt="yolov123"></p>
<p>其中<br>$$<br>b_x=\sigma(t_x)+c_x \<br>b_y=\sigma(t_y)+c_y \<br>b_w=p_we^{t_w} \<br>b_h=p_he^{t_h} \<br>P_r(obj)*IOU(b,obj)=\sigma(t_0) \quad \sigma(t_0)是box的置信度\<br>其中\<br>t_x,t_y,t_w,t_h是直接由yolo预测得到的\<br>c_x,c_y是anchor的左上角坐标，p_w,p_h是anchor的宽度和高度\<br>c_x,c_y,p_w,p_h由宽度和高度归一化之后的结果\<br>b_x,b_y,b_w,b_h是最终的预测bbox<br>$$</p>
<p>所以，需要知道的是</p>
<ol>
<li><strong>yolo网络只预测偏移量</strong></li>
<li><strong>偏移量与默认的anchor box进行运算之后得到bbox</strong></li>
<li><strong>bbox与真实box不是同一个</strong></li>
</ol>
<h3 id="2-4-修正的特征"><a href="#2-4-修正的特征" class="headerlink" title="2.4 修正的特征"></a>2.4 修正的特征</h3><p>yolo使用了一种称为通道的技术，将$28\times 28\times 512$层reshape到$14\times 14\times 1024$，然后将这两层的feature拼接起来做预测。</p>
<p><img src="/images/blog/yolov123_14.png" alt="yolov123"><br>参考 </p>
<h3 id="2-5-多尺度训练"><a href="#2-5-多尺度训练" class="headerlink" title="2.5 多尺度训练"></a>2.5 多尺度训练</h3><p>移除全连接层的yolo可以接收不同尺度输入图像，如果输入图像宽和高双倍之后，我们需要预测4倍的grid cell。由于yolo是按照32倍下采样的，所以输入图像是32的倍数即可。每10个batch之后，yolo会随机选取其他尺度的图像来训练网络。</p>
<h3 id="2-6-使用不同优化方案之后的准确率比较"><a href="#2-6-使用不同优化方案之后的准确率比较" class="headerlink" title="2.6 使用不同优化方案之后的准确率比较"></a>2.6 使用不同优化方案之后的准确率比较</h3><p><img src="/images/blog/yolov123_15.png" alt="yolov123"></p>
<h3 id="2-7-速度提升"><a href="#2-7-速度提升" class="headerlink" title="2.7 速度提升"></a>2.7 速度提升</h3><p><strong>darknet</strong><br>为了进一步简化CNN结构，设计了一个darknet网络。其在ImageNet上获取了72.9%的top-1准确率和91.2%的top-5准确率。darknet大部分使用的是$3\times 3$卷积</p>
<p><img src="/images/blog/yolov123_16.png" alt="yolov123"></p>
<p>在做目标检测时，最后红色框的最后的卷积层使用$3\times 3$的卷积层替换，然后再使用$1\times 1$的卷积将$7\times 7\times 1024$的输出转换成$7\times 7\times 125$</p>
<h2 id="3-yolov3"><a href="#3-yolov3" class="headerlink" title="3 yolov3"></a>3 yolov3</h2><h3 id="3-1-类预测"><a href="#3-1-类预测" class="headerlink" title="3.1 类预测"></a>3.1 类预测</h3><p>大多数分类器认为目标分类是互斥的，所以yolov2用的是softmax，全部分类的概率之和为1，但是yolov3使用了多标签分类。比如，标签可能既是<code>行人</code>也可能是<code>小孩</code>。yolov3为每个分类使用独立的logistic分类器以计算输入属于特定分类的概率。yolov3给每个分类用的是二分交叉熵，而非MSE。此举同时降低了计算复杂度。</p>
<h3 id="3-2-bbox预测和损失函数"><a href="#3-2-bbox预测和损失函数" class="headerlink" title="3.2 bbox预测和损失函数"></a>3.2 bbox预测和损失函数</h3><p>yolov3使用logistic回归来预测每个bbox的为物体的置信度。yolov3修改计算损失函数的方式，如果bbox与真实标签obj重叠区域大于其他所有的，其对应的obj(物体)置信度为1（即选取一个与ground truth有最多重叠的anchor，其对应的obj score为1）。其他与真实标签obj重叠区域超过阈值(默认为0.5)的anchor，它们不计算损失。每个ground truth obj只分配给一个bbox。如果某个bbox没有被分配ground truth，不计算其分类和定位损失，只计算物体置信度。只使用$t_x,t_y$而非$b_x,b_y$计算损失</p>
<p>$$<br>b_x=\sigma(t_x)+c_x \<br>b_y=\sigma(t_y)+c_y \<br>b_w = p_we^{t_w} \<br>b_h = p_he^{t_h}<br>$$</p>
<h3 id="3-3-特征金字塔-FPN网络"><a href="#3-3-特征金字塔-FPN网络" class="headerlink" title="3.3 特征金字塔(FPN网络)"></a>3.3 特征金字塔(FPN网络)</h3><p>yolov3在每个位置上得到3个预测，每个预测由<code>1个bbox</code>,<code>一个obj物体得分</code>,<code>80个分类得分</code>组成。即$=N\times N\times [3\times (4+1+80)]$ 。</p>
<p>YOLOv3在三个不同尺度上预测</p>
<ol>
<li>最后一个特征map层</li>
<li>从最后一层往回走2层，2倍上采样。yolov3采用了一个更高分辨率的特征图，并将它与上采样层得到的特征图进行逐元素相加。在这个特征融合层，yolov3再进行卷积得到第二个预测集。</li>
<li>重复步骤2，得到的特征层有好的高层结构(语义级)信息和物体位置的空间信息。</li>
</ol>
<h3 id="3-4-特征抽取器"><a href="#3-4-特征抽取器" class="headerlink" title="3.4 特征抽取器"></a>3.4 特征抽取器</h3><p>使用一个新的darknet-53来替换前面的darknet-19，darknet主要由$3\times 3$和$1\times 1$卷积组成，以及一些类似ResNet的跳转连接。</p>
<p><img src="/images/blog/yolov123_17.png" alt="yolov123"></p>
<h3 id="3-5-准确率"><a href="#3-5-准确率" class="headerlink" title="3.5 准确率"></a>3.5 准确率</h3><p><img src="/images/blog/yolov123_18.png" alt="yolov123"></p>
<h2 id="4-损失函数变化"><a href="#4-损失函数变化" class="headerlink" title="4 损失函数变化"></a>4 损失函数变化</h2><h3 id="4-1-YOLOv1损失函数"><a href="#4-1-YOLOv1损失函数" class="headerlink" title="4.1 YOLOv1损失函数"></a>4.1 YOLOv1损失函数</h3><p>先回顾下YOLOv1的损失函数</p>
<p>$$<br> Loss= \sum <em>{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 +\<br>\lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] +\<br> \lambda <em>{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] +\<br>\sum <em>{i=0} ^{S^2}\sum</em>{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \<br>\lambda <em>{noobj}\sum</em>{i=0} ^{S^2}\sum <em>{j=0} ^B1</em>{ij} ^{noobj}(C_i-\hat C_i)^2<br>$$</p>
<h3 id="4-2-YOLOv2"><a href="#4-2-YOLOv2" class="headerlink" title="4.2 YOLOv2"></a>4.2 YOLOv2</h3><p>yolov2的损失函数只是在yolov1基础上改动了关于bbox的w和h的损失计算方式<br>即从<br>$$<br>\sum <em>{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2]<br>$$<br>改动到<br>$$<br>\sum <em>{i=0} ^{S^2}\sum _{j=0} ^B1</em>{ij} ^{obj} [(w_i-\hat {w_i})^2+(h_i-\hat {h_i})^2]<br>$$<br>去掉了w和h的二次根号，作者认为没有必要。</p>
<h3 id="4-3-yolov3"><a href="#4-3-yolov3" class="headerlink" title="4.3 yolov3"></a>4.3 yolov3</h3><p>YOLOv3的损失函数是在yolov2基础上改动的，最大的变动是分类损失换成了二分交叉熵，这是由于yolov3中剔除了softmax改用logistic。不过在不同的框架中具体的修改不一致。</p>
<p><strong>darknet</strong></p>
<p>darknet源码中只修改了分类损失的计算方法。也即从yolov2的分类损失部分<br>$$<br>\sum <em>{i=0} ^{S^2}\sum</em>{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \<br>\lambda <em>{noobj}\sum</em>{i=0} ^{S^2}\sum <em>{j=0} ^B1</em>{ij} ^{noobj}(C_i-\hat C_i)^2<br>$$<br>修改到<br>$$<br>\sum <em>{i=0} ^{S^2}\sum</em>{j=0} ^B1_{ij} ^{obj} (C_i log \hat C_i+(1-C_i)log (1-\hat C_i) )+\<br>\lambda <em>{noobj}\sum</em>{i=0} ^{S^2}\sum <em>{j=0} ^B1</em>{ij} ^{noobj}(C_i-\hat C_i)^2<br>$$</p>
<p><strong>Tensorflow</strong></p>
<p>Tensorflow版本的<a href="https://github.com/qqwweee/keras-yolo3" target="_blank" rel="noopener">yolov3</a> 对以下部分都做了交叉熵损失</p>
<ol>
<li>坐标的x,y<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xy_loss &#x3D; object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[..., 0:2], from_logits&#x3D;True)</span><br></pre></td></tr></table></figure></li>
<li>置信度损失</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confidence_loss &#x3D; object_mask * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits&#x3D;True) + (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits&#x3D;True) * ignore_mask</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>分类损失</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_loss &#x3D; object_mask * K.binary_crossentropy(true_class_probs, raw_pred[..., 5:], from_logits&#x3D;True)</span><br></pre></td></tr></table></figure>
<p><strong>没有对非目标的损失函数</strong></p>
<h2 id="5-注意-bbox数目变化"><a href="#5-注意-bbox数目变化" class="headerlink" title="5 注意 bbox数目变化"></a>5 注意 bbox数目变化</h2><ul>
<li><p>yolov1<br>$$<br>(S,S,B\times 5+c) = (7,7,2\times 5+20)=(7,7,30)<br>$$</p>
</li>
<li><p>yolov2<br>$$<br>7\times7\times (5(每个grid cell5个bbox)\times (4(位置)+1(box置信度)+20(20个分类概率)))<br>$$</p>
</li>
<li><p>yolov3</p>
</li>
</ul>
<p>$$<br>(N\times N +2(上采样)\times N\times N +4(上采样)\times (N\times N))\times [3\times (4+1+80(分类))]<br>$$</p>
<p><a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" target="_blank" rel="noopener">Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3</a><br><a href="https://www.tinymind.cn/articles/411" target="_blank" rel="noopener">从YOLOv1到YOLOv3，目标检测的进化之路</a><br><a href="https://blog.csdn.net/weixin_42078618/article/details/85005428" target="_blank" rel="noopener">yolov3 损失函数</a></p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-03-12-yolo-v123/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-03-12-yolo-v123/" title="从yolov1到yolov3">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    
        <!-- 文章列表 item -->
<article class="post">
    <header>
        <!-- 标签这有且只能显示一个 -->
        
        <a class="cat-link" href="/categories/blog/">blog</a>
        
        <!-- 文章标题 -->
        
    <h3 class="post-title">
    	<a href="http://shartoo.github.com/2019/12/23/2019-03-05-SIFT-feature/">
    		SIFT特征详解
    	</a>
    </h3>

    </header>
    <p class="post-meta">
        Jelon 发表于
        <time datetime="2019-12-23T10:45:59.716Z">2019-12-23</time>
        &nbsp;&nbsp;
        <span class="post-tags">
            标签：
            
        </span>
    </p>

    <div class="post-content">
        <div class="post-excerpt">
            
                <h2 id="1-概览"><a href="#1-概览" class="headerlink" title="1 概览"></a>1 概览</h2><p>SIFT特征即Scale-Invariant Feature Transform，是一种用于检测和描述数字图像中的局部特征的算法。它定位关键点并以量化信息呈现（所以称之为描述器），可以用来做目标检测。此特征可以被认为可以对抗不同变换（即同一个特征在不同变换下可能看起来不同）而保持不变。</p>
<p>可以通过这个<a href="http://weitz.de/sift/index.html?size=large" target="_blank" rel="noopener">网站 SIFT特征在线计算</a>，直接查看一张图片中的SIFT特征，你需要准备一张小图片，然后上传到网站，就会自动计算出该图像的SITF特征。如果对SIFT特征计算步骤缺乏形象的认识，可以去这个网站互动下，它可以可视化每个步骤。</p>
<p>SIFT特征的提取步骤</p>
<ol>
<li>生成高斯差分金字塔（DOG金字塔），尺度空间构建</li>
<li>空间极值点检测（关键点的初步查探）</li>
<li>稳定关键点的精确定位</li>
<li>稳定关键点方向信息分配</li>
<li>关键点描述</li>
<li>特征点匹配</li>
</ol>
<h2 id="2-生成差分高斯金字塔"><a href="#2-生成差分高斯金字塔" class="headerlink" title="2 生成差分高斯金字塔"></a>2 生成差分高斯金字塔</h2><p>参考 <a href="https://shartoo.github.io/image-pramid/" target="_blank" rel="noopener">图像处理中各种金字塔</a> 得到一组如下图</p>
<p><img src="/images/blog/sift_feature1.png" alt="sift1"> </p>
<h2 id="3-空间极值点检测"><a href="#3-空间极值点检测" class="headerlink" title="3 空间极值点检测"></a>3 空间极值点检测</h2><p>从第2步的差分高斯金字塔(DOG)中可以得到不同层级不同尺度的金字塔，<strong>所谓的特征就是一些强的、有区分力的点</strong>。DOG中这些有区分力的点就是极值点，即每个像素都要和相邻点(<strong>此处的相邻不仅仅是水平面的前后左右，还有上下尺度的前后左右</strong>)比较，看其是否比它的图像域(水平方向上)和尺度空间域(垂直方向上)的相邻点大或者小，下图是示意图：</p>
<p><img src="/images/blog/sift_feature2.png" alt="sift1"> </p>
<p>在二维图像空间，中心点与它3<em>3邻域内的8个点做比较，在同一组内的尺度空间上，中心点和上下相邻的两层图像的2</em>9个点作比较，如此可以保证检测到的关键点在尺度空间和二维图像空间上都是局部极值点。所以确定极值点，需要$3\times 3-1(当前像素点，上图中中间黑色X)+2\times 9=26个点$。<br>从第2小节中，我们计算得到的极值点由如下黄色和红色标记（其中黄色圆圈的标记表明，它虽然是极值点，但是由于绝对值过小，在后续处理时会被丢弃）</p>
<p><img src="/images/blog/sift_feature3.png" alt="sift1"> </p>
<p>我们选取图中间用蓝色矩形框标记的红色点，查看其灰度值(下图正中间红色点)，可以看到它在当前差分金字塔取得了极小值。</p>
<p><img src="/images/blog/sift_feature4.png" alt="sift1"> </p>
<h2 id="4-稳定关键点的精确定位"><a href="#4-稳定关键点的精确定位" class="headerlink" title="4 稳定关键点的精确定位"></a>4 稳定关键点的精确定位</h2><p>上面步骤得到的极值点中存在大量不稳定地点，有些可能是噪音导致的，比如第3节中黄色圆圈标记的点。我们需要去除这些不稳定地像素点。即去除DOG局部曲率非常不对称的像素。此步骤，需要计算空间尺度函数的二次泰勒展开式的极值来完成。同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力。</p>
<p>具体步骤如下：</p>
<ol>
<li><p>空间尺度函数泰勒展开式如下：<br>$$<br>D(x)=D+\frac{\partial D^T}{\partial x}x+\frac{1}{2}x^T\frac{\partial ^2D}{\partial x^2}x \<br>对上式求导，令其为0，得到精确地位置，有\<br>\hat x=-\frac{\partial ^2D^{-1}}{\partial x^2}\frac{\partial D}{\partial x}<br>$$</p>
</li>
<li><p>在已经检测到的特征点中，要去掉低对比度的特征点和不稳定地边缘响应点。去除低对比度的点：把$\hat x$的值代回，即在Dog 空间的极值点$D(x)$处取值，<strong>只取前两项</strong>可得：<br>$$<br>D(\hat x)=D+\frac{1}{2}\frac{\partial D^T}{\partial x}\hat x<br>$$<br>若 $|D(\hat x)\ge 0.003$，该特征点就保留下来，否则丢弃</p>
</li>
</ol>
<p>3.边缘响应的去除。一个定义不好的高斯差分算子的极值在横跨边缘的地方有较大的曲率，而在垂直边缘的方向有较小的曲率。主曲率通过一个$2\times 2$的Hessian矩阵H去求出：<br>$$<br>H=[ \begin{array} D_{xx} \quad D_{xy} &amp; \<br>  D_{xy}\quad D_{yy}  &amp;<br> \end{array} ]<br>$$<br>使用python计算hessian矩阵的代码可以参考</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def hessian(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculate the hessian matrix with finite differences</span><br><span class="line">    Parameters:</span><br><span class="line">       - x : ndarray</span><br><span class="line">    Returns:</span><br><span class="line">       an array of shape (x.dim, x.ndim) + x.shape</span><br><span class="line">       where the array[i, j, ...] corresponds to the second derivative x_ij</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x_grad &#x3D; np.gradient(x) </span><br><span class="line">    hessian &#x3D; np.empty((x.ndim, x.ndim) + x.shape, dtype&#x3D;x.dtype) </span><br><span class="line">    for k, grad_k in enumerate(x_grad):</span><br><span class="line">        # 遍历每个维度，在一阶导数的所有项再次使用梯度</span><br><span class="line">        tmp_grad &#x3D; np.gradient(grad_k) </span><br><span class="line">        for l, grad_kl in enumerate(tmp_grad):</span><br><span class="line">            hessian[k, l, :, :] &#x3D; grad_kl</span><br><span class="line">    return hessian</span><br><span class="line"></span><br><span class="line">x &#x3D; np.random.randn(100, 100, 100)</span><br><span class="line">hessian(x)</span><br></pre></td></tr></table></figure>
<p>导数由采样点相邻差估计得到。D的主曲率和H的特征值成正比，令α为较大特征值，β为较小的特征值，则<br>$$<br>Tr(H)=D_{xx}+D_{yy}=\alpha +\beta \<br>Det(H)=D_{xx}D_{yy}-(D_{xy})^2 = \alpha\beta \<br>令\alpha=\gamma \beta则 \<br>\frac{Tr(H)^2}{Det(H)} =\frac{(\alpha+\beta)^2}{\alpha \beta}=\frac{(\gamma\beta +\beta)^2}{\gamma \beta ^2}=\frac{(1+\gamma)^2}{\gamma}<br>$$<br>而$\frac{(1+\gamma)^2}{\gamma}$的值在两个特征值相等的时候最小，随着$\gamma$的增大而增大，因此，为了检测主曲率是否在某阈值$\gamma$下，只需检测<br>$$<br>\frac{Tr(H)^2}{Det(H)}&lt;\frac{(\gamma+1)^2}{\gamma}<br>$$<br>在SIFT特征提取的原论文中，提到<strong>如果$\frac{\alpha +\beta}{\alpha \beta}&gt;\frac{(\gamma +1)^2}{\gamma}$，则丢弃此像素点</strong>，论文中$\gamma=10$</p>
<h2 id="5-给特征点赋值一个128维方向参数"><a href="#5-给特征点赋值一个128维方向参数" class="headerlink" title="5 给特征点赋值一个128维方向参数"></a>5 给特征点赋值一个128维方向参数</h2><p>经过上面的步骤，我们已经确定了一些灰度值极值点。接下来，我们需要确定这些极值点的方向。为关键点分配方向信息所要解决的问题是使得关键点对图像角度和旋转具有不变性。方向的分配是通过求每个极值点的梯度来实现的。<br>对于任一关键点</p>
<ul>
<li>其梯度<strong>幅值</strong>表述为：</li>
</ul>
<p>$$<br>m(x,y) = \sqrt{((L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}<br>$$</p>
<ul>
<li>梯度<strong>方向</strong>为：<br>$$<br>\theta (x,y) = tan ^{-1}[\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}]<br>$$</li>
</ul>
<p><strong>分配给关键点的并不直接是关键点的梯度方向，而是按照一种梯度方向直方图方式给出的</strong><br>计算方法</p>
<ol>
<li>计算关键点为中心邻域内所有点的梯度方向。0-360度</li>
<li>每个方向10度，共36个方向。</li>
<li>统计累计落在每个方向点的关键点个数，依次生成梯度直方图</li>
</ol>
<p><img src="/images/blog/sift_feature5.png" alt="sift1"> </p>
<p>具体在图像实例中，某个极值点的梯度方向直方图如下：</p>
<p><img src="/images/blog/sift_feature6.png" alt="sift1"> </p>
<p>上图中，左图矩形框内的红色小圆圈代表点击的极值点，中间图案代表最终得到的极值点的主方向，右图为对应的梯度方向直方图。</p>
<p>除此之外，原论文中还包含了<strong>辅方向</strong>，辅方向定义为：若在梯度直方图中存在一个相当于主峰值80%能量的峰值，则认为是关键点的辅方向。辅方向的设计可以增强匹配的鲁棒性，Lowe指出，大概有15%的关键点具有辅方向，而恰恰是这15%的关键点对稳定匹配起到关键作用。</p>
<h2 id="6-计算SIFT特征描述子"><a href="#6-计算SIFT特征描述子" class="headerlink" title="6 计算SIFT特征描述子"></a>6 计算SIFT特征描述子</h2><p>此步骤与步骤5基本一样，也是计算每个关键点周围的梯度方向的直方图分布。不同之处在于，此时的邻居为一个圆，并且坐标体系被扭曲以匹配相关梯度方向。<br>具体思路是：对关键点周围像素区域分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象表述。<br>如下图，对于2<em>2块，每块的所有像素点的梯度做高斯加权，每块最终取8个方向，即可以生成2</em>2<em>8维度的向量，以这2</em>2*8维向量作为中心关键点的数学描述</p>
<p><img src="/images/blog/sift_feature7.png" alt="sift1"> </p>
<p>但是实际上，在原论文中证明，对每个关键点周围采用$4\times 4$块(每个块内依然是8个方向)的邻域描述子效果最佳</p>
<p><img src="/images/blog/sift_feature8.png" alt="sift1"> </p>
<p>所以，<strong>此时我们计算的不是一个梯度方向直方图，而是16个</strong>。每个梯度直方图对应的是新坐标系统的中心点附近的点以及圆形周围邻居梯度的分量。</p>
<p>下图是某个极值点用于生成的描述子的邻居以及坐标系统，即直方图（被归一化并以$4\times 4\times 8=128$个整型数字）。仔细看下图，会发现有16个直方图(16个块)，每个直方图有8个bins(代表每个块的8个主方向)。</p>
<p><img src="/images/blog/sift_feature9.png" alt="sift1"> </p>
<p><a href="https://blog.csdn.net/dcrmg/article/details/52577555" target="_blank" rel="noopener">CSDN SIFT特征</a><br><a href="https://blog.csdn.net/abcjennifer/article/details/7639681" target="_blank" rel="noopener">Rachel-Zhang SIFT</a><br><a href="https://stackoverflow.com/questions/31206443/numpy-second-derivative-of-a-ndimensional-array" target="_blank" rel="noopener">stackoverflow 计算hessian矩阵</a></p>

            
            <p class="more">
                <a href="http://shartoo.github.com/2019/12/23/2019-03-05-SIFT-feature/">阅读剩下更多</a>
            </p>
        </div>
        <div class="post-thumbnail" data-img="">
            <a href="http://shartoo.github.com/2019/12/23/2019-03-05-SIFT-feature/" title="SIFT特征详解">
                
                    <img class="thumbnail" src="/img/default.png" data-echo="/img/thumbnail/3.jpg" alt="默认配图" >
                
            </a>
        </div>
    </div>
</article>

    

    
    <nav class="page-navigator">
        <span class="page-number current">1</span><a class="page-number" href="/categories/blog/page/2/">2</a><a class="page-number" href="/categories/blog/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/categories/blog/page/10/">10</a><a class="extend next" rel="next" href="/categories/blog/page/2/">后一页</a>
    </nav>
    


            </div>

        </section>
        <!-- 侧栏部分 -->
<aside class="sidebar">
    <section class="widget">
        <h3 class="widget-hd"><strong>文章分类</strong></h3>
        <!-- 文章分类 -->
<ul class="widget-bd">
    
    <li>
        <a href="/categories/blog/">blog</a>
        <span class="badge">(94)</span>
    </li>
    
</ul>
    </section>

    
    <section class="widget">
        <h3 class="widget-hd"><strong>热门标签</strong></h3>
        <!-- 文章标签 -->
<div class="widget-bd tag-wrap">
  
</div>
    </section>
    

    

    
    <!-- 友情链接 -->
    <section class="widget">
        <h3 class="widget-hd"><strong>友情链接</strong></h3>
        <!-- 文章分类 -->
<ul class="widget-bd">
    
        <li>
            <a href="https://jelon.top" target="_blank" title="Jelon个人前端小站">前端博客小站</a>
        </li>
    
        <li>
            <a href="https://www.baidu.com" target="_blank" title="百度搜索">百度</a>
        </li>
    
</ul>
    </section>
    
</aside>
<!-- / 侧栏部分 -->
    </div>

    <!-- 博客底部 -->
    <footer class="footer">
    &copy;
    
        2016-2019
    

    <a href="/">Jelon Loves You</a>
</footer>
<div class="back-to-top" id="JELON__backToTop" title="返回顶部">返回顶部</div>

    <!--博客js脚本 -->
    <!-- 这里放网站js脚本 -->

<script src="/js/main.js"></script>

</body>
</html>