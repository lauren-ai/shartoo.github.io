---
layout: post
title: softmax函数
description: 深度学习基础理论
category: 深度学习
---

本文根据 [softmax函数定义](http://blog.csdn.net/hejunqing14/article/details/48980321)
整理

## softmax函数的定义

softmax是sigmoid函数的普遍形式，用于多分类。用在输出层，作为一个分类器，表述为多个分类上的概率分布。 softmax函数的常见形式如下：

$$
  P(i) =\frac{exp(\theta_i^Tx)}{\sum _{i=1}^Kexp(\theta_i^Tx)}
$$

通过softmax函数，可以使得 $P(i)$ 的范围在[0,1]之间。在回归和分类问题中，通常θ是待求参数，通过寻找使得 $P(i)$ 最大的 $\theta_i$作为最佳参数。回顾logistic函数形式

$$
  P(i)=\frac{1}{1+exp(-\theta^T_ix)}
$$

这个函数的作用就是使得 $P(i)$ 在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。同样，softmax函数加入了e的幂函数正是为了两极化：正样本的结果将趋近于1，而负样本的结果趋近于0。

## softmax的可以作为最终概率的证明

虽然Softmax函数得到的是一个[0,1]之间的值，且 $\sum _{i=1}^KP(i)=1$ ，但是这个softmax求出的概率是否就是真正的概率？换句话说，这个概率是否严格等于期望呢？为此在这里进行推导。

假设现在有K个类，样本属于类别 $i$ 的概率为 $ϕ(i),i=1,…,K$ ,由于 $\sum _{i=1}^KP(i)=1$ ,所以只需要 **前K-1**个参数即可：   

$$
 \phi _i =P(y=i,\phi),i=1,...K-1.\quad \phi _k=1-\sum_{i=1}^{K-1} \phi _i
$$

先引入T(y)，它是一个k-1维的向量，如下所示：

$$
 T(1)=\begin {bmatrix}
  1\\ 0\\0\\0\\.\\.\\0\end{bmatrix}
  \quad
  T(2)=\begin {bmatrix}
   0\\ 1\\0\\0\\.\\.\\0\end{bmatrix}
   \quad
   ...
   T(k-1)=\begin {bmatrix}
    0\\ 0\\0\\0\\.\\.\\1\end{bmatrix}
    \quad
    T(k)=\begin {bmatrix}
     0\\ 0\\0\\0\\.\\.\\0\end{bmatrix}
$$

样本属于第i类则第 $i$ 行元素为1，其余为0，即：$(T(i))i=1$ (注意第i个列的第i行)。因为y只能属于1类，故(y不等于k时)T(y)只有一个元素为1，其余元素都为0，则y的期望为：

$$
  E(T(y))_i=P(y=i)=\phi_i,i\neq K
$$

令 $\beta_i=log\frac{\phi_i}{\phi_K},i=1,...,K$(同时除以     $\phi_K$ ),则有:

$$
  e^{\beta_i}=\frac{\phi_i}{\phi_K}\Longrightarrow\phi_K=\frac{\phi_i}{\beta_i}\Longrightarrow\phi_K\sum_i^Ke^{\beta_i}=\sum_i^K\phi_i=1
$$

**注意观察：第三个式子是在第一个式子的左右乘以 $\phi_K$ ,然后累加求和。（第二个式子用在第二步）**

所以：

$$
  \phi_k=\frac{1}{\sum_{i=1}Ke^{\beta _i}}
$$

此时再将 $\phi_K=\frac{\phi_i}{\beta}$ 带入（上面推导的第二个式子），有：

$$
  \phi_i=\frac{e^{\beta_i}}{\sum_{i=1}^Ke^{\beta_i}}
$$

由于分母中是求和操作，可以将i换成K。得到

$$
  \phi_i=\frac{e^{\beta_i}}{\sum_{k=1}^Ke^{\beta_k}}
$$

所以实际的期望是具有softmax函数的形式的，当 $f_i(x)=\beta_i=log\frac{\phi_i}{\phi_k}$ 时实际期望与softmax函数严格相等，所求概率为真实值。
