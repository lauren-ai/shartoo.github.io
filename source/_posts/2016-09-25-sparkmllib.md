---
layout:     post
title:      大数据：spark mllib集成学习
category: 大数据
description: 大数据
---

此文可以参考[如何在MLlib中实现随机森林和梯度提升树（GBTs）](http://blog.jobbole.com/85408/)一起阅读

# 一  梯度提升树和随机森林

&emsp;&emsp;梯度提升树(Gradient-Boosted Trees，GBTs)和随机森林都是决策树的集成学习方法，但是训练过程不一。以下是两种之间的一些利弊：
+ GBTs一次训练一颗树，所以它会比随机森林耗时更长。随机森林可以并行训练多颗树。
   + 另一方面，给GBTs使用比随机森林更小的树比较合理，训练更小的树耗时更少。
+ 随机森林更不易过拟合。训练更多的决策树可以减少随机森林过拟合风险，但会增加GBTs过拟合风险。
+ 由于增加随机森林使用的决策树数目可以单调的提升性能，因而森林更容易调节。但对于GBTs，决策树数目过大时可能会导致性能的减弱。

# 二 随机森林

在分类和回归中，随机森林是最成功的机器学习方法。它结合多颗决策树以减少过拟合风险。比如决策树，随机森林可以处理类别特征，如果不需要数据规范化（关于数据规范化，可以参考[数据规范化](http://blog.csdn.net/memray/article/details/9023737),数据规范化的好处参考
[为什么feature scaling会使 gradient desent收敛更好](https://www.zhihu.com/question/37129350/answer/70964527)）的话可以拓展到多分类，并且可以处理非线性和特征交互问题。
**spark.mllib** 可以同时使用连续型数据和类别特征，为分类和逻辑回归提供二分类和多分类。直接使用了现有的决策树实现了随机森林。

## 2.1 基本算法

随机森林单独的训练集合中每一课决策树，因而可以并行执行。算法在训练过程中引入了随机性，使得每个决策树都不一样。结合每棵树的决策可以减少最终决策偏差，提高算法最终表现。

## 2.2 训练数据

随机森林算法中加入的随机性包括以下：
+ 每次迭代时从原始数据集中抽样部分数据，以保证每次的数据不同。
+ 每次切分树节点时会考虑特征的随机子集。

## 2.3 预测

&emsp;&emsp;为了在新数据上作出预测，随机森林需要从其决策树集合中合计出预测，这个过程在分类和回归中是完全不同的。
+ 分类：多数表决，每棵树的预测都会为某一个分类投一票，得票最多的分类即预测分类。
+ 回归： 平均主义，每棵树预测值是一个实数，预测的分类为所有预测值的均值。

## 2.4 小提示

&emsp;&emsp;以下两个参数微调可以提高算法性能
+ **numTrees**:森林中的决策树数目。
    + 增加数数目可以减少预测偏差，提高模型的测试时间准确率。
    + 训练时长会随着树数目增加而大致线性增长
+ **maxDepth**:森林中每棵树的最大深度
    + 增加深度会更强大的模型，同时会增加消耗。但是更深的树，训练时间更长，同时更容易产生过拟合问题。
    + 通常来说，与单一决策树相比，随机森林总更适合训练更深的树。单一决策树更容易产生过拟合问题。

&emsp;&emsp;以下连个参数通常不需要调节，但是可以用来加速训练过程
+ **subsample**: 此参数用来设置随机森林中每棵树训练时使用的数据集大小，值为原始数据集比例。推荐默认值1.0，但是减少此值可以加速训练过程。
+ **featureSubsetStraegy**: 每个树节点分裂候选的特征数。该值设置为分数或者关于总特征数的函数。减少此值可以加速训练过程，但是太低的话可能会影响性能。

## 2.5 代码示例

以下代码演示了如何载入 **LIBSVM data file** ，将其解析成**LabeledPoint**类型的RDD，然后使用随机森林来分类。使用测试误差来衡量算法准确率

