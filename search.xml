<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>模型剪枝和优化-torch和Tensorflow为例</title>
    <url>/2019/11/26/model-pruning/</url>
    <content><![CDATA[<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1 基本概念"></a>1 基本概念</h2><h3 id="1-1-基本问题"><a href="#1-1-基本问题" class="headerlink" title="1.1 基本问题"></a>1.1 基本问题</h3><p>网络剪枝目标是</p>
<ul>
<li>更小的模型</li>
<li>更快的推理(inference)速度</li>
<li>不对准确率精度等造成过多损失</li>
</ul>
<p>相关技术有</p>
<ul>
<li>权重共享</li>
<li>量化(quantization)</li>
<li>低阶近似(Low-Rank Approximation)</li>
<li>二元/三元网络(Binary / Ternary Net)</li>
<li>Winograd Transformation</li>
</ul>
<h3 id="1-2-当前神经网络遇到的一些挑战"><a href="#1-2-当前神经网络遇到的一些挑战" class="headerlink" title="1.2 当前神经网络遇到的一些挑战"></a>1.2 当前神经网络遇到的一些挑战</h3><ol>
<li>模型变得越来越大</li>
</ol>
<p><img src="/images/blog/model_pruning_1.png" alt="模型剪枝和优化"></p>
<ol>
<li>速度越来越慢</li>
</ol>
<p><img src="/images/blog/model_pruning_2.png" alt="模型剪枝和优化"></p>
<ol>
<li>能源效率</li>
</ol>
<p>AlphaGo 使用了1920个CPU和280个GPU，每场比赛消耗3000美元的电力。</p>
<h3 id="1-3-网络剪枝的原理"><a href="#1-3-网络剪枝的原理" class="headerlink" title="1.3 网络剪枝的原理"></a>1.3 网络剪枝的原理</h3><p>将原本的稠密连接网络，删去不必要的连接，变成右边相对稀疏的网络。<strong>稀疏网络易于压缩，并且可以在预测时跳过零值，提高推理速度</strong>。</p>
<p><img src="/images/blog/model_pruning_3.png" alt="模型剪枝和优化"></p>
<p>如果可以对网络的所有神经元贡献度排序，我们可以删除排在末尾的神经元，这样可就可以减小网络获得更快的推理速度。</p>
<p>可以使用神经元的权重的L1/L2正则来做排序。剪枝之后，准确率将会降低。通常会执行<code>训练$$\rightarrow$$剪枝$$\rightarrow$$训练$$\rightarrow$$剪枝</code>..的循环中。如果一次剪枝过多，网络可能会损坏，无法恢复。所以在实践中，这是一个迭代执行的步骤。</p>
<h2 id="2-剪枝技术"><a href="#2-剪枝技术" class="headerlink" title="2 剪枝技术"></a>2 剪枝技术</h2><h3 id="2-1-权重剪枝"><a href="#2-1-权重剪枝" class="headerlink" title="2.1 权重剪枝"></a>2.1 权重剪枝</h3><ul>
<li>将权重矩阵中孤立(没有与其他权重项有连接的)的权重设置为0。这对应着上图中删除了连接</li>
<li>此处，为了达到k%的稀疏度，我们将孤立的权重排序。在权重矩阵中，W对应了梯度，然后将最小的k%设置为0。下面的代码演示了这个过程</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f &#x3D; h5py.File(&quot;model_weights.h5&quot;,&#39;r+&#39;)</span><br><span class="line">for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:</span><br><span class="line"> ranks &#x3D; &#123;&#125;</span><br><span class="line"> for l in list(f[‘model_weights’])[:-1]:</span><br><span class="line"> data &#x3D; f[‘model_weights’][l][l][‘kernel:0’]</span><br><span class="line"> w &#x3D; np.array(data)</span><br><span class="line"> ranks[l]&#x3D;(rankdata(np.abs(w),method&#x3D;’dense’) — 1).astype(int).reshape(w.shape)</span><br><span class="line"> lower_bound_rank &#x3D; np.ceil(np.max(ranks[l])*k).astype(int)</span><br><span class="line"> ranks[l][ranks[l]&lt;&#x3D;lower_bound_rank] &#x3D; 0</span><br><span class="line"> ranks[l][ranks[l]&gt;lower_bound_rank] &#x3D; 1</span><br><span class="line"> w &#x3D; w*ranks[l]</span><br><span class="line"> data[…] &#x3D; w</span><br></pre></td></tr></table></figure>
<h3 id="2-2-神经元剪枝"><a href="#2-2-神经元剪枝" class="headerlink" title="2.2 神经元剪枝"></a>2.2 神经元剪枝</h3><ul>
<li>将神经元对应的权重矩阵中的一整列的值全部设为0，这等同于删除了对应的输出神经元</li>
<li>此处，要达到k%的稀疏度，我们对权重矩阵的列排序，排序规则是它们的L2正则，然后删除最小的k%。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f &#x3D; h5py.File(&quot;model_weights.h5&quot;,&#39;r+&#39;)</span><br><span class="line">for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:</span><br><span class="line"> ranks &#x3D; &#123;&#125;</span><br><span class="line"> for l in list(f[‘model_weights’])[:-1]:</span><br><span class="line">     data &#x3D; f[‘model_weights’][l][l][‘kernel:0’]</span><br><span class="line">     w &#x3D; np.array(data)</span><br><span class="line">     norm &#x3D; LA.norm(w,axis&#x3D;0)</span><br><span class="line">     norm &#x3D; np.tile(norm,(w.shape[0],1))</span><br><span class="line">     ranks[l] &#x3D; (rankdata(norm,method&#x3D;’dense’) — 1).astype(int).reshape(norm.shape)</span><br><span class="line">     lower_bound_rank &#x3D; np.ceil(np.max(ranks[l])*k).astype(int)</span><br><span class="line">     ranks[l][ranks[l]&lt;&#x3D;lower_bound_rank] &#x3D; 0</span><br><span class="line">     ranks[l][ranks[l]&gt;lower_bound_rank] &#x3D; 1</span><br><span class="line">     w &#x3D; w*ranks[l]</span><br><span class="line">     data[…] &#x3D; w</span><br></pre></td></tr></table></figure>
<p>通常随着你增加稀疏度，并且删除越来越多的神经元，模型的性能会下降，此时就需要对模型性能和稀疏度作出取舍了。</p>
<h3 id="2-3-权重稀疏和神经元稀疏的对比"><a href="#2-3-权重稀疏和神经元稀疏的对比" class="headerlink" title="2.3 权重稀疏和神经元稀疏的对比"></a>2.3 权重稀疏和神经元稀疏的对比</h3><p><img src="/images/blog/model_pruning_4.png" alt="模型剪枝和优化"></p>
<p>看起来权重稀疏更柔和一些。</p>
<p><img src="/images/blog/model_pruning_5.png" alt="模型剪枝和优化"></p>
<p>权重稀疏和神经元稀疏在减小网络尺寸上效果相同。</p>
<h3 id="2-4-剪枝的问题"><a href="#2-4-剪枝的问题" class="headerlink" title="2.4 剪枝的问题"></a>2.4 剪枝的问题</h3><p>参考自<a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" target="_blank" rel="noopener">Pruning deep neural networks to make them fast and small</a>，说明尽管有诸多剪枝的论文，但是在现实世界里很少使用剪枝，究其原因，可能有如下</p>
<ul>
<li>按照贡献度排序的方法目前为止上不够完善，精度损失过高</li>
<li>难以实现</li>
<li>一些公司使用了剪枝技术，但是没有公开这个秘密</li>
</ul>
<h2 id="3-剪枝实践"><a href="#3-剪枝实践" class="headerlink" title="3 剪枝实践"></a>3 剪枝实践</h2><h3 id="3-1-剪枝为了速度VS为了更小的模型"><a href="#3-1-剪枝为了速度VS为了更小的模型" class="headerlink" title="3.1 剪枝为了速度VS为了更小的模型"></a>3.1 剪枝为了速度VS为了更小的模型</h3><p>VGG模型90%的权重在后面的全连接层，但是只贡献了1%的浮点运算。最近，人们才开始专注裁剪全连接层，通过替换全连接层模型尺寸会大幅度缩减。此处只关注于裁剪整个卷积层，但是它有个很好的副作用就是同事减小了内存消耗，如论文<a href="https://arxiv.org/abs/1611.06440" target="_blank" rel="noopener">1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference</a>所述，网络层越深，越容易被裁剪。这表明最后的卷积层会大幅度被裁剪，全连接后面的诸多神经元也会被抛弃。</p>
<p>对卷积层裁剪时，同时也可以对每个卷积核做权重衰减，或者移除某个卷积核的某个特定维度(列)，这样会得到稀疏的卷积核，这么得来的结果无法得到计算速度的提升。最近的研究提倡<code>结构稀疏</code>,即整个卷积核被裁剪掉。</p>
<p>另外一个重要提示是<strong>通过训练然后裁剪一个大网络，尤其在迁移学习时，其结果比从头训练一个小网络要好得多</strong></p>
<h3 id="3-2-裁剪卷积核"><a href="#3-2-裁剪卷积核" class="headerlink" title="3.2 裁剪卷积核"></a>3.2 裁剪卷积核</h3><p>参考论文<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="noopener">Pruning filters for effecient convents</a>.</p>
<p>此论文提倡裁剪掉整个卷积核。裁剪一个卷积核的索引k，影响的是它所在网络层，以及后续的网络层。所有在索引k处的输入通道，在后续网络层会被移除掉，如下图。</p>
<p><img src="/images/blog/model_pruning_6.png" alt="模型剪枝和优化"></p>
<p>假若后续层是全连接层，以及feature map的通道的尺寸会是$M\times N$，那么将会从全连接层中移除$M\times N$个神经元。</p>
<p><strong>神经元的排序相当简单，即它们每个卷积核的权重的L1 norm。</strong></p>
<p>每次剪枝迭代都会对所有卷积核的权重L1 norm排序，裁剪掉末尾的m个filter，重新训练，并重复。</p>
<h3 id="3-3-结构剪枝"><a href="#3-3-结构剪枝" class="headerlink" title="3.3 结构剪枝"></a>3.3 结构剪枝</h3><p>参考论文<a href="https://arxiv.org/abs/1512.08571" target="_blank" rel="noopener">1512.08571 Structured Pruning of Deep Convolutional Neural Networks</a></p>
<p>论文内容与上面差不多，但是排序算法复杂得多。论文使用了一个有N个粒子过滤器(particle filters)的集合，保存了N个即将被裁剪的卷积核。</p>
<p>如果粒子(particle)所代表的卷积核没有被mask划出，每个粒子(particle)被分配一个基于网络在验证集上准确率的得分。然后基于新的得分，会得到新的裁剪mask。<br>由于此步骤执行起来相对繁琐，论文使用了较小的验证集以衡量粒子得分。</p>
<h3 id="3-4-nvidia裁剪：卷积核裁剪以提升资源推理效率-Resource-Efficient-Inference"><a href="#3-4-nvidia裁剪：卷积核裁剪以提升资源推理效率-Resource-Efficient-Inference" class="headerlink" title="3.4 nvidia裁剪：卷积核裁剪以提升资源推理效率(Resource Efficient Inference)"></a>3.4 nvidia裁剪：卷积核裁剪以提升资源推理效率(Resource Efficient Inference)</h3><p>参考论文<a href="https://arxiv.org/abs/1611.06440" target="_blank" rel="noopener">1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference</a>。</p>
<p>首先，他们提出了<strong>将一个裁剪问题视为某种优化问题：选取权重B的子集，如果裁剪它们使得网络的损失变化得最小</strong></p>
<script type="math/tex; mode=display">
min _{w'}|C(D|W')-C(D|W)|\quad s.t\quad ||W'||_0\le B</script><p>注意：使用的是绝对值差异而非简单的差异，这样裁剪网络不会太多地缩减网络的性能，但是也应该不会增加。</p>
<p>这样一来，所有的排序方法可以使用此损失函数来衡量了。</p>
<h3 id="3-5-Oracle裁剪"><a href="#3-5-Oracle裁剪" class="headerlink" title="3.5  Oracle裁剪"></a>3.5  Oracle裁剪</h3><p>VGG16有4224个卷积核，完美的排序方法应该使用暴力裁剪每个卷积核，然后观察在训练集上损失函数变化，此方法称为oracle排序，最可能的排序方法。为了衡量其他排序方法的的效率，他们计算了其他方法与oracle的speraman协相关系数。令人惊讶的是，它们想到的排序方法(下文提到)与oracle协相关程度最高。</p>
<p>它们想到一个新的基于损失函数的泰勒一阶展开(代表最快的计算)神经元排序方法，裁剪一个卷积核$h$与将其清零相同。</p>
<p>$C(W,D)$是网络权重被设为W时在数据集D上的平均损失。现在，我们可以评估$C(W,D)$的在$C(W,D,h=0)$处的展开，它们 应该十分相近，因为移除单一卷积核不会对损失值造成太大影响。</p>
<p>$h$的排序为$C(W,D,h=0)-C(W,D)$的绝对值。</p>
<script type="math/tex; mode=display">
\Theta _{TE}(h_i)=|\triangle C(h_i)|=|C(D,h_i)-\frac{\partial C}{\partial h_i}h_i-C(D,h_i)|=|\frac{\partial C}{\partial h_i}h_i|\\
\Theta _{TE}(z_l ^{k})=|\frac{1}{M}\sum_m \frac{\partial C}{\partial z_{l,m} ^{(k)}}z_{l,m} ^{(k)}</script><p>每一层的排序都会那一整层的排序的L2 norm的排序再次normalized。这有点经验主义，不太确定是否真有必要，但是极大地影响剪枝质量。</p>
<p>这种排序是相当直觉性的，我们不能同时使用排序方法本身所使用的激活函数、梯度。如果(激活函数、梯度)任意一个很高，代表其对输出有较大影响。将它们相乘，根据梯度或者激活函数值非常高或低，可以让我们得以衡量，是抛弃还是继续保留该卷积核。</p>
<p>这让我很好奇，他们到底有没有将剪枝问题视为最小化网络损失函数值差异，然后想出的泰勒展开式，还是说相反的，网络损失值差异是他们的某种备份的新方法。</p>
<h2 id="4-剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则"><a href="#4-剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则" class="headerlink" title="4 剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则"></a>4 剪枝实践：对一个猫狗二分类器裁剪，使用泰勒展开式为排序准则</h2><p>使用1000张狗和1000张猫的图片，对VGG模型做迁移学习训练。猫狗图片来自<a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="noopener">kaggle猫狗分类</a>,使用400张猫和400张狗的图片作为测试集。</p>
<h3 id="4-1-剪枝之后的结果说明"><a href="#4-1-剪枝之后的结果说明" class="headerlink" title="4.1 剪枝之后的结果说明"></a>4.1 剪枝之后的结果说明</h3><ul>
<li>准确率从98.7%掉到97.5%</li>
<li>网络模型从538MB减小到150MB</li>
<li>在i7 CPU上推理时间从0.78秒减小到0.227秒。基本是原来的三分之一</li>
</ul>
<h3 id="4-2-第一步-训练一个大网络"><a href="#4-2-第一步-训练一个大网络" class="headerlink" title="4.2 第一步:训练一个大网络"></a>4.2 第一步:训练一个大网络</h3><p>使用一个VGG16，然后丢弃最后三个全连接层，然后添加新的三个全连接层，此过程会freeze所有的卷积层，只训练新的三个全连接层。</p>
<p>我们先准备数据集，从kaggle下载数据之后，从总分别选取1400张猫和1400张狗，其中1000张猫和1000张狗作为训练集，放在<code>train1000</code>目录下的<code>cat</code>和<code>dog</code>目录下，另外的400张猫和400张狗放在<code>val</code>目录下的<code>cat</code>和<code>dog</code>目录下。使用Tensorflow2.0的代码示例如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from keras_applications.vgg16 import VGG16</span><br><span class="line">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><br><span class="line">from tensorflow.keras.optimizers import Adam</span><br><span class="line">from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,ReduceLROnPlateau,Callback</span><br><span class="line"></span><br><span class="line">## global parameters</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-4</span><br><span class="line">input_width,input_height &#x3D; 224,224</span><br><span class="line"></span><br><span class="line">weight_save_path &#x3D; &quot;.&#x2F;vgg16_catdog_weights&#x2F;&quot;</span><br><span class="line">record_save_path &#x3D; &quot;.&#x2F;vgg16_catdog_tensorboard&#x2F;&quot;</span><br><span class="line">model_weight_file &#x3D; weight_save_path + &quot;vgg16_catdog_binary.h5&quot;</span><br><span class="line">optimizer &#x3D; Adam(lr&#x3D;0.0001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08)</span><br><span class="line"># Callback for early stopping the training</span><br><span class="line">early_stopping &#x3D; tf.keras.callbacks.EarlyStopping(monitor&#x3D;&#39;val_accuracy&#39;, min_delta&#x3D;0, patience&#x3D;15, verbose&#x3D;1, mode&#x3D;&#39;auto&#39;)</span><br><span class="line"># set model checkpoint callback (model weights will auto save in weight_save_path)</span><br><span class="line">checkpoint &#x3D; ModelCheckpoint(model_weight_file, monitor&#x3D;&#39;val_accuracy&#39;, verbose&#x3D;1, save_best_only&#x3D;True, mode&#x3D;&#39;max&#39;, period&#x3D;1)</span><br><span class="line"># monitor a learning indicator(reduce learning rate when learning effect is stagnant)</span><br><span class="line">reduceLRcallback &#x3D; ReduceLROnPlateau(monitor&#x3D;&#39;val_acc&#39;, factor&#x3D;0.7, patience&#x3D;5,</span><br><span class="line">                                     verbose&#x3D;1, mode&#x3D;&#39;auto&#39;, cooldown&#x3D;0, min_lr&#x3D;0)</span><br><span class="line"></span><br><span class="line">class LossHistory(Callback):</span><br><span class="line">    def on_train_begin(self, logs&#x3D;&#123;&#125;):</span><br><span class="line">        self.losses &#x3D; []</span><br><span class="line">        self.val_losses &#x3D; []</span><br><span class="line">        self.acc &#x3D; []</span><br><span class="line">        self.val_acc &#x3D; []</span><br><span class="line">        self.recall &#x3D; []</span><br><span class="line"></span><br><span class="line">    def on_epoch_end(self, batch, logs&#x3D;&#123;&#125;):</span><br><span class="line">        self.losses.append(logs.get(&#39;loss&#39;))</span><br><span class="line">        self.val_losses.append(logs.get(&#39;val_loss&#39;))</span><br><span class="line">        self.acc.append(logs.get(&#39;acc&#39;))</span><br><span class="line">        self.val_acc.append(logs.get(&#39;val_accuracy&#39;))</span><br><span class="line">        self.recall.append(logs.get(&#39;recall&#39;))</span><br><span class="line"></span><br><span class="line">def build_model(input_width,input_height,drop_prob&#x3D;0.5):</span><br><span class="line">    vgg &#x3D; VGG16(include_top&#x3D;False,weights&#x3D;&quot;imagenet&quot;,classes&#x3D;2,input_shape&#x3D;(input_width,input_height,3),backend &#x3D; tf.keras.backend, layers &#x3D; tf.keras.layers, models &#x3D; tf.keras.models, utils &#x3D; tf.keras.utils)</span><br><span class="line">    for layer in vgg.layers:</span><br><span class="line">        layer.trainable &#x3D;False</span><br><span class="line">    print(vgg.summary())</span><br><span class="line">    out &#x3D; tf.keras.layers.Flatten()(vgg.output)</span><br><span class="line">    dense1 &#x3D;tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;)(out)</span><br><span class="line">    drop1 &#x3D; tf.keras.layers.Dropout(drop_prob)(dense1)</span><br><span class="line">    dense2 &#x3D;tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;)(drop1)</span><br><span class="line">    drop2 &#x3D; tf.keras.layers.Dropout(drop_prob)(dense2)</span><br><span class="line">    dense3 &#x3D;tf.keras.layers.Dense(1,activation&#x3D;&quot;sigmoid&quot;)(drop2)</span><br><span class="line">    merged_model &#x3D; tf.keras.models.Model(vgg.input,dense3)</span><br><span class="line">    print(merged_model.summary())</span><br><span class="line">    return merged_model</span><br><span class="line"></span><br><span class="line">def train_val_generator(train_img_path,val_img_path):</span><br><span class="line">    train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1 &#x2F; 255.,</span><br><span class="line">                                           rotation_range&#x3D;45,</span><br><span class="line">                                           width_shift_range&#x3D;0.2,</span><br><span class="line">                                           # degree of horizontal offset(a ratio relative to image width)</span><br><span class="line">                                           height_shift_range&#x3D;0.2,</span><br><span class="line">                                           # degree of vertical offset(a ratio relatice to image height)</span><br><span class="line">                                           shear_range&#x3D;0.2, # the range of shear transformation(a ratio in 0 ~ 1)</span><br><span class="line">                                           zoom_range&#x3D;0.25,</span><br><span class="line">                                           # degree of random zoom(the zoom range will be [1 - zoom_range, 1 + zoom_range])</span><br><span class="line">                                           horizontal_flip&#x3D;True, # whether to perform horizontal flip</span><br><span class="line">                                           vertical_flip&#x3D;True, # whether to perform vertical flip</span><br><span class="line">                                           fill_mode&#x3D;&#39;nearest&#39; # mode list: nearest, constant, reflect, wrap</span><br><span class="line">                                           )</span><br><span class="line">    val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1 &#x2F; 255.)</span><br><span class="line"></span><br><span class="line">    train_generator &#x3D; train_datagen.flow_from_directory(</span><br><span class="line">            train_img_path,</span><br><span class="line">            shuffle&#x3D;True,</span><br><span class="line">            target_size&#x3D;(input_width,input_height),</span><br><span class="line">            batch_size&#x3D;batch_size,</span><br><span class="line">            class_mode&#x3D;&#39;binary&#39;)</span><br><span class="line"></span><br><span class="line">    validation_generator &#x3D; val_datagen.flow_from_directory(</span><br><span class="line">            val_img_path,</span><br><span class="line">            target_size&#x3D;(input_width,input_height),</span><br><span class="line">            batch_size&#x3D;batch_size,</span><br><span class="line">            class_mode&#x3D;&#39;binary&#39;)</span><br><span class="line">    return train_generator,validation_generator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_model(train_img_path,val_img_path,batch_size,epochs):</span><br><span class="line">    if os.path.exists(model_weight_file):</span><br><span class="line">        model &#x3D; tf.keras.models.load_model(model_weight_file)</span><br><span class="line">    else:</span><br><span class="line">        model &#x3D; build_model(input_width,input_height)</span><br><span class="line">        model.compile(optimizer&#x3D;optimizer,</span><br><span class="line">                      loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">                      metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">    train_generator, validation_generator &#x3D; train_val_generator(train_img_path,val_img_path)</span><br><span class="line">    train_sample_count &#x3D; len(train_generator.filenames)</span><br><span class="line">    val_sample_count &#x3D; len(validation_generator.filenames)</span><br><span class="line">    print(train_sample_count, val_sample_count)</span><br><span class="line">    history &#x3D; LossHistory()</span><br><span class="line">    model.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch&#x3D;int(train_sample_count &#x2F; batch_size) + 1,</span><br><span class="line">        epochs&#x3D;epochs,</span><br><span class="line">        validation_data&#x3D;validation_generator,</span><br><span class="line">        validation_steps&#x3D;int(val_sample_count &#x2F; batch_size) + 1,</span><br><span class="line">        callbacks&#x3D;[TensorBoard(log_dir&#x3D;record_save_path), early_stopping, history, checkpoint, reduceLRcallback]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    train_set_path &#x3D; &#39;E:&#x2F;data&#x2F;images&#x2F;dogs-vs-cats&#x2F;train1000&#39;</span><br><span class="line">    valid_set_path &#x3D; &#39;E:&#x2F;data&#x2F;images&#x2F;dogs-vs-cats&#x2F;val&#39;</span><br><span class="line">    batch_size &#x3D; 8</span><br><span class="line">    epochs &#x3D; 20</span><br><span class="line">    train_model(train_set_path, valid_set_path, batch_size,epochs)</span><br></pre></td></tr></table></figure>
<p>最后的准确率，没有作者那么高，只有90%，如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">236&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3744 - accuracy: 0.8231</span><br><span class="line">237&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3738 - accuracy: 0.8233</span><br><span class="line">238&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 2s - loss: 0.3747 - accuracy: 0.8230</span><br><span class="line">239&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3746 - accuracy: 0.8232</span><br><span class="line">240&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3746 - accuracy: 0.8229</span><br><span class="line">241&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3747 - accuracy: 0.8231</span><br><span class="line">242&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 1s - loss: 0.3738 - accuracy: 0.8239</span><br><span class="line">243&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 1s - loss: 0.3735 - accuracy: 0.8236</span><br><span class="line">244&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 1s - loss: 0.3728 - accuracy: 0.8243</span><br><span class="line">245&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8240</span><br><span class="line">246&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8242</span><br><span class="line">247&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8239</span><br><span class="line">248&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3746 - accuracy: 0.8231</span><br><span class="line">249&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8228</span><br><span class="line">250&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8235</span><br><span class="line">Epoch 00020: val_accuracy did not improve from 0.90274</span><br><span class="line"></span><br><span class="line">251&#x2F;251 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 47s 188ms&#x2F;step - loss: 0.3742 - accuracy: 0.8227 - val_loss: 0.2761 - val_accuracy: 0.8815</span><br></pre></td></tr></table></figure>
<p>查看对应的验证集的tensorboard如下</p>
<p><img src="/images/blog/model_pruning_7.jpg" alt="模型剪枝和优化"></p>
<h3 id="4-3-对卷积核排序"><a href="#4-3-对卷积核排序" class="headerlink" title="4.3  对卷积核排序"></a>4.3  对卷积核排序</h3><p>为了计算泰勒展开指标，我们需要在数据集上做一个<code>前向+后向传播</code>(可以在一个较小的数据集上)。</p>
<p>现在需要获取卷积层的梯度和激活函数。可以在梯度计算时注册一个hook，当这些东西就绪时会调用这个callback。</p>
<p>现在，我们可以从<code>self.activations</code>中获得激活函数值，当梯度就绪时会执行计算排序的方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def compute_rank(self, grad):</span><br><span class="line"> activation_index &#x3D; len(self.activations) - self.grad_index - 1</span><br><span class="line"> activation &#x3D; self.activations[activation_index]</span><br><span class="line"> values &#x3D; \</span><br><span class="line">  torch.sum((activation * grad), dim &#x3D; 0).\</span><br><span class="line">   sum(dim&#x3D;2).sum(dim&#x3D;3)[0, :, 0, 0].data</span><br><span class="line">	</span><br><span class="line"> # Normalize the rank by the filter dimensions</span><br><span class="line"> values &#x3D; \</span><br><span class="line">  values &#x2F; (activation.size(0) * activation.size(2) * activation.size(3))</span><br><span class="line"></span><br><span class="line"> if activation_index not in self.filter_ranks:</span><br><span class="line">  self.filter_ranks[activation_index] &#x3D; \</span><br><span class="line">   torch.FloatTensor(activation.size(1)).zero_().cuda()</span><br><span class="line"></span><br><span class="line"> self.filter_ranks[activation_index] +&#x3D; values</span><br><span class="line"> self.grad_index +&#x3D; 1</span><br></pre></td></tr></table></figure>
<h2 id="5-剪枝实践：使用Tensorflow-训练剪枝MNIST模型为例"><a href="#5-剪枝实践：使用Tensorflow-训练剪枝MNIST模型为例" class="headerlink" title="5. 剪枝实践：使用Tensorflow 训练剪枝MNIST模型为例"></a>5. 剪枝实践：使用Tensorflow 训练剪枝MNIST模型为例</h2><p>下面使用tensorflow api为例，其他API也有类似功能。基于keras api的权重剪枝，在训练过程中迭代的删除一些没用的连接，基于连接的梯度。下面示范通过简单的使用一种通用文件压缩算法(如zip压缩)，就可以缩减keras模型</p>
<h3 id="5-1-训练一个剪枝的模型"><a href="#5-1-训练一个剪枝的模型" class="headerlink" title="5.1 训练一个剪枝的模型"></a>5.1 训练一个剪枝的模型</h3><p>tensorflow提供一个<code>prune_low_magnitude()</code>的API来训练模型，模型中会移除一些连接。基于Keras的API可以应用于独立的网络层，或者整个网络。在高层级，此技术是在给定规划和目标稀疏度的前提下，通过迭代的移除(即zeroing out)网络层之间的连接。</p>
<p>例如，典型的配置是目标稀疏度为75%，通过每迭代100步(epoch)裁剪一些连接，从第2000步(epoch)开始。更多配置需要查看官方文档。</p>
<h3 id="5-2-一层一层的构建一个剪枝的模型"><a href="#5-2-一层一层的构建一个剪枝的模型" class="headerlink" title="5.2 一层一层的构建一个剪枝的模型"></a>5.2 一层一层的构建一个剪枝的模型</h3><p>下面展示如何在网络层层面使用API，构建一个剪枝的分类模型。</p>
<ul>
<li>此时，<code>prune_low_magnitude()</code>接收一个想要被裁剪的网络层作为参数。</li>
<li>此函数需要一个剪枝参数，配置的是在训练过程中的剪枝算法。以下是相关参数的意义<ul>
<li><strong>Sparsity</strong>: 整个训练过程中使用的是多项式递减(PolynomialDecay)。从50%的稀疏度开始，然后逐渐地训练模型以达到90%的稀疏度。x%的稀疏度代表x%的权重标量将会被裁剪掉</li>
<li><strong>Schedule</strong>：从第2000步开始到训练结束，网络层之间的连接会逐渐被裁剪掉，并且是每100步执行一次。究其原因是，要训练一个在几个步骤内稳定达到一定准确率的模型，以帮助其收敛。同时，也让模型在每次裁剪之后能恢复，所以并不是每一步都要裁剪。我们可以将裁剪频率设为100.</li>
</ul>
</li>
</ul>
<p>为了演示如何保存并重新载入裁剪的模型，我们先训练一个模型10个epoch，保存，然后载入模型并继续训练2个epoch。逐渐地稀疏，四个重要参数是<strong><code>begin_sparsity</code>,<code>final_sparsity</code>,<code>begin_step</code>,<code>end_step</code></strong>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from tensorflow_model_optimization.sparsity import keras as sparsity</span><br><span class="line"></span><br><span class="line">epochs &#x3D; 12</span><br><span class="line">l &#x3D; tf.keras.layers</span><br><span class="line">num_train_samples &#x3D; x_train.shape[0]</span><br><span class="line">end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">print(&#39;End step: &#39; + str(end_step))</span><br><span class="line">pruning_params &#x3D; &#123;</span><br><span class="line">      &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                   final_sparsity&#x3D;0.90,</span><br><span class="line">                                                   begin_step&#x3D;2000,</span><br><span class="line">                                                   end_step&#x3D;end_step,</span><br><span class="line">                                                   frequency&#x3D;100)</span><br><span class="line">&#125;</span><br><span class="line">pruned_model &#x3D; tf.keras.Sequential([</span><br><span class="line">    sparsity.prune_low_magnitude(</span><br><span class="line">        l.Conv2D(32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        input_shape&#x3D;input_shape,</span><br><span class="line">        **pruning_params),</span><br><span class="line">    l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">    l.BatchNormalization(),</span><br><span class="line">    sparsity.prune_low_magnitude(</span><br><span class="line">        l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;), **pruning_params),</span><br><span class="line">    l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">    l.Flatten(),</span><br><span class="line">    sparsity.prune_low_magnitude(l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">                                 **pruning_params),</span><br><span class="line">    l.Dropout(0.4),</span><br><span class="line">    sparsity.prune_low_magnitude(l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;),</span><br><span class="line">                                 **pruning_params)</span><br><span class="line">])</span><br><span class="line">pruned_model.summary()</span><br></pre></td></tr></table></figure>
<p>作为对比，我们训练了一个MNSIT数据集的分类模型，首先，我们准备的数据和参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tempfile</span><br><span class="line">import zipfile</span><br><span class="line">import os</span><br><span class="line">import tensorboard</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow_model_optimization.sparsity import keras as sparsity</span><br><span class="line"></span><br><span class="line">## global parameters</span><br><span class="line">batch_size &#x3D; 128</span><br><span class="line">num_classes &#x3D; 10</span><br><span class="line">epochs &#x3D; 10</span><br><span class="line"># input image dimensions</span><br><span class="line">img_rows, img_cols &#x3D; 28, 28</span><br><span class="line">logdir &#x3D; tempfile.mkdtemp()</span><br><span class="line">print(&#39;Writing training logs to &#39; + logdir)</span><br><span class="line"></span><br><span class="line">def prepare_trainval(img_rows, img_cols):</span><br><span class="line">    # the data, shuffled and split between train and test sets</span><br><span class="line">    (x_train, y_train), (x_test, y_test) &#x3D; tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    if tf.keras.backend.image_data_format() &#x3D;&#x3D; &#39;channels_first&#39;:</span><br><span class="line">      x_train &#x3D; x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)</span><br><span class="line">      x_test &#x3D; x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)</span><br><span class="line">      input_shape &#x3D; (1, img_rows, img_cols)</span><br><span class="line">    else:</span><br><span class="line">      x_train &#x3D; x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)</span><br><span class="line">      x_test &#x3D; x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)</span><br><span class="line">      input_shape &#x3D; (img_rows, img_cols, 1)</span><br><span class="line"></span><br><span class="line">    x_train &#x3D; x_train.astype(&#39;float32&#39;)</span><br><span class="line">    x_test &#x3D; x_test.astype(&#39;float32&#39;)</span><br><span class="line">    x_train &#x2F;&#x3D; 255</span><br><span class="line">    x_test &#x2F;&#x3D; 255</span><br><span class="line">    print(&#39;x_train shape:&#39;, x_train.shape)</span><br><span class="line">    print(x_train.shape[0], &#39;train samples&#39;)</span><br><span class="line">    print(x_test.shape[0], &#39;test samples&#39;)</span><br><span class="line"></span><br><span class="line">    # convert class vectors to binary class matrices</span><br><span class="line">    y_train &#x3D; tf.keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">    y_test &#x3D; tf.keras.utils.to_categorical(y_test, num_classes)</span><br><span class="line">    return x_train,x_test,y_train,y_test</span><br></pre></td></tr></table></figure>
<h4 id="5-2-1-构建原始的MNIST分类模型"><a href="#5-2-1-构建原始的MNIST分类模型" class="headerlink" title="5.2.1 构建原始的MNIST分类模型"></a>5.2.1 构建原始的MNIST分类模型</h4><p>使用keras构建一个简单的keras模型如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def build_clean_model(input_shape):</span><br><span class="line">    l &#x3D; tf.keras.layers</span><br><span class="line">    model &#x3D; tf.keras.Sequential([</span><br><span class="line">        l.Conv2D(</span><br><span class="line">            32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;, input_shape&#x3D;input_shape),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.BatchNormalization(),</span><br><span class="line">        l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.Flatten(),</span><br><span class="line">        l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        l.Dropout(0.4),</span><br><span class="line">        l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;)</span><br><span class="line">    ])</span><br><span class="line">    model.compile(</span><br><span class="line">        loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">        optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">        metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">    model.summary()</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure><br>训练模型代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train_clean_model(x_train,x_test,y_train,y_test,epochs,ori_mnist_model_file):</span><br><span class="line">    callbacks &#x3D; [tf.keras.callbacks.TensorBoard(log_dir&#x3D;logdir, profile_batch&#x3D;0)]</span><br><span class="line">    input_shape &#x3D; (img_rows, img_cols, 1)</span><br><span class="line">    model &#x3D; build_clean_model(input_shape)</span><br><span class="line">    model.fit(x_train, y_train,</span><br><span class="line">              batch_size&#x3D;batch_size,</span><br><span class="line">              epochs&#x3D;epochs,</span><br><span class="line">              verbose&#x3D;1,</span><br><span class="line">              callbacks&#x3D;callbacks,</span><br><span class="line">              validation_data&#x3D;(x_test, y_test))</span><br><span class="line">    score &#x3D; model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">    print(&#39;Saving model to: &#39;,ori_mnist_model_file)</span><br><span class="line">    tf.keras.models.save_model(model,ori_mnist_model_file, include_optimizer&#x3D;False)</span><br><span class="line">    print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">    print(&#39;Test accuracy:&#39;, score[1])</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test &#x3D; prepare_trainval(img_rows, img_cols)</span><br><span class="line">ori_mnist_model_file &#x3D; &quot;.&#x2F;ori_mnist_classifier.h5&quot;</span><br><span class="line">train_clean_model(x_train,x_test,y_train,y_test,epochs,ori_mnist_model_file)</span><br></pre></td></tr></table></figure>
<p>模型训练结果输出:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">45568&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;........] - ETA: 0s - loss: 0.0119 - accuracy: 0.9962</span><br><span class="line">46720&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.......] - ETA: 0s - loss: 0.0120 - accuracy: 0.9962</span><br><span class="line">47872&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.......] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961</span><br><span class="line">49024&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;......] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">50176&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.....] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">51328&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.....] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961</span><br><span class="line">52480&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0123 - accuracy: 0.9961</span><br><span class="line">53632&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0121 - accuracy: 0.9962</span><br><span class="line">54784&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0125 - accuracy: 0.9961</span><br><span class="line">56064&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">57216&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">58368&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9961</span><br><span class="line">59520&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9962</span><br><span class="line">60000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 3s 49us&#x2F;sample - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0297 - val_accuracy: 0.9919</span><br><span class="line">Saving model to: .&#x2F;ori_mnist_classifier.h5</span><br><span class="line">Test loss: 0.029679151664800906</span><br><span class="line">Test accuracy: 0.9919</span><br></pre></td></tr></table></figure>
<h4 id="5-2-2-构建剪枝的MNIST分类模型"><a href="#5-2-2-构建剪枝的MNIST分类模型" class="headerlink" title="5.2.2 构建剪枝的MNIST分类模型"></a>5.2.2 构建剪枝的MNIST分类模型</h4><p>注意和上面的5.2.1构建原始分类模型的代码对比<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def build_prune_model(input_shape,end_step):</span><br><span class="line">    l &#x3D; tf.keras.layers</span><br><span class="line">    print(&#39;End step: &#39; + str(end_step))</span><br><span class="line">    pruning_params &#x3D; &#123;</span><br><span class="line">          &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                       final_sparsity&#x3D;0.90,</span><br><span class="line">                                                       begin_step&#x3D;2000,</span><br><span class="line">                                                       end_step&#x3D;end_step,</span><br><span class="line">                                                       frequency&#x3D;100)</span><br><span class="line">    &#125;</span><br><span class="line">    pruned_model &#x3D; tf.keras.Sequential([</span><br><span class="line">        sparsity.prune_low_magnitude(</span><br><span class="line">            l.Conv2D(32, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">            input_shape&#x3D;input_shape,</span><br><span class="line">            **pruning_params),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.BatchNormalization(),</span><br><span class="line">        sparsity.prune_low_magnitude(</span><br><span class="line">            l.Conv2D(64, 5, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;), **pruning_params),</span><br><span class="line">        l.MaxPooling2D((2, 2), (2, 2), padding&#x3D;&#39;same&#39;),</span><br><span class="line">        l.Flatten(),</span><br><span class="line">        sparsity.prune_low_magnitude(l.Dense(1024, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">                                     **pruning_params),</span><br><span class="line">        l.Dropout(0.4),</span><br><span class="line">        sparsity.prune_low_magnitude(l.Dense(num_classes, activation&#x3D;&#39;softmax&#39;),</span><br><span class="line">                                     **pruning_params)</span><br><span class="line">    ])</span><br><span class="line">    pruned_model.compile(</span><br><span class="line">        loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">        optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">        metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">    pruned_model.summary()</span><br><span class="line">    return pruned_model</span><br></pre></td></tr></table></figure><br>训练剪枝模型<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train_prune_model(x_train,x_test,y_train,y_test,epochs,prune_model_file):</span><br><span class="line">    input_shape &#x3D; (img_rows, img_cols,1)</span><br><span class="line">    num_train_samples &#x3D; x_train.shape[0]</span><br><span class="line">    end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">    pruned_model &#x3D; build_prune_model(input_shape,end_step)</span><br><span class="line">    # Add a pruning step callback to peg the pruning step to the optimizer&#39;s</span><br><span class="line">    # step. Also add a callback to add pruning summaries to tensorboard</span><br><span class="line">    callbacks &#x3D; [</span><br><span class="line">        sparsity.UpdatePruningStep(),</span><br><span class="line">        sparsity.PruningSummaries(log_dir&#x3D;logdir, profile_batch&#x3D;0)</span><br><span class="line">    ]</span><br><span class="line">    pruned_model.fit(x_train, y_train,</span><br><span class="line">              batch_size&#x3D;batch_size,</span><br><span class="line">              epochs&#x3D;10,</span><br><span class="line">              verbose&#x3D;1,</span><br><span class="line">              callbacks&#x3D;callbacks,</span><br><span class="line">              validation_data&#x3D;(x_test, y_test))</span><br><span class="line">    score &#x3D; pruned_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">    print(&#39;Saving pruned model to: &#39;, prune_model_file)</span><br><span class="line">    # 保存模型时要设置 include_optimizer 为True by default.</span><br><span class="line">    tf.keras.models.save_model(pruned_model,prune_model_file, include_optimizer&#x3D;True)</span><br><span class="line">    print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">    print(&#39;Test accuracy:&#39;, score[1])</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test &#x3D; prepare_trainval(img_rows, img_cols)</span><br><span class="line">prune_model_file &#x3D; &quot;.&#x2F;prune_mnist_classifier.h5&quot;</span><br><span class="line">train_prune_model(x_train,x_test,y_train,y_test,epochs,prune_model_file)</span><br></pre></td></tr></table></figure></p>
<p>训练结果输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">52224&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0127 - accuracy: 0.9961</span><br><span class="line">53120&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;....] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961</span><br><span class="line">54016&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0125 - accuracy: 0.9962</span><br><span class="line">54912&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0124 - accuracy: 0.9962</span><br><span class="line">55808&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;...] - ETA: 0s - loss: 0.0124 - accuracy: 0.9962</span><br><span class="line">56704&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">57600&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">58368&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">59264&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9962</span><br><span class="line">60000&#x2F;60000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 4s 69us&#x2F;sample - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0226 - val_accuracy: 0.9920</span><br><span class="line">Saving pruned model to: .&#x2F;prune_mnist_classifier.h5</span><br><span class="line">Test loss: 0.022609539373161534</span><br><span class="line">Test accuracy: 0.992</span><br></pre></td></tr></table></figure>
<p>如果我们要载入剪枝的模型，我们得使用<strong>prune_scope()会话</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with sparsity.prune_scope():</span><br><span class="line">  restored_model &#x3D; tf.keras.models.load_model(checkpoint_file)</span><br><span class="line"></span><br><span class="line">restored_model.fit(x_train, y_train,</span><br><span class="line">                   batch_size&#x3D;batch_size,</span><br><span class="line">                   epochs&#x3D;2,</span><br><span class="line">                   verbose&#x3D;1,</span><br><span class="line">                   callbacks&#x3D;callbacks,</span><br><span class="line">                   validation_data&#x3D;(x_test, y_test))</span><br><span class="line"></span><br><span class="line">score &#x3D; restored_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">print(&#39;Test accuracy:&#39;, score[1])</span><br></pre></td></tr></table></figure>
<p>在训练和载入剪枝模型时有两点需要注意</p>
<ol>
<li>保存模型时， <code>include_optimizer</code>必须设置为<code>True</code>。因为剪枝过程需要保存optimizer的状态。</li>
<li>载入剪枝模型时需要在<code>prune_scope()</code>会话中来解序列化。</li>
</ol>
<h4 id="5-2-3-对照：如何使用剪枝模型"><a href="#5-2-3-对照：如何使用剪枝模型" class="headerlink" title="5.2.3 对照：如何使用剪枝模型"></a>5.2.3 对照：如何使用剪枝模型</h4><p><strong>构建模型时</strong></p>
<p><img src="/images/blog/model_pruning_8.png" alt="模型剪枝和优化"></p>
<p>我们对比发现，只有需要计算梯度的网络层需要使用剪枝的包装。同时需要设定好剪枝的规划。</p>
<p><strong>训练模型时</strong></p>
<p><img src="/images/blog/model_pruning_9.png" alt="模型剪枝和优化"></p>
<p>没有太大的区别，除了以下两点</p>
<ol>
<li>需要新增关于剪枝的统计</li>
<li>保存模型时需要将optimizer也一起保存</li>
</ol>
<p>使用netron打开两个保存的模型，效果如下，可以看到裁剪的模型都被放在了<code>PruneLowMagnitude</code>中。</p>
<p><img src="/images/blog/model_pruning_10.png" alt="模型剪枝和优化"></p>
<h3 id="5-3-对整个模型剪枝"><a href="#5-3-对整个模型剪枝" class="headerlink" title="5.3 对整个模型剪枝"></a>5.3 对整个模型剪枝</h3><p>函数<code>prune_low_magnitude</code>可以应用于整个keras模型。此时算法会被应用于所有对权重剪枝<strong>友好</strong>(Keras api的知道的)的网络层，<strong>不友好</strong>的网络层会直接忽略掉，<strong>未知</strong>的网络层可能会报错。</p>
<p>如果模型的网络层是API不知道如何剪枝的，但是非常适合不剪枝，那么交给API来修剪每层的basis即可(即不修剪卷积核的权重，只修剪basis)。</p>
<p>除去剪枝配置参数，相同的配置可以应用于网络的所有的剪枝层。同时需要注意的是，剪枝不保留原模型的优化器optimizer，需要对剪枝的模型重新训练一个新的优化器optimizer。</p>
<p>开始之前，假设我们已经有一个已经序列化过的预训练的Keras模型，想对其权重剪枝。以前面的MNIST模型为例。先载入模型<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Load the serialized model</span><br><span class="line">loaded_model &#x3D; tf.keras.models.load_model(keras_file)</span><br></pre></td></tr></table></figure><br>然后可以剪枝模型然后编译剪枝之后的模型并训练。此时的训练将重新从第0步开始，鉴于模型此时已经达到了一定的准确率，我们可以直接开始剪枝。将开始步骤设置为0，然后只训练4个epochs。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epochs &#x3D; 4</span><br><span class="line">end_step &#x3D; np.ceil(1.0 * num_train_samples &#x2F; batch_size).astype(np.int32) * epochs</span><br><span class="line">print(end_step)</span><br><span class="line"></span><br><span class="line">new_pruning_params &#x3D; &#123;</span><br><span class="line">      &#39;pruning_schedule&#39;: sparsity.PolynomialDecay(initial_sparsity&#x3D;0.50,</span><br><span class="line">                                                   final_sparsity&#x3D;0.90,</span><br><span class="line">                                                   begin_step&#x3D;0,</span><br><span class="line">                                                   end_step&#x3D;end_step,</span><br><span class="line">                                                   frequency&#x3D;100)</span><br><span class="line">&#125;</span><br><span class="line">new_pruned_model &#x3D; sparsity.prune_low_magnitude(model, **new_pruning_params)</span><br><span class="line">new_pruned_model.summary()</span><br><span class="line">new_pruned_model.compile(</span><br><span class="line">    loss&#x3D;tf.keras.losses.categorical_crossentropy,</span><br><span class="line">    optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">    metrics&#x3D;[&#39;accuracy&#39;])</span><br></pre></td></tr></table></figure>
<p>再训练4个epochs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Add a pruning step callback to peg the pruning step to the optimizer&#39;s</span><br><span class="line"># step. Also add a callback to add pruning summaries to tensorboard</span><br><span class="line">callbacks &#x3D; [</span><br><span class="line">    sparsity.UpdatePruningStep(),</span><br><span class="line">    sparsity.PruningSummaries(log_dir&#x3D;logdir, profile_batch&#x3D;0)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">new_pruned_model.fit(x_train, y_train,</span><br><span class="line">          batch_size&#x3D;batch_size,</span><br><span class="line">          epochs&#x3D;epochs,</span><br><span class="line">          verbose&#x3D;1,</span><br><span class="line">          callbacks&#x3D;callbacks,</span><br><span class="line">          validation_data&#x3D;(x_test, y_test))</span><br><span class="line"></span><br><span class="line">score &#x3D; new_pruned_model.evaluate(x_test, y_test, verbose&#x3D;0)</span><br><span class="line">print(&#39;Test loss:&#39;, score[0])</span><br><span class="line">print(&#39;Test accuracy:&#39;, score[1])</span><br></pre></td></tr></table></figure>
<p>模型导出到serving</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">final_model &#x3D; sparsity.strip_pruning(pruned_model)</span><br><span class="line">final_model.summary()</span><br><span class="line"></span><br><span class="line">_, new_pruned_keras_file &#x3D; tempfile.mkstemp(&#39;.h5&#39;)</span><br><span class="line">print(&#39;Saving pruned model to: &#39;, new_pruned_keras_file)</span><br><span class="line">tf.keras.models.save_model(final_model, new_pruned_keras_file, </span><br><span class="line">                        include_optimizer&#x3D;False)</span><br><span class="line"></span><br><span class="line"># 压缩之后的模型大小与前面一层层剪枝的大小一样</span><br><span class="line">_, zip3 &#x3D; tempfile.mkstemp(&#39;.zip&#39;)</span><br><span class="line">with zipfile.ZipFile(zip3, &#39;w&#39;, compression&#x3D;zipfile.ZIP_DEFLATED) as f:</span><br><span class="line">  f.write(new_pruned_keras_file)</span><br><span class="line">print(&quot;Size of the pruned model before compression: %.2f Mb&quot; </span><br><span class="line">      % (os.path.getsize(new_pruned_keras_file) &#x2F; float(2**20)))</span><br><span class="line">print(&quot;Size of the pruned model after compression: %.2f Mb&quot; </span><br><span class="line">      % (os.path.getsize(zip3) &#x2F; float(2**20)))</span><br></pre></td></tr></table></figure>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505" target="_blank" rel="noopener">medium Pruning Deep Neural Networks</a></li>
<li><a href="https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb" target="_blank" rel="noopener">tensorflow mnist 剪枝</a></li>
<li><a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" target="_blank" rel="noopener">Pruning deep neural networks to make them fast and small</a></li>
<li><a href="https://stackoverflow.com/questions/43839431/tensorflow-how-to-replace-or-modify-gradient/43948872" target="_blank" rel="noopener">stackoverflow 如何在tensorflow计算梯度时更改计算方式</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient" target="_blank" rel="noopener">Tensorflow官方API 如何更改梯度计算方式</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>理解pytorch的计算逻辑</title>
    <url>/2019/10/28/-understand-pytorch/</url>
    <content><![CDATA[<h2 id="1-线性回归问题"><a href="#1-线性回归问题" class="headerlink" title="1 线性回归问题"></a>1 线性回归问题</h2><p>假定我们以一个线性回归问题来逐步解释pytorch过程中的一些操作和逻辑。线性回归公式如下</p>
<script type="math/tex; mode=display">
 y = a+bx+e\quad \quad 此处假定a=1,b=2的一个线性回归函数</script><h3 id="1-1-先用普通的numpy来展示线性回归过程"><a href="#1-1-先用普通的numpy来展示线性回归过程" class="headerlink" title="1.1 先用普通的numpy来展示线性回归过程"></a>1.1 先用普通的numpy来展示线性回归过程</h3><p>我们随机生成100个数据，并以一定的随机概率扰动数据集，训练集和验证集八二分，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Data Generation</span><br><span class="line">np.random.seed(42)</span><br><span class="line">x &#x3D; np.random.rand(100, 1)</span><br><span class="line">y &#x3D; 1 + 2 * x + .1 * np.random.randn(100, 1)</span><br><span class="line"></span><br><span class="line"># Shuffles the indices</span><br><span class="line">idx &#x3D; np.arange(100)</span><br><span class="line">np.random.shuffle(idx)</span><br><span class="line"></span><br><span class="line"># Uses first 80 random indices for train</span><br><span class="line">train_idx &#x3D; idx[:80]</span><br><span class="line"># Uses the remaining indices for validation</span><br><span class="line">val_idx &#x3D; idx[80:]</span><br><span class="line"></span><br><span class="line"># Generates train and validation sets</span><br><span class="line">x_train, y_train &#x3D; x[train_idx], y[train_idx]</span><br><span class="line">x_val, y_val &#x3D; x[val_idx], y[val_idx]</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/understand_pytorch_1.png" alt=""></p>
<p>上面这是我们已经知道的是一个线性回归数据分布，并且回归的参数是$a=1,b=2$，如果我们只知道数据<code>x_train</code>和<code>y_train</code>，需要求这两个参数$a,b$呢，一般是使用梯度下降方法。</p>
<p>注意，下面的梯度下降方法是全量梯度，一次计算了所有的数据的梯度，只是在迭代了1000个epoch，通常训练时会把全量数据分成多个batch，每次都是小批量更新。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 初始化线性回归的参数 a 和 b</span><br><span class="line">np.random.seed(42)</span><br><span class="line">a &#x3D; np.random.randn(1)</span><br><span class="line">b &#x3D; np.random.randn(1)</span><br><span class="line">print(&quot;初始化的 a : %d 和 b : %d&quot;%(a,b))</span><br><span class="line">leraning_rate &#x3D; 1e-2</span><br><span class="line">epochs &#x3D; 1000</span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    pred &#x3D; a+ b*x_train</span><br><span class="line">    # 计算预测值和真实值之间的误差</span><br><span class="line">    error &#x3D; y_train-pred</span><br><span class="line">    # 使用MSE 来计算回归误差</span><br><span class="line">    loss &#x3D; (error**2).mean()</span><br><span class="line">    # 计算参数 a 和 b的梯度</span><br><span class="line">    a_grad &#x3D; -2*error.mean()</span><br><span class="line">    b_grad &#x3D; -2*(x_train*error).mean()</span><br><span class="line">    # 更新参数：用学习率和梯度</span><br><span class="line">    a &#x3D; a-leraning_rate*a_grad</span><br><span class="line">    b &#x3D; b -leraning_rate*b_grad</span><br><span class="line"></span><br><span class="line">print(&quot;最终获得参数为 a : %.2f, b :%.2f &quot;%(a,b))</span><br></pre></td></tr></table></figure>
<p>得到的输出如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">初始化的 a : 0 和 b : 0</span><br><span class="line">最终获得参数为 a : 0.98, b :1.94</span><br></pre></td></tr></table></figure>
<p>再验证下是否与sklearn的LinearRegression回归算法得到的结果相同。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检查下，我们获得结果是否与sklearn的结果一致</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">linr &#x3D; LinearRegression()</span><br><span class="line">linr.fit(x_train,y_train)</span><br><span class="line">print(linr.intercept_,linr.coef_[0])</span><br></pre></td></tr></table></figure>
<p>得到的参数如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0.98312156] [1.94067463]</span><br></pre></td></tr></table></figure>
<h2 id="2-pytorhc-来解决回归问题"><a href="#2-pytorhc-来解决回归问题" class="headerlink" title="2 pytorhc 来解决回归问题"></a>2 pytorhc 来解决回归问题</h2><h3 id="2-1-pytorch的一些基础问题"><a href="#2-1-pytorch的一些基础问题" class="headerlink" title="2.1 pytorch的一些基础问题"></a>2.1 pytorch的一些基础问题</h3><ul>
<li>如果将numpy数组转化为pytorch的tensor呢？使用<code>torch.from_numpy(data)</code></li>
<li>如果想将计算的数据放入GPU计算：<code>data.to(device)</code>(其中的device就是GPU或cpu)</li>
<li>数据类型转换示例： <code>data.float()</code></li>
<li>如果确定数据位于CPU还是GPU:<code>data.type()</code>会得到类似于<code>torch.cuda.FloatTensor</code>的结果，表明在GPU中</li>
<li>从GPU中把数据转化成numpy：先取出到cpu中，再转化成numpy数组。<code>data.cpu().numpy()</code></li>
</ul>
<h3 id="2-2-使用pytorch构建参数"><a href="#2-2-使用pytorch构建参数" class="headerlink" title="2.2 使用pytorch构建参数"></a>2.2 使用pytorch构建参数</h3><p>如何区分普通数据和参数/权重呢？<strong>需要计算梯度的是参数，否则就是普通数据</strong>。参数需要用梯度来更新，我们需要选项<code>requires_grad=True</code>。使用了这个选项就是告诉pytorch，我们要计算此变量的梯度了。</p>
<p>我们可以使用如下三种方式来构建参数</p>
<ol>
<li>此方法构建出来的参数全部都在cpu中<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></li>
<li>此方法尝试把tensor参数传入到gpu<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float).to(device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float).to(device)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
此时如果查看输出，会发现两个tensor ，$a和b$的梯度选项没了（没了requires_grad=True）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([0.5158], device&#x3D;&#39;cuda:0&#39;, grad_fn&#x3D;&lt;CopyBackwards&gt;) tensor([0.0246], device&#x3D;&#39;cuda:0&#39;, grad_fn&#x3D;&lt;CopyBackwards&gt;)</span><br></pre></td></tr></table></figure></li>
<li>先将tensor传入gpu，然后再使用<code>requires_grad_()</code>选项来重构tensor的属性。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; torch.randn(1, dtype&#x3D;torch.float).to(device)</span><br><span class="line">b &#x3D; torch.randn(1, dtype&#x3D;torch.float).to(device)</span><br><span class="line"># and THEN set them as requiring gradients...</span><br><span class="line">a.requires_grad_()</span><br><span class="line">b.requires_grad_()</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></li>
<li>最佳策略当然是初始化的时候直接赋予<code>requires_grad=True</code>属性了<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># We can specify the device at the moment of creation - RECOMMENDED!</span><br><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
查看tensor的属性<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([0.6226], device&#x3D;&#39;cuda:0&#39;, requires_grad&#x3D;True) tensor([1.4505], device&#x3D;&#39;cuda:0&#39;, requires_grad&#x3D;True)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-3-自动求导-Autograd"><a href="#2-3-自动求导-Autograd" class="headerlink" title="2.3 自动求导 Autograd"></a>2.3 自动求导 Autograd</h3><p>Autograd是Pytorch的自动求导包，有了它，我们就不必担忧偏导数和链式法则等一系列问题。Pytorch计算所有梯度的方法是<code>backward()</code>。计算梯度之前，我们需要先计算损失，那么需要调用对应(损失)变量的求导方法，如<code>loss.backward()</code>。</p>
<ul>
<li>计算所有变量的梯度(假设损失变量是loss): <code>loss.back()</code></li>
<li>获取某个变量的实际的梯度值(假设变量为att):<code>att.grad</code></li>
<li>由于梯度是累加的，每次用梯度更新参数之后，需要清零(假设梯度变量是att):<code>att.zero_()</code>,下划线是一种运算符，相当于直接作用于原变量上，等同于<code>att=0</code>(不要手动赋值，因为此过程可能涉及到GPU、CPU之间数据传输，容易出错)</li>
</ul>
<p>我们接下来尝试下手工更新参数和梯度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    error &#x3D; y_train_tensor - yhat</span><br><span class="line">    loss &#x3D; (error ** 2).mean()</span><br><span class="line"></span><br><span class="line">    # 这个是numpy的计算梯度的方式</span><br><span class="line">    # a_grad &#x3D; -2 * error.mean()</span><br><span class="line">    # b_grad &#x3D; -2 * (x_tensor * error).mean()</span><br><span class="line">    </span><br><span class="line">    # 告诉pytorch计算损失loss，计算所有变量的梯度</span><br><span class="line">    loss.backward()</span><br><span class="line">    # Let&#39;s check the computed gradients...</span><br><span class="line">    print(a.grad)</span><br><span class="line">    print(b.grad)  </span><br><span class="line">    </span><br><span class="line">    # 1. 手动更新参数，会出错 AttributeError: &#39;NoneType&#39; object has no attribute &#39;zero_&#39;</span><br><span class="line">    # 错误的原因是，我们重新赋值时会丢掉变量的 梯度属性</span><br><span class="line">    # a &#x3D; a - lr * a.grad</span><br><span class="line">    # b &#x3D; b - lr * b.grad</span><br><span class="line">    # print(a)</span><br><span class="line">    # 2. 再次手动更新参数，这次我们没有重新赋值，而是使用in-place的方式赋值  RuntimeError: a leaf Variable that requires grad has been used in an in- place operation.</span><br><span class="line">    # 这是因为 pytorch 给所有需要计算梯度的python操作以及依赖都纳入了动态计算图，稍后会解释</span><br><span class="line">    # a -&#x3D; lr * a.grad</span><br><span class="line">    # b -&#x3D; lr * b.grad        </span><br><span class="line"></span><br><span class="line">    # 3. 如果我们真想手动更新，不使用pytorch的计算图呢，必须使用no_grad来将此参数移除自动计算梯度变量之外。</span><br><span class="line">    # 这是源于pytorch的动态计算图DYNAMIC GRAPH，后面会有详细的解释</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        a -&#x3D; lr * a.grad</span><br><span class="line">        b -&#x3D; lr * b.grad</span><br><span class="line">    </span><br><span class="line">    # PyTorch is &quot;clingy&quot; to its computed gradients, we need to tell it to let it go...</span><br><span class="line">    a.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
<h3 id="2-4-动态计算图"><a href="#2-4-动态计算图" class="headerlink" title="2.4 动态计算图"></a>2.4 动态计算图</h3><p>如果想可视化计算图，可以使用辅助包<a href="https://github.com/szagoruyko/pytorchviz" target="_blank" rel="noopener">torchviz</a>，需要自己安装。使用其<code>make_dot(变量)</code>方法来可视化与当前给定变量相关的计算图。示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line"></span><br><span class="line">yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">error &#x3D; y_train_tensor - yhat</span><br><span class="line">loss &#x3D; (error ** 2).mean()</span><br><span class="line">make_dot(yhat)</span><br></pre></td></tr></table></figure>
<p>使用<code>make_dot(yhat)</code>会得到相关的三个计算图如下</p>
<p><img src="/images/blog/understand_pytorch_2.png" alt=""></p>
<p>各个组件，解释如下</p>
<ul>
<li><strong>蓝色盒子</strong>：作为参数的tensor，需要pytorch计算梯度的</li>
<li><strong>灰色盒子</strong>：与计算梯度相关的或者计算梯度依赖的，python操作</li>
<li><strong>绿色盒子</strong>：与灰色盒子一样，区别是，它是计算梯度的起始点（假设<code>backward()</code>方法是需要可视化图的变量调用的）-计算图自底向上构建。</li>
</ul>
<p>上图的<code>error</code>(图中)和<code>loss</code>(图右)，与左图的唯一区别就是中间步骤(灰色盒子)的数目。看左边的绿色盒子，有两个箭头指向该绿色盒子，代表两个变量相加。<code>a</code>和<code>b*x</code>。再看该图中的灰色盒子，它执行的是乘法计算，即<code>b*x</code>，但是为啥只有一个箭头指向呢，只有来自蓝色盒子的参数<code>b</code>，为啥没有数据<code>x</code>?因为我们不需要为数据<code>x</code>计算梯度（<strong>不计算梯度的变量不会出现在计算图中</strong>）。那么，如果我们去掉变量的<code>requires_grad</code>属性(设置为False)会怎样？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a_nongrad &#x3D; torch.randn(1,requires_grad&#x3D;False,dtype&#x3D;torch.float,device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1,requires_grad&#x3D;True,dtype&#x3D;torch.float,device&#x3D;device)</span><br><span class="line">yhat &#x3D; a_nongrad+b*x_train_tensor</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/understand_pytorch_3.png" alt=""></p>
<p>可以看到，对应参数<code>a</code>的蓝色盒子没有了，所以很简单明了，<strong>不计算梯度，就不出现在计算图中</strong>。</p>
<h2 id="3-优化器-Optimizer"><a href="#3-优化器-Optimizer" class="headerlink" title="3 优化器 Optimizer"></a>3 优化器 Optimizer</h2><p>到目前为止，我们都是手动计算梯度并更新参数的，如果有非常多的变量。我们可以使用pytorch的优化器，像<code>SGD</code>或者<code>Adam</code>。</p>
<p>优化器需要指定需要优化的参数，以及学习率，然后使用<code>step()</code>方法来更新，此外，<strong>我们不必再一个个的去将梯度赋值为0了，只需要使用优化器的<code>zero_grad()</code>方法即可。</strong>。</p>
<p>代码示例，使用SGD优化器更新参数<code>a</code>和<code>b</code>的梯度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Defines a SGD optimizer to update the parameters</span><br><span class="line">optimizer &#x3D; optim.SGD([a, b], lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    # 第一步，计算损失</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    error &#x3D; y_train_tensor - yhat</span><br><span class="line">    loss &#x3D; (error ** 2).mean()</span><br><span class="line">    # 第二步，后传损失</span><br><span class="line">    loss.backward()    </span><br><span class="line">    </span><br><span class="line">    # 不用再手动更新参数了</span><br><span class="line">    # with torch.no_grad():</span><br><span class="line">    # a -&#x3D; lr * a.grad</span><br><span class="line">    # b -&#x3D; lr * b.grad</span><br><span class="line">    # 使用优化器的step方法一步到位</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    # 也不用告诉pytorch需要对哪些梯度清零操作了，优化器的zero_grad()一步到位</span><br><span class="line">    # a.grad.zero_()</span><br><span class="line">    # b.grad.zero_()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
<h2 id="4-计算损失loss"><a href="#4-计算损失loss" class="headerlink" title="4  计算损失loss"></a>4  计算损失loss</h2><p>pytorch提供了很多损失函数，可以直接调用。简单使用如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line">a &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">b &#x3D; torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float, device&#x3D;device)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># 此处定义了损失函数为MSE</span><br><span class="line">loss_fn &#x3D; nn.MSELoss(reduction&#x3D;&#39;mean&#39;)</span><br><span class="line"></span><br><span class="line">optimizer &#x3D; optim.SGD([a, b], lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    yhat &#x3D; a + b * x_train_tensor</span><br><span class="line">    </span><br><span class="line">    # 不用再手动计算损失了</span><br><span class="line">    # error &#x3D; y_tensor - yhat</span><br><span class="line">    # loss &#x3D; (error ** 2).mean()</span><br><span class="line">    # 直接调用定义好的损失函数即可</span><br><span class="line">    loss &#x3D; loss_fn(y_train_tensor, yhat)</span><br><span class="line"></span><br><span class="line">    loss.backward()    </span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure>
<h2 id="5-模型"><a href="#5-模型" class="headerlink" title="5 模型"></a>5 模型</h2><p>pytorch中模型由一个继承自<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" target="_blank" rel="noopener">Module</a>的Python类来定义。需要实现两个最基本的方法</p>
<ol>
<li><code>__init__(self)</code>:定义了模型由哪几部分组成，当前模型只有两个变量<code>a</code>和<code>b</code>。模型可以定义更多的参数，并且可以将其他模型或者网络层定义为其参数</li>
<li><code>forwad(self,x)</code>:真实执行计算的方法，它对给定输入<code>x</code>输出模型预测值。不要显示调用此<code>forward(x)</code>方法，而是直接调用模型本身，即<code>model(x)</code>。</li>
</ol>
<p>简单的回归模型如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ManualLinearRegression(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # To make &quot;a&quot; and &quot;b&quot; real parameters of the model, we need to wrap them with nn.Parameter</span><br><span class="line">        self.a &#x3D; nn.Parameter(torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float))</span><br><span class="line">        self.b &#x3D; nn.Parameter(torch.randn(1, requires_grad&#x3D;True, dtype&#x3D;torch.float))</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Computes the outputs &#x2F; predictions</span><br><span class="line">        return self.a + self.b * x</span><br></pre></td></tr></table></figure>
<p>在<code>__init__(self)</code>方法中，我们使用<code>Parameters()</code>类定义了两个参数<code>a</code>和<code>b</code>，告诉Pytorch，这两个tensor要被作为模型的参数的属性。这样，我们就可以使用模型的<code>parameters()</code>方法来找到模型每次迭代时的所有参数值了，即便模型是嵌套模型都可以找得到，这样就能将参数喂入优化器optimizer来计算了(而非手动维护一张参数表)。并且，我们可以使用模型的<code>state_dict()</code>方法来获取所有参数的当前值。</p>
<p><strong>注意：模型应当与数据出于相同位置(GPU/CPU)，如果数据时GPU tensor，我们的模型也必须在GPU中</strong></p>
<p>代码示例如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line"></span><br><span class="line"># Now we can create a model and send it at once to the device</span><br><span class="line">model &#x3D; ManualLinearRegression().to(device)</span><br><span class="line"># We can also inspect its parameters using its state_dict</span><br><span class="line">print(model.state_dict())</span><br><span class="line"></span><br><span class="line">lr &#x3D; 1e-1</span><br><span class="line">n_epochs &#x3D; 1000</span><br><span class="line"></span><br><span class="line">loss_fn &#x3D; nn.MSELoss(reduction&#x3D;&#39;mean&#39;)</span><br><span class="line">optimizer &#x3D; optim.SGD(model.parameters(), lr&#x3D;lr)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    #  注意，模型一般都有个train()方法，但是不要手动调用，此处只是为了说明此时是在训练，防止有些模型在训练模型和验证模型时操作不一致，训练时有dropout之类的</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    # No more manual prediction!</span><br><span class="line">    # yhat &#x3D; a + b * x_tensor</span><br><span class="line">    yhat &#x3D; model(x_train_tensor)</span><br><span class="line">    </span><br><span class="line">    loss &#x3D; loss_fn(y_train_tensor, yhat)</span><br><span class="line">    loss.backward()    </span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">print(model.state_dict())</span><br></pre></td></tr></table></figure>
<h2 id="6-训练步"><a href="#6-训练步" class="headerlink" title="6 训练步"></a>6 训练步</h2><p>我们定义了<code>optimizer</code>,<code>loss function</code>,<code>model</code>为模型三要素，同时需要提供训练时用的特征(<code>feature</code>)和对应的标签(<code>label</code>)数据。一个完整的模型训练有以下组成</p>
<ul>
<li>模型三要素<ul>
<li>优化器optimizer</li>
<li>损失函数loss</li>
<li>模型 model</li>
</ul>
</li>
<li>数据<ul>
<li>特征数据feature</li>
<li>数据标签label</li>
</ul>
</li>
</ul>
<p>我们可以写一个包含模型三要素的通用的训练函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def make_train_step(model, loss_fn, optimizer):</span><br><span class="line">    # Builds function that performs a step in the train loop</span><br><span class="line">    def train_step(x, y):</span><br><span class="line">        # Sets model to TRAIN mode</span><br><span class="line">        model.train()</span><br><span class="line">        # Makes predictions</span><br><span class="line">        yhat &#x3D; model(x)</span><br><span class="line">        # Computes loss</span><br><span class="line">        loss &#x3D; loss_fn(y, yhat)</span><br><span class="line">        # Computes gradients</span><br><span class="line">        loss.backward()</span><br><span class="line">        # Updates parameters and zeroes gradients</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        # Returns the loss</span><br><span class="line">        return loss.item()</span><br><span class="line">    </span><br><span class="line">    # Returns the function that will be called inside the train loop</span><br><span class="line">    return train_step</span><br></pre></td></tr></table></figure>
<p>然后在每个epoch时迭代模型训练</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Creates the train_step function for our model, loss function and optimizer</span><br><span class="line">train_step &#x3D; make_train_step(model, loss_fn, optimizer)</span><br><span class="line">losses &#x3D; []</span><br><span class="line"></span><br><span class="line"># For each epoch...</span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">    # Performs one train step and returns the corresponding loss</span><br><span class="line">    loss &#x3D; train_step(x_train_tensor, y_train_tensor)</span><br><span class="line">    losses.append(loss)</span><br><span class="line">    </span><br><span class="line"># Checks model&#39;s parameters</span><br><span class="line">print(model.state_dict())</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e" target="_blank" rel="noopener">medium understand pytorch</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>使用pyod做离群点检测</title>
    <url>/2019/09/24/outlier-detection/</url>
    <content><![CDATA[<p><a href="https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/</a></p>
<h3 id="1-什么是离群点"><a href="#1-什么是离群点" class="headerlink" title="1 什么是离群点"></a>1 什么是离群点</h3><p>一个极大的偏离正常值的数据点。下面是一些常见的离群点</p>
<ul>
<li>一个学生的平均得分超过剩下90%的得分，而其他人的得分平均仅为70%。显然的离群点</li>
<li>分析某个顾客的购买行为，大部分集中在100-块，突然出现1000块的消费。</li>
</ul>
<p>有多种类型的离群点</p>
<ul>
<li><strong>单变量的</strong>： 只有一个变量的值会出现极端值</li>
<li><strong>多变量的</strong>： 至少两个以上的变量值的综合得分极端。</li>
</ul>
<h3 id="2-为什么需要检测离群点"><a href="#2-为什么需要检测离群点" class="headerlink" title="2 为什么需要检测离群点"></a>2 为什么需要检测离群点</h3><p>离群点会影响我们的正常的数据分析和建模，如下图左边是包含离群点的模型，右边是处理掉离群点之后的模型结构。</p>
<p><img src="/images/blog/outlier_sample.png" alt=""></p>
<p>但是，<strong>离群点并非一直都是不好的</strong>。简单的移除离群点并非明智之举，我们需要去理解离群点。</p>
<p>现在的趋势是使用直接的方式如盒图、直方图和散点图来检测离群点。但是<strong>在处理大规模数据集和需要在更大数据集中识别某种模式时，专用的离群点检测算法是非常有价值的</strong>。</p>
<p>某些应用，如金融欺诈识别和网络安全里面的入侵检测需要及时响应的以及精确的技术来识别离群点。</p>
<h3 id="3-为什么要使用PyOD-来做离群点检测"><a href="#3-为什么要使用PyOD-来做离群点检测" class="headerlink" title="3 为什么要使用PyOD 来做离群点检测"></a>3 为什么要使用PyOD 来做离群点检测</h3><p>现有的一些实现，比如PyNomaly，并非为了做离群点而设计的（尽管依然值得一试）。PyOD是一个可拓展的Python工具包用来检测多变量数据中的离群点。提供了接近20种离群点检测算法。</p>
<h3 id="4-PyOD的特征"><a href="#4-PyOD的特征" class="headerlink" title="4 PyOD的特征"></a>4 PyOD的特征</h3><ul>
<li>开源、并附有详细的说明文档和实例。</li>
<li>支持先进的模型，包括神经网络，深度学习和离群点检测集成学习方法</li>
<li>使用JIT优化加速，以及使用numba和joblib并行化</li>
<li>python2 和3 都可以用</li>
</ul>
<h3 id="5-安装使用PyOD"><a href="#5-安装使用PyOD" class="headerlink" title="5 安装使用PyOD"></a>5 安装使用PyOD</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install pyod</span><br><span class="line">pip install --upgrade pyod # to make sure that the latest version is installed!</span><br></pre></td></tr></table></figure>
<p>注意，PyOD包含了一些神经网络模型，基于keras。但是它不会自动安装Keras或者Tensorflow。需要手动安装这两个库，才能使用其神经网络模型。安装过程的依赖有点多。</p>
<h3 id="6-使用PyOD来做离群点检测"><a href="#6-使用PyOD来做离群点检测" class="headerlink" title="6 使用PyOD来做离群点检测"></a>6 使用PyOD来做离群点检测</h3><p>注意，我们使用的是<strong>离群得分</strong>，即每个模型都会给每个数据点打分而非直接根据一个阈值判定某个点是否为离群点。</p>
<p><strong>Angle_Based Outlier Detection (ABOD)</strong></p>
<ul>
<li>它考虑了每个数据点和其邻居的关系，但是不考虑邻居之间的关系。<ul>
<li>ABOD在多维度数据上表现较好</li>
<li>PyOD提供了两种不同版本的ABOD</li>
</ul>
</li>
<li>Fast ABOD：使用KNN来近似</li>
<li>Original ABOD：以高时间复杂度来考虑所有训练数据点</li>
</ul>
<p><strong>KNN 检测器</strong></p>
<ul>
<li>对于任意数据点，其到第k个邻居的距离可以作为其离群得分</li>
<li>PyOD提供三种不同的KNN检测器<ul>
<li><code>Largest</code>： 使用第k个邻居的距离来作为离群得分</li>
<li><code>Mean</code>: 使用全部k个邻居的平均距离作为离群得分</li>
<li><code>Median</code>:使用k个邻居的距离的中位数作为离群得分</li>
</ul>
</li>
</ul>
<p><strong>Isolation Forest</strong></p>
<ul>
<li>内部使用sklearn，此方法中，使用一个集合的树来完成数据分区。孤立森林提供农一个离群得分来判定一个数据点在结构中有多孤立。其离群得分用来将它与正常观测数据区分开来。</li>
<li>孤立森林在多维数据上表现很好</li>
</ul>
<p><strong>Histogram-based Outiler Detection</strong></p>
<ul>
<li>一种高效的无监督方法，它假设特征之间独立，然后通过构建直方图来计算离群得分</li>
<li>比多变量方法快得多，但是要损失一些精度</li>
</ul>
<p><strong>Local Correlation Integral(LOCI)</strong></p>
<ul>
<li>LOCI在离群检测和离群点分组上十分高效。它为每个数据点提供一个LOCI plot，该plot反映了数据点在附近数据点的诸多信息，确定集群、微集群、它们的半径以及它们的内部集群距离</li>
<li>现存的所有离群检测算法都无法超越此特性，因为他们的输出仅仅是给每个数据点的输出一个单一值。</li>
</ul>
<p><strong>Feature Bagging</strong></p>
<ul>
<li>一个特征集合检测器，它在数据集的一系列子集上拟合了大量的基准检测器。它使用平均或者其他结合方法来提高预测准确率</li>
<li>默认使用LOF(Local Outiler Factor)作为基准评估器。但是其他检测器，如KNN，ABOD都可以作为基准检测器</li>
<li>Feature Bagging首先通过随机选取特征子集来构建n个子样本。这带来了基准评估器的多样性。最终，通过取所有基准评估器的平均或者最大值来预测得分。</li>
</ul>
<p><strong>*Clustering Based  Local Outiler Factor</strong></p>
<ul>
<li>它将数据分为小聚类簇和大聚类簇。离群得分基于数据点所属的聚类簇的大小来计算，距离计算方式为到最近大聚类簇的距离。</li>
</ul>
<h3 id="7-PyOD在-Big-Mart-Sales-问题上的表现"><a href="#7-PyOD在-Big-Mart-Sales-问题上的表现" class="headerlink" title="7 PyOD在 Big Mart Sales 问题上的表现"></a>7 PyOD在 Big Mart Sales 问题上的表现</h3><p><a href="https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/?utm_source=outlierdetectionpyod&amp;utm_medium=blog" target="_blank" rel="noopener">Big Mart Sales Problem</a>。需要注册然后下载数据集，附件中有</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from scipy import stats</span><br><span class="line">from pyod.models.abod import ABOD</span><br><span class="line">from pyod.models.cblof import CBLOF</span><br><span class="line">from pyod.models.feature_bagging import FeatureBagging</span><br><span class="line">from pyod.models.hbos import HBOS</span><br><span class="line">from pyod.models.iforest import IForest</span><br><span class="line">from pyod.models.knn import KNN</span><br><span class="line">from pyod.models.lof import LOF</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.font_manager as mfm</span><br><span class="line"></span><br><span class="line">df &#x3D; pd.read_csv(&quot;train.csv&quot;)</span><br><span class="line">print(df.describe())</span><br><span class="line">show &#x3D; False</span><br><span class="line">if show:</span><br><span class="line">    plt.figure(figsize&#x3D;(10,10))</span><br><span class="line">    plt.scatter(df[&#39;Item_MRP&#39;],df[&#39;Item_Outlet_Sales&#39;])</span><br><span class="line">    plt.xlabel(&quot;Item_MRF&quot;)</span><br><span class="line">    plt.ylabel(&quot;Item_Outlet_Sales&quot;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">scaler &#x3D; MinMaxScaler(feature_range&#x3D;(0,1))</span><br><span class="line">df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]] &#x3D; scaler.fit_transform(df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]])</span><br><span class="line">print(df[[&#39;Item_MRP&#39;,&#39;Item_Outlet_Sales&#39;]].head())</span><br><span class="line"></span><br><span class="line">x1 &#x3D; df[&#39;Item_MRP&#39;].values.reshape(-1,1)</span><br><span class="line">x2 &#x3D; df[&#39;Item_Outlet_Sales&#39;].values.reshape(-1,1)</span><br><span class="line">x &#x3D; np.concatenate((x1,x2),axis&#x3D;1)</span><br><span class="line"># 设置 5%的离群点数据</span><br><span class="line">random_state &#x3D; np.random.RandomState(42)</span><br><span class="line">outliers_fraction &#x3D; 0.05</span><br><span class="line"># 定义7个后续会使用的离群点检测模型</span><br><span class="line">classifiers &#x3D; &#123;</span><br><span class="line">    &quot;Angle-based Outlier Detector(ABOD)&quot; : ABOD(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Cluster-based Local Outiler Factor (CBLOF)&quot;: CBLOF(contamination &#x3D; outliers_fraction,check_estimator&#x3D;False,random_state &#x3D; random_state),</span><br><span class="line">    &quot;Feature Bagging&quot; : FeatureBagging(LOF(n_neighbors&#x3D;35),contamination&#x3D;outliers_fraction,check_estimator&#x3D;False,random_state &#x3D; random_state),</span><br><span class="line">    &quot;Histogram-base Outlier Detection(HBOS)&quot; : HBOS(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Isolation Forest&quot; :IForest(contamination&#x3D;outliers_fraction,random_state &#x3D; random_state),</span><br><span class="line">    &quot;KNN&quot; : KNN(contamination&#x3D;outliers_fraction),</span><br><span class="line">    &quot;Average KNN&quot; :KNN(method&#x3D;&#39;mean&#39;,contamination&#x3D;outliers_fraction)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#逐一 比较模型</span><br><span class="line">xx,yy &#x3D; np.meshgrid(np.linspace(0,1,200),np.linspace(0,1,200))</span><br><span class="line">for i ,(clf_name,clf) in enumerate(classifiers.items()):</span><br><span class="line">    clf.fit(x)</span><br><span class="line">    # 预测利群得分</span><br><span class="line">    scores_pred &#x3D; clf.decision_function(x)*-1</span><br><span class="line">    # 预测数据点是否为 离群点</span><br><span class="line">    y_pred &#x3D; clf.predict(x)</span><br><span class="line">    n_inliers &#x3D; len(y_pred)-np.count_nonzero(y_pred)</span><br><span class="line">    n_outliers &#x3D; np.count_nonzero(y_pred&#x3D;&#x3D;1)</span><br><span class="line">    plt.figure(figsize&#x3D;(10,10))</span><br><span class="line"></span><br><span class="line">    # 复制一份数据</span><br><span class="line">    dfx &#x3D; df</span><br><span class="line">    dfx[&#39;outlier&#39;] &#x3D; y_pred.tolist()</span><br><span class="line">    # IX1 非离群点的特征1，IX2 非利群点的特征2</span><br><span class="line">    IX1 &#x3D; np.array(dfx[&#39;Item_MRP&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;0]).reshape(-1,1)</span><br><span class="line">    IX2 &#x3D; np.array(dfx[&#39;Item_Outlet_Sales&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;0]).reshape(-1,1)</span><br><span class="line">    # OX1 离群点的特征1，OX2离群点特征2</span><br><span class="line">    OX1 &#x3D; np.array(dfx[&#39;Item_MRP&#39;][dfx[&#39;outlier&#39;]&#x3D;&#x3D;1]).reshape(-1,1)</span><br><span class="line">    OX2 &#x3D; np.array(dfx[&#39;Item_Outlet_Sales&#39;][dfx[&#39;outlier&#39;] &#x3D;&#x3D; 1]).reshape(-1, 1)</span><br><span class="line">    print(&quot;模型 %s 检测到的&quot;%clf_name,&quot;离群点有 &quot;,n_outliers,&quot;非离群点有&quot;,n_inliers)</span><br><span class="line"></span><br><span class="line">    # 判定数据点是否为离群点的 阈值</span><br><span class="line">    threshold &#x3D; stats.scoreatpercentile(scores_pred,100*outliers_fraction)</span><br><span class="line">    # 决策函数来计算原始的每个数据点的离群点得分</span><br><span class="line">    z &#x3D; clf.decision_function(np.c_[xx.ravel(),yy.ravel()]) * -1</span><br><span class="line">    z &#x3D; z.reshape(xx.shape)</span><br><span class="line">    # 最小离群得分和阈值之间的点 使用蓝色填充</span><br><span class="line">    plt.contourf(xx,yy,z,levels&#x3D;np.linspace(z.min(),threshold,7),cmap&#x3D;plt.cm.Blues_r)</span><br><span class="line">    # 离群得分等于阈值的数据点 使用红色填充</span><br><span class="line">    a &#x3D; plt.contour(xx,yy,z,levels&#x3D;[threshold],linewidths &#x3D;2,colors&#x3D;&#39;red&#39;)</span><br><span class="line">    # 离群得分在阈值和最大离群得分之间的数据 使用橘色填充</span><br><span class="line">    plt.contourf(xx,yy,z,levels&#x3D;[threshold,z.max()],colors&#x3D;&#39;orange&#39;)</span><br><span class="line">    b &#x3D; plt.scatter(IX1,IX2,c&#x3D;&#39;white&#39;,s&#x3D;20,edgecolor &#x3D;&#39;k&#39;)</span><br><span class="line">    c &#x3D; plt.scatter(OX1,OX2,c&#x3D;&#39;black&#39;,s&#x3D;20,edgecolor &#x3D; &#39;k&#39;)</span><br><span class="line">    plt.axis(&#39;tight&#39;)</span><br><span class="line">    # loc &#x3D; 2 用来左上角</span><br><span class="line">    plt.legend(</span><br><span class="line">        [a.collections[0],b,c],</span><br><span class="line">        [&#39;learned decision function&#39;,&#39;inliers&#39;,&#39;outliers&#39;],</span><br><span class="line">        prop&#x3D;mfm.FontProperties(size&#x3D;20),</span><br><span class="line">        loc&#x3D;2</span><br><span class="line">    )</span><br><span class="line">    plt.xlim((0,1))</span><br><span class="line">    plt.ylim((0,1))</span><br><span class="line">    plt.title(clf_name)</span><br><span class="line">    plt.savefig(&quot;%s.png&quot;%clf_name)</span><br><span class="line">    #plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下；</p>
<p><img src="/images/blog/outlier_detection_ABOD.png" alt=""><br><img src="/images/blog/outlier_detection_avg_knn.png" alt=""><br><img src="/images/blog/outlier_detection_CBLOF.png" alt=""><br><img src="/images/blog/outlier_detection_feature_bagging.png" alt=""><br><img src="/images/blog/outlier_detection_HBOS.png" alt=""><br><img src="/images/blog/outlier_detection_Isolation_Forest.png" alt=""><br><img src="/images/blog/outlier_detection_KNN.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>使用StyleGAN训练自己的数据集.md</title>
    <url>/2019/05/12/edit-stylegan-humanface/</url>
    <content><![CDATA[<p>参考： <a href="https://www.gwern.net/Faces#compute" target="_blank" rel="noopener">https://www.gwern.net/Faces#compute</a></p>
<h2 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1 数据准备"></a>1 数据准备</h2><p>执行StyleGAN的最大难点在于准备数据集，不像其他的GAN可以接受文件夹输入，它只能接收<code>.tfrecords</code>作为输入，它将每张图片不同分辨率存储为数组。因此，输入文件必须是完美正态分布的，通过特定的dataset_tools.py工具将图片转成.tfrecords，这会导致实际存储尺寸达到原图的19倍。</p>
<p>注意：</p>
<ul>
<li>StyleGAN的数据集必须由相同的方式组成，$512\times 512$ 或 $1024\times 1024$( $513\times 513$就不行)</li>
<li>必须是相同的颜色空间，不能既有sRGB又有灰度图JPGs。</li>
<li>文件类型必须是与你要重新训练的模型所使用的图像格式相同的，比如，你不能用PNG图片来重新训练一个用JPG格式图像的模型。</li>
<li>不可以有细微的错误，比如CRC校验失败。</li>
</ul>
<h2 id="2-准备脸部数据"><a href="#2-准备脸部数据" class="headerlink" title="2 准备脸部数据"></a>2 准备脸部数据</h2><ol>
<li>下载原始数据集 <a href="https://www.gwern.net/Danbooru2018#download" target="_blank" rel="noopener">Danbooru2018</a></li>
<li>从Danbooru2018的metadata的JSON文件中抽取所有的图像子集的ID，如果需要指定某个特定的Danbooru标签,使用<code>jq</code>以及shell脚本</li>
<li>将原图裁剪。可以使用<a href="https://github.com/nagadomi/lbpcascade_animeface" target="_blank" rel="noopener">nagadomi</a>的人脸裁剪算法，普通的人脸检测算法无法适用于这个卡通人脸。</li>
<li>删除空文件，单色图，灰度图，删掉重名文件</li>
<li>转换成JPG格式</li>
<li>将所有图片上采样到目标分辨率即$512\times 512$，可以使用 <a href="https://github.com/nagadomi/waifu2x" target="_blank" rel="noopener">waifu2x</a></li>
<li>将所有图像转换成 $512\times 512$的sRGB JPG格式图像<br>8.可以人工筛选出质量高的图像，使用<code>findimagedupes</code>删除近似的图像，并用预训练的GAN Discriminator过滤掉部分。</li>
<li>使用StyleGAN的<code>data_tools.py</code>将图片转换成tfrecords</li>
</ol>
<p>目标是将此图</p>
<p><img src="/images/blog/stylegan_owndata_1.png" alt=""></p>
<p>转换成</p>
<p><img src="/images/blog/stylegan_owndata_2.png" alt=""></p>
<p>下面使用了一些脚本进行数据处理，可以使用<a href="https://github.com/reidsanders/danbooru-utility" target="_blank" rel="noopener">danbooru-utility</a>协助。</p>
<h3 id="2-1-裁剪"><a href="#2-1-裁剪" class="headerlink" title="2.1 裁剪"></a>2.1 裁剪</h3><p>原始的<a href="https://www.gwern.net/Danbooru2018#download" target="_blank" rel="noopener">Danbooru2018</a>可以使用磁链下载，提供了JSON的metadata，被压缩到<code>metadata/2*</code>和目录结构为<code>{original,512px}/{0-999}/$ID.{png,jpg}</code>。可以使用Danbooru2018<code>512像素</code>版本在整个SFW图像集上的训练，但是将所有图像缩放到512像素并非明智之举，因为会丢失大量面部信息，而保留高质量面部图像是个挑战。可以从<code>512px/</code>目录下的文件名中直接抽取SFW IDs，或者从metadata中抽取<code>id</code>和<code>rating</code>字段并存入某个文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find .&#x2F;512px&#x2F; -type f | sed -e &#39;s&#x2F;.*\&#x2F;\([[:digit:]]*\)\.jpg&#x2F;\1&#x2F;&#39;</span><br><span class="line"># 967769</span><br><span class="line"># 1853769</span><br><span class="line"># 2729769</span><br><span class="line"># 704769</span><br><span class="line"># 1799769</span><br><span class="line"># ...</span><br><span class="line">tar xf metadata.json.tar.xz</span><br><span class="line">cat metadata&#x2F;* | jq &#39;[.id, .rating]&#39; -c | fgrep &#39;&quot;s&quot;&#39; | cut -d &#39;&quot;&#39; -f 2 # &quot;</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure>
<p>可以安装和使用<a href="https://github.com/nagadomi/lbpcascade_animeface" target="_blank" rel="noopener">lbpcascade_animeface</a>以及opencv，使用简单的一个脚本<a href="https://github.com/nagadomi/lbpcascade_animeface/issues/1#issue-205363706" target="_blank" rel="noopener">lbpcascade_animeface issue</a>来裁剪图像。在Danbooru图像上表现惊人，大概有90%的高质量面部图像，5%低质量的，以及5%的错误图像(没有脸部)。也可以通过给脚本更多的限制，比如要求$256\times 256px$区域，可以消除大部分低质量的面部和错误。以下是<code>crop.py</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import sys</span><br><span class="line">import os.path</span><br><span class="line"></span><br><span class="line">def detect(cascade_file, filename, outputname):</span><br><span class="line">    if not os.path.isfile(cascade_file):</span><br><span class="line">        raise RuntimeError(&quot;%s: not found&quot; % cascade_file)</span><br><span class="line"></span><br><span class="line">    cascade &#x3D; cv2.CascadeClassifier(cascade_file)</span><br><span class="line">    image &#x3D; cv2.imread(filename)</span><br><span class="line">    gray &#x3D; cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    gray &#x3D; cv2.equalizeHist(gray)</span><br><span class="line"></span><br><span class="line">    ## Suggested modification: increase minSize to &#39;(250,250)&#39; px,</span><br><span class="line">    ## increasing proportion of high-quality faces &amp; reducing</span><br><span class="line">    ## false positives. Faces which are only 50x50px are useless</span><br><span class="line">    ## and often not faces at all.</span><br><span class="line"></span><br><span class="line">    faces &#x3D; cascade.detectMultiScale(gray,</span><br><span class="line">                                     # detector options</span><br><span class="line">                                     scaleFactor &#x3D; 1.1,</span><br><span class="line">                                     minNeighbors &#x3D; 5,</span><br><span class="line">                                     minSize &#x3D; (50, 50))</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    for (x, y, w, h) in faces:</span><br><span class="line">        cropped &#x3D; image[y: y + h, x: x + w]</span><br><span class="line">        cv2.imwrite(outputname+str(i)+&quot;.png&quot;, cropped)</span><br><span class="line">        i&#x3D;i+1</span><br><span class="line"></span><br><span class="line">if len(sys.argv) !&#x3D; 4:</span><br><span class="line">    sys.stderr.write(&quot;usage: detect.py &lt;animeface.xml file&gt; &lt;input&gt; &lt;output prefix&gt;\n&quot;)</span><br><span class="line">    sys.exit(-1)</span><br><span class="line"></span><br><span class="line">detect(sys.argv[1], sys.argv[2], sys.argv[3])</span><br></pre></td></tr></table></figure>
<p>IDs可以和提供的<code>lbpcascade_animeface</code>脚本使用<code>xargs</code>结合起来，但是这样还是太慢，使用并行策略<code>xargs --max-args=1 --max-procs=16</code>或者参数<code>parallel</code>更有效。<code>lbpcascade_animeface</code>脚本似乎使用了所有的GPU显存，但是没有可见的提升，我发现可以通过设置<code>CUDA_VISIBLE_DEVICES=&quot;&quot;</code>来禁用GPU（此步骤还是使用多核CPU更有效）。</p>
<p>一切就绪之后，可以按照如下方式在整个Danbooru2018数据子集上使用并行的面部图像切割</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cropFaces() &#123;</span><br><span class="line">    BUCKET&#x3D;$(printf &quot;%04d&quot; $(( $@ % 1000 )) )</span><br><span class="line">    ID&#x3D;&quot;$@&quot;</span><br><span class="line">    CUDA_VISIBLE_DEVICES&#x3D;&quot;&quot; nice python ~&#x2F;src&#x2F;lbpcascade_animeface&#x2F;examples&#x2F;crop.py \</span><br><span class="line">     ~&#x2F;src&#x2F;lbpcascade_animeface&#x2F;lbpcascade_animeface.xml \</span><br><span class="line">     .&#x2F;original&#x2F;$BUCKET&#x2F;$ID.* &quot;.&#x2F;faces&#x2F;$ID&quot;</span><br><span class="line">&#125;</span><br><span class="line">export -f cropFaces</span><br><span class="line"></span><br><span class="line">mkdir .&#x2F;faces&#x2F;</span><br><span class="line">cat sfw-ids.txt | parallel --progress cropFaces</span><br></pre></td></tr></table></figure>
<h3 id="2-2-上采样和使用GAN的Discriminator进行数据清洗"><a href="#2-2-上采样和使用GAN的Discriminator进行数据清洗" class="headerlink" title="2.2 上采样和使用GAN的Discriminator进行数据清洗"></a>2.2 上采样和使用GAN的Discriminator进行数据清洗</h3><p>在训练GAN一段时间之后，重新用Disciminator对真实的数据点进行排序。通常情况下，被Disciminator判定最低得分的图片通常也是质量较差的，可以移除，这样也有助于提升GAN。然后GAN可以在新的干净数据集上重新训练，得以提升GAN。</p>
<p>由于对图像排序是Disciminator默认会做的事，所有不需要额外的训练或算法。下面是一个简单的ranker.py脚本，载入StyleGAN的<code>.pkl</code>模型，然后运行图片名列表，并打印D得分</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    tflib.init_tf()</span><br><span class="line">    _G, D, _Gs &#x3D; pickle.load(open(sys.argv[1], &quot;rb&quot;))</span><br><span class="line">    image_filenames &#x3D; sys.argv[2:]</span><br><span class="line"></span><br><span class="line">    for i in range(0, len(image_filenames)):</span><br><span class="line">        img &#x3D; np.asarray(PIL.Image.open(image_filenames[i]))</span><br><span class="line">        img &#x3D; img.reshape(1, 3,512,512)</span><br><span class="line">        score &#x3D; D.run(img, None)</span><br><span class="line">        print(image_filenames[i], score[0][0])</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>使用示例如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find &#x2F;media&#x2F;gwern&#x2F;Data&#x2F;danbooru2018&#x2F;characters-1k-faces&#x2F; -type f | xargs -n 9000 --max-procs&#x3D;1 \</span><br><span class="line">    python ranker.py results&#x2F;02086-sgan-portraits-2gpu&#x2F;network-snapshot-058662.pkl \</span><br><span class="line">    | tee portraitfaces-rank.txt</span><br><span class="line">fgrep &#x2F;media&#x2F;gwern&#x2F; 2019-04-22-portraitfaces-rank.txt | \</span><br><span class="line">    sort --field-separator &#39; &#39; --key 2 --numeric-sort | head -100</span><br><span class="line"># ...&#x2F;megurine.luka&#x2F;7853120.jpg -708.6835</span><br><span class="line"># ...&#x2F;remilia.scarlet&#x2F;26352470.jpg -707.39856</span><br><span class="line"># ...&#x2F;z1.leberecht.maass..kantai.collection.&#x2F;26703440.jpg -702.76904</span><br><span class="line"># ...&#x2F;suzukaze.aoba&#x2F;27957490.jpg -700.5606</span><br><span class="line"># ...&#x2F;jack.the.ripper..fate.apocrypha.&#x2F;31991880.jpg -700.0554</span><br><span class="line"># ...&#x2F;senjougahara.hitagi&#x2F;4947410.jpg -699.0976</span><br><span class="line"># ...&#x2F;ayase.eli&#x2F;28374650.jpg -698.7358</span><br><span class="line"># ...&#x2F;ayase.eli&#x2F;16185520.jpg -696.97845</span><br><span class="line"># ...&#x2F;illustrious..azur.lane.&#x2F;31053930.jpg -696.8634</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure>
<p>你可以选择删除一定数量，或者最靠近末尾的TOP N%的图片。同时也应该检查最靠前的TOP的图像，有些十分异常的也需要删除。可以使用ranker.py提高生成的样本质量，简单示例。</p>
<h3 id="2-3-质量检测和数据增强"><a href="#2-3-质量检测和数据增强" class="headerlink" title="2.3 质量检测和数据增强"></a>2.3 质量检测和数据增强</h3><p>我们可以对图像质量进行人工校验，逐个浏览成百上千的图片，使用<code>findimagedupes -t 99%</code>来寻找近似相近的面部。在Danbooru2018中，可以有600-700000张脸，这已足够训练StyleGAN并且最终数据集有点大，会增加19倍。</p>
<p>但是如果我们需要在单一特征的小数据集上做，数据增强就比较有必要了。不需要做上下/左右翻转了，StyleGAN内部有做。我们可以做的是，颜色变换，锐化，模糊，增加/减小对比度，裁剪等操作。</p>
<h3 id="2-4-上采样和转换"><a href="#2-4-上采样和转换" class="headerlink" title="2.4 上采样和转换"></a>2.4 上采样和转换</h3><p>将图像转换成JPG可以大概节省33%的存储空间。但是切记，StyleGAN模型只接收在与其训练时所使用的相同的图片格式，像FFHQ数据集所使用的是PNG.</p>
<p>鉴于<code>dataset_tool.py</code>脚本在转换图片到tfrecords时太诡异，最好是打印每个处理完的图片，一旦程序崩溃，可以排错。对<code>dataset_tool.py</code>的简单修改如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:</span><br><span class="line">         order &#x3D; tfr.choose_shuffled_order() if shuffle else np.arange(len(image_filenames))</span><br><span class="line">         for idx in range(order.size):</span><br><span class="line">  print(image_filenames[order[idx]])</span><br><span class="line">             img &#x3D; np.asarray(PIL.Image.open(image_filenames[order[idx]]))</span><br><span class="line">             if channels &#x3D;&#x3D; 1:</span><br><span class="line">                 img &#x3D; img[np.newaxis, :, :] # HW &#x3D;&gt; CHW</span><br></pre></td></tr></table></figure>
<h2 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3 训练模型"></a>3 训练模型</h2><p><strong>参数配置</strong></p>
<ol>
<li><p><code>train/training_loop.py</code>:关键配置参数是training_loop.py的112行起。关键参数</p>
<ul>
<li><code>G_smoothing_kimg</code> 和<code>D_repeats</code>(影响学习的动态learning dynamics),</li>
<li><code>network_snapshot_ticks</code>(多久存储一次中间模型)</li>
<li><code>resume_run_id</code>: 设置为<code>latest</code></li>
<li><code>resume_kimg</code>.注意，它决定了模型训练的阶段，如果设置为0，模型会从头开始训练而无视之前的训练结果，即从最低分辨率开始。如果要做迁移学习，需要将其设置为一个足够高的数目，如10000，这样一来，模型就可以在最高分辨率，如$512\times 512$的阶段开始训练。</li>
<li>建议将<code>minibatch_repeats = 5</code>改为<code>minibatch_repeats = 1</code>。此处我怀疑ProGAN/StyleGAN中的梯度累加的实现，这样会使得训练过程更加稳定、更快。</li>
<li>注意，一些参数如学习率，会在<code>train.py</code>中被覆盖。最好是在覆盖的地方修改，</li>
</ul>
</li>
<li><p><code>train.py</code> (以前是<code>config.py</code>):设置GPU的数目，图像分辨率，数据集，学习率，水平翻转/镜像数据增强，以及minibatch-size。(此文件包含了ProGAN的一些配置参数，你并不是突然开启了ProGAN)。学习率和minbatch通常不用管（除非你想在训练的末尾阶段降低学习率以提升算法能力）。图像分辨率/dataset/mirroring需要设置，如</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">desc +&#x3D; &#39;-faces&#39;; dataset &#x3D; EasyDict(tfrecord_dir&#x3D;&#39;faces&#39;, resolution&#x3D;512); train.mirror_augment &#x3D; True</span><br></pre></td></tr></table></figure>
<p>此处设置了$512\times 512$的脸部数据集，我们前面创建的<code>datasets/faces</code>，启用mirror。假如没有8个GPU，必须修改<code>-preset</code>以匹配你的GPU数量，StyleGAN不会自动修改的。对于两块 2080ti，设置如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">desc +&#x3D; &#39;-preset-v2-2gpus&#39;; submit_config.num_gpus &#x3D; 2; sched.minibatch_base &#x3D; 8; sched.minibatch_dict &#x3D; \</span><br><span class="line">    &#123;4: 256, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16, 256: 8&#125;; sched.G_lrate_dict &#x3D; &#123;512: 0.0015, 1024: 0.002&#125;; \</span><br><span class="line">    sched.D_lrate_dict &#x3D; EasyDict(sched.G_lrate_dict); train.total_kimg &#x3D; 99000</span><br></pre></td></tr></table></figure>
<p>最后的结果会被保存到<code>results/00001-sgan-faces-2gpu</code>（<code>00001</code>代表递增ID,<code>sgan</code>因为使用的是StyleGAN而非ProGAN,<code>-faces</code>是训练的数据集,<code>-2gpu</code>即我们使用的多GPU）。</p>
<h2 id="4-运行过程"><a href="#4-运行过程" class="headerlink" title="4 运行过程"></a>4 运行过程</h2><p>相比于训练其他GAN，StyleGAN更稳定更好训练，但是也容易出问题。</p>
<h3 id="4-1-Crashproofing"><a href="#4-1-Crashproofing" class="headerlink" title="4.1 Crashproofing"></a>4.1 Crashproofing</h3><p>StyleGAN容易在混合GPU(1080ti+Titan V)上训练时崩溃，低版本的Tensorflow上也是，可以升级解决。如果崩溃了，代码无法自动继续上一次的训练迭代次数，需要手工在<code>training_loop.py</code>中修改<code>resume_run_id</code>为最后崩溃时的迭代次数。建议将此处的<code>resume_run_id</code>参数修改为<code>resume_run_id=latest</code>。</p>
<h3 id="4-2-调节学习率"><a href="#4-2-调节学习率" class="headerlink" title="4.2 调节学习率"></a>4.2 调节学习率</h3><p>学习率这个是最重要的超参数之一：在小batch size数据过大的更新会极大破坏GAN的稳定性和最终结果。论文在FFHQ数据集上，8个GPU，32的batch size时使用的学习率是0.003，但是在我们的动画数据集上，batch size=8更低的学习率效果更好。学习率与batch size非常相关，越难的数据集学习率应该更小。</p>
<h3 id="4-3-G-D的均衡"><a href="#4-3-G-D的均衡" class="headerlink" title="4.3 G/D的均衡"></a>4.3 G/D的均衡</h3><p>在后续的训练中，如果G没有产生很好的进步，没有朝着0.5的损失前进（而对应的D的损失朝着0.5大幅度缩减），并且在-1.0左右卡住或者其他的问题。此时，有必要调节G/D的均衡了。有几种方法可以完成此事，最简单的办法是在<code>train.py</code>中调节sched.G_lrate_dict的学习率参数。</p>
<p><img src="/images/blog/stylegan_owndata_3.png" alt=""></p>
<p>需要时刻关注G/D的损失，以及面部图像的perceptual质量，同时需要基于面部图像以及G/D的损失是否在爆炸或者严重不均衡而减小G和D的学习率（或者只减小D的学习率）。我们设想的是G/D的损失在一个确定的绝对损失值，同时质量有肉眼可见的提高，减小D的学习率有助于保持与G的均衡。当然如果超出你的耐心，或者时间不够，可以考虑同时减小D/G的学习率达到一个局部最优。</p>
<p>默认的0.003的学习率可能在达到高质量的面部和肖像图像时变得太高，可以将其减小三分之一或十分之一。如果任然不能收敛，D可能太强，可以单独的将其能力降低。由于训练的随机性和损失的相对性，可能需要在修改参数之后的很多小时或者很多天之后才能看到效果。</p>
<h3 id="4-4-跳过FID指标"><a href="#4-4-跳过FID指标" class="headerlink" title="4.4 跳过FID指标"></a>4.4 跳过FID指标</h3><p>一些指标用来计算日志。FID指标是ImageNet CNN的计算指标，可能在ImageNet中重要的特性在你的特定领域中其实是不相关的，并且一个大的FID如100是可以考虑的，FIDs为20或者增大都不太是个问题或者是个有用的指导，还不如直接看生成的样本呢。建议直接禁用FIDs指标（训练阶段并没有，所以直接禁用是安全的）。</p>
<p>可以直接通过注释<code>metrics.run</code>的调用来禁用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@@ -261,7 +265,7 @@ def training_loop()</span><br><span class="line">        if cur_tick % network_snapshot_ticks &#x3D;&#x3D; 0 or done or cur_tick &#x3D;&#x3D; 1:</span><br><span class="line">            pkl &#x3D; os.path.join(submit_config.run_dir, &#39;network-snapshot-%06d.pkl&#39; % (cur_nimg &#x2F;&#x2F; 1000))</span><br><span class="line">            misc.save_pkl((G, D, Gs), pkl)</span><br><span class="line">            # metrics.run(pkl, run_dir&#x3D;submit_config.run_dir, num_gpus&#x3D;submit_config.num_gpus, tf_config&#x3D;tf_config)</span><br></pre></td></tr></table></figure>
<h3 id="4-5-BLOB-斑块-和CRACK-裂缝-缺陷"><a href="#4-5-BLOB-斑块-和CRACK-裂缝-缺陷" class="headerlink" title="4.5 BLOB(斑块)和CRACK(裂缝)缺陷"></a>4.5 BLOB(斑块)和CRACK(裂缝)缺陷</h3><p>训练过程中，<code>blobs</code>(可以理解为斑块)时不时出现。这些blobs甚至出现在训练的后续阶段，在一些已经生成的高质量图像上，并且这些blob可能是与StyleGAN独有的(至少没有在其他GAN上出现过这个blob)。这些blob如此大并且刺眼。这些斑块出现的原因未知，据推测可能是$3\times 3$的卷积层导致的；可能使用额外的$1\times 1$卷积或者自相关层可以消除这个问题。</p>
<p>如果斑块出现得太频繁或者想完全消除，降低学习率达到一个局部最优可能有用。</p>
<p>训练动漫人物面部时，我看到了其他的缺陷，看起来像裂缝或者波浪或者皮肤上的皱纹，它们会一直伴随着训练直至最终。在小数据集做迁移学习时 会经常出现。与blob斑块相反，我目前怀疑裂缝的出现是过拟合的标识，而非StyleGAN的一种特质。当G开始记住最终的线条或像素上的精细细节的噪音时，目前的仅有的解决方案是要么停止训练要么增加数据。</p>
<h3 id="4-6-梯度累加"><a href="#4-6-梯度累加" class="headerlink" title="4.6 梯度累加"></a>4.6 梯度累加</h3><p>ProGAN/StyleGAN的代码宣称支持梯度累加，这是一种形似大的minibatch训练(batch_size=2048)的技巧，它通过不向后传播每个minibatch，但是累加多个minibatch，然后一次执行的方式实现。这是一种保持训练稳定的有效策略，增加minibatch尺寸有助于提高生成图像的质量。</p>
<p>但是ProGAN/StyleGAN的梯度累加的实现在Tensorflow或Pytorch中并没有类似的，<strong>以我个人的经验来看，最大可以加到4096，但是并没有看到什么区别，所以我怀疑这个实现是错误的。</strong></p>
<p>下面是我训练的动漫人脸的模型，训练了21980步，在2100万张图像上，38个GPU一天，尽管还没完全收敛，但是效果很好。<br><a href="https://www.gwern.net/images/gan/2019-03-16-stylegan-facestraining.mp4" target="_blank" rel="noopener">训练效果</a></p>
<h2 id="5-采样"><a href="#5-采样" class="headerlink" title="5 采样"></a>5 采样</h2><h3 id="5-1-PSI-Truncation-Trick"><a href="#5-1-PSI-Truncation-Trick" class="headerlink" title="5.1 PSI/Truncation Trick"></a>5.1 PSI/Truncation Trick</h3><p>截断技巧$\phi$  是所有StyleGAN生成器的最重要的超参数。它用在样本生成阶段，而非训练时。思路是，编辑latent 向量z，一个服从N(0,1)分布的向量，会自动删除所有大于特定值，比如0.5或1.0的变量。这看起来会避免极端的latent值，或者删除那些与G组合不太好的latent值。G不会生成与每个latent值在+1.5SD的点生成很多数据点。<br>代价便是这些依然是全部latent变量的何方区域，并且可以在训练期间被用来覆盖部分数据分布。因而，尽管latent变量接近0的均值才是最准确的模型，它们仅仅是全部可能的产生图像的数据空间上的一小部分。因而，我们可以从全部的无限制的正态分布$N(0,1)$上生成latent变量，也既可以截断如$+1SD或者+0.7SD$。</p>
<p>$\omega =0$时，多样性为0，并且所有生成的脸都是同一个角度(棕色眼睛，棕色头发的校园女孩，毫无例外的)，在$\omega \pm 0.5$时有更多区间的脸，在$\omega \pm 1.2$时会看到大量的多样性的脸/发型/一致性,但是也能看到大量的伪造像/失真像.参数$\omega$会极大地影响原始的输出。$\omega =1.2$时，得到的是异常原始但是极度真实或者失真。$\omega =0.5$时，具备一致连贯性，但是也很无聊。我的大部分采样，设置$\omega =0.7$可以得到最好的均衡。(就个人来说$\omega =1.2$时，采样最有趣)</p>
<h3 id="5-2-随机采样"><a href="#5-2-随机采样" class="headerlink" title="5.2 随机采样"></a>5.2 随机采样</h3><p>StyleGAN有个简单的脚本<code>prtrained_example.py</code>下载和生成单张人脸，为了复现效果，它在模型中指定了RNG随机数的种子，这样它会生成特定的人脸。然而，可以轻易地引入使用本地模型并生成，比如说1000张图像，指定参数$\omega =0.6$（此时会产生高质量图像，但是图像多样性较差）并保存结果到<code>results/example-{0-999}.png</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    tflib.init_tf()</span><br><span class="line">    _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02051-sgan-faces-2gpu&#x2F;network-snapshot-021980.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">    Gs.print_layers()</span><br><span class="line"></span><br><span class="line">    for i in range(0,1000):</span><br><span class="line">        rnd &#x3D; np.random.RandomState(None)</span><br><span class="line">        latents &#x3D; rnd.randn(1, Gs.input_shape[1])</span><br><span class="line">        fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">        images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;0.6, randomize_noise&#x3D;True, output_transform&#x3D;fmt)</span><br><span class="line">        os.makedirs(config.result_dir, exist_ok&#x3D;True)</span><br><span class="line">        png_filename &#x3D; os.path.join(config.result_dir, &#39;example-&#39;+str(i)+&#39;.png&#39;)</span><br><span class="line">        PIL.Image.fromarray(images[0], &#39;RGB&#39;).save(png_filename)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="5-3-Karras-et-al-2018图像"><a href="#5-3-Karras-et-al-2018图像" class="headerlink" title="5.3 Karras et al 2018图像"></a>5.3 Karras et al 2018图像</h3><p>此图像展示了使用1024像素的FFHQ 脸部模型(以及其他)，使用脚本<code>generate_figure.py</code>生成随机样本以及style noise的方面影响。此脚本需要大量修改来运行我的512像素的动漫人像。</p>
<ul>
<li><p>代码使用$\omega=1.0$截断，但是面部在$\omega=0.7$的时候看起来更好(好几个脚本都是用了<code>truncation_psi=</code>,但是严格来说，图3的<code>draw_style_mixiing_figure</code>将参数$\omega$隐藏在全局变量<code>sythesis_kwargs</code>中)</p>
</li>
<li><p>载入模型需要被换到动漫面部模型</p>
</li>
<li>需要将维度$1024\rightarrow 512$，其他被硬编码(hardcoded)的区间(ranges)必须被减小到521像素的图像。</li>
<li>截断技巧图8并没有足够的足够的面部来展示latent空间的用处，所以它需要被扩充来展示随机种子和面部图像，以及更多的$\omega$值。</li>
<li><code>bedroom/car/cat</code>样本应该被禁用</li>
</ul>
<p>代码改动如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> url_cars &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MJ6iCfNtMIRicihwRorsM3b7mmtmK9c3&#39; # karras2019stylegan-cars-512x384.pkl</span><br><span class="line"> url_cats &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MQywl0FNt6lHu8E_EUqnRbviagS7fbiJ&#39; # karras2019stylegan-cats-256x256.pkl</span><br><span class="line"></span><br><span class="line">-synthesis_kwargs &#x3D; dict(output_transform&#x3D;dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True), minibatch_size&#x3D;8)</span><br><span class="line">+synthesis_kwargs &#x3D; dict(output_transform&#x3D;dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True), minibatch_size&#x3D;8, truncation_psi&#x3D;0.7)</span><br><span class="line"></span><br><span class="line"> _Gs_cache &#x3D; dict()</span><br><span class="line"></span><br><span class="line"> def load_Gs(url):</span><br><span class="line">- if url not in _Gs_cache:</span><br><span class="line">- with dnnlib.util.open_url(url, cache_dir&#x3D;config.cache_dir) as f:</span><br><span class="line">- _G, _D, Gs &#x3D; pickle.load(f)</span><br><span class="line">- _Gs_cache[url] &#x3D; Gs</span><br><span class="line">- return _Gs_cache[url]</span><br><span class="line">+ _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02051-sgan-faces-2gpu&#x2F;network-snapshot-021980.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">+ return Gs</span><br><span class="line"></span><br><span class="line"> #----------------------------------------------------------------------------</span><br><span class="line"> # Figures 2, 3, 10, 11, 12: Multi-resolution grid of uncurated result images.</span><br><span class="line">@@ -85,7 +82,7 @@ def draw_noise_detail_figure(png, Gs, w, h, num_samples, seeds):</span><br><span class="line">     canvas &#x3D; PIL.Image.new(&#39;RGB&#39;, (w * 3, h * len(seeds)), &#39;white&#39;)</span><br><span class="line">     for row, seed in enumerate(seeds):</span><br><span class="line">         latents &#x3D; np.stack([np.random.RandomState(seed).randn(Gs.input_shape[1])] * num_samples)</span><br><span class="line">- images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;1, **synthesis_kwargs)</span><br><span class="line">+ images &#x3D; Gs.run(latents, None, **synthesis_kwargs)</span><br><span class="line">         canvas.paste(PIL.Image.fromarray(images[0], &#39;RGB&#39;), (0, row * h))</span><br><span class="line">         for i in range(4):</span><br><span class="line">             crop &#x3D; PIL.Image.fromarray(images[i + 1], &#39;RGB&#39;)</span><br><span class="line">@@ -109,7 +106,7 @@ def draw_noise_components_figure(png, Gs, w, h, seeds, noise_ranges, flips):</span><br><span class="line">     all_images &#x3D; []</span><br><span class="line">     for noise_range in noise_ranges:</span><br><span class="line">         tflib.set_vars(&#123;var: val * (1 if i in noise_range else 0) for i, (var, val) in enumerate(noise_pairs)&#125;)</span><br><span class="line">- range_images &#x3D; Gsc.run(latents, None, truncation_psi&#x3D;1, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">+ range_images &#x3D; Gsc.run(latents, None, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">         range_images[flips, :, :] &#x3D; range_images[flips, :, ::-1]</span><br><span class="line">         all_images.append(list(range_images))</span><br><span class="line"></span><br><span class="line">@@ -144,14 +141,11 @@ def draw_truncation_trick_figure(png, Gs, w, h, seeds, psis):</span><br><span class="line"> def main():</span><br><span class="line">     tflib.init_tf()</span><br><span class="line">     os.makedirs(config.result_dir, exist_ok&#x3D;True)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure02-uncurated-ffhq.png&#39;), load_Gs(url_ffhq), cx&#x3D;0, cy&#x3D;0, cw&#x3D;1024, ch&#x3D;1024, rows&#x3D;3, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;5)</span><br><span class="line">- draw_style_mixing_figure(os.path.join(config.result_dir, &#39;figure03-style-mixing.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, src_seeds&#x3D;[639,701,687,615,2268], dst_seeds&#x3D;[888,829,1898,1733,1614,845], style_ranges&#x3D;[range(0,4)]*3+[range(4,8)]*2+[range(8,18)])</span><br><span class="line">- draw_noise_detail_figure(os.path.join(config.result_dir, &#39;figure04-noise-detail.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, num_samples&#x3D;100, seeds&#x3D;[1157,1012])</span><br><span class="line">- draw_noise_components_figure(os.path.join(config.result_dir, &#39;figure05-noise-components.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, seeds&#x3D;[1967,1555], noise_ranges&#x3D;[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips&#x3D;[1])</span><br><span class="line">- draw_truncation_trick_figure(os.path.join(config.result_dir, &#39;figure08-truncation-trick.png&#39;), load_Gs(url_ffhq), w&#x3D;1024, h&#x3D;1024, seeds&#x3D;[91,388], psis&#x3D;[1, 0.7, 0.5, 0, -0.5, -1])</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure10-uncurated-bedrooms.png&#39;), load_Gs(url_bedrooms), cx&#x3D;0, cy&#x3D;0, cw&#x3D;256, ch&#x3D;256, rows&#x3D;5, lods&#x3D;[0,0,1,1,2,2,2], seed&#x3D;0)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure11-uncurated-cars.png&#39;), load_Gs(url_cars), cx&#x3D;0, cy&#x3D;64, cw&#x3D;512, ch&#x3D;384, rows&#x3D;4, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;2)</span><br><span class="line">- draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure12-uncurated-cats.png&#39;), load_Gs(url_cats), cx&#x3D;0, cy&#x3D;0, cw&#x3D;256, ch&#x3D;256, rows&#x3D;5, lods&#x3D;[0,0,1,1,2,2,2], seed&#x3D;1)</span><br><span class="line">+ draw_uncurated_result_figure(os.path.join(config.result_dir, &#39;figure02-uncurated-ffhq.png&#39;), load_Gs(url_ffhq), cx&#x3D;0, cy&#x3D;0, cw&#x3D;512, ch&#x3D;512, rows&#x3D;3, lods&#x3D;[0,1,2,2,3,3], seed&#x3D;5)</span><br><span class="line">+ draw_style_mixing_figure(os.path.join(config.result_dir, &#39;figure03-style-mixing.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, src_seeds&#x3D;[639,701,687,615,2268], dst_seeds&#x3D;[888,829,1898,1733,1614,845], style_ranges&#x3D;[range(0,4)]*3+[range(4,8)]*2+[range(8,16)])</span><br><span class="line">+ draw_noise_detail_figure(os.path.join(config.result_dir, &#39;figure04-noise-detail.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, num_samples&#x3D;100, seeds&#x3D;[1157,1012])</span><br><span class="line">+ draw_noise_components_figure(os.path.join(config.result_dir, &#39;figure05-noise-components.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, seeds&#x3D;[1967,1555], noise_ranges&#x3D;[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips&#x3D;[1])</span><br><span class="line">+ draw_truncation_trick_figure(os.path.join(config.result_dir, &#39;figure08-truncation-trick.png&#39;), load_Gs(url_ffhq), w&#x3D;512, h&#x3D;512, seeds&#x3D;[91,388, 389, 390, 391, 392, 393, 394, 395, 396], psis&#x3D;[1, 0.7, 0.5, 0.25, 0, -0.25, -0.5, -1])</span><br></pre></td></tr></table></figure>
<p>修改完之后，可以得到一些有趣的动漫人脸样本。</p>
<p><img src="/images/blog/stylegan_owndata_4.png" alt=""></p>
<p>上图是随机样本</p>
<p><img src="/images/blog/stylegan_owndata_5.png" alt=""></p>
<p>上图是使用风格混合样本。展示了编辑和差值(第一行是风格，左边列代表了要转变风格的图像)</p>
<p><img src="/images/blog/stylegan_owndata_6.png" alt=""></p>
<p>上图展示了使用阶段技巧的。10张随机面部，$\omega$区间为$[1,0.7,0.5,0.25,-0.25,-0.5,-1]$展示了在多样性/质量/平均脸之间的妥协。</p>
<h2 id="6-视频"><a href="#6-视频" class="headerlink" title="6 视频"></a>6 视频</h2><h3 id="6-1-训练剪辑"><a href="#6-1-训练剪辑" class="headerlink" title="6.1 训练剪辑"></a>6.1 训练剪辑</h3><p>最简单的样本时在训练过程中产生的中间结果，训练过程中由于分辨率递增和更精细细节的生成，样本尺寸也会增加，最后视频可能会很大(动漫人脸大概会有14MB)，所以有必要做一些压缩。使用工具<code>pngnq+advpng</code>或者将它们转成JPG格式(图像质量会降低)，在PNG图像上使用FFmpeg将训练过程中的图像转成视频剪辑。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat $(ls .&#x2F;results&#x2F;*faces*&#x2F;fakes*.png | sort --numeric-sort) | ffmpeg -framerate 10 \ # show 10 inputs per second</span><br><span class="line">    -i - # stdin</span><br><span class="line">    -r 25 # output frame-rate; frames will be duplicated to pad out to 25FPS</span><br><span class="line">    -c:v libx264 # x264 for compatibility</span><br><span class="line">    -pix_fmt yuv420p # force ffmpeg to use a standard colorspace - otherwise PNG colorspace is kept, breaking browsers (!)</span><br><span class="line">    -crf 33 # adequate high quality</span><br><span class="line">    -vf &quot;scale&#x3D;iw&#x2F;2:ih&#x2F;2&quot; \ # shrink the image by 2x, the full detail is not necessary &amp; saves space</span><br><span class="line">    -preset veryslow -tune animation \ # aim for smallest binary possible with animation-tuned settings</span><br><span class="line">    .&#x2F;stylegan-facestraining.mp4</span><br></pre></td></tr></table></figure>
<h3 id="6-2-差值"><a href="#6-2-差值" class="headerlink" title="6.2 差值"></a>6.2 差值</h3><p>原始的ProGAN仓库代码提供了配置文件来生成差值视频的，但是在StyleGAN中被移除了，<a href="https://colab.research.google.com/gist/kikko/d48c1871206fc325fa6f7372cf58db87/stylegan-experiments.ipynb" target="_blank" rel="noopener">Cyril Diagne的替代实现</a>(已经没法打开了)提供了三种视频</p>
<ol>
<li><p><code>random_grid_404.mp4</code>:标准差值视频，在latent空间中简单的随机游走。修改这些所有变量变量并做成动画，默认会作出$2\times 2$一共4个视频。几个差值视频可以从<a href="https://www.gwern.net/Faces#examples" target="_blank" rel="noopener">这里</a>看到 </p>
</li>
<li><p><code>interpolate.mp4</code>:粗糙的风格混合视频。生成单一的<code>源</code>面部图，一个二流的差值视频，在生成之前在latent空间中随机游走，每个随机步，其<code>粗糙(coarse)/高级(high-level)风格</code>噪音都会从随机步复制到<code>源</code>面部风格噪音数据中。对于面部来说，<code>源</code>面部会被各式各样地修改，比如方向、面部表情，但是基本面部可以被识别。</p>
</li>
</ol>
<p>下面是<code>video.py</code>代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">import PIL.Image</span><br><span class="line">import dnnlib</span><br><span class="line">import dnnlib.tflib as tflib</span><br><span class="line">import config</span><br><span class="line">import scipy</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    tflib.init_tf()</span><br><span class="line"></span><br><span class="line">    # Load pre-trained network.</span><br><span class="line">    # url &#x3D; &#39;https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id&#x3D;1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ&#39;</span><br><span class="line">    # with dnnlib.util.open_url(url, cache_dir&#x3D;config.cache_dir) as f:</span><br><span class="line">    ## NOTE: insert model here:</span><br><span class="line">    _G, _D, Gs &#x3D; pickle.load(open(&quot;results&#x2F;02047-sgan-faces-2gpu&#x2F;network-snapshot-013221.pkl&quot;, &quot;rb&quot;))</span><br><span class="line">    # _G &#x3D; Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.</span><br><span class="line">    # _D &#x3D; Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.</span><br><span class="line">    # Gs &#x3D; Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.</span><br><span class="line"></span><br><span class="line">    grid_size &#x3D; [2,2]</span><br><span class="line">    image_shrink &#x3D; 1</span><br><span class="line">    image_zoom &#x3D; 1</span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line">    random_seed &#x3D; 404</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;random_grid_%s.mp4&#39; % random_seed</span><br><span class="line">    minibatch_size &#x3D; 8</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line">    # Generate latent vectors</span><br><span class="line">    shape &#x3D; [num_frames, np.prod(grid_size)] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    all_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    import scipy</span><br><span class="line">    all_latents &#x3D; scipy.ndimage.gaussian_filter(all_latents, [smoothing_sec * mp4_fps] + [0] * len(Gs.input_shape), mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    all_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(all_latents)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def create_image_grid(images, grid_size&#x3D;None):</span><br><span class="line">        assert images.ndim &#x3D;&#x3D; 3 or images.ndim &#x3D;&#x3D; 4</span><br><span class="line">        num, img_h, img_w, channels &#x3D; images.shape</span><br><span class="line"></span><br><span class="line">        if grid_size is not None:</span><br><span class="line">            grid_w, grid_h &#x3D; tuple(grid_size)</span><br><span class="line">        else:</span><br><span class="line">            grid_w &#x3D; max(int(np.ceil(np.sqrt(num))), 1)</span><br><span class="line">            grid_h &#x3D; max((num - 1) &#x2F;&#x2F; grid_w + 1, 1)</span><br><span class="line"></span><br><span class="line">        grid &#x3D; np.zeros([grid_h * img_h, grid_w * img_w, channels], dtype&#x3D;images.dtype)</span><br><span class="line">        for idx in range(num):</span><br><span class="line">            x &#x3D; (idx % grid_w) * img_w</span><br><span class="line">            y &#x3D; (idx &#x2F;&#x2F; grid_w) * img_h</span><br><span class="line">            grid[y : y + img_h, x : x + img_w] &#x3D; images[idx]</span><br><span class="line">        return grid</span><br><span class="line"></span><br><span class="line">    # Frame generation func for moviepy.</span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        latents &#x3D; all_latents[frame_idx]</span><br><span class="line">        fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">        images &#x3D; Gs.run(latents, None, truncation_psi&#x3D;0.7,</span><br><span class="line">                              randomize_noise&#x3D;False, output_transform&#x3D;fmt)</span><br><span class="line"></span><br><span class="line">        grid &#x3D; create_image_grid(images, grid_size)</span><br><span class="line">        if image_zoom &gt; 1:</span><br><span class="line">            grid &#x3D; scipy.ndimage.zoom(grid, [image_zoom, image_zoom, 1], order&#x3D;0)</span><br><span class="line">        if grid.shape[2] &#x3D;&#x3D; 1:</span><br><span class="line">            grid &#x3D; grid.repeat(3, 2) # grayscale &#x3D;&gt; RGB</span><br><span class="line">        return grid</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">    # import scipy</span><br><span class="line">    # coarse</span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_seed &#x3D; 500</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w &#x3D; 512</span><br><span class="line">    h &#x3D; 512</span><br><span class="line">    #src_seeds &#x3D; [601]</span><br><span class="line">    dst_seeds &#x3D; [700]</span><br><span class="line">    style_ranges &#x3D; ([0] * 7 + [range(8,16)]) * len(dst_seeds)</span><br><span class="line"></span><br><span class="line">    fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">    synthesis_kwargs &#x3D; dict(output_transform&#x3D;fmt, truncation_psi&#x3D;0.7, minibatch_size&#x3D;8)</span><br><span class="line"></span><br><span class="line">    shape &#x3D; [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    src_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    src_latents &#x3D; scipy.ndimage.gaussian_filter(src_latents,</span><br><span class="line">                                                smoothing_sec * mp4_fps,</span><br><span class="line">                                                mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    src_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(src_latents)))</span><br><span class="line"></span><br><span class="line">    dst_latents &#x3D; np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in dst_seeds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    src_dlatents &#x3D; Gs.components.mapping.run(src_latents, None) # [seed, layer, component]</span><br><span class="line">    dst_dlatents &#x3D; Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]</span><br><span class="line">    src_images &#x3D; Gs.components.synthesis.run(src_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">    dst_images &#x3D; Gs.components.synthesis.run(dst_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    canvas &#x3D; PIL.Image.new(&#39;RGB&#39;, (w * (len(dst_seeds) + 1), h * 2), &#39;white&#39;)</span><br><span class="line"></span><br><span class="line">    for col, dst_image in enumerate(list(dst_images)):</span><br><span class="line">        canvas.paste(PIL.Image.fromarray(dst_image, &#39;RGB&#39;), ((col + 1) * h, 0))</span><br><span class="line"></span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        src_image &#x3D; src_images[frame_idx]</span><br><span class="line">        canvas.paste(PIL.Image.fromarray(src_image, &#39;RGB&#39;), (0, h))</span><br><span class="line"></span><br><span class="line">        for col, dst_image in enumerate(list(dst_images)):</span><br><span class="line">            col_dlatents &#x3D; np.stack([dst_dlatents[col]])</span><br><span class="line">            col_dlatents[:, style_ranges[col]] &#x3D; src_dlatents[frame_idx, style_ranges[col]]</span><br><span class="line">            col_images &#x3D; Gs.components.synthesis.run(col_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">            for row, image in enumerate(list(col_images)):</span><br><span class="line">                canvas.paste(PIL.Image.fromarray(image, &#39;RGB&#39;), ((col + 1) * h, (row + 1) * w))</span><br><span class="line">        return np.array(canvas)</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;interpolate.mp4&#39;</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line"></span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">    import scipy</span><br><span class="line"></span><br><span class="line">    duration_sec &#x3D; 60.0</span><br><span class="line">    smoothing_sec &#x3D; 1.0</span><br><span class="line">    mp4_fps &#x3D; 20</span><br><span class="line"></span><br><span class="line">    num_frames &#x3D; int(np.rint(duration_sec * mp4_fps))</span><br><span class="line">    random_seed &#x3D; 503</span><br><span class="line">    random_state &#x3D; np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    w &#x3D; 512</span><br><span class="line">    h &#x3D; 512</span><br><span class="line">    style_ranges &#x3D; [range(6,16)]</span><br><span class="line"></span><br><span class="line">    fmt &#x3D; dict(func&#x3D;tflib.convert_images_to_uint8, nchw_to_nhwc&#x3D;True)</span><br><span class="line">    synthesis_kwargs &#x3D; dict(output_transform&#x3D;fmt, truncation_psi&#x3D;0.7, minibatch_size&#x3D;8)</span><br><span class="line"></span><br><span class="line">    shape &#x3D; [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]</span><br><span class="line">    src_latents &#x3D; random_state.randn(*shape).astype(np.float32)</span><br><span class="line">    src_latents &#x3D; scipy.ndimage.gaussian_filter(src_latents,</span><br><span class="line">                                                smoothing_sec * mp4_fps,</span><br><span class="line">                                                mode&#x3D;&#39;wrap&#39;)</span><br><span class="line">    src_latents &#x2F;&#x3D; np.sqrt(np.mean(np.square(src_latents)))</span><br><span class="line"></span><br><span class="line">    dst_latents &#x3D; np.stack([random_state.randn(Gs.input_shape[1])])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    src_dlatents &#x3D; Gs.components.mapping.run(src_latents, None) # [seed, layer, component]</span><br><span class="line">    dst_dlatents &#x3D; Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def make_frame(t):</span><br><span class="line">        frame_idx &#x3D; int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))</span><br><span class="line">        col_dlatents &#x3D; np.stack([dst_dlatents[0]])</span><br><span class="line">        col_dlatents[:, style_ranges[0]] &#x3D; src_dlatents[frame_idx, style_ranges[0]]</span><br><span class="line">        col_images &#x3D; Gs.components.synthesis.run(col_dlatents, randomize_noise&#x3D;False, **synthesis_kwargs)</span><br><span class="line">        return col_images[0]</span><br><span class="line"></span><br><span class="line">    # Generate video.</span><br><span class="line">    import moviepy.editor</span><br><span class="line">    mp4_file &#x3D; &#39;results&#x2F;fine_%s.mp4&#39; % (random_seed)</span><br><span class="line">    mp4_codec &#x3D; &#39;libx264&#39;</span><br><span class="line">    mp4_bitrate &#x3D; &#39;5M&#39;</span><br><span class="line"></span><br><span class="line">    video_clip &#x3D; moviepy.editor.VideoClip(make_frame, duration&#x3D;duration_sec)</span><br><span class="line">    video_clip.write_videofile(mp4_file, fps&#x3D;mp4_fps, codec&#x3D;mp4_codec, bitrate&#x3D;mp4_bitrate)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ol>
<li><code>fine_503.mp4</code>：一个精细风格混合视频。</li>
</ol>
<h2 id="7-模型"><a href="#7-模型" class="headerlink" title="7 模型"></a>7 模型</h2><h3 id="7-1-动漫人脸"><a href="#7-1-动漫人脸" class="headerlink" title="7.1  动漫人脸"></a>7.1  动漫人脸</h3><p>训练的基准模型的数据来源是上面的数据预处理和训练阶段介绍过。是一个在218794张动漫人脸上，使用512像素的StyleGAN训练出来的，数据时所有Danboru2017数据集上裁剪的，清洗、上采样，并训练了21980次迭代，38个GPU天。</p>
<p>下载（推荐使用最近的<a href="https://www.gwern.net/Faces#portrait-results" target="_blank" rel="noopener">portrait StyleGAN</a>,除非需要特别剪切的脸部）</p>
<ul>
<li><p><a href="https://mega.nz/#!2DRDQIjJ!JKQ_DhEXCzeYJXjliUSWRvE-_rfrvWv_cq3pgRuFadw" target="_blank" rel="noopener">随机样本</a> 在2019年2月14日随机生成的，使用了一个极大的$\omega=1.2$(165MB,JPG)</p>
</li>
<li><p><a href="https://mega.nz/#!aPRFDKaC!FDpQi_FEPK443JoRBEOEDOmlLmJSblKFlqZ1A1XPt2Y" target="_blank" rel="noopener">StyleGAN 模型 This Waifu Does Not Exist</a>(294MBm<code>.pkl</code>)</p>
</li>
<li><p><a href="https://mega.nz/#!vawjXISI!F7s13yRicxDA3QYqYDL2kjnc2K7Zk3DwCIYETREmBP4" target="_blank" rel="noopener">动漫人脸StyleGAN模型</a>最近训练的。</p>
</li>
</ul>
<h2 id="8-迁移学习"><a href="#8-迁移学习" class="headerlink" title="8 迁移学习"></a>8 迁移学习</h2><p>特定的动漫人脸模型迁移学习到特定角色是很简单的：角色的图像太少，无法训练一个好的StyleGAN模型，同样的，采样不充分的StyleGAN的数据增强也不行，但是由于StyleGAN在所有类型的动漫人脸训练得到，StyleGAN学习到足够充分的特征空间，可以轻易地拟合到特定角色而不会出现过拟合。</p>
<p>制作特定脸部模型时，图像数量越多越好，但是一般n=500-5000足矣，甚至n=50都可以。论文中的结论</p>
<p><strong>尽管StyleGAN的 generator是在人脸数据集上训练得到的，但是其embeding算法足以表征更大的空间。论文中的图表示，虽然比不上生成人脸的效果，但是依然能获得不错的高质量的猫、狗甚至油画和车辆的表征</strong>如果说连如此不同的车辆都可以被成功编码进人脸的StyleGAN，那么很显然latent空间可以轻易地对一个新的人脸建模。因此，我们可以判断训练过程可能与学习新面孔不太相关，这样任务就简单许多。</p>
<p>由于StyleGAN目前是非条件生成网络也没有在限定领域文本或元数据上编码，只使用了海量图片，所有需要做的就是将新数据集编码，然后简单地在已有模型基础上开始训练就可以了。</p>
<ol>
<li>准备新数据集</li>
<li>编辑<code>train.py</code>,给<code>-desc</code>行重新赋值</li>
<li>正确地给<code>resume_kimg</code>赋值，<code>resume_run_id=&quot;latest&quot;</code></li>
<li>开始运行<code>python train.py</code>，就可以迁移学习了</li>
</ol>
<p>主要问题是，没法从头开始(第0次迭代)，我尝试过这么做，但是效果不好并且StyleGAN看起来可能直接忽视了预训练模型。我个人假设是，作为ProGAN的一部分，在额外的分辨率或网络层上增长或消退，StyleGAN简单的随机或擦除新的网络层并覆盖它们，这使得这么做没有意义。这很好避免，简单地跳过训练进程，直接到期望的分辨率。例如，开始一个512像素的数据集训练时，可以在<code>training_loop.py</code>中设置<code>resume_king=7000</code>。这会强行让StyleGAN跳过所有的progressing growing步骤，并载入全部的模型。如何校验呢？检查第一幅吐下你给(<code>fakes07000.png</code>或者其他的)，从之前的任何的迁移学习训练完成，它应当看起来像是原始模型在训练结束时的效果。接下来的训练样本应该表现出原始图像快速适应(变形到)新数据集（应该不会出现类似<code>fakes0000.png</code>的图像，因为这表明是从头开始训练）</p>
<h3 id="8-1-动漫人脸模型迁移到特定角色人脸"><a href="#8-1-动漫人脸模型迁移到特定角色人脸" class="headerlink" title="8.1 动漫人脸模型迁移到特定角色人脸"></a>8.1 动漫人脸模型迁移到特定角色人脸</h3><p>第一个迁移的角色是 Holo，使用了从Danboru2017的数据集中筛选出来的Holo面部图像，使用<code>waifu2x</code>缩放到512像素，手工清理，并做数据增强，从3900张增强到12600张图像，同时使用了镜像翻转，因为Holo面部是对称的。使用的预训练模型是2019年2月9号的一个动漫人脸模型，尚未完全收敛。</p>
<p>值得一提的是，这个数据集之前用ProGAN来训练的，但是几周的训练之后，ProGAN严重过拟合，并产生崩坏。<br>训练过程相当快，只有几百次迭代之后就可以看到肉眼可见的Holo的脸部图了。</p>
<p>StyleGAN要成功得多，尽管有几个失败的点出现在动漫人脸上。事实上，几百次迭代之后，它开始过拟合这些裂缝/伪影/脏点。最终使用的是迭代次数为11370的模型，而且依然有些过拟合。我个人认为总数n(数据增强之后)，Holo应该训练训练更长时间(FFHQ数据集的1/7)，但是显然不是。可能数据增强并没有太大价值，又或者要么多样性编码并没那么有用，要么这些操作有用，但是StyleGAN已经从之前的训练中学习到，并且需要更多真实数据来理解Holo的面部。</p>
<p>11370次迭代的<a href="https://mega.nz/#!afIjAAoJ!ATuVaw-9k5I5cL_URTuK2zI9mybdgFGYMJKUUHUfbk8" target="_blank" rel="noopener">模型下载</a></p>
<h3 id="8-2-动漫人脸迁移到FFHQ人脸"><a href="#8-2-动漫人脸迁移到FFHQ人脸" class="headerlink" title="8.2 动漫人脸迁移到FFHQ人脸"></a>8.2 动漫人脸迁移到FFHQ人脸</h3><p>如果StyleGAN可以平滑地表征动漫人脸，并使用参数$\omega$承载了全局的如头发长度+颜色属性转换，参数$\omega$可能一种快速的方式来空值单一角色的大尺度变化。例如，性别变换，或者动漫到真人的变换？（给定图像/latent向量，可以简单地改变正负号来将其变成相反的属性，这可以每个随机脸相反的版本，而且如果有人有编码器，就可以自动地转换了）。</p>
<p>数据来源：可以方便的使用FFHQ下载脚本，然后将图像下采样到512像素，甚至构建一个FFHQ+动漫头像的数据集。<br>最快最先要做的是，从动漫人脸到FFHQ真人脸的迁移学习。可能模型无法得到足够的动漫知识，然后去拟合，但是值得一试。早期的训练结果如下，有点像僵尸</p>
<p><img src="/images/blog/stylegan_owndata_7.png" alt=""></p>
<p>97次迭代(ticks)之后，模型收敛到一个正常的面孔，唯一可能保留的线索是一些训练样本中的过度美化的发型。</p>
<p><img src="/images/blog/stylegan_owndata_8.png" alt=""></p>
<h3 id="8-3-动漫脸—-gt-动漫脸-FFHQ脸"><a href="#8-3-动漫脸—-gt-动漫脸-FFHQ脸" class="headerlink" title="8.3 动漫脸—&gt;动漫脸+FFHQ脸"></a>8.3 动漫脸—&gt;动漫脸+FFHQ脸</h3><p>下一步是同时训练动漫脸和FFHQ脸模型，尽管开始时数据集的鲜明的不同，将会是正的VS负的$\omega$最终导致划分为真实VS动漫，并提供一个便宜并且简单的方法来转换任意脸部图像。</p>
<p>简单的合并512像素的FFHQ脸部图像和521像素的动漫脸部，并从之前的FFHQ模型基础上训练（我怀疑，一些动漫图像数据仍然在模型中，因此这将会比从原始的动漫脸部模型中训练要快一点）。我训练了812次迭代，11359-12171张图像，超过2个GPU天。</p>
<p>它确实能够较好地学习两种类型的面孔，清晰地分离样本如下</p>
<p><img src="/images/blog/stylegan_owndata_9.png" alt=""></p>
<p>但是，迁移学习和$\omega$采样的结果是不如意的，修改不同领域的风格混合，或者不同领域之间的转换的能力有限。截断技巧无法清晰地解耦期望的特征（事实上，多种$\omega$ 没法清晰对应什么）。</p>
<p><img src="/images/blog/stylegan_owndata_10.png" alt=""></p>
<p>StyleGAN的动漫+FFHQ的风格混合结果。</p>
<h2 id="9-逆转StyleGAN来控制和修改图像"><a href="#9-逆转StyleGAN来控制和修改图像" class="headerlink" title="9 逆转StyleGAN来控制和修改图像"></a>9 逆转StyleGAN来控制和修改图像</h2><p>一个非条件GAN架构，默认是单向的：latent向量z从众多$N(0,1)$变量中随机生成得到的，喂入GAN，并输出图像。没有办法让非条件GAN逆向，即喂入图像输出其latent。</p>
<p>最直接的方法是转向条件GAN架构，基于文本或者标签embeding。然后生成特定特征，戴眼镜，微笑。当前无法操作，因为生成一个带标签或者embedding并且训练的StyleGAN需要的不是一点半点的修改。这也不是一个完整的解决方案，因为它无法在现存的图像进行编辑。</p>
<p>对于非条件GAN，有两种实现方式来逆转G。</p>
<ol>
<li>神经网络可以做什么，另外一个神经网络就可以学到逆操作。<a href="https://arxiv.org/abs/1907.02544" target="_blank" rel="noopener">Donahue 2016</a>,<a href="https://arxiv.org/abs/1907.02544" target="_blank" rel="noopener">Donahue Simonyan 2019</a>.如果StyleGAN学习到了$z$到图像的映射，那么训练第二个神经网络来监督学习从图像到$z$的映射，</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>使用传统方法换脸算法</title>
    <url>/2019/04/28/cv-switch-face/</url>
    <content><![CDATA[<p>参考 <a href="https://matthewearl.github.io/2015/07/28/switching-eds-with-python/" target="_blank" rel="noopener">switch face with python</a></p>
<h3 id="1-使用dlib抽取面部关键点"><a href="#1-使用dlib抽取面部关键点" class="headerlink" title="1 使用dlib抽取面部关键点"></a>1 使用dlib抽取面部关键点</h3><p><img src="/images/blog/cv_switch_face_1.png" alt=""></p>
<p>关键代码如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PREDICTOR_PATH &#x3D; &quot;&#x2F;home&#x2F;matt&#x2F;dlib-18.16&#x2F;shape_predictor_68_face_landmarks.dat&quot;</span><br><span class="line"></span><br><span class="line">detector &#x3D; dlib.get_frontal_face_detector()</span><br><span class="line">predictor &#x3D; dlib.shape_predictor(PREDICTOR_PATH)</span><br><span class="line"></span><br><span class="line">def get_landmarks(im):</span><br><span class="line">    rects &#x3D; detector(im, 1)</span><br><span class="line">    </span><br><span class="line">    if len(rects) &gt; 1:</span><br><span class="line">        raise TooManyFaces</span><br><span class="line">    if len(rects) &#x3D;&#x3D; 0:</span><br><span class="line">        raise NoFaces</span><br><span class="line"></span><br><span class="line">    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])</span><br></pre></td></tr></table></figure></p>
<p>特征抽取器<code>predictor</code>传入一个矩形的人脸部分，预测内部的人脸的68个关键点坐标，即$68\times 2$个值。</p>
<h3 id="2-使用procrustes分析进行人脸对齐"><a href="#2-使用procrustes分析进行人脸对齐" class="headerlink" title="2 使用procrustes分析进行人脸对齐"></a>2 使用procrustes分析进行人脸对齐</h3><p>检测两张人脸的关键点之后，每个点的特定属性我们是知道的，比如第30个点代表的是鼻尖的坐标。我们接下来要做的是，如何扭曲、转换、以及缩放第一个人脸的点，使得它与目标关键点尽可能接近。这个相同的转换步骤可以用，第二个人脸图像来覆盖第一个人脸图像来实现。</p>
<p>数学形式的解法为，我们寻找$T,s,R$最小化下面的等式:</p>
<script type="math/tex; mode=display">
\sum _{i=1} ^{68}|sRp_i ^T+T-q_i ^T|^2</script><p>其中$R$是一个$2\times 2$的正交矩阵，$s$是个标量，$T$是一个2向量，$p_ihe q_i$是上面计算得到的68个关键点。此问题等价于求解一个<strong>正交procrustes分析</strong>问题。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def transformation_from_points(points1, points2):</span><br><span class="line">    points1 &#x3D; points1.astype(numpy.float64)</span><br><span class="line">    points2 &#x3D; points2.astype(numpy.float64)</span><br><span class="line"></span><br><span class="line">    c1 &#x3D; numpy.mean(points1, axis&#x3D;0)</span><br><span class="line">    c2 &#x3D; numpy.mean(points2, axis&#x3D;0)</span><br><span class="line">    points1 -&#x3D; c1</span><br><span class="line">    points2 -&#x3D; c2</span><br><span class="line"></span><br><span class="line">    s1 &#x3D; numpy.std(points1)</span><br><span class="line">    s2 &#x3D; numpy.std(points2)</span><br><span class="line">    points1 &#x2F;&#x3D; s1</span><br><span class="line">    points2 &#x2F;&#x3D; s2</span><br><span class="line"></span><br><span class="line">    U, S, Vt &#x3D; numpy.linalg.svd(points1.T * points2)</span><br><span class="line">    R &#x3D; (U * Vt).T</span><br><span class="line"></span><br><span class="line">    return numpy.vstack([numpy.hstack(((s2 &#x2F; s1) * R,</span><br><span class="line">                                       c2.T - (s2 &#x2F; s1) * R * c1.T)),</span><br><span class="line">                         numpy.matrix([0., 0., 1.])])</span><br></pre></td></tr></table></figure>
<p>以上代码执行了如下步骤</p>
<ol>
<li>将所有输入转换为浮点型，便于后续的计算</li>
<li>减去每个点集合的中心坐标(即减去均值)。一旦结果点集合的最优变换和扭曲解找到，中心的<code>c1</code>和<code>c2</code>可以用来求解全局解。</li>
<li>类似的，每个点除以标准差。消除尺度影响</li>
<li>使用SVD计算扭曲比率，需要去查看<strong>正交Procrustes问题</strong>的求解过程才能了解。</li>
<li>返回完整的转换为放射变换矩阵。</li>
</ol>
<p>结果可以用Opencv的<code>cv2.wrapAffine</code>函数来映射第二张图到第一张图。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def warp_im(im, M, dshape):</span><br><span class="line">    output_im &#x3D; numpy.zeros(dshape, dtype&#x3D;im.dtype)</span><br><span class="line">    cv2.warpAffine(im,</span><br><span class="line">                   M[:2],</span><br><span class="line">                   (dshape[1], dshape[0]),</span><br><span class="line">                   dst&#x3D;output_im,</span><br><span class="line">                   borderMode&#x3D;cv2.BORDER_TRANSPARENT,</span><br><span class="line">                   flags&#x3D;cv2.WARP_INVERSE_MAP)</span><br><span class="line">    return output_im</span><br></pre></td></tr></table></figure>
<p>其实就是为了让两张图的点位能对得上，将某张图进行旋转，缩放，使得两张图的人脸的关键点能处于相同的坐标位置。</p>
<p><img src="/images/blog/cv_switch_face_2.gif" alt=""></p>
<h3 id="3-目标图的轮廓纠正"><a href="#3-目标图的轮廓纠正" class="headerlink" title="3 目标图的轮廓纠正"></a>3 目标图的轮廓纠正</h3><p><img src="/images/blog/cv_switch_face_3.png" alt=""></p>
<p>Non colour-corrected overlay<br>接下来需要解决的问题是，两张图像的不同肤色和光照差异，会导致连接处边缘的突兀。下面的方法是尝试纠正这个问题。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">COLOUR_CORRECT_BLUR_FRAC &#x3D; 0.6</span><br><span class="line">LEFT_EYE_POINTS &#x3D; list(range(42, 48))</span><br><span class="line">RIGHT_EYE_POINTS &#x3D; list(range(36, 42))</span><br><span class="line"></span><br><span class="line">def correct_colours(im1, im2, landmarks1):</span><br><span class="line">    blur_amount &#x3D; COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(</span><br><span class="line">                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis&#x3D;0) -</span><br><span class="line">                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis&#x3D;0))</span><br><span class="line">    blur_amount &#x3D; int(blur_amount)</span><br><span class="line">    if blur_amount % 2 &#x3D;&#x3D; 0:</span><br><span class="line">        blur_amount +&#x3D; 1</span><br><span class="line">    im1_blur &#x3D; cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)</span><br><span class="line">    im2_blur &#x3D; cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)</span><br><span class="line"></span><br><span class="line">    # Avoid divide-by-zero errors.</span><br><span class="line">    im2_blur +&#x3D; 128 * (im2_blur &lt;&#x3D; 1.0)</span><br><span class="line"></span><br><span class="line">    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) &#x2F;</span><br><span class="line">                                                im2_blur.astype(numpy.float64))</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/cv_switch_face_4.png" alt=""></p>
<p>此方法尝试改变第二张图像的轮廓去匹配第一张图的。做法是：<strong>第二张图除以其高斯模糊，然后乘以第一张图的高斯模糊</strong>。此算法源于<a href="https://en.wikipedia.org/wiki/Color_balance#Scaling_monitor_R.2C_G.2C_and_B" target="_blank" rel="noopener">RGB的缩放轮廓纠正</a>,但是对所有图像使用了一个常量的缩放因子，每个像素有其局部缩放因子。</p>
<p>由此方法，两张图像的光照差异可以在某种程度上累加。例如，如果第一张图某一边在发光二第二张图有均衡的光照，那么轮廓纠正之后图二会出现出现某些暗处。也就是说这是个比较粗暴的方案，合适大小的高斯核是问题的关键。太小的话会导致图一种某些面部特征会出现在图二中，太大的话会导致核外面某些像素重叠，并出现变色。此处使用的是$0.6\times 瞳孔距离$。</p>
<h3 id="4-将图二特征渲染回图一"><a href="#4-将图二特征渲染回图一" class="headerlink" title="4 将图二特征渲染回图一"></a>4 将图二特征渲染回图一</h3><p>使用一个mask从图二中抽取部分，渲染到图一中。</p>
<p><img src="/images/blog/cv_switch_face_5.png" alt=""></p>
<ul>
<li>值为1的区域(上图中的白色区域)对应的是图二中人脸特征选取的部分</li>
<li>值为0的区域(上图中黑色区域)对应的是图一该出现的部分。0到1之间的区域是两张图的混合。</li>
</ul>
<p>下面代码是生成如上区域的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LEFT_EYE_POINTS &#x3D; list(range(42, 48))</span><br><span class="line">RIGHT_EYE_POINTS &#x3D; list(range(36, 42))</span><br><span class="line">LEFT_BROW_POINTS &#x3D; list(range(22, 27))</span><br><span class="line">RIGHT_BROW_POINTS &#x3D; list(range(17, 22))</span><br><span class="line">NOSE_POINTS &#x3D; list(range(27, 35))</span><br><span class="line">MOUTH_POINTS &#x3D; list(range(48, 61))</span><br><span class="line">OVERLAY_POINTS &#x3D; [</span><br><span class="line">    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,</span><br><span class="line">    NOSE_POINTS + MOUTH_POINTS,</span><br><span class="line">]</span><br><span class="line">FEATHER_AMOUNT &#x3D; 11</span><br><span class="line"></span><br><span class="line">def draw_convex_hull(im, points, color):</span><br><span class="line">    points &#x3D; cv2.convexHull(points)</span><br><span class="line">    cv2.fillConvexPoly(im, points, color&#x3D;color)</span><br><span class="line"></span><br><span class="line">def get_face_mask(im, landmarks):</span><br><span class="line">    im &#x3D; numpy.zeros(im.shape[:2], dtype&#x3D;numpy.float64)</span><br><span class="line"></span><br><span class="line">    for group in OVERLAY_POINTS:</span><br><span class="line">        draw_convex_hull(im,</span><br><span class="line">                         landmarks[group],</span><br><span class="line">                         color&#x3D;1)</span><br><span class="line"></span><br><span class="line">    im &#x3D; numpy.array([im, im, im]).transpose((1, 2, 0))</span><br><span class="line"></span><br><span class="line">    im &#x3D; (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) &gt; 0) * 1.0</span><br><span class="line">    im &#x3D; cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)</span><br><span class="line"></span><br><span class="line">    return im</span><br><span class="line"></span><br><span class="line">mask &#x3D; get_face_mask(im2, landmarks2)</span><br><span class="line">warped_mask &#x3D; warp_im(mask, M, im1.shape)</span><br><span class="line">combined_mask &#x3D; numpy.max([get_face_mask(im1, landmarks1), warped_mask],</span><br><span class="line">                          axis&#x3D;0)</span><br></pre></td></tr></table></figure>
<p>上面代码分解如下</p>
<ol>
<li><p><code>get_face_mask()</code>方法是用来定义生成图像的一个mask和一个面部关键点的矩阵。它画了两个白色凸多边形:一个环绕着眼睛区域，一个环绕着鼻子和嘴巴区域。接着它会羽化mask的边缘11个像素。羽化有助于隐藏遗留的颜色不连续问题</p>
</li>
<li><p>此类面部的mask会给两张图都生成。图二的mask会被转换进图一的坐标空间，使用步骤2相同的转换。</p>
</li>
<li>此mask接下来会逐像素取最大值。结合两个mask可以保证图一中的特征会被覆盖，同时图二中的特征也会被展示。</li>
</ol>
<p>最后，mask用于得到最终的合成结果。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">output_im &#x3D; im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/cv_switch_face_6.png" alt=""></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>Mask Embedding in conditional GAN for Guided Synthesis of High Resolution Images</title>
    <url>/2019/04/22/Mask%20Embedding%20in%20conditional%20GAN/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/pdf/1907.01710.pdf" target="_blank" rel="noopener">论文来源</a></p>
<h2 id="1-引入"><a href="#1-引入" class="headerlink" title="1 引入"></a>1 引入</h2><p>目前大多数从语义分割mask生成照片的方法要么是使用从粗糙到精细的级联网络，要么是设计特定的损失函数来增加模型稳定性。目前，使用语义分割mask为指引合成包含丰富局部特征高分辨率的图像依然有难度。</p>
<p>本文就提出了一种关联提供的语义分割mask，同时保留了丰富的局部细节的合成图像方法。</p>
<p>图像翻译模型如Pix2Pix使用Unet风格的生成器直接映射图像的抽象表征，但是没有合理的机制来随机特征的实现。这通常会导致模型的输出是特征空间不同分辨率之间的模棱两可的特征，如下图。通常这会导致模糊的图像和糟糕的纹理细节。解决方案之一是Pix2PixHD论文中，使用了一个粗糙到精细的方法，以及风格损失函数来修正图像的输出质量。但是这个方法需要很大的模型，但仍然没解决特征映射的基础问题。一个更加合理的解决方案是Tub-GAN，使用了latent向量z(选中的分布的噪音)和一个语义分割的mask作为条件输入，允许模型学习其联合分布。虽然他们提出了一个融合策略:投影latent特征以及投影mask特征并不能保证是天然关联的，因而此模型也是受限的仅仅能生成背景空间的低频信息。</p>
<p><img src="/images/blog/mask_embedding_gan_1.jpg" alt=""></p>
<p>图说明：生成的样本图像和卡通示例了：训练期间一个mask引导的面部生成会遇到的采样空间映射的挑战：图像翻译模型会被训练着映射相同的圆形模式到各种各样的球形模式上。<strong>模可能学习到了训练数据集的平均的篮球模式而不能完整的独立地映射它们。相同的问题存在于训练一个生成器使用相似的mask表征来复制完全不同的脸</strong>。</p>
<p>本文提出了当前可用的以mask为指引像素级语义输入的生成模型的两种主要问题</p>
<ol>
<li>合成的结果中的精细的纹理细节缺乏多样性，源于mask到图像域的不充分的映射。</li>
<li>当前的多条件输入架构设计中的低参数效率</li>
</ol>
<p>对于第一个问题，我们认为带语义分割map输入的多个latent向量会取得更好的样本空间映射，这也会使得合成结果里纹理细节的多样性。对于第二个问题，我们的解决方案是<strong>在初始的特征投影之前将mask embeding反射回latent向量输入中，此操作显著提升了合成结果中的纹理细节</strong>。将mask embeding 向量和latent向量结合起来是一个有效的方法来添加mask限制，因为它允许初始的特征投影可匹敌于像素级别mask限制。与Tube-GAN相反，我们使用的是投影的mask特征作为latent特征的主要限制，使得网络的上采样路径能够保留其大部分能力来实现精细的局部纹理特征。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><h3 id="2-1-Conditional-条件GAN"><a href="#2-1-Conditional-条件GAN" class="headerlink" title="2.1 (Conditional)条件GAN"></a>2.1 (Conditional)条件GAN</h3><p>条件GAN可以通过联合latent向量和条件输入来控制生成器的输出。许多研究使用了cGAN，使用的是向量形式(比如标签)的图像属性来控制图像合成。Pix2Pix和Pix2PixHD首次提出在编码器-解码器风格的图像到图像翻译的结构中使用语义分割为输入。有些研究应用输入embeding来转换高维属性，如将语义分割mask转换成压缩的低维形式。CCGAN提出使用包含图像特色的句子embeding作为特征来进行cycle-GAN训练。他们的研究表明，凝练的文本信息可以被用来与生成器latent特征融合作为条件来做图像合成。Disentangling Multiple Conditional Inputs in GANs使用binary(二值)mask embeding作为条件输入的部分来控制生成的服饰的形状。但是这个论文表明，mask embedding向量不足以完成像素级以mask为指引的受限图像合成。它们的结构中输出的形状并不总是与输入的mask一致的。</p>
<h3 id="2-2-State-of-art的Pix2Pix的风格生成器"><a href="#2-2-State-of-art的Pix2Pix的风格生成器" class="headerlink" title="2.2 State-of-art的Pix2Pix的风格生成器"></a>2.2 State-of-art的Pix2Pix的风格生成器</h3><p>Pix2Pix在图像翻译时，无法生成稳定多样的精细纹理特征。Pix2Pix-HD模型提出从粗糙到精细的级联结构，加上风格损失和多尺度的判别器。主要思想是使用额外的损失项来正则惩罚膨胀的模型能力，尤其是拼接的精修网络(refinedment networks)。尽管此模型有通过样例级别的特征embedding来随机纹理的机制，局部纹理细节的多样性依然取决于实例标签映射的镜像扰动。某种程度上来说，此机制允许随机纹理生成，但是纹理映射仅与物体的形状扰动相关。换句话说，此图像翻译模型仍然受限于一对一的映射(如上图)，由于这种原因，图像质量和多样性依然相当低。</p>
<h3 id="2-3-ProGAN-Progressive-Growing"><a href="#2-3-ProGAN-Progressive-Growing" class="headerlink" title="2.3 ProGAN(Progressive Growing )"></a>2.3 ProGAN(Progressive Growing )</h3><p>ProGAN是一种训练方法：它渐进式地给生成器和判别器增加卷积层来获得更好的稳定性和更快的收敛。此技术使得使用轻量级的修改的DCGAN来合成高清图像成为可能。</p>
<h2 id="3-生成器中的Mask-Embedding"><a href="#3-生成器中的Mask-Embedding" class="headerlink" title="3 生成器中的Mask Embedding"></a>3 生成器中的Mask Embedding</h2><p>为了控制生成器的输出的形状，mask经常被用来作为编码器-解码器形式网络的中生成器的仅有输入来增强像素级限制。此类图像翻译模型的主要定律是构建一种翻译$G(v)\rightarrow \lbrace r \rbrace$，其中一对一的翻译由输入$v$限定的。使用诸如dropout或者噪音z覆盖作为输入$v$的机制，一对多的关联$G(v,z)\rightarrow \lbrace r_1,r_2,…r_m \rbrace$ 原理上就称为可能。然而，受限于卷积操作和目标函数，Pix2Pix表明，覆盖的噪音常常会被模型忽视。模型输出严重依赖于语义分割输入的mask和dropout，使得高频纹理特征的多样性也是受限的。话句话说，给定一个局部最优的图像到图像的翻译网络$G`$，其采样形式在实际中可能变成了$G’(v,z)\rightarrow \lbrace r_1,r_2,..r_n \rbrace,其中n&lt;&lt;m$。映射的样本空间就变得非常稀疏了。如下图(不同映射机制下，可达的采样空间)所示，我们复制了不同的策略(Pix2Pix,Pix2Pix-HD)以及我们的策略，允许模型的采样空间增加到更大的子集，直至整个域空间，反过来证明了越大的生成器在多样性、分辨率和实现上更好。</p>
<p><img src="/images/blog/mask_embedding_gan_2.jpg" alt=""></p>
<h3 id="3-1-像素级的Mask限制和模型设计"><a href="#3-1-像素级的Mask限制和模型设计" class="headerlink" title="3.1 像素级的Mask限制和模型设计"></a>3.1 像素级的Mask限制和模型设计</h3><p>我们提出的生成器架构如下图所示，借用了PGAN生成器的架构，其中的生成器投影了一个latent向量到latent空间，然后接着几个上采样和卷积层来生成输出图像。为了插入语义分割信息，我们构建了一系列的mask特征并拼接二者到对应的latent特征中。这种形式的UNet架构与Pix2Pix的实现很像，但是没有Pix2Pix没有latent向量的输入。</p>
<p><img src="/images/blog/mask_embedding_gan_3.jpg" alt=""></p>
<p>然而，我们观察到此方案的初始的实现，与原始的PGAN架构相比，带有严重的质量下滑。我们将此问题看做是一个空间采样问题，即mask引入了特征投影路径上的限制，从早期层到后续层的特征映射变得越来越<strong>不依赖于</strong>带mask为输入所引入的在空间和形态学上的限制了，这也导致了模型的训练过程的不稳定和模型能力的衰退。解决方案是，实现一种机制，允许初始的latent特征映射基本与mask限制吻合。然后模型可以使用mask特征的短连接(上图中的水平的箭头)仅作为一种办法来增强像素级限制，而不必抵消过多的模型能力来完成全局的图像结构。我们通过构建一个mask embedding向量并将其嵌入到latent输入向量中来完成此操作(上图左下方)。</p>
<h3 id="3-2-公式"><a href="#3-2-公式" class="headerlink" title="3.2 公式"></a>3.2 公式</h3><p>尽管在空间采样问题中同时出现了mask限制和局部精修的纹理细节问题，此条件下上采样大部分由卷积层督导完成。mask输入并不能识别真实图像数据集中某一张图像，但是与一簇真实图片相关。因此，从数据集中收集的mask定义了一部分的真实图像的manifolds(副本)。下图中左上的椭圆说明了一个两部分的猫和狗的mask。我们也展示了由更小的椭圆设定的低分辨率特征。由一系列的卷积层链接，是一个限定接收域的局部操作，扇区是层次继承的并且每个manifold获得了类似的几何结构。我们的结构首先分两步正确地在最低分辨率的manifold上采样了一个mask限制点(1)通过mask embedding定位准确的扇区。(2)通过一个latent 特征向量在扇区内采样一个点。然后一个上采样步骤精修了细节并通过垂直的mask信息注射增强了mask限制。<strong>两部分，latent特征向量和mask embedding，是我们的方法与其他方法的根本区别</strong>。</p>
<p><img src="/images/blog/mask_embedding_gan_4.jpg" alt=""></p>
<p>图示说明：展示了使用一个狗的mask为指引的图像生成过程。左边：展示了图像生成过程中使用一系列卷积层处理的特征空间。右边：使用和未使用mask embeding的两个生成狗照片的示例。在inference时，带mask embeding的完美的模型投影基础特征到正确的manifold并使用卷积层执行合适的上采样。然而，没有mask embeding的模型学习到的是(1)只投影平均的基础图像(2)即便按照mask 限制，也只能低效地映射基础图像到狗。</p>
<p>现在着重说说latent特征向量和mask embedding。<strong>如果没有latent 特征向量</strong>，Pix2Pix或Pix2Pix-HD这类模型只有mask输入，而没有足够的随机性。因此，其生成的图像几乎完全没mask限定到唯一了。没有latent特征向量的模型生成的图像的多样性是十分有限的。相反，我们的模型有latent向量，它编码了一个大的多样性的细节。给定一个mask，我们可以生成十分不同的图像，并保有精良的细节。</p>
<p><strong>如果没有mask embeding</strong>，比如Tube-GAN,其限制在低维特征得不到重视，以及后面层中的参数可能需要纠正分辨率图像的一些错误，这限制了它在表达细节的能力。而我们的模型使用mask embedding潜在地找到了正确的扇区，big生成正确的latent空间表征。因此，所有的后续层可以更集中在生成细节上。上图就用卡通图展示了这种对比。蓝色点线，是我们模型的处理过程并从第二列到最优列生成以不同分辨率生成狗的图像。后续的网络层，卷积层和mask嵌入将图像从猫纠正到狗。不幸的是，最后的图像看起来像狗，但是分辨率太低。上图并不是真实的，但是在现实场景中我们观察到类似的场景。这些观测说明，嵌入mask embedding显著提升了特征映射效率。</p>
<h3 id="3-3-架构"><a href="#3-3-架构" class="headerlink" title="3.3 架构"></a>3.3 架构</h3><p>我们提出的模型由mask 投影路径和latent投影路径分别对应了UNet的contracting和expanding路径。输入到mask投影路径是一个二值面部边缘map。mask经过一系列blocks之后，每个block由2个卷积层，strides分别为1和2.每个block输出一个递增数目的特征到接下来的网络层，并仅连接前面的8个特征到latent投影路径来形成mask限制。</p>
<p>mask投影路径有两个主要的函数。第一个给latent投影路径上的特征上采样过程提供空间限制。第二个，输出mask embedding告知latent投影层其特征特征聚类簇，极可能与特定mask关联。为了反映事实：左边的contracting路径上的mask特征主要扮演着限制者角色，只有8个mask特征被拼接到网络的latent投影路径上。此设计基于两点</p>
<ul>
<li>更多的特征需要更大的模型能力来将它们合适的融合到投影的latent特征上</li>
<li>我们之前的实验表明一个训练过的模型会基本上将mask特征相差无几地投影到特定模式上。</li>
</ul>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4 训练"></a>4 训练</h2><p>三个模型都是用WGAN-GP的损失函数等式。Pix2Pix的baseline直接以目标分辨率训练了25个epochs。我们的带mask embedding和不带mask embedding模型都是用了渐进式progressive growing训练策略(源于pGAN)。我们从第一个输出分辨率$8\times 8$开始，训练了45k步，然后在新卷积blocks中fade加倍输入和输出分辨率。鉴于mask投影路径的轻量的权重，没有实现其fading连接。</p>
<p>为了对比mask embedding机制的效果，训练过程的超参数batch size，learing rate和每个生成器的判别器的优化数目都是一样的。是用了$batch size=256$,输出分辨率$8\times 8$。每次加倍输出分辨率的时候缩减一半的batch size。学习率初始设定为0.001，并且在模型的输出分辨率达到256时，增加到0.002.使用Tensorflow写的代码，每个模型在4块Nvidia V100上训练2天达到最终的分辨率512.</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p>我们对比了Pix2Pix baseline(23.23M)的生成器，和我们的不带embedding的baseline(23.07M)，和我们带embedding的模型，在图像合成任务中，使用Celeba-HQ数据集。没有对比Pix2Pix-HD，因为其个体实例级的特征embedding机制依赖于mask的perturbation来生成多样图像。而我们的模型在latent’投影路径上(对Pix2Pix来说是上采样路径)来说是保持相似的。我们的两个模型的baseline的判别器和模型相同都包含了23.07M个参数。生成器的性能使用切片的Wassertein 距离来衡量(SWD)。</p>
<h3 id="5-1-Celeba-HQ数据集"><a href="#5-1-Celeba-HQ数据集" class="headerlink" title="5.1 Celeba-HQ数据集"></a>5.1 Celeba-HQ数据集</h3><p>使用此数据集，我们使用python的Dlib包抽取了每个面部的68个关键点，关键点的检测是在分辨率为1024的图像上做的。<strong>面部关键点与原始标注的属性严重不符的图像被删掉，最后总共有27000张图像被选做训练集</strong>。</p>
<h3 id="5-2-量化评估"><a href="#5-2-量化评估" class="headerlink" title="5.2 量化评估"></a>5.2 量化评估</h3><p>我们使用切片的SWD距离来评估模型的效果，下表展示了之前的一些参数设置。由于内存限制，SWD在批量的每一对合成图和真实上计算平均值。玩魔兽先计算240对(真图-合成图)图像，然后重复直至覆盖了8192对。我们首先生成图像从$512\times 512$到$16\times 16$分辨率的拉普拉斯金字塔。每个层级的金字塔我们抽取128个$7\times 7$的patches，正则化然后计算与每一层的真图的平均距离。下图，SWD指标集中于我们的方法和其他方法的不同表现。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Configurations</th>
<th>512</th>
<th>256</th>
<th>128</th>
<th>64</th>
<th>32</th>
<th>16</th>
<th>avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Real</td>
<td>10.82</td>
<td>9.98</td>
<td>10.14</td>
<td>9.75</td>
<td>9.83</td>
<td>7.52</td>
<td>9.67</td>
</tr>
<tr>
<td>Pix2Pix</td>
<td>67.74</td>
<td>27.72</td>
<td>25.08</td>
<td>20.46</td>
<td>19.05</td>
<td>151.78</td>
<td>65.52</td>
</tr>
<tr>
<td>不带embedding</td>
<td>58.20</td>
<td>27.7722.19</td>
<td>18.25</td>
<td>17.58</td>
<td>70.49</td>
<td>35.57</td>
</tr>
<tr>
<td>带embedding</td>
<td>43.74</td>
<td>22.46</td>
<td>17.48</td>
<td>14.83</td>
<td>13.65</td>
<td>37.57</td>
<td>24.96</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到使用mask embedding可以取得更好的合成图像质量。</p>
<h3 id="5-3-量化对比"><a href="#5-3-量化对比" class="headerlink" title="5.3 量化对比"></a>5.3 量化对比</h3><p><img src="/images/blog/mask_embedding_gan_5.jpg" alt=""></p>
<ul>
<li>a: 输入的mask</li>
<li>b: 使用Pix2Pix合成的图。【只能生成十分相似的粗糙的风格】</li>
<li>c: 使用我们的不带mask embedding baseline模型合成的效果【无法生成高保真的纹理特征】</li>
<li>d: 使用我们的带mask embedding模型合成的效果</li>
</ul>
<p>不带embedding的模型无法生成高稳定性的纹理特征。生成的图像包含了主要的噪音实现和上采样的人工痕迹或多或少地增加了模型能力。观测结果也符合我们的假设，即不带embedding的模型会被强制投影出示的特征到样本分布的重叠空间，这就导致了模糊的特征纹理以及模棱两可的结构。生成器能力不充分的结果便是模型会生成明显的人工痕迹，比如对角直线和棋盘纹理模式。</p>
<h3 id="5-4-改变Latent输入"><a href="#5-4-改变Latent输入" class="headerlink" title="5.4 改变Latent输入"></a>5.4 改变Latent输入</h3><p><img src="/images/blog/mask_embedding_gan_6.jpg" alt=""></p>
<ul>
<li>a: 输入的mask</li>
<li>b: 原图</li>
<li>c,d,e: 使用相同mask，但是不同latent 向量合成的图</li>
</ul>
<p>上图展示了，相同的mask输入可以与不同的latent向量并生成不同的人脸。我们也注意到latent向量和mask embedding并不完全是不关联的。latent向量更多的是侧重图像风格，即头发、皮肤、面部头发。另一方面，面部关键点标记应该是由提供的mask图决定的。不足之处是，与各种各样的不同的面部相比图像数量过少。我们观察到一些面部mask与某些特性粘合，比如性别、皮肤颜色等与我们给定的面部mask不是明显相关的。后续可以增加更多的抽象mask和对应的数据集可能可以克服这个问题。并且，实现随机模糊和mask特征的dropout可以帮助增加输出的多样性。</p>
<h2 id="5-代码逻辑"><a href="#5-代码逻辑" class="headerlink" title="5 代码逻辑"></a>5 代码逻辑</h2><p><img src="/images/blog/mask_embedding_gan_7.jpg" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>图像质量评估</title>
    <url>/2019/04/21/image-quality-assessment/</url>
    <content><![CDATA[<p>参考 <a href="https://github.com/ocampor/notebooks/blob/master/notebooks/image/quality/brisque.ipynb?source=post_page-----391a6be52c11----------------------" target="_blank" rel="noopener">Image Quality Assessment</a></p>
<h2 id="1-说明"><a href="#1-说明" class="headerlink" title="1 说明"></a>1 说明</h2><p>图像质量评估(Image Quality Assessment(IQA))看起来是个非常主观的事，但是可以借鉴一些方法，目前主流方法分两类</p>
<ol>
<li>基于引用比较的评估。</li>
<li>无引用的评估</li>
</ol>
<p>主要区别是，基于引用的评估方法需要依赖一张高质量的图片作为评估源，来比较两张图之间的区别。常用的基于引用的评估方法是结构相似性索引(Structural Similarity Index(SSIM))。</p>
<h2 id="2-无引用图片质量评估"><a href="#2-无引用图片质量评估" class="headerlink" title="2 无引用图片质量评估"></a>2 无引用图片质量评估</h2><p>它不需要引用图片，仅仅依赖于接收到的图片信息，称为盲测方法。分为两步(1)计算能够描述图片结构的特征(2)计算与人类对于图片质量观点相关的特征。TID2008是一个基于方法论的数据集，它描述了如何从引用图片中评估人类的观点，被广泛用于对比IQA算法的性能。</p>
<h3 id="2-1-Blind-referenceless-image-spatial-quality-evaluator-BRISQUE"><a href="#2-1-Blind-referenceless-image-spatial-quality-evaluator-BRISQUE" class="headerlink" title="2.1 Blind/referenceless image spatial quality evaluator (BRISQUE)"></a>2.1 Blind/referenceless image spatial quality evaluator (BRISQUE)</h3><p>BRISQUE是一种仅使用图像像素来计算特征(其他方法都是基于图像转换到其他空间，比如wavelet 或者DCT)。非常高效，因为它不需要其他任何信息来计算其特征。</p>
<p>它依赖于空间域中局部正规化的亮度系数的空间自然场景统计(Spatial Natural Scene Statistics(NSS))模型，以及这些系数的点与点之间的内积的模型。</p>
<h3 id="2-2-方法论"><a href="#2-2-方法论" class="headerlink" title="2.2 方法论"></a>2.2 方法论</h3><h4 id="2-2-1-Natural-Scene-Statistics-in-the-Spatial-Domain"><a href="#2-2-1-Natural-Scene-Statistics-in-the-Spatial-Domain" class="headerlink" title="2.2.1 Natural Scene Statistics in the Spatial Domain"></a>2.2.1 Natural Scene Statistics in the Spatial Domain</h4><p>给定图像$I(i,j)$，首先通过减去局部均值$\mu (i,j)$，然后除以局部方差$\delta(i,j)$ 来计算局部亮度系数$\hat I(i,j)$。加上$C$是为了避免除以0.</p>
<script type="math/tex; mode=display">
\hat I(i,j) = \frac{I(i,j)-\mu(i,j)}{\delta(i,j)+c}\\
其中如果I(i,j)\in [0,255]，则C=1,如果 \in [0,1]，那么C=1/255</script><p>为了计算局部归一化的亮度，即平均减去的对比度归一化(MSCN)系数，首先，我们需要计算局部均值。</p>
<script type="math/tex; mode=display">
\mu (i,j) = \sum _{k=-K} ^{K} \sum _{I=-k} ^L w_{k,l}I_{k,l}(i,j) \\
其中w是尺寸为(K,L)的高斯核</script><p>计算代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def normalize_kernel(kernel):</span><br><span class="line">    return kernel &#x2F; np.sum(kernel)</span><br><span class="line"></span><br><span class="line">def gaussian_kernel2d(n, sigma):</span><br><span class="line">    Y, X &#x3D; np.indices((n, n)) - int(n&#x2F;2)</span><br><span class="line">    gaussian_kernel &#x3D; 1 &#x2F; (2 * np.pi * sigma ** 2) * np.exp(-(X ** 2 + Y ** 2) &#x2F; (2 * sigma ** 2)) </span><br><span class="line">    return normalize_kernel(gaussian_kernel)</span><br><span class="line"></span><br><span class="line">def local_mean(image, kernel):</span><br><span class="line">    return signal.convolve2d(image, kernel, &#39;same&#39;)</span><br></pre></td></tr></table></figure>
<p>然后计算局部偏差</p>
<script type="math/tex; mode=display">
\sigma(i,j)\sqrt{\sum_{k=-K} ^K \sum _{l=-L} ^L w_{k,l}(I_{k,l}(i,j)-\mu(i,j))^2 }</script><p>代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def local_deviation(image, local_mean, kernel):</span><br><span class="line">    &quot;Vectorized approximation of local deviation&quot;</span><br><span class="line">    sigma &#x3D; image ** 2</span><br><span class="line">    sigma &#x3D; signal.convolve2d(sigma, kernel, &#39;same&#39;)</span><br><span class="line">    return np.sqrt(np.abs(local_mean ** 2 - sigma)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算得到MSCN系数</p>
<script type="math/tex; mode=display">
\hat I(i,j) = \frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}</script><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def calculate_mscn_coefficients(image, kernel_size&#x3D;6, sigma&#x3D;7&#x2F;6):</span><br><span class="line">    C &#x3D; 1&#x2F;255</span><br><span class="line">    kernel &#x3D; gaussian_kernel2d(kernel_size, sigma&#x3D;sigma)</span><br><span class="line">    local_mean &#x3D; signal.convolve2d(image, kernel, &#39;same&#39;)</span><br><span class="line">    local_var &#x3D; local_deviation(image, local_mean, kernel)</span><br><span class="line">    </span><br><span class="line">    return (image - local_mean) &#x2F; (local_var + C)</span><br></pre></td></tr></table></figure>
<p>作者发现一个扭曲的图片的MSCN系数服从一个广义高斯分布(GGD)</p>
<script type="math/tex; mode=display">
f(x;\alpha,\sigma ^2)=\frac{\alpha}{2\beta T(1/\alpha)}e^{-(\frac{|x|}{\beta})^{\alpha}} \\
其中 \beta = \sigma\sqrt{\frac{T(\frac{1}{\alpha})}{T(\frac{3}{\alpha})}},T是伽马函数，\alpha的形状控制形状以及\sigma ^2的方差</script><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def generalized_gaussian_dist(x, alpha, sigma):</span><br><span class="line">    beta &#x3D; sigma * np.sqrt(special.gamma(1 &#x2F; alpha) &#x2F; special.gamma(3 &#x2F; alpha))</span><br><span class="line">    coefficient &#x3D; alpha &#x2F; (2 * beta() * special.gamma(1 &#x2F; alpha))</span><br><span class="line">    return coefficient * np.exp(-(np.abs(x) &#x2F; beta) ** alpha)</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-相邻MSCN系数的点对内积"><a href="#2-2-2-相邻MSCN系数的点对内积" class="headerlink" title="2.2.2 相邻MSCN系数的点对内积"></a>2.2.2 相邻MSCN系数的点对内积</h4><p>邻接的系数的符号也代表了某种特定结构，可能是某种扭曲的分布。邻接MSCN系数点对内积的沿着四个方向</p>
<ol>
<li>水平方向</li>
<li>垂直方向</li>
<li>主对角(main-diagonal)D1</li>
<li>次对角(secondary-diagonal)D2</li>
</ol>
<script type="math/tex; mode=display">
D2(i,j) = \hat I(i,j)\hat I(i+1,j+1)</script><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def calculate_pair_product_coefficients(mscn_coefficients):</span><br><span class="line">    return collections.OrderedDict(&#123;</span><br><span class="line">        &#39;mscn&#39;: mscn_coefficients,</span><br><span class="line">        &#39;horizontal&#39;: mscn_coefficients[:, :-1] * mscn_coefficients[:, 1:],</span><br><span class="line">        &#39;vertical&#39;: mscn_coefficients[:-1, :] * mscn_coefficients[1:, :],</span><br><span class="line">        &#39;main_diagonal&#39;: mscn_coefficients[:-1, :-1] * mscn_coefficients[1:, 1:],</span><br><span class="line">        &#39;secondary_diagonal&#39;: mscn_coefficients[1:, :-1] * mscn_coefficients[:-1, 1:]</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>广义高斯分布不能很好的拟合系数内积的经验直方图。因此，又提出了 Asymmetric Generalized Gaussian Distribution (AGGD)<a href="Multiscale skewed heavy-tailed model for texture analysis. Proceedings - International Conference on Image Processing">非对称广义高斯分布模型</a></p>
<script type="math/tex; mode=display">
f(x;v,\sigma _l ^2,\sigma _r ^2) = \frac{v}{(\beta _l+\beta _r)T(\frac{1}{v})}e^{(-(\frac{-x}{\beta _l})^v)} \quad\quad x<0 \\
f(x;v,\sigma _l ^2,\sigma _r ^2) = \frac{v}{(\beta _l+\beta _r)T(\frac{1}{v})}e^{(-(\frac{-x}{\beta _r})^v)} \quad\quad x\gt 0 \\
其中 \beta _{side} = \sigma _{side}\sqrt{\frac{T(\frac{1}{v})}{T(\frac{3}{v})}} ,side 可以是 r 或 l \\
前面没有提到的参数是均值 m = (\beta _r-\beta_l)\frac{T(\frac{2}{v})}{T(frac{1}{v})}</script><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def asymmetric_generalized_gaussian(x, nu, sigma_l, sigma_r):</span><br><span class="line">    def beta(sigma):</span><br><span class="line">        return sigma * np.sqrt(special.gamma(1 &#x2F; nu) &#x2F; special.gamma(3 &#x2F; nu))</span><br><span class="line">    </span><br><span class="line">    coefficient &#x3D; nu &#x2F; ((beta(sigma_l) + beta(sigma_r)) * special.gamma(1 &#x2F; nu))</span><br><span class="line">    f &#x3D; lambda x, sigma: coefficient * np.exp(-(x &#x2F; beta(sigma)) ** nu)</span><br><span class="line">        </span><br><span class="line">    return np.where(x &lt; 0, f(-x, sigma_l), f(x, sigma_r))</span><br></pre></td></tr></table></figure>
<h4 id="2-2-3-拟合AGGD"><a href="#2-2-3-拟合AGGD" class="headerlink" title="2.2.3 拟合AGGD"></a>2.2.3 拟合AGGD</h4><ol>
<li><p>计算$\hat \gamma，其中N_l$是负样本数量，而$N_r$是正样本数量.</p>
<script type="math/tex; mode=display">
\hat \gamma = \frac{\sqrt{\frac{1}{N_l}\sum_{k=1,x_k<0} ^{N_l}x_k ^2}}{\sqrt{\frac{1}{N_r}\sum_{k=1,x_k<0} ^{N_r}x_k ^2}}</script></li>
<li><p>计算$\hat r$</p>
<script type="math/tex; mode=display">
\hat r = \frac{(\frac{\sum|x_k|}{N_l+N_r})^2}{\frac{\sum x_k ^2}{N_l+N_r}}</script></li>
<li><p>使用$\hat \gamma ,\hat r$计算$\hat R$</p>
<script type="math/tex; mode=display">
\hat R = \hat r\frac{(\hat \gamma ^3+1)(\hat \gamma +1)}{(\hat \gamma ^2+1)^2}</script></li>
<li><p>使用反广义高斯比率计算$\alpha$ </p>
<script type="math/tex; mode=display">
\rho (\alpha) =\frac{T(2/\alpha)^2}{T(1/\alpha)T(3/\alpha)}</script></li>
<li><p>评估左右scale参数</p>
<script type="math/tex; mode=display">
\sigma _l = sqrt{\frac{1}{N_l-1}\sum _{k=l,x_k<0} ^{N_l} x_k ^2} \\
\sigma _r = sqrt{\frac{1}{N_r-1}\sum _{k=r,x_k\gt 0} ^{N_r} x_k ^2}</script></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def asymmetric_generalized_gaussian_fit(x):</span><br><span class="line">    def estimate_phi(alpha):</span><br><span class="line">        numerator &#x3D; special.gamma(2 &#x2F; alpha) ** 2</span><br><span class="line">        denominator &#x3D; special.gamma(1 &#x2F; alpha) * special.gamma(3 &#x2F; alpha)</span><br><span class="line">        return numerator &#x2F; denominator</span><br><span class="line"></span><br><span class="line">    def estimate_r_hat(x):</span><br><span class="line">        size &#x3D; np.prod(x.shape)</span><br><span class="line">        return (np.sum(np.abs(x)) &#x2F; size) ** 2 &#x2F; (np.sum(x ** 2) &#x2F; size)</span><br><span class="line"></span><br><span class="line">    def estimate_R_hat(r_hat, gamma):</span><br><span class="line">        numerator &#x3D; (gamma ** 3 + 1) * (gamma + 1)</span><br><span class="line">        denominator &#x3D; (gamma ** 2 + 1) ** 2</span><br><span class="line">        return r_hat * numerator &#x2F; denominator</span><br><span class="line"></span><br><span class="line">    def mean_squares_sum(x, filter &#x3D; lambda z: z &#x3D;&#x3D; z):</span><br><span class="line">        filtered_values &#x3D; x[filter(x)]</span><br><span class="line">        squares_sum &#x3D; np.sum(filtered_values ** 2)</span><br><span class="line">        return squares_sum &#x2F; ((filtered_values.shape))</span><br><span class="line"></span><br><span class="line">    def estimate_gamma(x):</span><br><span class="line">        left_squares &#x3D; mean_squares_sum(x, lambda z: z &lt; 0)</span><br><span class="line">        right_squares &#x3D; mean_squares_sum(x, lambda z: z &gt;&#x3D; 0)</span><br><span class="line"></span><br><span class="line">        return np.sqrt(left_squares) &#x2F; np.sqrt(right_squares)</span><br><span class="line"></span><br><span class="line">    def estimate_alpha(x):</span><br><span class="line">        r_hat &#x3D; estimate_r_hat(x)</span><br><span class="line">        gamma &#x3D; estimate_gamma(x)</span><br><span class="line">        R_hat &#x3D; estimate_R_hat(r_hat, gamma)</span><br><span class="line"></span><br><span class="line">        solution &#x3D; optimize.root(lambda z: estimate_phi(z) - R_hat, [0.2]).x</span><br><span class="line"></span><br><span class="line">        return solution[0]</span><br><span class="line"></span><br><span class="line">    def estimate_sigma(x, alpha, filter &#x3D; lambda z: z &lt; 0):</span><br><span class="line">        return np.sqrt(mean_squares_sum(x, filter))</span><br><span class="line">    </span><br><span class="line">    def estimate_mean(alpha, sigma_l, sigma_r):</span><br><span class="line">        return (sigma_r - sigma_l) * constant * (special.gamma(2 &#x2F; alpha) &#x2F; special.gamma(1 &#x2F; alpha))</span><br><span class="line">    </span><br><span class="line">    alpha &#x3D; estimate_alpha(x)</span><br><span class="line">    sigma_l &#x3D; estimate_sigma(x, alpha, lambda z: z &lt; 0)</span><br><span class="line">    sigma_r &#x3D; estimate_sigma(x, alpha, lambda z: z &gt;&#x3D; 0)</span><br><span class="line">    </span><br><span class="line">    constant &#x3D; np.sqrt(special.gamma(1 &#x2F; alpha) &#x2F; special.gamma(3 &#x2F; alpha))</span><br><span class="line">    mean &#x3D; estimate_mean(alpha, sigma_l, sigma_r)</span><br><span class="line">    </span><br><span class="line">    return alpha, mean, sigma_l, sigma_r</span><br></pre></td></tr></table></figure>
<h4 id="2-2-4-计算BRISQUE特征"><a href="#2-2-4-计算BRISQUE特征" class="headerlink" title="2.2.4 计算BRISQUE特征"></a>2.2.4 计算BRISQUE特征</h4><p>计算图像质量的特征即拟合MSCN系数的结果并移动shifted内积到广义高斯分布。首先，我们需要拟合MSCN系数到GDD，然后点对内积到AGGD。特征概要如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>FeatureID</th>
<th>Feature Description</th>
<th>Computation Procedure</th>
</tr>
</thead>
<tbody>
<tr>
<td>$f_1-f_2 $</td>
<td>Shape and variance</td>
<td>Fit GGD to MSCN coefficients</td>
</tr>
<tr>
<td>$f_3-f_6$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>H</strong> pairwise products</td>
</tr>
<tr>
<td>$f<em>7-f</em>{10}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>V</strong> pairwise products</td>
</tr>
<tr>
<td>$f<em>{11}-f</em>{14}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>D1</strong> pairwise products</td>
</tr>
<tr>
<td>$f<em>{15}-f</em>{18}$</td>
<td>Shape, mean, left variance, right variance</td>
<td>Fit AGGD to <strong>D2</strong> pairwise products</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def calculate_brisque_features(image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6):</span><br><span class="line">    def calculate_features(coefficients_name, coefficients, accum&#x3D;np.array([])):</span><br><span class="line">        alpha, mean, sigma_l, sigma_r &#x3D; asymmetric_generalized_gaussian_fit(coefficients)</span><br><span class="line"></span><br><span class="line">        if coefficients_name &#x3D;&#x3D; &#39;mscn&#39;:</span><br><span class="line">            var &#x3D; (sigma_l ** 2 + sigma_r ** 2) &#x2F; 2</span><br><span class="line">            return [alpha, var]</span><br><span class="line">        </span><br><span class="line">        return [alpha, mean, sigma_l ** 2, sigma_r ** 2]</span><br><span class="line">    </span><br><span class="line">    mscn_coefficients &#x3D; calculate_mscn_coefficients(image, kernel_size, sigma)</span><br><span class="line">    coefficients &#x3D; calculate_pair_product_coefficients(mscn_coefficients)</span><br><span class="line">    </span><br><span class="line">    features &#x3D; [calculate_features(name, coeff) for name, coeff in coefficients.items()]</span><br><span class="line">    flatten_features &#x3D; list(chain.from_iterable(features))</span><br><span class="line">    return np.array(flatten_features)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-计算图像质量"><a href="#2-3-计算图像质量" class="headerlink" title="2.3 计算图像质量"></a>2.3 计算图像质量</h3><p>首先，我们需要两个辅助函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_histogram(x, label):</span><br><span class="line">    n, bins &#x3D; np.histogram(x.ravel(), bins&#x3D;50)</span><br><span class="line">    n &#x3D; n &#x2F; np.max(n)</span><br><span class="line">    plt.plot(bins[:-1], n, label&#x3D;label, marker&#x3D;&#39;o&#39;)</span><br></pre></td></tr></table></figure>
<ol>
<li>载入图像</li>
<li>计算系数.计算完MSCN系数和点对的内积之后，我们可以确定其分布实际上是不同的。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mscn_coefficients &#x3D; calculate_mscn_coefficients(gray_image, 7, 7&#x2F;6)</span><br><span class="line">coefficients &#x3D; calculate_pair_product_coefficients(mscn_coefficients)</span><br><span class="line">for name, coeff in coefficients.items():</span><br><span class="line">    plot_histogram(coeff.ravel(), name)</span><br><span class="line">plt.axis([-2.5, 2.5, 0, 1.05])</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ol>
<li>拟合系数到广义高斯分布</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brisque_features &#x3D; calculate_brisque_features(gray_image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6)</span><br></pre></td></tr></table></figure>
<ol>
<li>resize图像并计算BRISQUE特征</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ownscaled_image &#x3D; cv2.resize(gray_image, None, fx&#x3D;1&#x2F;2, fy&#x3D;1&#x2F;2, interpolation &#x3D; cv2.INTER_CUBIC)</span><br><span class="line">downscale_brisque_features &#x3D; calculate_brisque_features(downscaled_image, kernel_size&#x3D;7, sigma&#x3D;7&#x2F;6)</span><br><span class="line"></span><br><span class="line">brisque_features &#x3D; np.concatenate((brisque_features, downscale_brisque_features))</span><br></pre></td></tr></table></figure>
<ol>
<li>缩放特征并喂入SVR.作者提供了一个与训练的SVR模型来计算质量评估。但是，为了有个好的结果，我们需要将特征缩放到[-1,1]。对于后者，我们需要用预缩放特征向量相同的参数。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def scale_features(features):</span><br><span class="line">    with open(&#39;normalize.pickle&#39;, &#39;rb&#39;) as handle:</span><br><span class="line">        scale_params &#x3D; pickle.load(handle)</span><br><span class="line">    </span><br><span class="line">    min_ &#x3D; np.array(scale_params[&#39;min_&#39;])</span><br><span class="line">    max_ &#x3D; np.array(scale_params[&#39;max_&#39;])</span><br><span class="line">    </span><br><span class="line">    return -1 + (2.0 &#x2F; (max_ - min_) * (features - min_))</span><br><span class="line"></span><br><span class="line">def calculate_image_quality_score(brisque_features):</span><br><span class="line">    model &#x3D; svmutil.svm_load_model(&#39;brisque_svm.txt&#39;)</span><br><span class="line">    scaled_brisque_features &#x3D; scale_features(brisque_features)</span><br><span class="line">    </span><br><span class="line">    x, idx &#x3D; svmutil.gen_svm_nodearray(</span><br><span class="line">        scaled_brisque_features,</span><br><span class="line">        isKernel&#x3D;(model.param.kernel_type &#x3D;&#x3D; svmutil.PRECOMPUTED))</span><br><span class="line">    </span><br><span class="line">    nr_classifier &#x3D; 1</span><br><span class="line">    prob_estimates &#x3D; (svmutil.c_double * nr_classifier)()</span><br><span class="line">    </span><br><span class="line">    return svmutil.libsvm.svm_predict_probability(model, x, prob_estimates)</span><br><span class="line">calculate_image_quality_score(brisque_features)</span><br></pre></td></tr></table></figure>
<h2 id="3-结论"><a href="#3-结论" class="headerlink" title="3 结论"></a>3 结论</h2><p>方法在TID2008数据集上测试，并且效果不错，即便与引用IQA方法比起来。后续可以用XGBoost,LightGBM方法来训练识别步骤来提高效率。</p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>从yolov1到yolov3</title>
    <url>/2019/03/12/yolo-v123/</url>
    <content><![CDATA[<h2 id="1-YOLOv1"><a href="#1-YOLOv1" class="headerlink" title="1 YOLOv1"></a>1 YOLOv1</h2><p>我们先看yolov1的检测效果</p>
<h3 id="1-1-Grid-Cell"><a href="#1-1-Grid-Cell" class="headerlink" title="1.1 Grid Cell"></a>1.1 Grid Cell</h3><p>YOLOv1把输入图片切分成$s\times s$个grid cell，<strong>每个grid cell只预测一个物体</strong>。比如下图中，黄色grid cell会预测中心坐标点落入其中的这个<code>person</code>物体。</p>
<p>注意这里的grid cell只是在图像上看起来是一个方格，实际是原图在经过yolo网络之后会变成$s\times s$个feature map，下图中的一个grid cell经过网络变换之后到最后的特征层变成了一个坐标点。比如原图为$448\times 448$经过yolo最后抽取得到的feature map为$7\times 7\times 30$</p>
<p><img src="/images/blog/yolov123_1.png" alt="yolov123"></p>
<p>每个grid cell只预测固定数目的bbox，当前示例中黄色grid cell做了两个bbox(2个蓝色框)预测来定位person这个物体的位置</p>
<p><img src="/images/blog/yolov123_2.png" alt="yolov123"></p>
<p>然而，一个grid cell只预测一个物体的规则限制了YOLO的预测能力。如果几个物体密集紧邻(多个物体的中心坐标可能会落在一个grid cell里面)，yolo就会遗漏一些物体。如下图，左下方有9个圣诞老人，但是yolo只能检测出来5个。</p>
<p><img src="/images/blog/yolov123_3.png" alt="yolov123"></p>
<p>对于每个grid cell </p>
<ul>
<li>预测B个bbox，并且每个bbox有一个box置信度。</li>
<li>即便上面预测了B个box，但实际<strong>只会检测一个物体</strong>(只保留一个)</li>
<li>预测C个条件分类概率(每个目标分类的概率)</li>
</ul>
<p>比如，对<strong>VOC数据集，YOLO使用了$7\times 7$(S=7)的grids，每个grid cell预测2(B=2)个bbox，以及20个分类(C=20)。</strong></p>
<p>每个bbox有个5元组$(x,y,w,h,conf)$，分别为bbox的坐标位置和置信度。这个置信度反映的是这个bbox包含目标(物体)的概率，以及此bbox的精确度。我们会对坐标位置进行归一化，变成0-1之间的比率（这一点，在准备yolo的训练数据时得到体现）。注意，<strong>其中的x,y是相对于当前cell的偏移量</strong>。每个grid cell有20个条件分类概率，此条件分类概率是检测到的物体属于20个分类里某一分类的概率(每个grid cell为会所有的分类都有一个概率,此处20个分类，会有20个概率)。因此，yolo最终的预测矩阵为</p>
<script type="math/tex; mode=display">
(S,S,B\times 5+c) = (7,7,2\times 5+20)=(7,7,30)</script><p>这样会产生非常多的bbox（下图中间），但是只保留置信度高于一定阈值(0.25)的bbox作为最终预测（下图右边）。</p>
<p><img src="/images/blog/yolov123_4.png" alt="yolov123"></p>
<p>每个预测bbox的分类置信度计算公式如下</p>
<script type="math/tex; mode=display">
分类置信度= box置信度 \times 条件分类概率</script><p><strong>它同时在分类和定位上衡量预测的bbox的置信度</strong></p>
<p>上面的写法很容易混淆，一个详细的定义如下：</p>
<ul>
<li>$bbox置信度= P_r(object)\times IOU$</li>
<li>$条件分类概率= P_r(class_i|object)$</li>
<li>$分类置信度= P_r(class_i)\times IOU=bbox置信度\times 条件分类概率$</li>
</ul>
<p>其中</p>
<ul>
<li>$P_r(object)$是bbox包含物体的概率</li>
<li>$IOU$是预测bbox和真实bbox之间的IOU</li>
<li>$P_r(class_i|object)$为给定当前物体，预测其属于分类$class_i$的概率</li>
<li>$P_r(class_i)$是物体属于分类$class_i$的概率<h3 id="1-2-网络架构设计"><a href="#1-2-网络架构设计" class="headerlink" title="1.2 网络架构设计"></a>1.2 网络架构设计</h3></li>
</ul>
<p><img src="/images/blog/yolov123_5.png" alt="yolov123"></p>
<p>网络使用了24组卷积网络+2个全连接层。一些卷积层使用$1\times 1$以减小特征图深度。最后一个卷积层输出$7\times 7\times 1024$,<strong>再接2个全连接层实现一种线性回归</strong>，最终输出$(7,7,30)$。</p>
<h3 id="1-3-损失函数"><a href="#1-3-损失函数" class="headerlink" title="1.3 损失函数"></a>1.3 损失函数</h3><p>YOLO的每个grid cell预测多（VOC中是2个）个bbox。为了计算正阳性样本的损失，我们只要求它们之中的一个bbox对物体负责。因此，我们选取与真实标注(ground truth)有最高IOU的一个。这种策略导致bbox的的预测的特殊性，即每个预测在物体尺寸和比率上更准。</p>
<p>YOLO使用预测bbox和真实bbox的二次方差和来作为损失函数。损失函数由以下项组成</p>
<ul>
<li>分类损失</li>
<li>定位损失：预测bbox和真实bbox之间的误差</li>
<li>置信度损失：box包含物体的置信度</li>
</ul>
<h3 id="1-3-1-分类损失"><a href="#1-3-1-分类损失" class="headerlink" title="1.3.1 分类损失"></a>1.3.1 分类损失</h3><p>如果检测到物体，每个grid cell的分类损失是每个分类的条件概率的平方误差和。</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 \\
其中如果在grid \quad cell\quad  i中出现物体，则1 _i ^{obj}=1，否则1 _i ^{obj}=0  \\
\hat p_i(c) 代表grid\quad  cell\quad  i中分类c的条件分类概率</script><p>所以这个误差项可以这么理解：</p>
<ol>
<li>首先，当前这个grid cell—&gt;一共有 $7\times 7$个，中有物体的概率。此项有$49\times 2(每个grid \quad cell预测2个)=98$个</li>
<li>其次，这个物体分别属于20个分类的概率</li>
</ol>
<h4 id="1-3-2-回归损失"><a href="#1-3-2-回归损失" class="headerlink" title="1.3.2 回归损失"></a>1.3.2 回归损失</h4><p>回归损失衡量的是，预测bbox的位置和尺寸的误差。YOLOv1只计算负责检测物体那个bbox的误差</p>
<script type="math/tex; mode=display">
\lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] \\
+ \lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] \\
其中 如果第i个grid \quad cell中的第j个bbox是负责检测物体的，则1_{ij} ^{obj}=1，否则1_{ij} ^{obj}=0. \\
\lambda _{coord}增加bbox坐标的损失权重，默认为5</script><p>在YOLO看来大的bbox和小的bbox的2个像素的误差是相等的，为刻意强调这一点，YOLO没有直接预测bbox的宽和高，而是宽的二次方根和高的二次方根。除此之外，对此loss乘以一个权重$\lambda _{coord}$以增加bbox的准确率。</p>
<h4 id="1-3-3-置信度损失"><a href="#1-3-3-置信度损失" class="headerlink" title="1.3.3 置信度损失"></a>1.3.3 置信度损失</h4><p>如果某个物体在box中被检测到，其置信度损失（衡量的是box中的物体）为</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}\sum_{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2 \\
其中
+ \hat C_i是grid\quad cell\quad i中第j个box的置信度
+ 1_{ij} ^{obj}=1 如果第i个grid\quad cell的第j个box负责检测物体，否则为0</script><p>如果某个物体不在box中，其置信度损失为</p>
<script type="math/tex; mode=display">
\lambda _{noobj}\sum_{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{noobj}(C_i-\hat C_i)^2 \\
其中
+ 1_{ij} ^{noobj}是对1_{ij} ^{noobj}的一种补充
+ \hat C_i是第i个grid \quad cell的第j个box的置信度
+  \lambda _{noobj}降低背景检测损失的权重(noobj即背景)</script><p>由于大部分box不包含任何物体，这会导致分类(正负样本)的不均衡，因此，我们降低了背景检测损失的权重，即$\lambda _{noobj}$默认值为0.5</p>
<h4 id="1-3-4-最终损失"><a href="#1-3-4-最终损失" class="headerlink" title="1.3.4 最终损失"></a>1.3.4 最终损失</h4><p>最终损失为前面三种损失之和</p>
<script type="math/tex; mode=display">
 Loss= \sum _{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 +\\
\lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] +\\
 \lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] +\\
\sum _{i=0} ^{S^2}\sum_{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \\
\lambda _{noobj}\sum_{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{noobj}(C_i-\hat C_i)^2</script><h3 id="1-4-yolo评价"><a href="#1-4-yolo评价" class="headerlink" title="1.4 yolo评价"></a>1.4 yolo评价</h3><ul>
<li>优点 <ul>
<li>快速，实时检测</li>
<li>使用单一结构网络，可以端到端地训练</li>
<li>通用。用自然图片训练得到的网络也可用于艺术图片的检测</li>
<li>区域候选的方法限制了分类器识别特定区域。YOLO在预测边框时可以遍历整个图像。使用相关信息时，YOLO在背景区域检测更少的假阳性。</li>
</ul>
</li>
</ul>
<ul>
<li>YOLO每个grid cell只检测一个物体，这使得它在做预测时增加了空间多样性。</li>
</ul>
<h2 id="2-YOLOv2"><a href="#2-YOLOv2" class="headerlink" title="2 YOLOv2"></a>2 YOLOv2</h2><p>与基于区域候选的方法相比，YOLO有更高的定位损失以及更低的召回率。</p>
<h3 id="2-1-准确率提升"><a href="#2-1-准确率提升" class="headerlink" title="2.1 准确率提升"></a>2.1 准确率提升</h3><h4 id="2-1-1-BatchNorm"><a href="#2-1-1-BatchNorm" class="headerlink" title="2.1.1 BatchNorm"></a>2.1.1 BatchNorm</h4><p>每个卷积层之后添加一层BN,可以去掉dropout，同时提升2%的map</p>
<h4 id="2-1-2-高分辨率"><a href="#2-1-2-高分辨率" class="headerlink" title="2.1.2 高分辨率"></a>2.1.2 高分辨率</h4><p>我们先看看yolov1的训练步骤</p>
<ol>
<li>首先，训练一个类似VGG的分类网络【用的是$224\times 224$图片】</li>
<li>然后，使用全连接层替换卷积层</li>
<li>再端到端地训练目标检测【用的是$448\times 448$图片】</li>
</ol>
<p>YOLOv2的训练步骤</p>
<ol>
<li>训练一个分类器：先用$224\times 224$的图片训练，再用$448\times 448$图片接着训练（更少的epoch）<br>后续步骤一致。这使得yolov2的检测器更容易 训练，map也提升了4%。</li>
</ol>
<h4 id="2-1-3-anchor-box"><a href="#2-1-3-anchor-box" class="headerlink" title="2.1.3 anchor box"></a>2.1.3 anchor box</h4><p>从yolo的论文中，我们得知，训练阶段的早期会出现不稳定的梯度，因为yolo会随意预测物体尺度和位置。而实际中，一些物体的尺寸是有规律的。比如汽车的长宽比一般是0.41</p>
<p><img src="/images/blog/yolov123_6.png" alt="yolov123"><br>由于我们只需要一个bbox猜测值是对的就可以，因而如果我们按照实际物体比率来初始化这些预测的初始值，训练阶段就会稳定得多。例如，我们可以按照如下步骤构建5个anchor box</p>
<p><img src="/images/blog/yolov123_7.png" alt="yolov123"></p>
<p><strong>YOLOv2不直接预测bbox，而是预测其相对于上面5个bbox的偏移量。</strong>如果限制偏移量的值，我们可以保持预测的多样性，并使得每个预测集中于特定尺度，这样一来初始训练阶段就会稳定许多。</p>
<p>以下是对yolov1网络的改动</p>
<ol>
<li>移除了最后两个负责预测bbox的个全连接层</li>
</ol>
<p><img src="/images/blog/yolov123_8.png" alt="yolov123"></p>
<ol>
<li>将分类预测从cell级转向bbox级。这样一来，每个预测包含了预测bbox的4个位置参数，一个box置信度（是否包含物体）以及20个分类概率。每个grid cell5个bbox，每个bbox25个参数，共125个参数。与yolov1一样，物体预测依旧是预测预测的bbox与真实bbox之间的IOU。</li>
</ol>
<p><img src="/images/blog/yolov123_9.png" alt="yolov123"><br>yolov1是如何做预测的</p>
<p><img src="/images/blog/yolov123_10.png" alt="yolov123"></p>
<p>yolov2是如何做预测的.注意与yolov1区别</p>
<ol>
<li>为了生成$7\times7\times125$的预测，将网络的最后两层替换$3\times3$的卷积层，每个卷积层输出1024个通道，然后再用$1\times1$卷积将$7\times7\times1024$转换成$7\times7\times125$。</li>
<li>将网络输入图像尺寸从$448\times 448$改为$416\times 416$.由于网络中心一般是大物体坐标中心，之前的$8\times8$会出现不确定性预测。输入图像尺寸从$448\times448$改为$416\times416$之后，输出的特征空间尺度从$8\times8$变成$7\times7$。</li>
</ol>
<p><img src="/images/blog/yolov123_11.png" alt="yolov123"><br>使用anchor box使得map从69.5下降到69.2%，但是召回率从81%提升到88%。</p>
<h3 id="2-2-维度聚类"><a href="#2-2-维度聚类" class="headerlink" title="2.2 维度聚类"></a>2.2 维度聚类</h3><p>在诸多问题领域，bbox其实有很强的可识别模式。比如，自动驾驶领域，2个最常见的模式是汽车和行人。YOLOv2使用kmeans聚类在训练集上得到模式最多的K个bbox。</p>
<p>由于需要处理的是bbox，而非点，其距离衡量指标使用的是IOU。</p>
<p><img src="/images/blog/yolov123_12.png" alt="yolov123"></p>
<p>上图左边是在真实bbox中使用不同聚类中心数目得到的平均IOU，增加聚类中心数目，准确率提升。使用5个聚类中心得到比较均衡的结果，右边是5个聚类中心时的bbox分布。</p>
<h3 id="2-3-直接的位置预测"><a href="#2-3-直接的位置预测" class="headerlink" title="2.3 直接的位置预测"></a>2.3 直接的位置预测</h3><p><strong>预测的是位置相对于anchor左上角的偏移量。</strong>如果直接，无约束的预测会导致随机结果，YOLO预测的是5个参数$t_x,t_y,t_w,t_h$并使用sigma函数加以约束其取值范围。下图中蓝色框是预测的bbox，点线矩形框是anchor。左上角的$C_x,C_y$也是anchor的左上角位置，预测的是相对于此处的偏移。</p>
<p><img src="/images/blog/yolov123_13.png" alt="yolov123"></p>
<p>其中</p>
<script type="math/tex; mode=display">
b_x=\sigma(t_x)+c_x \\
b_y=\sigma(t_y)+c_y \\
b_w=p_we^{t_w} \\
b_h=p_he^{t_h} \\
P_r(obj)*IOU(b,obj)=\sigma(t_0) \quad \sigma(t_0)是box的置信度\\
其中\\
t_x,t_y,t_w,t_h是直接由yolo预测得到的\\
c_x,c_y是anchor的左上角坐标，p_w,p_h是anchor的宽度和高度\\
c_x,c_y,p_w,p_h由宽度和高度归一化之后的结果\\
b_x,b_y,b_w,b_h是最终的预测bbox</script><p>所以，需要知道的是</p>
<ol>
<li><strong>yolo网络只预测偏移量</strong></li>
<li><strong>偏移量与默认的anchor box进行运算之后得到bbox</strong></li>
<li><strong>bbox与真实box不是同一个</strong></li>
</ol>
<h3 id="2-4-修正的特征"><a href="#2-4-修正的特征" class="headerlink" title="2.4 修正的特征"></a>2.4 修正的特征</h3><p>yolo使用了一种称为通道的技术，将$28\times 28\times 512$层reshape到$14\times 14\times 1024$，然后将这两层的feature拼接起来做预测。</p>
<p><img src="/images/blog/yolov123_14.png" alt="yolov123"><br>参考 </p>
<h3 id="2-5-多尺度训练"><a href="#2-5-多尺度训练" class="headerlink" title="2.5 多尺度训练"></a>2.5 多尺度训练</h3><p>移除全连接层的yolo可以接收不同尺度输入图像，如果输入图像宽和高双倍之后，我们需要预测4倍的grid cell。由于yolo是按照32倍下采样的，所以输入图像是32的倍数即可。每10个batch之后，yolo会随机选取其他尺度的图像来训练网络。</p>
<h3 id="2-6-使用不同优化方案之后的准确率比较"><a href="#2-6-使用不同优化方案之后的准确率比较" class="headerlink" title="2.6 使用不同优化方案之后的准确率比较"></a>2.6 使用不同优化方案之后的准确率比较</h3><p><img src="/images/blog/yolov123_15.png" alt="yolov123"></p>
<h3 id="2-7-速度提升"><a href="#2-7-速度提升" class="headerlink" title="2.7 速度提升"></a>2.7 速度提升</h3><p><strong>darknet</strong><br>为了进一步简化CNN结构，设计了一个darknet网络。其在ImageNet上获取了72.9%的top-1准确率和91.2%的top-5准确率。darknet大部分使用的是$3\times 3$卷积</p>
<p><img src="/images/blog/yolov123_16.png" alt="yolov123"></p>
<p>在做目标检测时，最后红色框的最后的卷积层使用$3\times 3$的卷积层替换，然后再使用$1\times 1$的卷积将$7\times 7\times 1024$的输出转换成$7\times 7\times 125$</p>
<h2 id="3-yolov3"><a href="#3-yolov3" class="headerlink" title="3 yolov3"></a>3 yolov3</h2><h3 id="3-1-类预测"><a href="#3-1-类预测" class="headerlink" title="3.1 类预测"></a>3.1 类预测</h3><p>大多数分类器认为目标分类是互斥的，所以yolov2用的是softmax，全部分类的概率之和为1，但是yolov3使用了多标签分类。比如，标签可能既是<code>行人</code>也可能是<code>小孩</code>。yolov3为每个分类使用独立的logistic分类器以计算输入属于特定分类的概率。yolov3给每个分类用的是二分交叉熵，而非MSE。此举同时降低了计算复杂度。</p>
<h3 id="3-2-bbox预测和损失函数"><a href="#3-2-bbox预测和损失函数" class="headerlink" title="3.2 bbox预测和损失函数"></a>3.2 bbox预测和损失函数</h3><p>yolov3使用logistic回归来预测每个bbox的为物体的置信度。yolov3修改计算损失函数的方式，如果bbox与真实标签obj重叠区域大于其他所有的，其对应的obj(物体)置信度为1（即选取一个与ground truth有最多重叠的anchor，其对应的obj score为1）。其他与真实标签obj重叠区域超过阈值(默认为0.5)的anchor，它们不计算损失。每个ground truth obj只分配给一个bbox。如果某个bbox没有被分配ground truth，不计算其分类和定位损失，只计算物体置信度。只使用$t_x,t_y$而非$b_x,b_y$计算损失</p>
<script type="math/tex; mode=display">
b_x=\sigma(t_x)+c_x \\
b_y=\sigma(t_y)+c_y \\
b_w = p_we^{t_w} \\
b_h = p_he^{t_h}</script><h3 id="3-3-特征金字塔-FPN网络"><a href="#3-3-特征金字塔-FPN网络" class="headerlink" title="3.3 特征金字塔(FPN网络)"></a>3.3 特征金字塔(FPN网络)</h3><p>yolov3在每个位置上得到3个预测，每个预测由<code>1个bbox</code>,<code>一个obj物体得分</code>,<code>80个分类得分</code>组成。即$=N\times N\times [3\times (4+1+80)]$ 。</p>
<p>YOLOv3在三个不同尺度上预测</p>
<ol>
<li>最后一个特征map层</li>
<li>从最后一层往回走2层，2倍上采样。yolov3采用了一个更高分辨率的特征图，并将它与上采样层得到的特征图进行逐元素相加。在这个特征融合层，yolov3再进行卷积得到第二个预测集。</li>
<li>重复步骤2，得到的特征层有好的高层结构(语义级)信息和物体位置的空间信息。</li>
</ol>
<h3 id="3-4-特征抽取器"><a href="#3-4-特征抽取器" class="headerlink" title="3.4 特征抽取器"></a>3.4 特征抽取器</h3><p>使用一个新的darknet-53来替换前面的darknet-19，darknet主要由$3\times 3$和$1\times 1$卷积组成，以及一些类似ResNet的跳转连接。</p>
<p><img src="/images/blog/yolov123_17.png" alt="yolov123"></p>
<h3 id="3-5-准确率"><a href="#3-5-准确率" class="headerlink" title="3.5 准确率"></a>3.5 准确率</h3><p><img src="/images/blog/yolov123_18.png" alt="yolov123"></p>
<h2 id="4-损失函数变化"><a href="#4-损失函数变化" class="headerlink" title="4 损失函数变化"></a>4 损失函数变化</h2><h3 id="4-1-YOLOv1损失函数"><a href="#4-1-YOLOv1损失函数" class="headerlink" title="4.1 YOLOv1损失函数"></a>4.1 YOLOv1损失函数</h3><p>先回顾下YOLOv1的损失函数</p>
<script type="math/tex; mode=display">
 Loss= \sum _{i=0} ^{S^2}1 _i ^{obj} \sum _{c \in classes}(p_i(c)-\hat p_i(c))^2 +\\
\lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj}[(x_i-\hat x_i)^2+(y_i-\hat y_i)^2] +\\
 \lambda _{coord}\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2] +\\
\sum _{i=0} ^{S^2}\sum_{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \\
\lambda _{noobj}\sum_{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{noobj}(C_i-\hat C_i)^2</script><h3 id="4-2-YOLOv2"><a href="#4-2-YOLOv2" class="headerlink" title="4.2 YOLOv2"></a>4.2 YOLOv2</h3><p>yolov2的损失函数只是在yolov1基础上改动了关于bbox的w和h的损失计算方式<br>即从</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj} [(\sqrt{w_i}-\sqrt{\hat w_i})^2+((\sqrt{h_i}-\sqrt{\hat {h_i}}))^2]</script><p>改动到</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{obj} [(w_i-\hat {w_i})^2+(h_i-\hat {h_i})^2]</script><p>去掉了w和h的二次根号，作者认为没有必要。</p>
<h3 id="4-3-yolov3"><a href="#4-3-yolov3" class="headerlink" title="4.3 yolov3"></a>4.3 yolov3</h3><p>YOLOv3的损失函数是在yolov2基础上改动的，最大的变动是分类损失换成了二分交叉熵，这是由于yolov3中剔除了softmax改用logistic。不过在不同的框架中具体的修改不一致。</p>
<p><strong>darknet</strong></p>
<p>darknet源码中只修改了分类损失的计算方法。也即从yolov2的分类损失部分</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}\sum_{j=0} ^B1_{ij} ^{obj}(C_i-\hat C_I)^2+ \\
\lambda _{noobj}\sum_{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{noobj}(C_i-\hat C_i)^2</script><p>修改到</p>
<script type="math/tex; mode=display">
\sum _{i=0} ^{S^2}\sum_{j=0} ^B1_{ij} ^{obj} (C_i log \hat C_i+(1-C_i)log (1-\hat C_i) )+\\
\lambda _{noobj}\sum_{i=0} ^{S^2}\sum _{j=0} ^B1_{ij} ^{noobj}(C_i-\hat C_i)^2</script><p><strong>Tensorflow</strong></p>
<p>Tensorflow版本的<a href="https://github.com/qqwweee/keras-yolo3" target="_blank" rel="noopener">yolov3</a> 对以下部分都做了交叉熵损失</p>
<ol>
<li>坐标的x,y<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xy_loss &#x3D; object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[..., 0:2], from_logits&#x3D;True)</span><br></pre></td></tr></table></figure></li>
<li>置信度损失</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">confidence_loss &#x3D; object_mask * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits&#x3D;True) + (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits&#x3D;True) * ignore_mask</span><br></pre></td></tr></table></figure>
<ol>
<li>分类损失</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class_loss &#x3D; object_mask * K.binary_crossentropy(true_class_probs, raw_pred[..., 5:], from_logits&#x3D;True)</span><br></pre></td></tr></table></figure>
<p><strong>没有对非目标的损失函数</strong></p>
<h2 id="5-注意-bbox数目变化"><a href="#5-注意-bbox数目变化" class="headerlink" title="5 注意 bbox数目变化"></a>5 注意 bbox数目变化</h2><ul>
<li>yolov1<script type="math/tex; mode=display">
(S,S,B\times 5+c) = (7,7,2\times 5+20)=(7,7,30)</script></li>
<li><p>yolov2</p>
<script type="math/tex; mode=display">
7\times7\times (5(每个grid cell5个bbox)\times (4(位置)+1(box置信度)+20(20个分类概率)))</script></li>
<li><p>yolov3</p>
</li>
</ul>
<script type="math/tex; mode=display">
(N\times N +2(上采样)\times N\times N +4(上采样)\times (N\times N))\times [3\times (4+1+80(分类))]</script><p><a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" target="_blank" rel="noopener">Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3</a><br><a href="https://www.tinymind.cn/articles/411" target="_blank" rel="noopener">从YOLOv1到YOLOv3，目标检测的进化之路</a><br><a href="https://blog.csdn.net/weixin_42078618/article/details/85005428" target="_blank" rel="noopener">yolov3 损失函数</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>SIFT特征详解</title>
    <url>/2019/03/05/SIFT-feature/</url>
    <content><![CDATA[<h2 id="1-概览"><a href="#1-概览" class="headerlink" title="1 概览"></a>1 概览</h2><p>SIFT特征即Scale-Invariant Feature Transform，是一种用于检测和描述数字图像中的局部特征的算法。它定位关键点并以量化信息呈现（所以称之为描述器），可以用来做目标检测。此特征可以被认为可以对抗不同变换（即同一个特征在不同变换下可能看起来不同）而保持不变。</p>
<p>可以通过这个<a href="http://weitz.de/sift/index.html?size=large" target="_blank" rel="noopener">网站 SIFT特征在线计算</a>，直接查看一张图片中的SIFT特征，你需要准备一张小图片，然后上传到网站，就会自动计算出该图像的SITF特征。如果对SIFT特征计算步骤缺乏形象的认识，可以去这个网站互动下，它可以可视化每个步骤。</p>
<p>SIFT特征的提取步骤</p>
<ol>
<li>生成高斯差分金字塔（DOG金字塔），尺度空间构建</li>
<li>空间极值点检测（关键点的初步查探）</li>
<li>稳定关键点的精确定位</li>
<li>稳定关键点方向信息分配</li>
<li>关键点描述</li>
<li>特征点匹配</li>
</ol>
<h2 id="2-生成差分高斯金字塔"><a href="#2-生成差分高斯金字塔" class="headerlink" title="2 生成差分高斯金字塔"></a>2 生成差分高斯金字塔</h2><p>参考 <a href="https://shartoo.github.io/image-pramid/">图像处理中各种金字塔</a> 得到一组如下图</p>
<p><img src="/images/blog/sift_feature1.png" alt="sift1"> </p>
<h2 id="3-空间极值点检测"><a href="#3-空间极值点检测" class="headerlink" title="3 空间极值点检测"></a>3 空间极值点检测</h2><p>从第2步的差分高斯金字塔(DOG)中可以得到不同层级不同尺度的金字塔，<strong>所谓的特征就是一些强的、有区分力的点</strong>。DOG中这些有区分力的点就是极值点，即每个像素都要和相邻点(<strong>此处的相邻不仅仅是水平面的前后左右，还有上下尺度的前后左右</strong>)比较，看其是否比它的图像域(水平方向上)和尺度空间域(垂直方向上)的相邻点大或者小，下图是示意图：</p>
<p><img src="/images/blog/sift_feature2.png" alt="sift1"> </p>
<p>在二维图像空间，中心点与它3<em>3邻域内的8个点做比较，在同一组内的尺度空间上，中心点和上下相邻的两层图像的2</em>9个点作比较，如此可以保证检测到的关键点在尺度空间和二维图像空间上都是局部极值点。所以确定极值点，需要$3\times 3-1(当前像素点，上图中中间黑色X)+2\times 9=26个点$。<br>从第2小节中，我们计算得到的极值点由如下黄色和红色标记（其中黄色圆圈的标记表明，它虽然是极值点，但是由于绝对值过小，在后续处理时会被丢弃）</p>
<p><img src="/images/blog/sift_feature3.png" alt="sift1"> </p>
<p>我们选取图中间用蓝色矩形框标记的红色点，查看其灰度值(下图正中间红色点)，可以看到它在当前差分金字塔取得了极小值。</p>
<p><img src="/images/blog/sift_feature4.png" alt="sift1"> </p>
<h2 id="4-稳定关键点的精确定位"><a href="#4-稳定关键点的精确定位" class="headerlink" title="4 稳定关键点的精确定位"></a>4 稳定关键点的精确定位</h2><p>上面步骤得到的极值点中存在大量不稳定地点，有些可能是噪音导致的，比如第3节中黄色圆圈标记的点。我们需要去除这些不稳定地像素点。即去除DOG局部曲率非常不对称的像素。此步骤，需要计算空间尺度函数的二次泰勒展开式的极值来完成。同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力。</p>
<p>具体步骤如下：</p>
<ol>
<li><p>空间尺度函数泰勒展开式如下：</p>
<script type="math/tex; mode=display">
D(x)=D+\frac{\partial D^T}{\partial x}x+\frac{1}{2}x^T\frac{\partial ^2D}{\partial x^2}x \\
对上式求导，令其为0，得到精确地位置，有\\
\hat x=-\frac{\partial ^2D^{-1}}{\partial x^2}\frac{\partial D}{\partial x}</script></li>
<li><p>在已经检测到的特征点中，要去掉低对比度的特征点和不稳定地边缘响应点。去除低对比度的点：把$\hat x$的值代回，即在Dog 空间的极值点$D(x)$处取值，<strong>只取前两项</strong>可得：</p>
<script type="math/tex; mode=display">
D(\hat x)=D+\frac{1}{2}\frac{\partial D^T}{\partial x}\hat x</script><p>若 $|D(\hat x)\ge 0.003$，该特征点就保留下来，否则丢弃</p>
</li>
</ol>
<p>3.边缘响应的去除。一个定义不好的高斯差分算子的极值在横跨边缘的地方有较大的曲率，而在垂直边缘的方向有较小的曲率。主曲率通过一个$2\times 2$的Hessian矩阵H去求出：</p>
<script type="math/tex; mode=display">
H=[ \begin{array} D_{xx} \quad D_{xy} & \\
  D_{xy}\quad D_{yy}  & 
 \end{array} ]</script><p>使用python计算hessian矩阵的代码可以参考<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def hessian(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculate the hessian matrix with finite differences</span><br><span class="line">    Parameters:</span><br><span class="line">       - x : ndarray</span><br><span class="line">    Returns:</span><br><span class="line">       an array of shape (x.dim, x.ndim) + x.shape</span><br><span class="line">       where the array[i, j, ...] corresponds to the second derivative x_ij</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x_grad &#x3D; np.gradient(x) </span><br><span class="line">    hessian &#x3D; np.empty((x.ndim, x.ndim) + x.shape, dtype&#x3D;x.dtype) </span><br><span class="line">    for k, grad_k in enumerate(x_grad):</span><br><span class="line">        # 遍历每个维度，在一阶导数的所有项再次使用梯度</span><br><span class="line">        tmp_grad &#x3D; np.gradient(grad_k) </span><br><span class="line">        for l, grad_kl in enumerate(tmp_grad):</span><br><span class="line">            hessian[k, l, :, :] &#x3D; grad_kl</span><br><span class="line">    return hessian</span><br><span class="line"></span><br><span class="line">x &#x3D; np.random.randn(100, 100, 100)</span><br><span class="line">hessian(x)</span><br></pre></td></tr></table></figure><br>导数由采样点相邻差估计得到。D的主曲率和H的特征值成正比，令α为较大特征值，β为较小的特征值，则</p>
<script type="math/tex; mode=display">
Tr(H)=D_{xx}+D_{yy}=\alpha +\beta \\
Det(H)=D_{xx}D_{yy}-(D_{xy})^2 = \alpha\beta \\
令\alpha=\gamma \beta则 \\
\frac{Tr(H)^2}{Det(H)} =\frac{(\alpha+\beta)^2}{\alpha \beta}=\frac{(\gamma\beta +\beta)^2}{\gamma \beta ^2}=\frac{(1+\gamma)^2}{\gamma}</script><p>而$\frac{(1+\gamma)^2}{\gamma}$的值在两个特征值相等的时候最小，随着$\gamma$的增大而增大，因此，为了检测主曲率是否在某阈值$\gamma$下，只需检测</p>
<script type="math/tex; mode=display">
\frac{Tr(H)^2}{Det(H)}<\frac{(\gamma+1)^2}{\gamma}</script><p>在SIFT特征提取的原论文中，提到<strong>如果$\frac{\alpha +\beta}{\alpha \beta}&gt;\frac{(\gamma +1)^2}{\gamma}$，则丢弃此像素点</strong>，论文中$\gamma=10$</p>
<h2 id="5-给特征点赋值一个128维方向参数"><a href="#5-给特征点赋值一个128维方向参数" class="headerlink" title="5 给特征点赋值一个128维方向参数"></a>5 给特征点赋值一个128维方向参数</h2><p>经过上面的步骤，我们已经确定了一些灰度值极值点。接下来，我们需要确定这些极值点的方向。为关键点分配方向信息所要解决的问题是使得关键点对图像角度和旋转具有不变性。方向的分配是通过求每个极值点的梯度来实现的。<br>对于任一关键点</p>
<ul>
<li>其梯度<strong>幅值</strong>表述为：</li>
</ul>
<script type="math/tex; mode=display">
m(x,y) = \sqrt{((L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}</script><ul>
<li>梯度<strong>方向</strong>为：<script type="math/tex; mode=display">
\theta (x,y) = tan ^{-1}[\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}]</script></li>
</ul>
<p><strong>分配给关键点的并不直接是关键点的梯度方向，而是按照一种梯度方向直方图方式给出的</strong><br>计算方法</p>
<ol>
<li>计算关键点为中心邻域内所有点的梯度方向。0-360度</li>
<li>每个方向10度，共36个方向。</li>
<li>统计累计落在每个方向点的关键点个数，依次生成梯度直方图</li>
</ol>
<p><img src="/images/blog/sift_feature5.png" alt="sift1"> </p>
<p>具体在图像实例中，某个极值点的梯度方向直方图如下：</p>
<p><img src="/images/blog/sift_feature6.png" alt="sift1"> </p>
<p>上图中，左图矩形框内的红色小圆圈代表点击的极值点，中间图案代表最终得到的极值点的主方向，右图为对应的梯度方向直方图。</p>
<p>除此之外，原论文中还包含了<strong>辅方向</strong>，辅方向定义为：若在梯度直方图中存在一个相当于主峰值80%能量的峰值，则认为是关键点的辅方向。辅方向的设计可以增强匹配的鲁棒性，Lowe指出，大概有15%的关键点具有辅方向，而恰恰是这15%的关键点对稳定匹配起到关键作用。</p>
<h2 id="6-计算SIFT特征描述子"><a href="#6-计算SIFT特征描述子" class="headerlink" title="6 计算SIFT特征描述子"></a>6 计算SIFT特征描述子</h2><p>此步骤与步骤5基本一样，也是计算每个关键点周围的梯度方向的直方图分布。不同之处在于，此时的邻居为一个圆，并且坐标体系被扭曲以匹配相关梯度方向。<br>具体思路是：对关键点周围像素区域分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象表述。<br>如下图，对于2<em>2块，每块的所有像素点的梯度做高斯加权，每块最终取8个方向，即可以生成2</em>2<em>8维度的向量，以这2</em>2*8维向量作为中心关键点的数学描述</p>
<p><img src="/images/blog/sift_feature7.png" alt="sift1"> </p>
<p>但是实际上，在原论文中证明，对每个关键点周围采用$4\times 4$块(每个块内依然是8个方向)的邻域描述子效果最佳</p>
<p><img src="/images/blog/sift_feature8.png" alt="sift1"> </p>
<p>所以，<strong>此时我们计算的不是一个梯度方向直方图，而是16个</strong>。每个梯度直方图对应的是新坐标系统的中心点附近的点以及圆形周围邻居梯度的分量。</p>
<p>下图是某个极值点用于生成的描述子的邻居以及坐标系统，即直方图（被归一化并以$4\times 4\times 8=128$个整型数字）。仔细看下图，会发现有16个直方图(16个块)，每个直方图有8个bins(代表每个块的8个主方向)。</p>
<p><img src="/images/blog/sift_feature9.png" alt="sift1"> </p>
<p><a href="https://blog.csdn.net/dcrmg/article/details/52577555" target="_blank" rel="noopener">CSDN SIFT特征</a><br><a href="https://blog.csdn.net/abcjennifer/article/details/7639681" target="_blank" rel="noopener">Rachel-Zhang SIFT</a><br><a href="https://stackoverflow.com/questions/31206443/numpy-second-derivative-of-a-ndimensional-array" target="_blank" rel="noopener">stackoverflow 计算hessian矩阵</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>HOG特征详解</title>
    <url>/2019/03/04/HOG-feature/</url>
    <content><![CDATA[<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0 简介"></a>0 简介</h2><p>HOG特征即 Histogram of oriented gradients，源于2005年一篇<a href="https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf" target="_blank" rel="noopener">CVPR论文</a>，使用HOG+SVM做行人检测，由于效果良好而被广泛应用。大体效果如下，具体使用HOG+SVM做行人检测时再讨论详细代码。</p>
<p><img src="/images/blog/hog_feature_1.jpg" alt="sift1"> </p>
<p>算法计算步骤概览</p>
<ol>
<li>图像预处理。<code>伽马矫正</code>(减少光度影响)和<code>灰度化</code>(也可以在RGB图上做，只不过对三通道颜色值计算，取梯度值最大的)【可选步骤】</li>
<li>计算图像像素点梯度值，得到梯度图(尺寸和原图同等大小)</li>
<li>图像划分多个cell，统计cell内梯度直方向方图</li>
<li>将$2\times 2$个cell联合成一个block,对每个block做块内梯度归一化</li>
</ol>
<h2 id="1-图像预处理"><a href="#1-图像预处理" class="headerlink" title="1 图像预处理"></a>1 图像预处理</h2><h3 id="1-1-gamma矫正和灰度化"><a href="#1-1-gamma矫正和灰度化" class="headerlink" title="1.1 gamma矫正和灰度化"></a>1.1 gamma矫正和灰度化</h3><p><strong>作用</strong>：gamma矫正通常用于电视和监视器系统中重现摄像机拍摄的画面．在图像处理中也可用于调节图像的对比度，减少图像的光照不均和局部阴影．<br><strong>原理</strong>： 通过非线性变换，让图像从暴光强度的线性响应变得更接近人眼感受的响应，即将漂白（相机曝光）或过暗（曝光不足）的图片，进行矫正</p>
<p>gamma矫正公式： </p>
<script type="math/tex; mode=display">
f(x) =x^{\gamma}</script><p>即输出是输入的幂函数，指数为$\gamma$</p>
<p>代码实现如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">img &#x3D; cv2.imread(&#39;gamma0.jpg&#39;,0)</span><br><span class="line">img1 &#x3D; np.power(img&#x2F;float(np.max(img)), 1&#x2F;1.5)</span><br><span class="line">img2 &#x3D; np.power(img&#x2F;float(np.max(img)), 1.5)</span><br><span class="line"></span><br><span class="line">cv2.imshow(&#39;src&#39;,img)</span><br><span class="line">cv2.imshow(&#39;gamma&#x3D;1&#x2F;1.5&#39;,img1)</span><br><span class="line">cv2.imshow(&#39;gamma&#x3D;1.5&#39;,img2)</span><br><span class="line">cv2.waitKey(0)</span><br></pre></td></tr></table></figure>
<p>下图分别代表了处理之后的<code>原图</code>,<code>灰度图</code>，<code>gamma=1/1.5矫正</code>,<code>gamma=1.5矫正</code></p>
<p><img src="/images/blog/hog_feature_2.jpg" alt="sift1"> </p>
<h2 id="2-计算图像像素梯度图"><a href="#2-计算图像像素梯度图" class="headerlink" title="2 计算图像像素梯度图"></a>2 计算图像像素梯度图</h2><p>我们需要同时计算图像的<code>水平梯度图</code>和<code>垂直梯度图</code> 。如下图，假设我们要计算下图中像素点A的梯度值，</p>
<p><img src="/images/blog/hog_feature_3.jpg" alt="sift1"> </p>
<p>计算方法为</p>
<p><strong>梯度大小</strong></p>
<ul>
<li>水平梯度： $g_x=\sqrt {(L(x-1,y)-L(x+1,y))^2}=\sqrt{(30-20)^2}=\sqrt{10^2}=10$</li>
<li>垂直梯度： $g_y=\sqrt {(L(,y+1)-L(x,y-1))^2}=\sqrt{(32-64)^2}=\sqrt{32^2}=32$</li>
</ul>
<p><strong>梯度方向</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\theta (x,y) = arctan [\frac{g_x}{g_y}] =arctan\frac{10}{32}</script></li>
</ul>
<p>梯度方向会取绝对值，因此得到的角度范围是 $[0,180°]$</p>
<p>上面这些计算过程，在opencv中有对应的算子，称为Sobel算子，分别计算水平和垂直方向梯度的。</p>
<p><img src="/images/blog/hog_feature_4.jpg" alt="sift1"> </p>
<p>使用的python opencv代码为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">im &#x3D; cv2.imread(&#39;bolt.png&#39;)</span><br><span class="line">im &#x3D; np.float32(im) &#x2F; 255.0</span><br><span class="line"> </span><br><span class="line"># 计算梯度</span><br><span class="line">img &#x3D; cv2.G</span><br><span class="line">gx &#x3D; cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize&#x3D;1)</span><br><span class="line">gy &#x3D; cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize&#x3D;1)</span><br><span class="line"># 计算梯度幅度和方向</span><br><span class="line">mag, angle &#x3D; cv2.cartToPolar(gx, gy, angleInDegrees&#x3D;True)</span><br><span class="line">cv2.imshow(&quot;absolute x-gradient&quot;,gx)</span><br><span class="line">cv2.imshow(&quot;absolute y-gradient&quot;,gy)</span><br><span class="line">cv2.imshow(&quot;gradient magnitude&quot;,mag)</span><br><span class="line">cv2.imshow(&quot;gradient direction&quot;,angle)</span><br><span class="line">cv2.waitKey(0)</span><br></pre></td></tr></table></figure>
<p>效果如下，分别为<code>原图</code>,<code>x方向梯度绝对值</code>,<code>y方向梯度绝对值图</code>,<code>梯度幅度图</code>,<code>梯度方向图</code><br><strong>下图是没有使用归一化效果</strong></p>
<p><img src="/images/blog/hog_feature_5.jpg" alt="sift1"> </p>
<p><strong>使用归一化之后的效果</strong></p>
<p><img src="/images/blog/hog_feature_6.jpg" alt="sift1"> </p>
<p>可以看到</p>
<ul>
<li>x方向梯度图会强化垂直方向的特征，可以观察到左侧白色斜线更加明显，但是底部一些水平线没有了。</li>
<li>y方向梯度图会强化水平方向特征，底部水平线强化了，左侧垂直线不是那么明显了。</li>
</ul>
<p>梯度图移除了大量非显著性特征，并加强了显著特征。三通道的彩色图中，每个像素的梯度幅度是三个通道中最大的那个，而梯度方向是梯度幅度最大的那个通道上的方向。</p>
<h2 id="3-计算梯度直方图"><a href="#3-计算梯度直方图" class="headerlink" title="3 计算梯度直方图"></a>3 计算梯度直方图</h2><p>经过上一步计算之后，每个像素点都会有两个值：<strong>梯度方向和梯度幅度</strong>。</p>
<p>但是，也看到了，梯度幅度和梯度方向图与原图等同大小，实际如果使用这些特征，会存在两个问题</p>
<ul>
<li>计算量很大，基本就是原图</li>
<li>特征稀疏。图中其实只有少量稀疏的显著特征，大部分可能是0</li>
</ul>
<p>以上是个人理解。</p>
<p>HOG特征在此步骤选择联合一个$8\times 8$的小格子内部一些像素，计算其梯度幅度和梯度方向的统计直方图，这样一来就可以以这个梯度直方图来代替原本庞大的矩阵。每个像素有一个梯度幅度和梯度方向两个取值，那么一个$8\times 8$的小格子一共有$8\times 8\times 2=128$个取值。</p>
<p>上面提到，梯度方向取值范围是$[0,180]$，以每20°为一个单元，所有的梯度方向可以划分为9组，这就是统计直方图的分组数目。如下图，我们选取划分格子之后的第二行第二列一个小单元，计算得到右边的<code>梯度方向图</code>和<code>梯度幅度图</code>，同时以以梯度方向为index，统计分组数量。</p>
<p><img src="/images/blog/hog_feature_7.jpg" alt="sift1"> </p>
<p>得到的统计频率直方图如下</p>
<p><img src="/images/blog/hog_feature_8.jpg" alt="sift1"> </p>
<p>从上图可以看到，更多的点的梯度方向是倾向于0度和160度，也就是说这些点的梯度方向是向上或者向下，表明图像这个位置存在比较明显的横向边缘。因此HOG是对边角敏感的，由于这样的统计方法，也是对部分像素值变化不敏感的，所以能够适应不同的环境。</p>
<p>至于为什么选取$8\times 8$为一个单元格，是因为HOG特征当初设计时是用来做行人检测的。在行人图片中$8\times8$的矩阵被缩放成$64\times 128$的网格时，足以捕获一些特征，比如脸部或者头部特征等。</p>
<h2 id="4-block归一化"><a href="#4-block归一化" class="headerlink" title="4 block归一化"></a>4 block归一化</h2><p>目的：降低光照的影响<br>方法：向量的每一个值除以向量的模长</p>
<p>比如对于一个$(128,64,32)$的三维向量来说，模长是$ \sqrt{128^2+64^2+32^2}=146.64$,那么归一化后的向量变成了$(0.87,0.43,0.22)$。</p>
<p>HOG在选取$8\times 8$为一个单元格的基础之上，再以$2\times 2$个单元格为一组，称为block。作者提出要对block进行归一化，由于每个单元格cell有9个向量，$2\times 2$个单元格则有36个向量，需要对这36个向量进行归一化。下图演示了如何在图像中抽取block</p>
<p><img src="/images/blog/hog_feature_9.gif" alt="sift1"> </p>
<h2 id="5-HOG特征描述"><a href="#5-HOG特征描述" class="headerlink" title="5  HOG特征描述"></a>5  HOG特征描述</h2><p>每一个$16\times 16$大小的block将会得到36大小的vector。那么对于一个$64\times128$大小的图像，按照上图的方式提取block，将会有7个水平位置和15个竖直位可以取得，所以一共有$7\times15=105$个block，所以我们整合所有block的vector，形成一个大的一维vector的大小将会是$36\times105=3780$。</p>
<h2 id="6-参考代码"><a href="#6-参考代码" class="headerlink" title="6 参考代码"></a>6 参考代码</h2><p>计算图像HOG特征时，我们使用如下代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from skimage.feature import hog</span><br><span class="line">from skimage import data, exposure</span><br><span class="line">image &#x3D; cv2.imread(&#39;C:&#x2F;Users&#x2F;dell&#x2F;Desktop&#x2F;123.png&#39;)</span><br><span class="line"></span><br><span class="line">fd, hog_image &#x3D; hog(image, orientations&#x3D;8, pixels_per_cell&#x3D;(16, 16),</span><br><span class="line">                    cells_per_block&#x3D;(1, 1), visualize&#x3D;True, multichannel&#x3D;True)</span><br><span class="line">fig, (ax1, ax2) &#x3D; plt.subplots(1, 2, figsize&#x3D;(8, 4), sharex&#x3D;True, sharey&#x3D;True)</span><br><span class="line">ax1.axis(&#39;off&#39;)</span><br><span class="line">ax1.imshow(image, cmap&#x3D;plt.cm.gray)</span><br><span class="line">ax1.set_title(&#39;Input image&#39;)</span><br><span class="line"># Rescale histogram for better display</span><br><span class="line">hog_image_rescaled &#x3D; exposure.rescale_intensity(hog_image, in_range&#x3D;(0, 10))</span><br><span class="line">ax2.axis(&#39;off&#39;)</span><br><span class="line">ax2.imshow(hog_image_rescaled, cmap&#x3D;plt.cm.gray)</span><br><span class="line">ax2.set_title(&#39;Histogram of Oriented Gradients&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果如下</p>
<p><img src="/images/blog/hog_feature_10.jpg" alt="sift1"></p>
<h2 id="7-参考"><a href="#7-参考" class="headerlink" title="7 参考"></a>7 参考</h2><p><a href="https://zhuanlan.zhihu.com/p/40960756" target="_blank" rel="noopener">知乎 图像HOG特征计算</a><br><a href="https://blog.csdn.net/akadiao/article/details/79679306" target="_blank" rel="noopener">图像gamma矫正</a><br><a href="https://www.learnopencv.com/histogram-of-oriented-gradients/" target="_blank" rel="noopener">梯度计算</a><br><a href="https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html" target="_blank" rel="noopener">skimage 计算图像HOG特征</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>图像处理基础Haar特征</title>
    <url>/2019/03/02/img-haar-feature/</url>
    <content><![CDATA[<h2 id="1-Haar特征"><a href="#1-Haar特征" class="headerlink" title="1 Haar特征"></a>1 Haar特征</h2><p>Haar特征最早是由应用于人脸特征表示时提出的，此特征反映的是图像部分区域内灰度变化情况，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。<br>Haar特征有4类: <code>边缘特征</code>,<code>线性特征</code>,<code>中心特征</code>,<code>对角线特征</code>，如下图示例，这些，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。</p>
<p><img src="/images/blog/haar_1.jpg" alt="haar特征"></p>
<p><strong>如何计算上面特征</strong></p>
<ul>
<li>对于A和B ：$特征值=sum <em>白-sum</em>黑$</li>
<li>对于C：$特征值=sum <em>白-2\times sum</em>黑$,白色乘以2的原因是，保持与黑色区域一样的像素数目。</li>
</ul>
<p>通过改变特征模板的大小和位置，可在图像子窗口中穷举出大量的特征。上图的特征模板称为“特征原型”。而特征类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征。这就带来了两个问题</p>
<ol>
<li>如何快速计算那么多的特征： <strong>积分图</strong></li>
<li>哪些矩形特征才是对分类器分类最有效的：<strong>训练分类算法</strong>，如AdaBoost</li>
</ol>
<h2 id="2-积分图"><a href="#2-积分图" class="headerlink" title="2 积分图"></a>2 积分图</h2><p>积分图就是只遍历一次图像就可以求出图像中所有区域像素和的快速算法。核心思想是：将图像从起点开始到各个点所形成的矩形区域像素之和作为一个数组的元素保存在内存中，当要计算某个区域的像素和时可以直接索引数组的元素，不用重新计算这个区域的像素和，从而加快了计算。</p>
<p>积分图是一种能够描述全局信息的<strong>矩阵表示</strong>方法。<strong>积分图的构造方式是位置（i,j）处的值ii(i,j)是原图像(i,j)左上角方向所有像素的和</strong>。</p>
<p>积分图的构建算法</p>
<ol>
<li>用$s(i,j)$表示行方向的累加和，初始化$s(i,-1)=0$</li>
<li>用$ii(i,j)$表示一个积分图像，初始化$ii(-1,i)=0$</li>
<li>逐行扫描图像，递归计算每个像素$(i,j)$行方向的累加和$s(i,j)$和积分图像$ii(i,j)$的值。<script type="math/tex; mode=display">
s(i,j)=s(i,j-1)+f(i,j) \\
ii(i,j)=ii(i-1,j)+s(i,j)</script></li>
<li>扫描图像一遍，当到达图像右下角像素时，积分图像ii就构造好了。</li>
</ol>
<p><strong>计算方块内的像素和</strong><br>计算好了积分图，我们接下来就可以利用积分图来加速计算某个方块内部的像素的和</p>
<p><img src="/images/blog/haar_2.jpg" alt="haar特征"></p>
<p>如上图，假设我们想计算区域D的像素和。上图中D的四个顶点分别是1,2,3,4。令$rectsum_n$代表顶点$n$左上角的所有像素和，那么区域D内的像素和为: $rectsum_4-rectsum_2-rectsum_3+rectsum_1，注意顶点1的坐上所有像素和被减了2次，所以要加一次$</p>
<h2 id="3-Adaboost-算法"><a href="#3-Adaboost-算法" class="headerlink" title="3 Adaboost 算法"></a>3 Adaboost 算法</h2><p>opencv中关于adaboost训练过程参考 <a href="https://docs.opencv.org/3.3.0/dc/d88/tutorial_traincascade.html" target="_blank" rel="noopener">opencv tutorial_traincascade</a><br>后面再讲到 Adaboost算法时，再详细说明。</p>
<h2 id="4-opencv-中使用haar特征检测人脸"><a href="#4-opencv-中使用haar特征检测人脸" class="headerlink" title="4 opencv 中使用haar特征检测人脸"></a>4 opencv 中使用haar特征检测人脸</h2><p>opencv已经预先包含了多种训练好的<code>人脸分类器</code>，<code>眼睛分类器</code>，<code>微笑分类器</code>等。可以自己定义和训练一个，此处直接使用opencv已经训练好的人脸级联分类器，它的算法原理就是使用了Haar特征+Adaboost算法训练出的级联分类器。</p>
<p>首先，载入xml定义的分类器，载入的图像必须是灰度图<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">face_cascade &#x3D; cv.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;)</span><br><span class="line">eye_cascade &#x3D; cv.CascadeClassifier(&#39;haarcascade_eye.xml&#39;)</span><br><span class="line">img &#x3D; cv.imread(&#39;sachin.jpg&#39;)</span><br><span class="line">gray &#x3D; cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure><br>接着开始检测人脸，如果有检测到则画出矩形框，下图是检测结果(同时包含了眼睛检测)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">faces &#x3D; face_cascade.detectMultiScale(gray, 1.3, 5)</span><br><span class="line">for (x,y,w,h) in faces:</span><br><span class="line">    cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)</span><br><span class="line">    roi_gray &#x3D; gray[y:y+h, x:x+w]</span><br><span class="line">    roi_color &#x3D; img[y:y+h, x:x+w]</span><br><span class="line">    eyes &#x3D; eye_cascade.detectMultiScale(roi_gray)</span><br><span class="line">    for (ex,ey,ew,eh) in eyes:</span><br><span class="line">        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)</span><br><span class="line">cv.imshow(&#39;img&#39;,img)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/haar_3.jpg" alt="haar特征"></p>
<h2 id="5-Haar特征拓展"><a href="#5-Haar特征拓展" class="headerlink" title="5 Haar特征拓展"></a>5 Haar特征拓展</h2><h3 id="5-1-ABH特征"><a href="#5-1-ABH特征" class="headerlink" title="5.1 ABH特征"></a>5.1 ABH特征</h3><p>为提高二分Haar特征的区分能力，我们提出 了一种多二分Haar特征，并使用它们的共现性作为新的特征，成为ABH(Asseming Binary Haar)特征。</p>
<p>下图演示了ABH特征的一个例子。</p>
<p><img src="/images/blog/haar_4.jpg" alt="haar特征"></p>
<p>上图中ABH特征集成了三个二分Haar特征，当三个二分Haar特征值分别为1,1,0时，ABH特征为 a(b1,b2,b3)=(110)2=6<br>其中 a为三个二分Haar特征 b1,b2,b3 的ABH特征计算函数， (.)2 是一个从二进制转十进制的操作。特征值说明了对 2F个不同结合的index，其中F是结合的二分特征数。</p>
<h3 id="5-2-LAB-Locally-Assembing-Binary-特征"><a href="#5-2-LAB-Locally-Assembing-Binary-特征" class="headerlink" title="5.2 LAB(Locally Assembing Binary)特征"></a>5.2 LAB(Locally Assembing Binary)特征</h3><p>ABH特征的数目巨大。为了枚举所有的特征，需要几个自由参数，比如二分Haar特征的集合数目，每个二分Haar特征的大小，每个二分Haar特征的坐标位置。从如此巨大的特征池中学习是不可逆的。我们发现了一种对应的用于人脸检测的缩减集合，称为LAB Haar特征。</p>
<p>ABH特征之中，LAB特征是那些结合8个局部邻接2-矩形的二分Haar特征，它们大小相同并且共享同一个中心矩形。下图展示了一个8个二分Haar特征用以集合为一个LAB特征。</p>
<p><img src="/images/blog/haar_5.jpg" alt="haar特征"></p>
<p>下图是一个2个LAB特征的示例</p>
<p><img src="/images/blog/haar_6.jpg" alt="haar特征"></p>
<p>图中展示了两个不同的LAB特征，中心的黑色矩形被8个相邻的二分Haar特征共享，所有9个矩形都是相同的大小。</p>
<p>从公式上看，一个LAB特征可以用一个四元组表示 l(x,y,w,h) ，其中 x,y 分别代表了左上角的x和y轴坐标，(w,h) 代表了矩形的宽和高。</p>
<p>LAB特征保留了所有二分Haar特征的优势，同时又很强的区分能力，大小也很小。LAB特征抓取了图像的局部强度。计算LAB特征需要计算8个2-矩形Haar特征。LAB特征值区间为 {0,…255}，每个值对应了特别的局部结构。</p>
<p><a href="https://docs.opencv.org/3.4.3/d7/d8b/tutorial_py_face_detection.html" target="_blank" rel="noopener">opencv haar特征进行人脸检测</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>图像中的各种金字塔</title>
    <url>/2019/02/24/image-pramid/</url>
    <content><![CDATA[<h2 id="0-定义"><a href="#0-定义" class="headerlink" title="0 定义"></a>0 定义</h2><p>问题：假设要进行人脸识别，但是人脸与摄像头之间距离忽远忽近，单一分辨率的识别算法无法识别所有距离下的人脸特征。</p>
<p>图像金字塔是一种以多分辨率来解释图像的结构，通过对原始图像进行多尺度像素采样的方式，生成N个不同分辨率的图像。<br>把具有最高级别分辨率的图像放在底部，以金字塔形状排列，往上是一系列像素（尺寸）逐渐降低的图像，一直到金字塔的顶部只包含一个像素点的图像，这就构成了传统意义上的图像金字塔。</p>
<p><strong>示例图形金字塔</strong></p>
<p><img src="/images/blog/image_praid_example.png" alt="图像金字塔"></p>
<p><strong>获取金字塔步骤</strong></p>
<ol>
<li>利用低通滤波器平滑图像</li>
<li>对平滑图像进行采样。有两种采样方式：<code>上采样</code>（分辨率逐渐升高）,<code>下采样</code>(分辨率直接按降低)</li>
</ol>
<p><strong>图像金字塔层数与图像大小关系</strong></p>
<p>以$512\times512$为例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>金字塔层数</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像大小</td>
<td>512</td>
<td>216</td>
<td>128</td>
<td>64</td>
<td>16</td>
<td>8</td>
<td>4</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>尺寸变化时不够除的会进行四舍五入。</p>
<p><strong>上采样和下采样</strong></p>
<ul>
<li><strong>上采样</strong>:如果想放大图像，则需要通过向上取样操作得到，具体做法如下<ol>
<li>将图像在每个方向扩大为原来的俩倍，新增的行和列以0填充</li>
<li>使用先前同样的内核（乘以4）与放大后的图像卷积，获得新增像素的近似值</li>
</ol>
</li>
</ul>
<p>在opencv中的代码很简单<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">src &#x3D; cv.pyrUp(src, dstsize&#x3D;(2 * cols, 2 * rows))</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>下采样</strong>:为了获取层级为 $G_i+1$ 的金字塔图像，我们采用如下方法:<ol>
<li>对图像G_i进行高斯内核卷积</li>
<li>将所有偶数行和列去除</li>
</ol>
</li>
</ul>
<p>在opencv中的代码<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">src &#x3D; cv.pyrDown(src, dstsize&#x3D;(cols &#x2F; 2, rows &#x2F; 2))</span><br></pre></td></tr></table></figure><br>显而易见，结果图像只有原图的四分之一。通过对输入图像$G_i$(原始图像)不停迭代以上步骤就会得到整个金字塔。同时我们也可以看到，向下取样会逐渐丢失图像的信息。 以上就是对图像的向下取样操作，即缩小图像</p>
<p><strong>两种图像金字塔</strong></p>
<ul>
<li>高斯金字塔</li>
<li>Laplace金字塔</li>
</ul>
<h2 id="2-SIFT中的高斯金字塔"><a href="#2-SIFT中的高斯金字塔" class="headerlink" title="2 SIFT中的高斯金字塔"></a>2 SIFT中的高斯金字塔</h2><p>高斯金字塔不是一个金字塔，而<strong>是很多组(Octave)金字塔,而且每组金字塔包含若干层</strong>。在opencv官方文档中的高斯金字塔看起来只是一上下采样，而且每一组只有一层。</p>
<p>构建过程</p>
<ol>
<li>先将原图像扩大一倍之后作为高斯金字塔的第1组第1层，将第1组第1层图像经高斯卷积（其实就是高斯平滑或称高斯滤波）之后作为第1组金字塔的第2层，高斯卷积函数为：</li>
</ol>
<script type="math/tex; mode=display">
G(x,y)=\frac{1}{2\pi \sigma ^2}e^{-\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma ^2}}</script><p>对于参数σ，在Sift算子中取的是固定值1.6。</p>
<ol>
<li><p>将σ乘以一个比例系数k,等到一个新的平滑因子σ=k*σ，用它来平滑第1组第2层图像，结果图像作为第3层。</p>
</li>
<li><p>如此这般重复，最后得到L层图像，在同一组中，每一层图像的尺寸都是一样的，只是平滑系数不一样。它们对应的平滑系数分别为：$0，σ，kσ，k^2σ,k^3σ……k^{L-2}σ$。</p>
</li>
<li><p>将第1组倒数第三层图像作比例因子为2的降采样，得到的图像作为第2组的第1层，然后对第2组的第1层图像做平滑因子为σ的高斯平滑，得到第2组的第2层，就像步骤2中一样，如此得到第2组的L层图像，同组内它们的尺寸是一样的，对应的平滑系数分别为：$0，σ，kσ，k^2σ,k^3σ……k^{(L-2)}σ$。但是在尺寸方面第2组是第1组图像的一半。</p>
</li>
</ol>
<p>这样反复执行，就可以得到一共O组，每组L层，共计O*L个图像，这些图像一起就构成了高斯金字塔，结构如下：</p>
<p><img src="/images/blog/gauss_image_praid.png" alt="图像金字塔"></p>
<p>上图第一行的单独一副图像是原图经过了双线性插值做了上采样使得图像尺寸扩充了4倍(高度和宽度各扩充一倍)。图像一共4行6列，代表了图像金字塔有4层，6组(也称之为八度)。同一列，从上至下是降采样过程，可以看到图像尺寸不断缩小；同一行，从左往右是使用不同平滑系数进行高斯模糊过程，可以看到图像越来越模糊。【注意此图是灰度图演示过程，下面的图是在原图基础上做的，所以效果不一样】</p>
<p><strong>代码示例</strong><br>我们以下图的lnea.jpg为例<br><img src="/images/blog/lnea.jpg" alt="图像金字塔"><br>得到的图像金字塔结果如下<br><img src="/images/blog/image_pyramid_result.png" alt="图像金字塔"></p>
<p>代码位于 <a href="https://github.com/shartoo/BeADataScientist/blob/master/codes/4_4-image/image_pyramid.py" target="_blank" rel="noopener">python实现图像金字塔</a></p>
<h2 id="3-差分金字塔DOG"><a href="#3-差分金字塔DOG" class="headerlink" title="3 差分金字塔DOG"></a>3 差分金字塔DOG</h2><p>DOG（差分金字塔）金字塔是在高斯金字塔的基础上构建起来的，其实生成高斯金字塔的目的就是为了构建DOG差分金字塔。</p>
<p><strong>构建过程</strong></p>
<p>差分金字塔的第1组第1层是由高斯金字塔的第1组第2层减第1组第1层得到的。以此类推，逐组逐层生成每一个差分图像，所有差分图像构成差分金字塔。</p>
<p>概括为差分金字塔的第o组第l层图像是有高斯金字塔的第o组第l+1层减第o组第l层得到的。图示如下</p>
<p><img src="/images/blog/image_dog_result.png" alt="图像金字塔"> </p>
<p>可以看到结果都是黑的，人眼看不到效果。实际计算结果包含了大量信息点。<br>我们对结果进行归一化操作，此时就变成了laplace金字塔了。</p>
<h2 id="4-laplace金字塔"><a href="#4-laplace金字塔" class="headerlink" title="4 laplace金字塔"></a>4 laplace金字塔</h2><p>之前一直没弄清楚，差分金字塔和laplace金字塔之间的关系。直到看到这个<a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/DoG_vs_LoG.pdf" target="_blank" rel="noopener">文档</a> </p>
<p>我们先看差分金字塔的公式定义：</p>
<script type="math/tex; mode=display">
Dog(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) =L(x,y,k\sigma)-L(x,y,\sigma) \\
其中 G(x,y,\sigma)代表了高斯核，G(x,y,\sigma)=\frac{1}{\sqrt{2\pi}}e^{\frac{x^2+y^2}{2\sigma ^2}},而L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)</script><p>缩放之后的LoG表达式为：</p>
<script type="math/tex; mode=display">
LoG(x,y,\sigma)=\sigma ^2\bigtriangledown ^2 L(x,y,\sigma) \\
= \sigma ^2(L_{xx}+L_{yy})</script><p>最终推导结果如下：</p>
<script type="math/tex; mode=display">
(k-1)\sigma ^2LoG \approx =DoG</script><p>可以看到，DoG近似等于将LoG尺度缩放到一个常量$k-1$.</p>
<p>我们来看实际效果，借助opencv的api</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cv2.normalize(im, None, alpha&#x3D;0, beta&#x3D;1, norm_type&#x3D;cv2.NORM_MINMAX, dtype&#x3D;cv2.CV_32F)</span><br></pre></td></tr></table></figure>
<p>得到结果如下，可以看到清晰地特征。</p>
<p><img src="/images/blog/image_dog_norm_result.png" alt="图像金字塔"> </p>
<p>代码位于 <a href="https://github.com/shartoo/BeADataScientist/blob/master/codes/4_4-image/image_pyramid.py" target="_blank" rel="noopener">python实现图像金字塔</a></p>
<p>此特征可以等价理解成Laplace特征。</p>
<p><strong>参考</strong></p>
<p><a href="https://blog.csdn.net/qq_27806947/article/details/80769339" target="_blank" rel="noopener">OpenCV(Python3)_16(图像金字塔)</a></p>
<p><a href="http://www.10tiao.com/html/295/201609/2651988200/3.html" target="_blank" rel="noopener">IO头条 图像金字塔算法</a></p>
<p><a href="https://blog.csdn.net/dcrmg/article/details/52561656" target="_blank" rel="noopener">csdn Sift中尺度空间、高斯金字塔、差分金字塔（DOG金字塔）、图像金字塔</a></p>
<p><a href="https://docs.opencv.org/3.4/d4/d1f/tutorial_pyramids.html" target="_blank" rel="noopener">opencv 官方文档</a></p>
<p><a href="https://www.uio.no/studier/emner/matnat/its/UNIK4690/v16/forelesninger/lecture_2_3_blending.pdf" target="_blank" rel="noopener">图像金字塔的算法构建图示(强烈推荐)</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>merlin中文语音合成概览</title>
    <url>/2019/01/08/merlin_mandarin_summary/</url>
    <content><![CDATA[<p><img src="/images/blog/merlin-mandarin1.png" alt="merlin中文语音合成"><br><img src="/images/blog/merlin-mandarin2.png" alt="merlin中文语音合成"><br><img src="/images/blog/merlin-mandarin3.png" alt="merlin中文语音合成"><br><img src="/images/blog/merlin-mandarin4.png" alt="merlin中文语音合成"><br><img src="/images/blog/merlin-mandarin5.png" alt="merlin中文语音合成"><br><img src="/images/blog/merlin-mandarin6.png" alt="merlin中文语音合成"></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>语音合成：merlin使用概览</title>
    <url>/2019/01/07/merlin_summary/</url>
    <content><![CDATA[<h3 id="0-概览"><a href="#0-概览" class="headerlink" title="0 概览"></a>0 概览</h3><p>本文详细解释Merlin Mandarin_voice下脚本一步一步所做的事。</p>
<h3 id="01-setup"><a href="#01-setup" class="headerlink" title="01_setup"></a>01_setup</h3><p>脚本<code>merlin/egs/mandarin_voice/s1/01_setup.sh</code></p>
<p>主要工作是创建一个目录，做好准备工作。主要创建了如下文件夹:</p>
<ul>
<li>experiments<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">─ mandarin_voice(voice name)</span><br><span class="line">    ├── acoustic_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    ├── duration_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    └── test_synthesis</span><br><span class="line">        ├── gen-lab</span><br><span class="line">        ├── prompt-lab</span><br><span class="line">        ├── test_id_list.scp</span><br><span class="line">        └── wav</span><br></pre></td></tr></table></figure></li>
<li>database<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> feats</span><br><span class="line">│ ├── bap</span><br><span class="line">│ ├── lf0</span><br><span class="line">│ └── mgc</span><br><span class="line">├── labels</span><br><span class="line">│ └── label_phone_align</span><br><span class="line">├── prompt-lab</span><br><span class="line">│ ├── A11_0.lab</span><br><span class="line">│ ├── A11_1.lab</span><br><span class="line">│ ├── A11_2.lab</span><br><span class="line">...</span><br><span class="line">└── wav</span><br><span class="line">    ├── A11_0.wav</span><br><span class="line">    ├── A11_100.wav</span><br><span class="line">    ├── A11_101.wav</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>将一些基本参数写入到<code>conf/global_setting.cfg</code>文件中</p>
<h3 id="02-prepare-lab"><a href="#02-prepare-lab" class="headerlink" title="02_prepare_lab"></a>02_prepare_lab</h3><p>需要两个参数：</p>
<ul>
<li>lab_dir: 第一步中的标注目录 <code>database/labels</code></li>
<li>prompt_lab_dir :第一步中生成的<code>database/prompt-lab</code></li>
</ul>
<h4 id="2-1-准备文件夹"><a href="#2-1-准备文件夹" class="headerlink" title="2.1 准备文件夹"></a>2.1 准备文件夹</h4><ul>
<li>将 <code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件分别复制到<code>experiments/mandarin_voice/duration_model/data</code>（时域模型）和<code>experiments/mandarin_voice/acoustic_model/data</code>（声学模型）下。【用于训练】</li>
<li>将<code>database/prompt-lab</code>下的lab文件复制到<code>experiments/mandarin_voice/test_synthesis</code>下【用于测试（合成）】</li>
</ul>
<h4 id="2-2-生成文件列表"><a href="#2-2-生成文件列表" class="headerlink" title="2.2 生成文件列表"></a>2.2 生成文件列表</h4><ul>
<li>将<code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件列表写入到<code>experiments/mandarin_voice/duration_model/FileIdList&#39;和</code>experiments/mandarin_voice/acoustic_model/FileIdList’。并移除文件后缀【训练集文件列表】</li>
<li>将<code>database/prompt-lab</code>下的lab文件列表写入到<code>experiments/mandarin_voice/test_synthesis/test_id_list.scp</code>文件中，并移除文件后缀【用于合成语音的文本列表】</li>
</ul>
<h3 id="03-prepare-acoustic-feature"><a href="#03-prepare-acoustic-feature" class="headerlink" title="03_prepare_acoustic_feature"></a>03_prepare_acoustic_feature</h3><p>需要两个参数</p>
<ul>
<li><strong>wav_dir</strong>: 使用的是第一步中的<code>database/wav</code>，下面存放的是所有的wav音频文件</li>
<li><strong>feat_dir</strong>:输出文件目录<code>database/feats</code>，是当前脚本输出的特征存放文件目录<h4 id="3-1-使用声码器抽取声学特征"><a href="#3-1-使用声码器抽取声学特征" class="headerlink" title="3.1 使用声码器抽取声学特征"></a>3.1 使用声码器抽取声学特征</h4></li>
</ul>
<p>使用<code>merlin/misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本抽取，注意，其中的声码器可以是<code>WORLD</code>也可以是其他的，比如<code>straight</code>,<code>WORLD_2</code>。其实依然是在python中调用以下脚本：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">world &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;WORLD&quot;)</span><br><span class="line">sptk &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;SPTK-3.9&quot;)</span><br><span class="line">reaper &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;REAPER&quot;)</span><br></pre></td></tr></table></figure><br>生成的特征目录如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sp_dir &#x3D; os.path.join(feat_dir, &#39;sp&#39; )</span><br><span class="line">mgc_dir &#x3D; os.path.join(feat_dir, &#39;mgc&#39;)</span><br><span class="line">ap_dir &#x3D; os.path.join(feat_dir, &#39;ap&#39; )</span><br><span class="line">bap_dir &#x3D; os.path.join(feat_dir, &#39;bap&#39;)</span><br><span class="line">f0_dir &#x3D; os.path.join(feat_dir, &#39;f0&#39; )</span><br><span class="line">lf0_dir &#x3D; os.path.join(feat_dir, &#39;lf0&#39;)</span><br></pre></td></tr></table></figure><br>如果我们使用world作为vocoder的话，会使用<code>misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本，生成步骤其实是：</p>
<ol>
<li>直接从原始wav文件，使用<code>world analysis</code>抽取 <code>sp</code>,<code>bapd</code>特征。<code>straight</code>vocoder 会产生 <code>ap</code>,如果使用reaper会产生<code>f0</code>特征。</li>
<li><code>f0</code>$\rightarrow$ <code>lf0</code>,<code>bapd</code>$\rightarrow$ <code>bap</code>,<code>sp</code>$\rightarrow$ <code>mgc</code></li>
</ol>
<h4 id="3-2-复制特征到声学特征目录下"><a href="#3-2-复制特征到声学特征目录下" class="headerlink" title="3.2 复制特征到声学特征目录下"></a>3.2 复制特征到声学特征目录下</h4><p>将所有<code>feat_dir</code>下的所有文件,包括<code>sp</code>,<code>mgc</code>,<code>ap</code>,<code>bap</code>,<code>f0</code>,<code>lf0</code>复制到<code>experiments/mandarin_voice/acoustic_model/data</code>下。</p>
<h2 id="04-prepare-conf-files"><a href="#04-prepare-conf-files" class="headerlink" title="04_prepare_conf_files"></a>04_prepare_conf_files</h2><p>执行<code>./scripts/prepare_config_files.sh</code></p>
<p><strong>duration相关配置</strong></p>
<ul>
<li><p>先从<code>merlin/misc/recipes/duration_demo.conf</code>复制一份到<code>conf/duration_mandarin_voice.conf</code>，并修改<code>conf/duration_mandarin_voice.conf</code>中的一些目录</p>
<ul>
<li>MerlinDir</li>
<li>WorkDir</li>
<li>TOPLEVEL</li>
<li>FileIdList</li>
</ul>
</li>
<li><p>修改Label相关的配置项【Labels】</p>
<ul>
<li>silence_pattern：修改为 <code>[&#39;*-sil+*&#39;]</code></li>
<li>label_type:<code>state_align</code> 或 <code>phone_align</code>，修改之后为<code>phone_align</code></li>
<li>label_align: 即配置音素对齐文件的目录<code>/experiments/mandarin_voice/duration_model/data/label_phone_align</code></li>
<li>question_file_name:<code>/misc/questions/questions-mandarin.hed</code>问题集</li>
</ul>
</li>
<li><p>修改输出配置【Outputs】，label_type有<code>state_align</code> 或 <code>phone_align</code>，如果是<code>state_align</code>会在【outputs】处指定<code>dur=5</code>,如果是<code>phone_align</code>则指定<code>dur=1</code></p>
</li>
<li>神经网络的架构配置，如果当前声音文件是<code>demo</code>则修改<code>hidden_layer_size</code> 【architechture】</li>
<li>修改训练、验证、测试数据数量。【data】<ul>
<li>train_file_number: 200</li>
<li>valid_file_number: 25</li>
<li>test_file_number: 25</li>
</ul>
</li>
</ul>
<p><strong>acoustic相关配置</strong></p>
<ul>
<li>复制文件<code>conf/acoustic_mandarin_voice.conf</code>，修改变量，label配置都和duration相关配置一样。</li>
<li>修改输出配置【outputs】<ul>
<li>mgc</li>
<li>dmgc</li>
<li>bap</li>
<li>dbap</li>
<li>lf0</li>
<li>dlf0</li>
</ul>
</li>
<li>波形文件设置【waveform】<ul>
<li>framelength</li>
<li>minimum_phase_order</li>
<li>fw_alpha</li>
</ul>
</li>
<li>其他的【architechture】和【data】都和duration相关配置一样。</li>
</ul>
<p>执行<code>./scripts/prepare_config_files_for_synthesis.sh</code>配置测试（或合成）语音相关的参数。基本和上面的<code>./scripts/prepare_config_files.sh</code>一样，需要配置<code>duration</code>和<code>ascoustic</code>参数。新增了【Processes】</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DurationModel: True</span><br><span class="line">GenTestList: True</span><br><span class="line"># sub-processes</span><br><span class="line">NORMLAB: True</span><br><span class="line">MAKEDUR: False</span><br><span class="line">MAKECMP: False</span><br><span class="line">NORMCMP: False</span><br><span class="line">TRAINDNN: False</span><br><span class="line">DNNGEN: True</span><br><span class="line">CALMCD: False</span><br></pre></td></tr></table></figure>
<h3 id="05-train-duration-model"><a href="#05-train-duration-model" class="headerlink" title="05_train_duration_model"></a>05_train_duration_model</h3><p>实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/duration_mandarin_voice.conf</code></p>
<p>其中<code>./scripts/submit.sh</code>是theano相关参数的配置。</p>
<h3 id="06-train-acoustic-model"><a href="#06-train-acoustic-model" class="headerlink" title="06_train_acoustic_model"></a>06_train_acoustic_model</h3><p>训练声学模型，实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/acoustic_mandarin_voice.conf</code></p>
<h3 id="07-run-merlin"><a href="#07-run-merlin" class="headerlink" title="07_run_merlin"></a>07_run_merlin</h3><p>需要两个参数</p>
<ul>
<li>test_dur_config_file: 语音合成的时域配置文件</li>
<li>test_synth_config_file:语音合成的</li>
</ul>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习目标检测网络汇总对比</title>
    <url>/2019/01/03/obj_detect_summary/</url>
    <content><![CDATA[<p>参考 ：<a href="https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</a></p>
<p><strong>概览图</strong></p>
<p><img src="/images/blog/obj_detect_summary-1.png" alt="目标检测概览"></p>
<h3 id="0-说明"><a href="#0-说明" class="headerlink" title="0 说明"></a>0 说明</h3><p>关于目标检测的好坏，很难有一个统一明确的比较。我们一般都是在速度和准确率之间妥协，除此之外，我们还需要注意，以下因素会影响性能</p>
<ul>
<li>特征抽取网络(<code>VGG16</code>,<code>ResNet</code>,<code>Inception</code>,<code>MobileNet</code>)</li>
<li>特征抽取网络的输出strides</li>
<li>输入图像的分辨率</li>
<li>匹配策略和IOU阈值（在预测时计算损失的方法）</li>
<li>NMS IOU阈值</li>
<li>难样本挖掘比率(即正负样本anchor比例)</li>
<li>候选或者预测的数目</li>
<li>bounding box的编码</li>
<li>数据增强</li>
<li>训练集</li>
<li>训练或测试时是否使用多尺度图像(图像裁剪)</li>
<li>哪个特征map层用来做目标检测</li>
<li>定位损失函数</li>
<li>所使用的深度学习平台</li>
<li>训练配置包括batch_size,输入图像的resize，学习率，以及学习率的递减</li>
</ul>
<h2 id="1性能评测结果"><a href="#1性能评测结果" class="headerlink" title="1性能评测结果"></a>1性能评测结果</h2><h3 id="1-1-FasterRCNN"><a href="#1-1-FasterRCNN" class="headerlink" title="1.1 FasterRCNN"></a>1.1 FasterRCNN</h3><p>在PASCAL VOC 2012测试集上的表现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>生成的区域候选数目</th>
<th>测试数据</th>
<th>mAP(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>SelectiveSearch</td>
<td>2000</td>
<td>voc2012</td>
<td>65.7</td>
</tr>
<tr>
<td>SelectiveSearch</td>
<td>2000</td>
<td>voc2007+voc2012</td>
<td>68.4</td>
</tr>
<tr>
<td>RPN+VGG,shared</td>
<td>300</td>
<td>voc2012</td>
<td>67.0</td>
</tr>
<tr>
<td>RPN+VGG,shared</td>
<td>300</td>
<td>voc2007+voc2012</td>
<td>70.4</td>
</tr>
<tr>
<td>RPN+VGG,shared</td>
<td>300</td>
<td>voc2007+voc2012+coco</td>
<td>75.9</td>
</tr>
</tbody>
</table>
</div>
<p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-2.png" alt="目标检测概览"></p>
<h3 id="1-2-R-FCN"><a href="#1-2-R-FCN" class="headerlink" title="1.2 R-FCN"></a>1.2 R-FCN</h3><p><strong>voc数据集</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>训练集</th>
<th>mAP(%)</th>
<th>测试时间(sec/image)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FasterRCNN</td>
<td>voc 2007+voc2012</td>
<td>73.8</td>
<td>0.42</td>
</tr>
<tr>
<td>FasterRCNN++</td>
<td>voc 2007+voc2012+coco</td>
<td>83.8</td>
<td>3.36</td>
</tr>
<tr>
<td>F-FCN多尺度训练</td>
<td>voc2007+voc2012</td>
<td>77.6</td>
<td>0.17</td>
</tr>
<tr>
<td>F-FCN多尺度训练</td>
<td>voc2007+voc2012+coco</td>
<td>82.0</td>
<td>0.17</td>
</tr>
</tbody>
</table>
</div>
<p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-3.png" alt="目标检测概览"></p>
<h3 id="1-3-SSD"><a href="#1-3-SSD" class="headerlink" title="1.3 SSD"></a>1.3 SSD</h3><p><strong>voc数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-4.png" alt="目标检测概览"></p>
<p>性能</p>
<p><img src="/images/blog/obj_detect_summary-5.png" alt="目标检测概览"></p>
<p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-6.png" alt="目标检测概览"></p>
<h3 id="1-4-YOLO"><a href="#1-4-YOLO" class="headerlink" title="1.4 YOLO"></a>1.4 YOLO</h3><p><strong>voc2007</strong></p>
<p><img src="/images/blog/obj_detect_summary-7.png" alt="目标检测概览"></p>
<p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-8.png" alt="目标检测概览"></p>
<h3 id="1-5-yolov3"><a href="#1-5-yolov3" class="headerlink" title="1.5 yolov3"></a>1.5 yolov3</h3><p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-9.png" alt="目标检测概览"></p>
<p><strong>性能</strong></p>
<p><img src="/images/blog/obj_detect_summary-10.png" alt="目标检测概览"></p>
<h3 id="1-6-FPN"><a href="#1-6-FPN" class="headerlink" title="1.6 FPN"></a>1.6 FPN</h3><p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-11.png" alt="目标检测概览"></p>
<h3 id="1-7-RetinaNet"><a href="#1-7-RetinaNet" class="headerlink" title="1.7 RetinaNet"></a>1.7 RetinaNet</h3><p><strong>coco数据集</strong></p>
<p><img src="/images/blog/obj_detect_summary-12.png" alt="目标检测概览"></p>
<p><strong>性能</strong></p>
<p><img src="/images/blog/obj_detect_summary-13.png" alt="目标检测概览"></p>
<h2 id="2-论文结果比较"><a href="#2-论文结果比较" class="headerlink" title="2 论文结果比较"></a>2 论文结果比较</h2><p>下图是用VOC2007+voc2012的数据集训练的，mAP的计算方式是VOC2012。</p>
<ul>
<li>对于SSD，输入图像尺寸有300x300和512x512</li>
<li>对于yolo，输入图像尺寸有288x288,416x416,544x544<br>更高的分辨率可以得到更好的准确率，但是速度会相应下降。</li>
</ul>
<p><img src="/images/blog/obj_detect_summary-13.png" alt="目标检测概览"></p>
<p>输入图像的分辨率和特征抽取对速度有极大影响。下面是最高和最低的FPS，当然下图可能在使用不同mAP时结果有较大出入</p>
<p><img src="/images/blog/obj_detect_summary-14.png" alt="目标检测概览"></p>
<p><strong>coco数据集的表现</strong></p>
<p><img src="/images/blog/obj_detect_summary-15.png" alt="目标检测概览"></p>
<p>可以看到，FPN和FasterRCNN有很高的准确率，但是RetinaNet最高。取得最高准确率的RetinaNet是借助了</p>
<ul>
<li>金字塔特征</li>
<li>特征抽取器的复杂</li>
<li>Focal Loss</li>
</ul>
<h2 id="3-google的研究结果"><a href="#3-google的研究结果" class="headerlink" title="3 google的研究结果"></a>3 google的研究结果</h2><h3 id="3-1-特征抽取器"><a href="#3-1-特征抽取器" class="headerlink" title="3.1 特征抽取器"></a>3.1 特征抽取器</h3><p>研究了特征抽取器对准确率的影响，其中FasterRCNN和R-FCN可以利用一个更好的特征抽取器，但是对SSD效果提升程度不大。</p>
<p><img src="/images/blog/obj_detect_summary-16.png" alt="目标检测概览"></p>
<p>上图中x轴 是每个特征抽取器在分类上的top1的准确率。</p>
<h3 id="3-2-物体尺寸"><a href="#3-2-物体尺寸" class="headerlink" title="3.2 物体尺寸"></a>3.2 物体尺寸</h3><p>对于大目标SSD即使使用很简单的抽取器也可以表现很好，如果使用更好的抽取其，SSD甚至可以达到其他分类器的准确率。但是<strong>SSD在小目标上表现很差</strong></p>
<p><img src="/images/blog/obj_detect_summary-17.png" alt="目标检测概览"></p>
<h3 id="3-3-输入图像的分辨率"><a href="#3-3-输入图像的分辨率" class="headerlink" title="3.3 输入图像的分辨率"></a>3.3 输入图像的分辨率</h3><p>更高的分辨率有利于提升小目标的检测准确率，对大目标也有帮助。对分辨率在长宽维度上以因子2递减，准确率平均降低15.88%，但是对应的inference时间也会平均以因子 27.4%下降。</p>
<p><img src="/images/blog/obj_detect_summary-18.png" alt="目标检测概览"></p>
<h3 id="3-4-区域候选的数目"><a href="#3-4-区域候选的数目" class="headerlink" title="3.4 区域候选的数目"></a>3.4 区域候选的数目</h3><p>区域候选的数目可以极大地影响FasterRCNN(FRCNN)，而对准确率不会有太大降低。例如，Inception ResNet,FasterRCNN可以提升三倍速度，如果使用50个区域候选而不是300个的话，对应的准确率只降低了4%。但是R-FCN对每个ROI只有少得多的操作需要做，所以减少区域候选，对它的速度的提升并不显著。</p>
<p><img src="/images/blog/obj_detect_summary-19.png" alt="目标检测概览"></p>
<h3 id="3-5-GPU时间"><a href="#3-5-GPU时间" class="headerlink" title="3.5 GPU时间"></a>3.5 GPU时间</h3><p>下面是不同模型使用不同特征抽取器的GPU时间</p>
<p><img src="/images/blog/obj_detect_summary-20.png" alt="目标检测概览"></p>
<p>大部分论文使用FLOPS(浮点运算)来衡量模型复杂度，但是这个没法反映准确的速度。模型密度(稀疏和稠密模型)影响的是所耗费的时间。讽刺的是，欠稠密模型通常平均需要更长的时间来完成一个浮点运算。</p>
<h3 id="3-6-内存"><a href="#3-6-内存" class="headerlink" title="3.6 内存"></a>3.6 内存</h3><p>MobileNet有最少的参数，它需要不到1GB的内存。</p>
<p><img src="/images/blog/obj_detect_summary-21.png" alt="目标检测概览"></p>
<h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h2><ul>
<li>R-FCN和SSD模型平均速度更快，但是如果不考虑速度，它们准确率不如FasterRCNN</li>
<li>FasterRCNN每张图需要至少100ms</li>
<li>只使用低分辨率的feature map会极大地损伤检测准确率</li>
<li>输入分辨率极大的影响准确率。减少一半的图像尺寸(长和宽都减少一半)会导致准确率下降15.88%，对应的inference时间减少27.4%</li>
<li>特征抽取器的选取对FasterRCNN和R-FCN有较大影响，但是对SSD没太大影响。</li>
<li>后续处理，包括NMS(只能在CPU上运行)，对最快的模型耗费了最多的时间，大概有40ms，这也导致了其速度降到25FPS</li>
<li>如果mAP的计算只使用了单一IoU，那么使用mAP@IoU=0.75</li>
<li>在使用InceptionResNet网络作为特征抽取器时，stride=8比stride=16会将mAP提升5%，但是运行时间增加了63%。</li>
</ul>
<p><strong>最准确的模型</strong></p>
<ul>
<li>最准确的单一模型，使用FasterRCNN，使用InceptionResNet，和300个候选。一张图片的检测需要1秒钟。</li>
<li>最准确的模型是一个多次裁剪inference的模型集合。它使用平均准确率向量来选取5个最不同的模型</li>
</ul>
<p><strong>最快的模型</strong></p>
<ul>
<li>使用mobilenet的SSD是在最快速度和最佳准确率之间一个最好的均衡</li>
<li>SSD表现卓越，但是对小目标较差</li>
<li>对于大目标，SSD可以达到与FasterRCNN和R-FCN一样的准确率，但是用的是更小更轻的特征抽取器。</li>
</ul>
<p><strong>速度与准确率之间的均衡</strong></p>
<ul>
<li>FasterRCNN如果只使用50个区域候选的话，它可以达到与R-FCN和SSD一样的速度，准确率为32mAP</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>Mobilenet以及其变种网络高效的原因</title>
    <url>/2019/01/02/efficient_tinynet/</url>
    <content><![CDATA[<p>本文翻译自： <a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="noopener">https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d</a></p>
<h2 id="1-高效网络中所使用的Block模块"><a href="#1-高效网络中所使用的Block模块" class="headerlink" title="1 高效网络中所使用的Block模块"></a>1 高效网络中所使用的Block模块</h2><p>假设我们有如下的卷积过程：</p>
<p><img src="/images/blog/effient-smallnet-1.png" alt="iou1"></p>
<p>假设$H\times W$代表了上一层的输出<strong>空间尺寸</strong>，即当前卷积的特征输入尺寸，其中的N代表了输入通道，$K\times K$代表了卷积核的尺寸,M代表了输出的特征通道数。这样一来，标准卷积的计算量是$H\times W\times N\times K\times K\times M$。</p>
<p>此处需要注意的是标准卷积的计算消耗与下面三点是成比例的：</p>
<ul>
<li>前一层的输出特征的空间尺寸$H\times W$，即当前卷积的输入尺寸。</li>
<li>卷积核尺寸$K\times K$</li>
<li>输入特征通道数N、输出的特征通道数M。</li>
</ul>
<p>当卷积同时在空间(宽度)以及通道(深度)上做卷积操作时，上述计算消耗都是必须的。CNN可以被以下的卷积分解方法来加速卷积。</p>
<h3 id="1-2-卷积"><a href="#1-2-卷积" class="headerlink" title="1.2 卷积"></a>1.2 卷积</h3><p>我们在输入输出之间连线来可视化输入输出之间的依赖，连线的数量大概地展示卷积分别在空间和通道域上的计算消耗。</p>
<p><img src="/images/blog/effient-smallnet-2.png" alt="iou1"></p>
<p>例如，常见的$3\times3$卷积可以向上图展示所示。我们可以看到<strong>输入和输出在空间尺度上是局部连接的(上图左)，而在通道尺度上是全连接(上图右)</strong>。接下来我们看$1\times1$卷积，也即点卷积，用来改变通道数，如下图。其卷积计算消耗是$H\times W\times N\times M$，因为卷积核是$1\times1$，导致计算量只有$3\times3$的1/9。此卷积用来混杂通道之间的信息。</p>
<p><img src="/images/blog/effient-smallnet-3.png" alt="iou1"></p>
<h3 id="1-2-1-1x1卷积的效果"><a href="#1-2-1-1x1卷积的效果" class="headerlink" title="1.2.1 1x1卷积的效果"></a>1.2.1 1x1卷积的效果</h3><ul>
<li>先看看$3\times 3$卷积效果</li>
</ul>
<p><img src="/images/blog/small_effient_3x3.gif" alt="卷积1"></p>
<ul>
<li>再看$1\times 1$卷积效果<br><img src="/images/blog/small_effient_1x1.gif" alt="卷积2"></li>
</ul>
<h3 id="1-3-分组卷积"><a href="#1-3-分组卷积" class="headerlink" title="1.3 分组卷积"></a>1.3 分组卷积</h3><p>分组卷积为卷积的变种，特点是<strong>输入特征的通道数被分组，并且每个分组独立地进行卷积操作</strong>（标准卷积相当于只有一个分组，所有通道之间信息会互通）。</p>
<p>以G代表分组数，分组卷积的计算消耗是$H\times W\times N\times K\times K\times M/G$,其计算消耗只有标准卷积的$\frac{1}{G}$。下图是一个示例，$3\times3$卷积，分组$G=2$。我们可以看到通道域的连接数变小了，也就是计算消耗变小。</p>
<p><img src="/images/blog/effient-smallnet-4.png" alt="iou1"></p>
<p>进一步，分组数变为$G=3$时的连接示意图，更加稀疏。</p>
<p><img src="/images/blog/effient-smallnet-5.png" alt="iou1"></p>
<p>卷积$1\times1$分组数为$G=2$时的示意图如下，卷积$1\times 1$也可以被分组，即ShuffleNet所使用的卷积。</p>
<p><img src="/images/blog/effient-smallnet-6.png" alt="iou1"></p>
<p>进一步的，分组变成$G=3$时的示例</p>
<p><img src="/images/blog/effient-smallnet-7.png" alt="iou1"></p>
<h3 id="1-4-depthwise卷积"><a href="#1-4-depthwise卷积" class="headerlink" title="1.4 depthwise卷积"></a>1.4 depthwise卷积</h3><p>depthwise卷积其实是在<strong>输入通道上独立地做卷积操作</strong>，也可以认为是分组卷积的一种极端情况，<strong>即输入输出通道数目相同，分组G等于通道数</strong></p>
<p><img src="/images/blog/effient-smallnet-8.png" alt="iou1"></p>
<p>depthwise卷积通过略去通道域上的卷积极大的减小了计算消耗。</p>
<h3 id="1-5-通道混排"><a href="#1-5-通道混排" class="headerlink" title="1.5 通道混排"></a>1.5 通道混排</h3><p>通道混排是一个改变了通道的顺序操作(层)，被用在ShuffleNet中。可以使用tensor reshape和transpose操作实现。</p>
<p>假设$GN’(=N)$表示输入通道数目，输入通道维度首先reshape成$(G,N’)$，然后将$(G,N’)$转置（transpose）到$(N’,G)$最终faltten到与输入维度一致。此处G代表了分组卷积的分组数，它同样被用到ShuffleNet中。</p>
<p>通道混排的计算消耗<strong>无法用乘-加(multiply-add)操作来定义</strong>。如下图，通道混排，分组G=2，没有执行卷积，只是改变了通道的顺序。</p>
<p><img src="/images/blog/effient-smallnet-9.png" alt="iou1"></p>
<p>G=3的通道混排</p>
<p><img src="/images/blog/effient-smallnet-10.png" alt="iou1"></p>
<h2 id="2-高效网络"><a href="#2-高效网络" class="headerlink" title="2 高效网络"></a>2 高效网络</h2><h3 id="2-1-ResNet-Bottleneck-版本"><a href="#2-1-ResNet-Bottleneck-版本" class="headerlink" title="2.1 ResNet(Bottleneck 版本)"></a>2.1 ResNet(Bottleneck 版本)</h3><p>ResNet中的bottleneck架构的残差单元如下：</p>
<p><img src="/images/blog/effient-smallnet-11.png" alt="iou1"></p>
<p>可以看到残差单元都是由 $1\times1$和$3\times3$组成的。</p>
<ul>
<li>第一个$1\times1$卷积减小了输入通道的维度，减小了接下来的相对耗费计算资源的$3\times3$卷积</li>
<li>最后一个$1\times1$卷积恢复了输出通道的维度</li>
</ul>
<h3 id="2-2-ResNeXt"><a href="#2-2-ResNeXt" class="headerlink" title="2.2 ResNeXt"></a>2.2 ResNeXt</h3><p>ResNeXt可以看做是ResNet的特殊版本，其$3\times3$卷积部分被替换为分组的$3\times3$卷积。使用分组卷积之后，$1\times1$所造成的通道缩减率变得比ResNet温和一些，这也导致其比ResNet在同等计算消耗情况下更好的准确率。</p>
<p><img src="/images/blog/effient-smallnet-12.png" alt="iou1"></p>
<h3 id="2-3-MobileNet（分离卷积）"><a href="#2-3-MobileNet（分离卷积）" class="headerlink" title="2.3 MobileNet（分离卷积）"></a>2.3 MobileNet（分离卷积）</h3><p>MobileNet是一个分离卷积的堆，由depthwise卷积和$1\times1$卷积组成。</p>
<p><img src="/images/blog/effient-smallnet-13.png" alt="iou1"></p>
<p>分离卷积在空间尺度和通道域执行卷积。卷积因子显著地将计算消耗从$H\times W\times N\times K\times K\times M$减小到$H\times W\times N\times K\times K$（depthwise）+$H\times W\times N\times M$（$1\times1$卷积），即总共计算消耗为$(H\times W\times N)\times(K^2+M)$.由于$M&gt;&gt;K(例如 K=3并且M\ge 32)$，计算量缩减到1/8至1/9.</p>
<p>最重要的点，此时的计算瓶颈在$1\times1$卷积。</p>
<h3 id="2-4-ShuffleNet"><a href="#2-4-ShuffleNet" class="headerlink" title="2.4 ShuffleNet"></a>2.4 ShuffleNet</h3><p>ShuffleNet的动机是上面提到的计算瓶颈变成了$1\times1$卷积。但是$1\times1$卷积已经高效了，似乎已经没有优化空间，但是此时可以<strong>用分组的$1\times1$卷积</strong>。</p>
<p><img src="/images/blog/effient-smallnet-14.png" alt="iou1"></p>
<p>上图展示了ShuffleNet的模块，此处的重要block是通道混排层，它会混排分组卷积的不同组之间的通道顺序。如果没有通道混排，分组卷积之间的输出都不会发生联系，这会导致准确率的衰弱。</p>
<h3 id="2-5-MobileNetv2"><a href="#2-5-MobileNetv2" class="headerlink" title="2.5 MobileNetv2"></a>2.5 MobileNetv2</h3><p>MobileNetv2使用了一个类似ResNet的残差单元的网络架构，修改版本的残差单元中$3\times3$卷积被depthwise卷积替代。</p>
<p><img src="/images/blog/effient-smallnet-15.png" alt="iou1"></p>
<p>从上图可以看到，与标准bottleneck架构相比，第一个$1\times1$卷积增加了通道维度，然后执行depthwise卷积，最后再减少通道维度。通过如下图这样重排building block，我们可以看到这个架构是如何起作用的（此重排序过程没有改变网络架构，因为mobilenetv2架构就是此模块的堆叠）。</p>
<p><img src="/images/blog/effient-smallnet-16.png" alt="iou1"></p>
<p>也即此模块被当做修改版本的分离卷积，其中的分离卷积中的单个$1\times1$卷积被扩充到2个$1\times1$卷积。将T作为通道方向上的扩充因子，两个$1\times1$卷积的计算消耗是 $2\times W\times H\times N^2/T$，而分离卷积中的$1\times1$卷积的计算消耗是$H\times W\times T^2$</p>
<h3 id="2-6-FD-Mobilenet"><a href="#2-6-FD-Mobilenet" class="headerlink" title="2.6 FD-Mobilenet"></a>2.6 FD-Mobilenet</h3><p> Fast-Downsampling MobileNet (FD-MobileNet),此网络架构与MobileNet相比其下采样在网络的更早的层就开始了，这个简单的trick可以减小计算消耗。原因在于传统的下采样策略和可分离卷积的计算消耗。</p>
<p>从VGGnet开始大部分的网络都 采取相同的下采样策略：执行下采样然后在接下来的网络层将通道数翻倍。对于标准卷积，其计算消耗并没有并没有在下采样之后减小，因为是由公式$H\times W\times N\times K^2\times M$定义的。然而，对于分离卷积，其计算消耗在下采样之后变小了，从$H\times W\times N\times(K^2+M)$到$H/2\times W/2\times 2\times N\times(K^2+2M)=H\times W\times N(K^2/2+M)$。当M不是很大（比如网络的前面的层）时是有一些比较优势的。</p>
<p><img src="/images/blog/effient-smallnet-17.png" alt="iou1"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>卷积,深度分离卷积,空间分离卷积</title>
    <url>/2019/01/01/conv_sepatiable/</url>
    <content><![CDATA[<h2 id="1-常规卷积"><a href="#1-常规卷积" class="headerlink" title="1 常规卷积"></a>1 常规卷积</h2><p>不知道常规卷积计算的，可以去<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">各种卷积的在线演示</a>看看。假设我们有个尺寸为$12\times 12\times 3$的图像，其中的3位RGB三通道。需要执行一个$5\times 5$的卷积，$stride=1,padding=valid$，所以卷积之后的feature map尺寸为$8\times 8(12-5+1=8)$。</p>
<p>由于图像是三通道，所以我们的卷积核也必须是三通道的。所以每次卷积核在图像上移动一次的时候的计算的不是简单的$5\times 5=25$，实际上是$5\times 5\times 3=75$次乘法操作。即，实际是每次对25个像素矩阵做乘法计算，然后输出一个数值。经过$5\times 5\times 5\times 3$的卷积核之后，$12\times 12\times 3$的图像变成了$$8\times 8\times 1$的特征图。</p>
<p><img src="/images/blog/sepatiable_cnn_1.png" alt="cnn"></p>
<p>如果想增加输出特征图的通道数，比如说增加到256，使用256个卷积核，然后把所有结果堆叠起来即可。</p>
<p><img src="/images/blog/sepatiable_cnn_2.png" alt="cnn"></p>
<p>很显然，这并非矩阵乘法（不是用一整张图与卷积核相乘），而是每次单独地与图像的一部分相乘。</p>
<h2 id="2-深度分离卷积"><a href="#2-深度分离卷积" class="headerlink" title="2 深度分离卷积"></a>2 深度分离卷积</h2><p>深度分离卷积分为两部分</p>
<ul>
<li>深度卷积</li>
<li>逐点卷积</li>
</ul>
<h3 id="2-1-逐通道卷积"><a href="#2-1-逐通道卷积" class="headerlink" title="2.1 逐通道卷积"></a>2.1 逐通道卷积</h3><p>假设图像依然是$12\times 12\times 3$，这次使用的是3个$5\times 5\times 1$的卷积。</p>
<p><img src="/images/blog/sepatiable_cnn_3.png" alt="cnn"></p>
<p>每个$5\times 5\times 1$卷积迭代图像的<strong>一个通道</strong>，即每次都是25个像素的点乘，然后输出一个$8\times 8\times 1$的图像</p>
<h3 id="2-2-逐点卷积"><a href="#2-2-逐点卷积" class="headerlink" title="2.2 逐点卷积"></a>2.2 逐点卷积</h3><p>前面，我们把$12\times 12\times 3$的图像卷积变成了$8\times 8\times 3$的图像，现在我们需要增加每个图像的通道数。</p>
<p>逐点卷积这个叫法源于它使用的是$1\times 1$的卷积核，你可以看做它迭代的计算图像上每个像素点。它有与输入图像同样多的通道数，当前示例中的通道数为3。因此，在$8\times 8\times 3$的图像上迭代$1\times 1\times 3$，可以得到一个$8\times 8\times 1$的特征图像。</p>
<p><img src="/images/blog/sepatiable_cnn_4.png" alt="cnn"></p>
<p>我们也可以使用256个$1\times 1\times 3$的卷积核，每个卷积之后输出输出一个$8\times 8\times 1$图像，堆叠起来就有$8\times 8\times 256$的特征图</p>
<p><img src="/images/blog/sepatiable_cnn_5.png" alt="cnn"></p>
<p>我们将一个卷积操作分离成了逐通道卷积和逐点卷积。更直观的说明：</p>
<ul>
<li>原始的卷积的计算步骤： $12\times 12\times 3—5\times 5\times 3\times 256\rightarrow 12\times 12\times 256$</li>
<li>深度分离卷积计算步骤: $12\times 12\times 3—5\times 5\times 1\times 1\rightarrow 1\times 1\times 3\times 256\rightarrow 12\times 12\times 256$</li>
</ul>
<h3 id="2-3-深度分离卷积的意义是什么呢"><a href="#2-3-深度分离卷积的意义是什么呢" class="headerlink" title="2.3 深度分离卷积的意义是什么呢"></a>2.3 深度分离卷积的意义是什么呢</h3><p>主要是减少计算量，加快计算过程。</p>
<ul>
<li>原始的卷积的计算过程。256个$5\times 5\times 3$的卷积，移动$8\times 8$次。总计算量是 $256\times 3\times 5\times 5\times 8\times 8=1228800$次乘法操作</li>
<li>分离卷积之后。使用3个$5\times 5\times 1$的卷积，移动$8\times 8$次，是$3\times 5\times 5\times 8\times 8=4800$次乘法。在逐像素卷积步骤，有256个$1\times 1$移动$8\times 8$次，总共$256\times 1\times 1\times 3\times 8\times 8=49512$次乘法。加起来总共$53952$次乘法。</li>
</ul>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>主要区别是什么？<strong>常规卷积中，我们对图像进行了245次转换，每次转换使用$5\times 5\times 3 \times 8\times 8=4800$次乘法。在分离卷积中，我们真正转换操作只进行在逐通道卷积上了一次，然后仅仅将其拉长到256个通道上。此时已经没有对图像做转换。</strong></p>
<h2 id="4-空间分离卷积"><a href="#4-空间分离卷积" class="headerlink" title="4 空间分离卷积"></a>4 空间分离卷积</h2><p>空间分离卷积的思想很简单，<strong>就是把一个二维卷积分解成2个一维卷积</strong>，比如说一个$3\times 3$的卷积分离成一个$3\times 1$和一个$1\times 3$的卷积。如下</p>
<p><img src="/images/blog/sepatiable_cnn_6.png" alt="cnn"></p>
<p>对应在图像上的计算步骤，与常规卷积相比多了一个中间图像</p>
<p><img src="/images/blog/sepatiable_cnn_7.png" alt="cnn"></p>
<p>最有名的空间分离卷积是Sobel卷积算子，</p>
<p><img src="/images/blog/sepatiable_cnn_8.png" alt="cnn"></p>
<p>空间分离卷积的局限性在于，不是所有的卷积都可以这么分解。影响了其普适性。</p>
<ul>
<li><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728" target="_blank" rel="noopener">A Basic Introduction to Separable Convolutions</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>图像处理：opencv的目标追踪方法总结</title>
    <url>/2018/12/06/opencv_obj_track/</url>
    <content><![CDATA[<p>参考自： <a href="https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/" target="_blank" rel="noopener">https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/</a> 和 <a href="https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/</a></p>
<h2 id="什么是目标追踪"><a href="#什么是目标追踪" class="headerlink" title="什么是目标追踪"></a>什么是目标追踪</h2><p>在视频后续帧中定位一个物体，称为追踪。虽然定义简单，但是目标追踪是一个相对广义的定义，比如以下问题 也属于目标追踪问题：</p>
<ol>
<li><strong>稠密光流</strong>：此类算法用来评估一个视频帧中的<strong>每个像素的运动向量</strong></li>
<li><strong>稀疏光流</strong>：此类算法，像Kanade-Lucas-Tomashi(KLT)特征追踪，追踪一张图片中<strong>几个特征点</strong>的位置</li>
<li><strong>Kalman Filtering</strong>：一个非常出名的<strong>信号处理算法</strong>基于先前的运动信息用来预测运动目标的位置。早期用于导弹的导航</li>
<li><strong>MeanShift和Camshift</strong>：这些算法是用来<strong>定位密度函数的最大值</strong>，也用于追踪</li>
<li><strong>单一目标追踪</strong>：此类追踪器中，第一帧中的用矩形标识目标的位置。然后在接下来的帧中用追踪算法。日常生活中，此类追踪器用于与目标检测混合使用。</li>
<li><strong>多目标追踪查找算法</strong>：如果我们有一个非常快的目标检测器，在每一帧中检测多个目标，然后运行一个追踪查找算法，来识别当前帧中某个矩形对应下一帧中的某个矩形。</li>
</ol>
<h2 id="追踪-VS-检测"><a href="#追踪-VS-检测" class="headerlink" title="追踪 VS 检测"></a>追踪 VS 检测</h2><p>如果你使用过opencv 的人脸检测算法，你就知道算法可以实时，并且很准确的检测到每一帧中的人脸。那么，为什么首先要追踪呢？我们首先考虑几个问题：</p>
<ol>
<li><p><strong>跟踪比检测更快</strong>：通常<strong>跟踪算法比检测算法更快</strong>。原因很简单，当你跟踪前一帧中的某个物体时，你已经知道了此物体的外观信息。同时你也知道前一帧的位置，以及运行的速度和方向。因而，在下一帧你可以用所有的信息来预测下一帧中物体的位置，以及在一个很小范围内搜索即可得到目标的位置。好的追踪算法会利用所有已知信息来追踪点，但是检测算法每次都要重头开始。<br>所以，通常，如果我们在第n帧开始检测，那么我们需要在第n-1帧开始跟踪。那么为什么不简单地第一帧开始检测，并从后续所有帧开始跟踪。因为跟踪会利用其已知信息，但是也可能会丢失目标，因为目标可能被障碍物遮挡，甚至于目标移动速度太快，算法跟不上。通常，跟踪算法会累计误差，而且bbox 会慢慢偏离目标。为了修复这些问题，需要不断运行检测算法。检测算法用大量样本训练之后，更清楚目标类别的大体特征。另一方面，跟踪算法更清楚它所跟踪的类别中某一个特定实例。</p>
</li>
<li><p><strong>检测失败的话，跟踪可以帮忙</strong>：如果你在视频中检测人脸，然后人脸被某个物体遮挡了，人脸检测算法大概率会失败。好的跟踪算法，另一方面可以处理一定程度的遮挡</p>
</li>
<li><p><strong>跟踪会保存实体</strong>：目标检测算法的输出是包含物体的一个矩形的数组，。但是没有此物体的个体信息，</p>
</li>
</ol>
<h2 id="Opencv-3-的跟踪API"><a href="#Opencv-3-的跟踪API" class="headerlink" title="Opencv 3 的跟踪API"></a>Opencv 3 的跟踪API</h2><p>opencv实现了7中跟踪算法，但是3.4.1以及以上版本才有完整的7种。<code>BOOSTING</code>, <code>MIL</code>, <code>KCF</code>, <code>TLD</code>, <code>MEDIANFLOW</code>, <code>GOTURN</code>, <code>MOSSE</code>,<code>CSRT</code>。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>C++代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;opencv2&#x2F;opencv.hpp&gt;</span><br><span class="line">#include &lt;opencv2&#x2F;tracking.hpp&gt;</span><br><span class="line">#include &lt;opencv2&#x2F;core&#x2F;ocl.hpp&gt;</span><br><span class="line"> </span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace std;</span><br><span class="line"> </span><br><span class="line">&#x2F;&#x2F; Convert to string</span><br><span class="line">#define SSTR( x ) static_cast&lt; std::ostringstream &amp; &gt;( \</span><br><span class="line">( std::ostringstream() &lt;&lt; std::dec &lt;&lt; x ) ).str()</span><br><span class="line"> </span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; List of tracker types in OpenCV 3.4.1</span><br><span class="line">    string trackerTypes[8] &#x3D; &#123;&quot;BOOSTING&quot;, &quot;MIL&quot;, &quot;KCF&quot;, &quot;TLD&quot;,&quot;MEDIANFLOW&quot;, &quot;GOTURN&quot;, &quot;MOSSE&quot;, &quot;CSRT&quot;&#125;;</span><br><span class="line">    &#x2F;&#x2F; vector &lt;string&gt; trackerTypes(types, std::end(types));</span><br><span class="line"> </span><br><span class="line">    &#x2F;&#x2F; Create a tracker</span><br><span class="line">    string trackerType &#x3D; trackerTypes[2];</span><br><span class="line"> </span><br><span class="line">    Ptr&lt;Tracker&gt; tracker;</span><br><span class="line"> </span><br><span class="line">    #if (CV_MINOR_VERSION &lt; 3)</span><br><span class="line">    &#123;</span><br><span class="line">        tracker &#x3D; Tracker::create(trackerType);</span><br><span class="line">    &#125;</span><br><span class="line">    #else</span><br><span class="line">    &#123;</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;BOOSTING&quot;)</span><br><span class="line">            tracker &#x3D; TrackerBoosting::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;MIL&quot;)</span><br><span class="line">            tracker &#x3D; TrackerMIL::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;KCF&quot;)</span><br><span class="line">            tracker &#x3D; TrackerKCF::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;TLD&quot;)</span><br><span class="line">            tracker &#x3D; TrackerTLD::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;MEDIANFLOW&quot;)</span><br><span class="line">            tracker &#x3D; TrackerMedianFlow::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;GOTURN&quot;)</span><br><span class="line">            tracker &#x3D; TrackerGOTURN::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;MOSSE&quot;)</span><br><span class="line">            tracker &#x3D; TrackerMOSSE::create();</span><br><span class="line">        if (trackerType &#x3D;&#x3D; &quot;CSRT&quot;)</span><br><span class="line">            tracker &#x3D; TrackerCSRT::create();</span><br><span class="line">    &#125;</span><br><span class="line">    #endif</span><br><span class="line">    &#x2F;&#x2F; Read video</span><br><span class="line">    VideoCapture video(&quot;videos&#x2F;chaplin.mp4&quot;);</span><br><span class="line">     </span><br><span class="line">    &#x2F;&#x2F; Exit if video is not opened</span><br><span class="line">    if(!video.isOpened())</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; &quot;Could not read video file&quot; &lt;&lt; endl; </span><br><span class="line">        return 1; </span><br><span class="line">    &#125; </span><br><span class="line"> </span><br><span class="line">    &#x2F;&#x2F; Read first frame </span><br><span class="line">    Mat frame; </span><br><span class="line">    bool ok &#x3D; video.read(frame); </span><br><span class="line"> </span><br><span class="line">    &#x2F;&#x2F; Define initial bounding box </span><br><span class="line">    Rect2d bbox(287, 23, 86, 320); </span><br><span class="line"> </span><br><span class="line">    &#x2F;&#x2F; Uncomment the line below to select a different bounding box </span><br><span class="line">    &#x2F;&#x2F; bbox &#x3D; selectROI(frame, false); </span><br><span class="line">    &#x2F;&#x2F; Display bounding box. </span><br><span class="line">    rectangle(frame, bbox, Scalar( 255, 0, 0 ), 2, 1 ); </span><br><span class="line"> </span><br><span class="line">    imshow(&quot;Tracking&quot;, frame); </span><br><span class="line">    tracker-&gt;init(frame, bbox);</span><br><span class="line">     </span><br><span class="line">    while(video.read(frame))</span><br><span class="line">    &#123;     </span><br><span class="line">        &#x2F;&#x2F; Start timer</span><br><span class="line">        double timer &#x3D; (double)getTickCount();</span><br><span class="line">         </span><br><span class="line">        &#x2F;&#x2F; Update the tracking result</span><br><span class="line">        bool ok &#x3D; tracker-&gt;update(frame, bbox);</span><br><span class="line">         </span><br><span class="line">        &#x2F;&#x2F; Calculate Frames per second (FPS)</span><br><span class="line">        float fps &#x3D; getTickFrequency() &#x2F; ((double)getTickCount() - timer);</span><br><span class="line">         </span><br><span class="line">        if (ok)</span><br><span class="line">        &#123;</span><br><span class="line">            &#x2F;&#x2F; Tracking success : Draw the tracked object</span><br><span class="line">            rectangle(frame, bbox, Scalar( 255, 0, 0 ), 2, 1 );</span><br><span class="line">        &#125;</span><br><span class="line">        else</span><br><span class="line">        &#123;</span><br><span class="line">            &#x2F;&#x2F; Tracking failure detected.</span><br><span class="line">            putText(frame, &quot;Tracking failure detected&quot;, Point(100,80), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(0,0,255),2);</span><br><span class="line">        &#125;</span><br><span class="line">         </span><br><span class="line">        &#x2F;&#x2F; Display tracker type on frame</span><br><span class="line">        putText(frame, trackerType + &quot; Tracker&quot;, Point(100,20), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(50,170,50),2);</span><br><span class="line">         </span><br><span class="line">        &#x2F;&#x2F; Display FPS on frame</span><br><span class="line">        putText(frame, &quot;FPS : &quot; + SSTR(int(fps)), Point(100,50), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(50,170,50), 2);</span><br><span class="line"> </span><br><span class="line">        &#x2F;&#x2F; Display frame.</span><br><span class="line">        imshow(&quot;Tracking&quot;, frame);</span><br><span class="line">         </span><br><span class="line">        &#x2F;&#x2F; Exit if ESC pressed.</span><br><span class="line">        int k &#x3D; waitKey(1);</span><br><span class="line">        if(k &#x3D;&#x3D; 27)</span><br><span class="line">        &#123;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Python代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import sys</span><br><span class="line">(major_ver, minor_ver, subminor_ver) &#x3D; (cv2.__version__).split(&#39;.&#39;)￼</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39; :</span><br><span class="line">    # Set up tracker.</span><br><span class="line">    # Instead of MIL, you can also use</span><br><span class="line">    tracker_types &#x3D; [&#39;BOOSTING&#39;, &#39;MIL&#39;,&#39;KCF&#39;, &#39;TLD&#39;, &#39;MEDIANFLOW&#39;, &#39;GOTURN&#39;, &#39;MOSSE&#39;, &#39;CSRT&#39;]</span><br><span class="line">    tracker_type &#x3D; tracker_types[2]</span><br><span class="line">    if int(minor_ver) &lt; 3:</span><br><span class="line">        tracker &#x3D; cv2.Tracker_create(tracker_type)</span><br><span class="line">    else:</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;BOOSTING&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerBoosting_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;MIL&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerMIL_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;KCF&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerKCF_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;TLD&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerTLD_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;MEDIANFLOW&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerMedianFlow_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;GOTURN&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerGOTURN_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &#39;MOSSE&#39;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerMOSSE_create()</span><br><span class="line">        if tracker_type &#x3D;&#x3D; &quot;CSRT&quot;:</span><br><span class="line">            tracker &#x3D; cv2.TrackerCSRT_create()</span><br><span class="line">    # Read video</span><br><span class="line">    video &#x3D; cv2.VideoCapture(&quot;videos&#x2F;chaplin.mp4&quot;)</span><br><span class="line">    # Exit if video not opened.</span><br><span class="line">    if not video.isOpened():</span><br><span class="line">        print &quot;Could not open video&quot;</span><br><span class="line">        sys.exit()</span><br><span class="line">    # Read first frame.</span><br><span class="line">    ok, frame &#x3D; video.read()</span><br><span class="line">    if not ok:</span><br><span class="line">        print &#39;Cannot read video file&#39;</span><br><span class="line">        sys.exit()</span><br><span class="line">     </span><br><span class="line">    # Define an initial bounding box</span><br><span class="line">    bbox &#x3D; (287, 23, 86, 320)</span><br><span class="line">    # Uncomment the line below to select a different bounding box</span><br><span class="line">    bbox &#x3D; cv2.selectROI(frame, False)</span><br><span class="line">    # Initialize tracker with first frame and bounding box</span><br><span class="line">    ok &#x3D; tracker.init(frame, bbox)</span><br><span class="line">    while True:</span><br><span class="line">        # Read a new frame</span><br><span class="line">        ok, frame &#x3D; video.read()</span><br><span class="line">        if not ok:</span><br><span class="line">            break</span><br><span class="line">        # Start timer</span><br><span class="line">        timer &#x3D; cv2.getTickCount()</span><br><span class="line">        # Update tracker</span><br><span class="line">        ok, bbox &#x3D; tracker.update(frame)</span><br><span class="line">        # Calculate Frames per second (FPS)</span><br><span class="line">        fps &#x3D; cv2.getTickFrequency() &#x2F; (cv2.getTickCount() - timer);</span><br><span class="line">        # Draw bounding box</span><br><span class="line">        if ok:</span><br><span class="line">            # Tracking success</span><br><span class="line">            p1 &#x3D; (int(bbox[0]), int(bbox[1]))</span><br><span class="line">            p2 &#x3D; (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))</span><br><span class="line">            cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)</span><br><span class="line">        else :</span><br><span class="line">            # Tracking failure</span><br><span class="line">            cv2.putText(frame, &quot;Tracking failure detected&quot;, (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)</span><br><span class="line"> </span><br><span class="line">        # Display tracker type on frame</span><br><span class="line">        cv2.putText(frame, tracker_type + &quot; Tracker&quot;, (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);</span><br><span class="line">     </span><br><span class="line">        # Display FPS on frame</span><br><span class="line">        cv2.putText(frame, &quot;FPS : &quot; + str(int(fps)), (100,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50), 2);</span><br><span class="line"> </span><br><span class="line">        # Display result</span><br><span class="line">        cv2.imshow(&quot;Tracking&quot;, frame)</span><br><span class="line"> </span><br><span class="line">        # Exit if ESC pressed</span><br><span class="line">        k &#x3D; cv2.waitKey(1) &amp; 0xff</span><br><span class="line">        if k &#x3D;&#x3D; 27 : break</span><br></pre></td></tr></table></figure>
<h2 id="追踪算法详细"><a href="#追踪算法详细" class="headerlink" title="追踪算法详细"></a>追踪算法详细</h2><p>首先，思考下跟踪时我们的目标是在当前帧找到在前面的所有或绝大部分帧中正确跟踪的目标。由于我们追踪目标到当前帧，所以我们已知它是如何运动的，也即我们知道运行模型的参数。所谓运行模型，就是我们知道前面帧中的目标的位置和速度。即便你对目标一无所知，但是根据当前运行模型也可以预测目标的可能位置，并且这个预测可能会很准确。</p>
<p>但是，我们有更多的信息，比如我们可以对目标进行编码来代表目标的外观，这样的外观模型。此模型可以用来搜索，由运动模型预测的临近范围内的目标来获得更加准确的预测。</p>
<p><strong><em>运动模型预测目标的大概位置，外观模型微调此预估，来获得一个更准确的预测</em></strong></p>
<p>如果说目标很简单，并且外观改变不大的话，可以简单地使用一个模板作为外观模型并在图像中搜索模板就可以。<br>分类器的任务就是简单地判别一个矩形框是目标还是背景。分类器的输入是一个图像patch，返回一个0到1之间的分值。分值为0代表是背景，1则为目标。<br>在机器学习中，我们通常用<strong>在线学习</strong>代表算法可以在运行时飞快地训练完，而离线分类器需要几千个样本来训练一个分类器，而在线算法仅需要几个样本就可以。</p>
<p>分类器由正样本(目标)和负样本(非目标)来训练得到，以此分类器学习到目标与非目标之间的差异。在训练在线分类器时，我们没有数量众多的正负样本。</p>
<h3 id="Boost-追踪器"><a href="#Boost-追踪器" class="headerlink" title="Boost 追踪器"></a>Boost 追踪器</h3><p>此跟踪器基于<strong>在线版本的AdaBoost</strong>，这个是以Haar特征级联的人脸检测器内部使用。此分类器需要在运行时以正负样本来训练。</p>
<ol>
<li>其初始框由用户指定，作为追踪的正样本，而在框范围之外许多其他patch都作为背景。</li>
<li>在新的一帧图像中，分类器在前一帧框的周围的每个像素上分类，并给出得分。</li>
<li>目标的新位置即得分最高的</li>
<li>这样一来有新的正样本来重新训练分类器。依次类推。</li>
</ol>
<p><strong>优点</strong>：几乎没有，几十年前的技术。<br><strong>缺点</strong>：追踪性能一般，它无法感知追踪什么时候会失败。</p>
<h3 id="MIL追踪"><a href="#MIL追踪" class="headerlink" title="MIL追踪"></a>MIL追踪</h3><p>算法与Boost很像，唯一的区别是，它会考虑当前标定框周围小部分框同时作为正样本，你可能认为这个想法比较烂，因为大部分的这些<code>正样本</code>其实目标并不在中心。</p>
<p>这就是MIL(Multiple Instance Learning)的独特之处，在MIL中你不需要指定正负样本，而是<strong>正负样包(bags)</strong>。在正样本包中的并不全是正样本，而是仅需要一个样本是正样本即可。当前示例中，正样本包里面的样本包含的是处于中心位置的框，以及中心位置周围的像素所形成的框。即便当前位置的跟踪目标不准确，从以当前位置为中心在周围像素抽取的样本框所构成的正样本包中，仍然有很大概率命中一个恰好处于中心位置的框。</p>
<p><strong>优点</strong>：性能很好，不会像Boost那样会偏移，即便出现部分遮挡依然表现不错。 <strong>在Opencv3.0中效果最好的追踪器，如果在更高版本里，选择KCF</strong><br><strong>缺点</strong>：没法应对全遮挡，追踪失败也较难的得到反馈。</p>
<h3 id="KCF-追踪"><a href="#KCF-追踪" class="headerlink" title="KCF 追踪"></a>KCF 追踪</h3><p>KCF即Kernelized Correlation Filters,思路借鉴了前面两个。注意到MIL所使用的多个正样本之间存在交大的重叠区域。这些重叠数据可以引出一些较好的数学特性，这些特性同时可以用来构造更快更准确的分类器。</p>
<p><strong>优点</strong>：速度和精度都比MIL效果好，并且追踪失败反馈比Boost和MIL好。Opencv 3.1以上版本最好的分类器。<br><strong>缺点</strong>：完全遮挡之后没法恢复。</p>
<h3 id="TLD追踪"><a href="#TLD追踪" class="headerlink" title="TLD追踪"></a>TLD追踪</h3><p>TLD即Tracking, learning and detection，如其名此算法由三部分组成<code>追踪</code>,<code>学习</code>,<code>检测</code>。追踪器逐帧追踪目标，检测器定位所有到当前为止观察到的外观，如果有必要则纠正追踪器。<code>学习</code>会评估检测器的错误并更新，以避免进一步出错。<br>此追踪器可能会产生跳跃，比如你正在追踪一个人，但是场景中存在多个人，此追踪器可能会突然跳到另外一个行人进行追踪。<br>优势是：此追踪器可以应对大尺度变换，运行以及遮挡问题。如果说你的视频序列中，某个物体隐藏在另外一个物体之后，此追踪器可能是个好选择。 </p>
<p><strong>优势</strong>：多帧中被遮挡依然可以被检测到，目标尺度变换也可以被处理。<br><strong>缺点</strong>：容易误跟踪导致基本不可用。</p>
<h3 id="MedianFlow-追踪"><a href="#MedianFlow-追踪" class="headerlink" title="MedianFlow 追踪"></a>MedianFlow 追踪</h3><p>此追踪器在视频的前向时间和后向时间同时追踪目标，然后评估两个方向的轨迹的误差。最小化前后向误差，使得其可以有效地检测追踪失败的清情形，并在视频中选择相对可靠的轨迹。 </p>
<p>实测时发现，<strong>此追踪器在运动可预测以及运动速度较小时性能最好。而不像其他追踪器那样，即便追踪失败继续追踪，此追踪器很快就知道追踪失败</strong></p>
<p><strong>优点</strong>：对追踪失败反馈敏锐，当场景中运动可预测以及没有遮挡时，较好<br><strong>缺点</strong>：急速运动场景下会失败。</p>
<h3 id="GoTurn-追踪"><a href="#GoTurn-追踪" class="headerlink" title="GoTurn 追踪"></a>GoTurn 追踪</h3><p>这个是唯一使用CNN方法的追踪器，此算法对视点变化，光照变换以及形变等问题的鲁棒性较好。但是无法处理遮挡问题。</p>
<p><strong>注意</strong>：它使用caffe模型来追踪，需要下载caffe模型以及proto txt文件。</p>
<p>参考<a href="https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/" target="_blank" rel="noopener">这篇文章</a>详细介绍如何使用 GoTurn</p>
<h3 id="MOSSE-追踪"><a href="#MOSSE-追踪" class="headerlink" title="MOSSE 追踪"></a>MOSSE 追踪</h3><p>MOSSE即Minimum Output Sum of Squared Error，使用一个自适应协相关来追踪，产生稳定的协相关过滤器，并使用单帧来初始化。MOSSE鲁棒性较好，可以应对光线变换，尺度变换，姿势变动以及非网格化的变形。它也能基于峰值与旁瓣比例来检测遮挡，这使得追踪可以在目标消失时暂停并在目标出现时重启。MOSSE可以在高FPS(高达450以上)的场景下运行。易于实现，并且与其他复杂追踪器一样精确，并且可以更快。但是，在性能尺度上看，大幅度落后于基于深度学习的追踪器。</p>
<h3 id="CSRT-追踪"><a href="#CSRT-追踪" class="headerlink" title="CSRT 追踪"></a>CSRT 追踪</h3><p>在 Discriminative Correlation Filter with Channel and Spatial Reliability （DCF-CSR）中，我们使用空间依赖图来调整过滤器支持</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>如果需要更高的准确率，并且可以容忍延迟的话，使用CSRT</li>
<li>如果需要更快的FPS，并且可以容许稍低一点的准确率的话，使用KCF</li>
<li>如果纯粹的需要速度的话，用MOSSE</li>
</ul>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>merlin语音合成过程中 中文前端处理过程详细</title>
    <url>/2018/10/11/merlin-mandarin-fronted-process/</url>
    <content><![CDATA[<h2 id="0-概述"><a href="#0-概述" class="headerlink" title="0 概述"></a>0 概述</h2><p>语音合成过程，需要处理两部分内容，分别是：</p>
<ul>
<li><p>文本(Text)处理： 假设我们的输入是<code>你好看啊</code></p>
</li>
<li><p>音频(speech)处理： 对应<code>你好看啊.wav</code></p>
</li>
</ul>
<h2 id="1-文本处理"><a href="#1-文本处理" class="headerlink" title="1 文本处理"></a>1 文本处理</h2><h3 id="1-1-规范化"><a href="#1-1-规范化" class="headerlink" title="1.1 规范化"></a>1.1 规范化</h3><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等</p>
<p>有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音[7]。例如</p>
<ul>
<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”；</li>
<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>
</ul>
<p>对于英文而言，如：</p>
<ul>
<li><strong>类别为年份（NYER）</strong>： 2011 $\rightarrow$ twenty eleven</li>
<li><strong>类别为货币(MONEY)</strong>: £100 $\rightarrow$  one hundred pounds</li>
<li><strong>类别为非单词，需要拟音(ASWD)</strong>:  IKEA $\rightarrow$  apply letter-to-sound</li>
<li><strong>类别为数字(NUM)</strong> : 100 NUM $\rightarrow$ one hundred</li>
<li><strong>类别为字母(LSEQ)</strong> :  DVD  $\rightarrow$ dee vee dee</li>
</ul>
<h3 id="1-2-转化为拼音"><a href="#1-2-转化为拼音" class="headerlink" title="1.2 转化为拼音"></a>1.2 转化为拼音</h3><p>参考<a href="http://www.moe.edu.cn/s78/A19/yxs_left/moe_810/s230/195802/t19580201_186000.html" target="_blank" rel="noopener">国家汉语拼音方案</a></p>
<p>使用一个汉语拼音词典，将<code>你好看啊</code>转换为： <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>。此过程需要注意有些多音词需要处理，可以只是使用python的<strong>pypinyin</strong></p>
<h3 id="1-3-拼音转换为音调表示"><a href="#1-3-拼音转换为音调表示" class="headerlink" title="1.3 拼音转换为音调表示"></a>1.3 拼音转换为音调表示</h3><p>目前支持将汉语拼音中的<code>一</code>,<code>二</code>,<code>三</code>,<code>四</code>声转换为 <code>1</code>,<code>2</code>,<code>3</code>,<code>4</code>,<code>5</code>（5代表轻声）</p>
<p> <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>$\rightarrow$ <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<p>事实上<strong>pypinyin</strong>可以一步从<code>你好看啊</code>转换为 <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<h3 id="1-4-将音节分解为音素"><a href="#1-4-将音节分解为音素" class="headerlink" title="1.4  将音节分解为音素"></a>1.4  将音节分解为音素</h3><p>音素为汉语拼音的最小单元。包括<code>声母</code>,<code>韵母</code>,但是其中还会有一些整体认读音节。(注意：下面所列并非官方标准版，不同情形可以采取不同取舍，参考<a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">MTTS文本分析</a>)</p>
<p><strong>整体认读音节</strong></p>
<p>16个整体认读音节分别是：<code>zhi 、chi、shi、ri、zi、ci、si、yi、wu、yu、ye、yue、yuan、yin 、yun、ying</code>，但是要注意没有yan，因为yan并不发作an音</p>
<p><strong>声母（23个）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b p m f d t n l g k h j q x zh ch sh r z c s y w</span><br></pre></td></tr></table></figure>
<p><strong>韵母（39个）</strong></p>
<ul>
<li>单韵母 a、o、e、 ê、i、u、ü、-i（前）、-i（后）、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 üe、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 üan 、en、in、uen、 ün 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<p><strong>韵母（39个）（转换标注后）</strong></p>
<ul>
<li>单韵母 a、o、e、ea、i、u、v、ic、ih、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 ve、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 van 、en、in、uen、 vn 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<h3 id="1-5-结果"><a href="#1-5-结果" class="headerlink" title="1.5 结果"></a>1.5 结果</h3><p>此步骤的结果为 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&#39;n&#39;, &#39;i3&#39;), (&#39;h&#39;, &#39;ao3&#39;), (&#39;k&#39;, &#39;an4&#39;), (&#39;a5&#39;,)]</span><br></pre></td></tr></table></figure>
<h2 id="2-合成基元选取"><a href="#2-合成基元选取" class="headerlink" title="2 合成基元选取"></a>2 合成基元选取</h2><p>合成基元就是合成语音所需的最小单元。由大到小来说：</p>
<ol>
<li>可以选择每个汉字，一共有6万多，会导致需要很大的训练集</li>
<li>可以选择所有拼音，数量会比汉字少很多</li>
<li>也可以选择声韵母，声韵母是组成音节的单元，21个声母+39个韵母，数据量大幅度减少。</li>
</ol>
<p>在实际语音中除了这些文本上的内容之外，还会存在开始和结束的<strong>静音</strong>，标点符号之间存在的<strong>短暂停顿</strong>。所以我们可以采取以下这套合成基元方案。</p>
<ul>
<li><strong>声母</strong>：  21个声母+wy（共23个）</li>
<li><strong>韵母</strong>： 39个韵母</li>
<li><strong>静音</strong>：<code>sil</code>, <code>pau</code>, <code>sp</code>。sil(silence) 表示句首和句尾的静音，pau(pause) 表示由逗号，顿号造成的停顿，句中其他的短停顿为sp(short pause)</li>
</ul>
<h2 id="3-上下文相关标注"><a href="#3-上下文相关标注" class="headerlink" title="3  上下文相关标注"></a>3  上下文相关标注</h2><p>上下文相关标注的规则要综合考虑有哪些上下文对当前音素发音的影响，总的来说，需要考虑发音基元及其前后基元的信息，以及发音基元所在的音节、词、韵律词、韵律短语、语句相关的信息。</p>
<p>此类标注对于不同任务可以自由设计，一种参考是<a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/mandarin_example_label.md" target="_blank" rel="noopener">MTTS普通话标注示例</a>。这里将参考中的一些内容作出一些解释：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层级（由小到达）</th>
<th>标注格式</th>
</tr>
</thead>
<tbody>
<tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6_p7</td>
</tr>
<tr>
<td>.</td>
<td>/A:a1_a2-a3_a4#a5</td>
</tr>
<tr>
<td>音节层</td>
<td>/B:b1_b2!b3_b4#b5@b6!b7+b8@b9#b10_b11</td>
</tr>
<tr>
<td>.</td>
<td>/C:c1+c2-c3=c4#c5</td>
</tr>
<tr>
<td>词层</td>
<td>/D:d1-d2 /E:e1&amp;e2^e3_e4 /F:f1-f2</td>
</tr>
<tr>
<td>韵律层</td>
<td>/G:g1-g2 /H:h1-h2@h3+h4 /I:i1-i2</td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/J:j1^j2=j3-j4 /K:k1=k2_k3^k4&amp;k5_k6 /L:l1^l2#l3-l4</td>
</tr>
<tr>
<td>语句层</td>
<td>/M:m1#m2+m3+m4!m5</td>
</tr>
</tbody>
</table>
</div>
<p>下面的（发音）基元指的是声韵母，HMM建模选用的单元是音节</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前基元在当前音节的位置（正序）</td>
</tr>
<tr>
<td>p7</td>
<td>当前基元在当前音节的位置（倒序） </td>
</tr>
<tr>
<td>a1</td>
<td>前一音节的首基元</td>
</tr>
<tr>
<td>a2</td>
<td>前一音节的末基元</td>
</tr>
<tr>
<td>a3，a4</td>
<td>前一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>前一音节的基元数目</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节的首基元</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节的末基元</td>
</tr>
<tr>
<td>b3，b4</td>
<td>当前音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>当前音节的基元数目</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节在词中的位置（正序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节在词中的位置（倒序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b9</td>
<td>当前音节在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b10</td>
<td>当前音节在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b11</td>
<td>当前音节在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>c1</td>
<td>后一音节的首基元</td>
</tr>
<tr>
<td>c2</td>
<td>后一音节的末基元</td>
</tr>
<tr>
<td>c3，c4</td>
<td>后一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>c5</td>
<td>后一音节的基元数目</td>
</tr>
<tr>
<td>d1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>d2</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>e1</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>e2</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>当前词在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>e4</td>
<td>当前词在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>f1</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>f2</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>g1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>g2</td>
<td>前一个韵律词的词数目</td>
</tr>
<tr>
<td> —-</td>
<td>——</td>
</tr>
<tr>
<td>h1</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>h2</td>
<td>当前韵律词的词数目</td>
</tr>
<tr>
<td>h3</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>h4</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td> —-</td>
<td>—</td>
</tr>
<tr>
<td>i1</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>i2</td>
<td>后一个韵律词的词数目</td>
</tr>
<tr>
<td> —</td>
<td>—-</td>
</tr>
<tr>
<td>j1</td>
<td>前一韵律短语的语调类型</td>
</tr>
<tr>
<td>j2</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>j3</td>
<td>前一韵律短语的词数目</td>
</tr>
<tr>
<td>j4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td> —-</td>
<td>——</td>
</tr>
<tr>
<td>k1</td>
<td>当前韵律短语的语调类型</td>
</tr>
<tr>
<td>k2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>k3</td>
<td>当前韵律短语的词数目</td>
</tr>
<tr>
<td>k4</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>k5</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>k6</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td> —-</td>
<td>—-</td>
</tr>
<tr>
<td>l1</td>
<td>后一韵律短语的语调类型</td>
</tr>
<tr>
<td>l2</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>l3</td>
<td>后一韵律短语的词数目</td>
</tr>
<tr>
<td>l4</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td> —-</td>
<td>—-</td>
</tr>
<tr>
<td>m1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>m2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>m3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>m4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>m5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody>
</table>
</div>
<h2 id="4-问题集设计"><a href="#4-问题集设计" class="headerlink" title="4 问题集设计"></a>4 问题集设计</h2><p>问题集(Question Set)即是决策树中条件判断的设计。问题集通常很大，由几百个判断条件组成。</p>
<p>问题集的设计依赖于不同语言的语言学知识，而且<strong>与上下文标注文件相匹配，改变上下文标注方法也需要相应地改变问题集</strong>，对于中文语音合成而言，问题集的设计的规则有:</p>
<ul>
<li><strong>前前个，前个，当前，下个，下下个声韵母分别是某个合成基元吗</strong>，合成基元共有65个(23声母+39韵母+3静音)，例如判断是否是元音a QS “LL-a” QS “L-a” QS “C-a” QS “R-a” QS “RR-a”</li>
<li><strong>声母特征划分</strong>，例如声母可以划分成塞音，擦音，鼻音，唇音等，声母特征划分24个</li>
<li><strong>韵母特征划分</strong>，例如韵母可以划分成单韵母，复合韵母，分别包含aeiouv的韵母，韵母特征划分8个</li>
<li><strong>其他信息划分</strong>，词性划分，26个词性; 声调类型，5个; 是否是声母或者韵母或者静音，3个</li>
<li><strong>韵律特征划分</strong>，如是否是重音，重音和韵律词/短语的位置数量</li>
<li><strong>位置和数量特征划分</strong></li>
</ul>
<p>对于三音素模型而言，对于每个划分的特征，都会产生3个判断条件，该音素是否满足条件，它的左音素（声韵母）和右音素（声韵母）是否满足条件，有时会扩展到左左音素和右右音素的情况，这样就有5个问题。其中，每个问题都是以 QS 命令开头，问题集的答案可以有多个，中间以逗号隔开，答案是一个包含通配符的字符串。当问题表达式为真时，该字符串成功匹配标注文件中的某一行标注。格式如：</p>
<p>QS 问题表达式 {答案 1，答案 2，答案 3，……}</p>
<p>QS “LL==Fricative” {f^<em>,s^</em>,sh^<em>,x^</em>,h^<em>,lh^</em>,hy^<em>,hh^</em>}</p>
<p>对于3音素上下文相关的基元模型的3个问题，例如： <em> 判断当前，前接，后接音素/单元是否为擦音 </em> QS ‘C_Fricative’ <em> QS ‘L_Fricative’ </em> QS ‘R_Fricative’</p>
<p>问题集示例参考 <a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/question.md" target="_blank" rel="noopener">MTTS问题集设计参考</a></p>
<p>值得注意的是，merlin中使用的问题集和HTS中有所不同，Merlin中新增加了CQS问题，Merlin处理Questions Set 的模块在merlin/src/frontend/label_normalisation 中的Class HTSLabelNormalisation</p>
<p><strong>Question Set 的格式是</strong></p>
<p>QS + 一个空格 + “question<em>name” + 任意空格+ {Answer1, answer2, answer3…} # 无论是QS还是CQS的answer中，前后的**不用加，加了也会被去掉 CQS + 一个空格 + “question_name” + 任意空格+ {Answer} #对于CQS，这里只能有一个answer 比如 CQS C-Syl-Tone {</em>(d+)+} merlin也支持浮点数类型，只需改为CQS C-Syl-Tone {_([d.]+)+}</p>
<p>参考 ：  <a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html</a></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>使用HOG+SVM+滑窗+NMS完成目标定位分类</title>
    <url>/2018/10/02/opencv-svm-hog-loc/</url>
    <content><![CDATA[<h2 id="0-概览"><a href="#0-概览" class="headerlink" title="0  概览"></a>0  概览</h2><p>整个过程如下:</p>
<ol>
<li>数据标注</li>
<li>抽取HOG</li>
<li>训练SVM</li>
<li>预测（滑窗，分类）</li>
<li>NMS</li>
</ol>
<h2 id="1-数据标注"><a href="#1-数据标注" class="headerlink" title="1 数据标注"></a>1 数据标注</h2><p>使用的是<code>extract_feature_from_fasterrcnn_labeled.py</code></p>
<p>本文直接使用了 YOLO的数据标注格式。YOLO的标注格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一个有9个物体，9个物体总共属于3个分类</span><br><span class="line">9,3</span><br><span class="line"># 分类yinliao的类别是0，xmin,ymin,xmax,ymax分别是56,103,261,317</span><br><span class="line">56,103,261,317,0,yinliao</span><br><span class="line">261,0,442,120,2,kele</span><br><span class="line">465,16,645,180,2,kele</span><br><span class="line">209,138,345,266,2,kele</span><br></pre></td></tr></table></figure>
<p>根据此标注文件可以从图片中抠取目标物体的ROI。如下图，根据标注的文件可以抽取的ROI示例，左上角为提取的ROI：</p>
<p><img src="/images/blog/hog_svm_loc1.jpg" alt="yolov123"> </p>
<h2 id="2-抽取特征"><a href="#2-抽取特征" class="headerlink" title="2 抽取特征"></a>2 抽取特征</h2><h3 id="2-1-抽取HOG特征"><a href="#2-1-抽取HOG特征" class="headerlink" title="2.1 抽取HOG特征"></a>2.1 抽取HOG特征</h3><p>注意：</p>
<ul>
<li>我最开始使用的是 Opencv的<code>hog = cv2.HOGDescriptor</code>的方式，后来修改为<code>skimage.feature</code>。</li>
<li>hog特征如果想要固定长度的话，提取对象(ROI)必须也是固定长度的，所以需要做个resize。否则的话在训练时，会出现数据维度不一致的问题。我之前以为HOG会把任意尺寸的图像转换为相同长度的特征，发现并不是。提取完之后检查输出文件的大小是否一样。</li>
<li>提取ROI的时候注意 y在前，x在后。比如<code>roi = gray[int(rec[1]):int(rec[3]),int(rec[0]):int(rec[2])]</code>,rec里面是顺序的(xmin,ymin,xmax,ymax)</li>
<li>使用 <code>sklearn.externals.joblib.dump(hog特征，保存路径)</code>的方式存储提取的HOG特征</li>
</ul>
<p>ROI和其HOG特征示例如下（左边为ROI，右边为对应的HOG）：</p>
<p><img src="/images/blog/hog_svm_loc2.jpg" alt="yolov123"> </p>
<p>当前的做法是每次提取一个正样本，就生成5个或者10个负样本。使用<code>generate_neg_box</code>方法。</p>
<h3 id="2-2-SURF特征"><a href="#2-2-SURF特征" class="headerlink" title="2.2 SURF特征"></a>2.2 SURF特征</h3><p>参考</p>
<p> <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html" target="_blank" rel="noopener">opencv3.0+的 surf特征使用</a></p>
<p><a href="http://www.dummies.com/programming/big-data/data-science/how-to-visualize-the-classifier-in-an-svm-supervised-learning-model/" target="_blank" rel="noopener">使用SVM对图像数据分类</a></p>
<p><a href="https://docs.opencv.org/3.4.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html" target="_blank" rel="noopener">opencv surf特征一些参数的解释</a></p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def extract_surf(img,pca &#x3D; 100,visual &#x3D;False):</span><br><span class="line">    image &#x3D; cv2.imread(im)</span><br><span class="line">    image &#x3D; cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)</span><br><span class="line">    surf &#x3D; cv2.xfeatures2d.SURF_create(1000, extended&#x3D;False)</span><br><span class="line">    (kps, descs) &#x3D; surf.detectAndCompute(image, None)</span><br><span class="line">    if visual:</span><br><span class="line">        print(len(kps),descs.shape)</span><br><span class="line">        print(&quot;&#x3D;&#x3D;&quot; * 30)</span><br><span class="line">        print(descs)</span><br><span class="line">        img2 &#x3D; cv2.drawKeypoints(image, kps, None, (255, 0, 0), 4)</span><br><span class="line">        cv2.imshow(&quot;with surf feature&quot;, img2)</span><br><span class="line">        cv2.waitKey(0)</span><br><span class="line">    return kps, descs</span><br></pre></td></tr></table></figure>
<ul>
<li><p>SURF_create:第一个参数是Hessian矩阵阈值，此值越大，生成的特征点越少；第二个参数是extended是是否需要拓展，False时每个特征点维度是64，True时每个特征点维度是128.</p>
</li>
<li><p>detectAndCompute的结果：有两个值kps和descs。其实descs包含kps，descs是一个二维数组，行数即特征点数目（不固定），列数固定为64或128.如下图：</p>
</li>
</ul>
<p><img src="/images/blog/hog_svm_loc3.jpg" alt="yolov123"> </p>
<h2 id="3-训练SVM"><a href="#3-训练SVM" class="headerlink" title="3 训练SVM"></a>3 训练SVM</h2><p>代码在 <a href="">classifier.py</a>中。训练SVM很简单，如下：</p>
<p><img src="/images/blog/hog_svm_loc4.jpg" alt="yolov123"> </p>
<p>过程为：</p>
<ol>
<li><p>添加 <code>(正样本，正标签)</code>和<code>(负样本，负标签)</code>.</p>
</li>
<li><p>声明一个SVM分类器</p>
</li>
<li><p>拟合SVM分类器</p>
</li>
<li><p>保存模型</p>
</li>
</ol>
<p>训练过程非常快。</p>
<h2 id="4-预测-滑窗，分类"><a href="#4-预测-滑窗，分类" class="headerlink" title="4 预测(滑窗，分类)"></a>4 预测(滑窗，分类)</h2><p>测试过程相对麻烦。需要处理几个问题</p>
<ol>
<li>训练的时候的ROI都是固定尺寸的(做了resize)，但是测试的时候可能物体有<strong>尺寸变化</strong>，如何应对尺寸变换-&gt;对原图做图像<strong>金字塔缩放</strong>(会生成大约9张不同尺寸的图)</li>
<li>如何在一张图中<strong>搜索物体</strong>-&gt;使用固定尺寸的<strong>滑窗遍历</strong>图像</li>
<li>临近位置的ROI可能会预测<strong>多个结果</strong>，合并结果-&gt;<strong>NMS</strong></li>
</ol>
<p>代码<code>classifier.py</code>的<code>test_model</code>方法截图：</p>
<p><img src="/images/blog/hog_svm_loc5.jpg" alt="yolov123"><br><img src="/images/blog/hog_svm_loc6.jpg" alt="yolov123"> </p>
<h3 id="4-1-图像金字塔"><a href="#4-1-图像金字塔" class="headerlink" title="4.1 图像金字塔"></a>4.1 图像金字塔</h3><p>注意生成图像金字塔的时候并没有使用<code>skimage.transform.pyramid_gaussian</code>的，因为这种方法对使用HOG特征不太友好。所以此代码中重写了<code>pyramid</code></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><ol>
<li>不能使用opencv的hog描述子来计算和训练，否则在预测的时候会出现数据维度或格式错误’</li>
<li>HOG特征的长度是跟图像的尺寸有关的，所以在计算HOG特征之前要统一resize到固定尺寸才行。虽然HOG特征计算时声称，只跟</li>
<li>使用SVM做二分类的时候要注意，负样本可能需要多一点。不然在预测时会出现很多误判。我刚开始时使用另外一个分类的ROI作为负样本，事实表明效果很差，最后采取了随机在正样本周围取样，效果会变好一点。</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><strong>classifier.py</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line">#encoding:utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">@author:</span><br><span class="line">@time:2017&#x2F;3&#x2F;19 11:08</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">import numpy as np</span><br><span class="line">import glob</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">from skimage.transform import pyramid_gaussian</span><br><span class="line">import imutils</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from skimage.io import imread</span><br><span class="line">from extract_feature_from_fasterrcnn_labeled import get_hog</span><br><span class="line">from skimage import feature</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def train_model(pos_feat_path,neg_feat_path,model_path):</span><br><span class="line">    fds &#x3D; []</span><br><span class="line">    labels &#x3D; []</span><br><span class="line">    # Load the positive features</span><br><span class="line">    for feat_path in glob.glob(os.path.join(pos_feat_path, &quot;*.feat&quot;)):</span><br><span class="line">        fd &#x3D; joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(1)</span><br><span class="line"></span><br><span class="line">    # Load the negative features</span><br><span class="line">    for feat_path in glob.glob(os.path.join(neg_feat_path, &quot;*.feat&quot;)):</span><br><span class="line">        fd &#x3D; joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(0)</span><br><span class="line"></span><br><span class="line">    clf &#x3D; LinearSVC()</span><br><span class="line">    print(&quot;Training a Linear SVM Classifier&quot;)</span><br><span class="line">    max_len &#x3D; 0</span><br><span class="line">    for fd in fds:</span><br><span class="line">        if len(fd)&gt;max_len:</span><br><span class="line">            max_len &#x3D;len(fd)</span><br><span class="line">    for i in range(len(fds)):</span><br><span class="line">        fd &#x3D; fds[i]</span><br><span class="line">        np.squeeze(fd[i],axis&#x3D;0)</span><br><span class="line">        if len(fd)&lt;max_len:</span><br><span class="line">            fds[i] &#x3D; np.concatenate((fds[i],np.array([0]*(max_len-len(fds[i])))),axis&#x3D;0)</span><br><span class="line">    clf.fit(fds, labels)</span><br><span class="line">    # If feature directories don&#39;t exist, create them</span><br><span class="line">    if not os.path.isdir(os.path.split(model_path)[0]):</span><br><span class="line">        os.makedirs(os.path.split(model_path)[0])</span><br><span class="line">    joblib.dump(clf, model_path)</span><br><span class="line">    print(&quot;Classifier saved to &#123;&#125;&quot;.format(model_path))</span><br><span class="line"></span><br><span class="line">def sliding_window(image, window_size, step_size):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    This function returns a patch of the input image &#96;image&#96; of size equal</span><br><span class="line">    to &#96;window_size&#96;. The first image returned top-left co-ordinates (0, 0)</span><br><span class="line">    and are increment in both x and y directions by the &#96;step_size&#96; supplied.</span><br><span class="line">    So, the input parameters are -</span><br><span class="line">    * &#96;image&#96; - Input Image</span><br><span class="line">    * &#96;window_size&#96; - Size of Sliding Window</span><br><span class="line">    * &#96;step_size&#96; - Incremented Size of Window</span><br><span class="line"></span><br><span class="line">    The function returns a tuple -</span><br><span class="line">    (x, y, im_window)</span><br><span class="line">    where</span><br><span class="line">    * x is the top-left x co-ordinate</span><br><span class="line">    * y is the top-left y co-ordinate</span><br><span class="line">    * im_window is the sliding window image</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    for y in range(0, image.shape[0], step_size[1]):</span><br><span class="line">        for x in range(0, image.shape[1], step_size[0]):</span><br><span class="line">            yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])</span><br><span class="line"></span><br><span class="line">def overlapping_area(detection_1, detection_2):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Function to calculate overlapping area&#39;si</span><br><span class="line">    &#96;detection_1&#96; and &#96;detection_2&#96; are 2 detections whose area</span><br><span class="line">    of overlap needs to be found out.</span><br><span class="line">    Each detection is list in the format -&gt;</span><br><span class="line">    [x-top-left, y-top-left, confidence-of-detections, width-of-detection, height-of-detection]</span><br><span class="line">    The function returns a value between 0 and 1,</span><br><span class="line">    which represents the area of overlap.</span><br><span class="line">    0 is no overlap and 1 is complete overlap.</span><br><span class="line">    Area calculated from -&gt;</span><br><span class="line">    http:&#x2F;&#x2F;math.stackexchange.com&#x2F;questions&#x2F;99565&#x2F;simplest-way-to-calculate-the-intersect-area-of-two-rectangles</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # Calculate the x-y co-ordinates of the</span><br><span class="line">    # rectangles</span><br><span class="line">    x1_tl &#x3D; detection_1[0]</span><br><span class="line">    x2_tl &#x3D; detection_2[0]</span><br><span class="line">    x1_br &#x3D; detection_1[0] + detection_1[3]</span><br><span class="line">    x2_br &#x3D; detection_2[0] + detection_2[3]</span><br><span class="line">    y1_tl &#x3D; detection_1[1]</span><br><span class="line">    y2_tl &#x3D; detection_2[1]</span><br><span class="line">    y1_br &#x3D; detection_1[1] + detection_1[4]</span><br><span class="line">    y2_br &#x3D; detection_2[1] + detection_2[4]</span><br><span class="line">    # Calculate the overlapping Area</span><br><span class="line">    x_overlap &#x3D; max(0, min(x1_br, x2_br)-max(x1_tl, x2_tl))</span><br><span class="line">    y_overlap &#x3D; max(0, min(y1_br, y2_br)-max(y1_tl, y2_tl))</span><br><span class="line">    overlap_area &#x3D; x_overlap * y_overlap</span><br><span class="line">    area_1 &#x3D; detection_1[3] * detection_2[4]</span><br><span class="line">    area_2 &#x3D; detection_2[3] * detection_2[4]</span><br><span class="line">    total_area &#x3D; area_1 + area_2 - overlap_area</span><br><span class="line">    return overlap_area &#x2F; float(total_area)</span><br><span class="line"></span><br><span class="line">def nms(detections, threshold&#x3D;.5):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    This function performs Non-Maxima Suppression.</span><br><span class="line">    &#96;detections&#96; consists of a list of detections.</span><br><span class="line">    Each detection is in the format -&gt;</span><br><span class="line">    [x-top-left, y-top-left, confidence-of-detections, width-of-detection, height-of-detection]</span><br><span class="line">    If the area of overlap is greater than the &#96;threshold&#96;,</span><br><span class="line">    the area with the lower confidence score is removed.</span><br><span class="line">    The output is a list of detections.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    if len(detections) &#x3D;&#x3D; 0:</span><br><span class="line">        return []</span><br><span class="line">    # Sort the detections based on confidence score</span><br><span class="line">    detections &#x3D; sorted(detections, key&#x3D;lambda detections: detections[2],</span><br><span class="line">            reverse&#x3D;True)</span><br><span class="line">    # Unique detections will be appended to this list</span><br><span class="line">    new_detections&#x3D;[]</span><br><span class="line">    # Append the first detection</span><br><span class="line">    new_detections.append(detections[0])</span><br><span class="line">    # Remove the detection from the original list</span><br><span class="line">    del detections[0]</span><br><span class="line">    # For each detection, calculate the overlapping area</span><br><span class="line">    # and if area of overlap is less than the threshold set</span><br><span class="line">    # for the detections in &#96;new_detections&#96;, append the</span><br><span class="line">    # detection to &#96;new_detections&#96;.</span><br><span class="line">    # In either case, remove the detection from &#96;detections&#96; list.</span><br><span class="line">    for index, detection in enumerate(detections):</span><br><span class="line">        for new_detection in new_detections:</span><br><span class="line">            if overlapping_area(detection, new_detection) &gt; threshold:</span><br><span class="line">                del detections[index]</span><br><span class="line">                break</span><br><span class="line">        else:</span><br><span class="line">            new_detections.append(detection)</span><br><span class="line">            del detections[index]</span><br><span class="line">    return new_detections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pyramid(image, scale&#x3D;1.5, minSize&#x3D;(30, 30)):</span><br><span class="line">    # yield the original image</span><br><span class="line">    yield image</span><br><span class="line"></span><br><span class="line">    # keep looping over the pyramid</span><br><span class="line">    while True:</span><br><span class="line">        # compute the new dimensions of the image and resize it</span><br><span class="line">        w &#x3D; int(image.shape[1] &#x2F; scale)</span><br><span class="line">        image &#x3D; imutils.resize(image, width&#x3D;w)</span><br><span class="line"></span><br><span class="line">        # if the resized image does not meet the supplied minimum</span><br><span class="line">        # size, then stop constructing the pyramid</span><br><span class="line">        if image.shape[0] &lt; minSize[1] or image.shape[1] &lt; minSize[0]:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        # yield the next image in the pyramid</span><br><span class="line">        yield image</span><br><span class="line"></span><br><span class="line">def test_model(model_path,image):</span><br><span class="line">    im &#x3D; cv2.cvtColor(cv2.imread(image), cv2.COLOR_RGB2GRAY)</span><br><span class="line">    min_wdw_sz &#x3D; (50, 100)</span><br><span class="line">    step_size &#x3D; (30, 30)</span><br><span class="line">    downscale &#x3D; 1.25</span><br><span class="line">    visualize_det &#x3D; True</span><br><span class="line">    # Load the classifier</span><br><span class="line">    clf &#x3D; joblib.load(model_path)</span><br><span class="line">    visualize_test &#x3D; True</span><br><span class="line">    # List to store the detections</span><br><span class="line">    detections &#x3D; []</span><br><span class="line">    # The current scale of the image</span><br><span class="line">    scale &#x3D; 0</span><br><span class="line">    fourcc &#x3D; cv2.VideoWriter_fourcc(*&#39;XVID&#39;)</span><br><span class="line">    video_out &#x3D; cv2.VideoWriter(&#39;D:&#x2F;data&#x2F;test_carmera&#x2F;svm&#x2F;hog_svm_slide_wid.avi&#39;, -1, 20.0, (480,600))</span><br><span class="line"></span><br><span class="line">    # Downscale the image and iterate</span><br><span class="line">    #for im_scaled in pyramid_gaussian(im, downscale&#x3D;downscale):</span><br><span class="line">    for (i, im_scaled) in enumerate(pyramid(im, scale&#x3D;1.25)):</span><br><span class="line">        # This list contains detections at the current scale</span><br><span class="line">        cd &#x3D; []</span><br><span class="line">        # If the width or height of the scaled image is less than</span><br><span class="line">        # the width or height of the window, then end the iterations.</span><br><span class="line">        if im_scaled.shape[0] &lt; min_wdw_sz[1] or im_scaled.shape[1] &lt; min_wdw_sz[0]:</span><br><span class="line">            break</span><br><span class="line">        for (x, y, im_window) in sliding_window(im_scaled, min_wdw_sz, step_size):</span><br><span class="line">            if im_window.shape[0] !&#x3D; min_wdw_sz[1] or im_window.shape[1] !&#x3D; min_wdw_sz[0]:</span><br><span class="line">               continue</span><br><span class="line">            #Calculate the HOG features</span><br><span class="line">            #fd &#x3D; get_hog(im_window)</span><br><span class="line">            #im_window &#x3D; imutils.auto_canny(im_window)</span><br><span class="line">            im_window &#x3D;  cv2.resize(im_window,(200,250))</span><br><span class="line">            fd &#x3D; feature.hog(im_window, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">            print(fd.shape)</span><br><span class="line">            if len(fd)&gt;1:</span><br><span class="line">                #fd &#x3D; np.transpose(fd)</span><br><span class="line">                fd &#x3D; fd.reshape(1, -1)</span><br><span class="line">                pred &#x3D; clf.predict(fd)</span><br><span class="line">                print(&quot;prediction:\t &quot;,pred)</span><br><span class="line">                if pred &#x3D;&#x3D; 1:# and clf.decision_function(fd)&gt;1:</span><br><span class="line">                    print(&quot;Detection:: Location -&gt; (&#123;&#125;, &#123;&#125;)&quot;.format(x, y))</span><br><span class="line">                    print(&quot;Scale -&gt;  &#123;&#125; | Confidence Score &#123;&#125; \n&quot;.format(scale, clf.decision_function(fd)))</span><br><span class="line">                    detections.append((x, y, clf.decision_function(fd),</span><br><span class="line">                                   int(min_wdw_sz[0] * (downscale ** scale)),</span><br><span class="line">                                   int(min_wdw_sz[1] * (downscale ** scale))))</span><br><span class="line"></span><br><span class="line">                    cd.append(detections[-1])</span><br><span class="line">                # If visualize is set to true, display the working</span><br><span class="line">                # of the sliding window</span><br><span class="line">                if visualize_det:</span><br><span class="line">                    clone &#x3D; im_scaled.copy()</span><br><span class="line">                    for x1, y1, _, _, _ in cd:</span><br><span class="line">                        # Draw the detections at this scale</span><br><span class="line">                        cv2.rectangle(clone, (x1, y1), (x1 + im_window.shape[1], y1 +</span><br><span class="line">                                                        im_window.shape[0]), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">                    cv2.rectangle(clone, (x, y), (x + im_window.shape[1], y +</span><br><span class="line">                                                  im_window.shape[0]), (255, 255, 255), thickness&#x3D;2)</span><br><span class="line">                    cv2.imshow(&quot;Sliding Window in Progress&quot;, clone)</span><br><span class="line">                    out_img_tempt&#x3D; cv2.resize(clone,(480,600))</span><br><span class="line">                    video_out.write(out_img_tempt)</span><br><span class="line">                    cv2.waitKey(10)</span><br><span class="line">        # Move the the next scale</span><br><span class="line">        scale +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    # Display the results before performing NMS</span><br><span class="line">    clone &#x3D; im.copy()</span><br><span class="line">    for (x_tl, y_tl, _, w, h) in detections:</span><br><span class="line">        # Draw the detections</span><br><span class="line">        cv2.rectangle(im, (x_tl, y_tl), (x_tl + w, y_tl + h), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">    cv2.imshow(&quot;Raw Detections before NMS&quot;, im)</span><br><span class="line">    cv2.waitKey()</span><br><span class="line"></span><br><span class="line">    # Perform Non Maxima Suppression</span><br><span class="line">    detections &#x3D; nms(detections, 0.5)</span><br><span class="line"></span><br><span class="line">    # Display the results after performing NMS</span><br><span class="line">    for (x_tl, y_tl, _, w, h) in detections:</span><br><span class="line">        # Draw the detections</span><br><span class="line">        cv2.rectangle(clone, (x_tl, y_tl), (x_tl + w, y_tl + h), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">    cv2.imshow(&quot;Final Detections after applying NMS&quot;, clone)</span><br><span class="line">    out_img_tempt &#x3D; cv2.resize(clone,(480,600))</span><br><span class="line">    #out_img_tempt[0:clone.shape[0], 0:clone.shape[1]] &#x3D; clone[:, :]</span><br><span class="line">    video_out.write(out_img_tempt)</span><br><span class="line">    cv2.waitKey()</span><br><span class="line">    video_out.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    hog_svm_model &#x3D; r&quot;D:\data\imgs\hog_svm_model_kele.model&quot;</span><br><span class="line">    pos_feat_path &#x3D; r&quot;D:\data\imgs\hog_feats\2&quot;</span><br><span class="line">    neg_feat_path &#x3D; r&quot;D:\data\imgs\hog_feats\2_neg&quot;</span><br><span class="line"></span><br><span class="line">    # hog_svm_model &#x3D; r&quot;D:\data\test_carmera\svm\hog_svm_model.model&quot;</span><br><span class="line">    # pos_feat_path &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats\0&quot;</span><br><span class="line">    # neg_feat_path &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats\0_neg&quot;</span><br><span class="line"></span><br><span class="line">    train_model(pos_feat_path,neg_feat_path,hog_svm_model)</span><br><span class="line">    test_img &#x3D; &quot;D:&#x2F;data&#x2F;test_carmera&#x2F;svm&#x2F;images&#x2F;frmaes_2.jpg&quot;</span><br><span class="line">    test_img1 &#x3D; r&quot;D:\data\imgs\images\0b24fb0ee9947292ffbb88c6e7c22a08.jpg&quot;</span><br><span class="line">    test_model(hog_svm_model,test_img1)</span><br><span class="line"></span><br><span class="line">    # gray &#x3D; cv2.cvtColor(cv2.imread(test_img), cv2.COLOR_BGR2GRAY)</span><br><span class="line">    # edged &#x3D; imutils.auto_canny(gray)</span><br><span class="line">    # cv2.imshow(&quot;edges&quot;,edged)</span><br><span class="line">    # # find contours in the edge map, keeping only the largest one which</span><br><span class="line">    # # is presumed to be the car logo</span><br><span class="line">    # (a,cnts, _) &#x3D; cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">    # c &#x3D; max(cnts, key&#x3D;cv2.contourArea)</span><br><span class="line">    # # extract the logo of the car and resize it to a canonical width</span><br><span class="line">    # # and height</span><br><span class="line">    # (x, y, w, h) &#x3D; cv2.boundingRect(c)</span><br><span class="line">    # logo &#x3D; gray[y:y + h, x:x + w]</span><br><span class="line">    # cv2.imshow(&quot;rect&quot;,logo)</span><br><span class="line">    # cv2.waitKey(0)</span><br><span class="line">    # t0 &#x3D; time.time()</span><br><span class="line">    # clf_type &#x3D; &#39;LIN_SVM&#39;</span><br><span class="line">    # fds &#x3D; []</span><br><span class="line">    # labels &#x3D; []</span><br><span class="line">    # num &#x3D; 0</span><br><span class="line">    # total &#x3D; 0</span><br><span class="line">    # for feat_path in glob.glob(os.path.join(train_feat_path, &#39;*.feat&#39;)):</span><br><span class="line">    #     data &#x3D; joblib.load(feat_path)</span><br><span class="line">    #     fds.append(data[:-1])</span><br><span class="line">    #     labels.append(data[-1])</span><br><span class="line">    # if clf_type is &#39;LIN_SVM&#39;:</span><br><span class="line">    #     clf &#x3D; LinearSVC()</span><br><span class="line">    #     print(&quot;Training a Linear SVM Classifier.&quot;)</span><br><span class="line">    #     clf.fit(fds, labels)</span><br><span class="line">    #     # If feature directories don&#39;t exist, create them</span><br><span class="line">    #     # if not os.path.isdir(os.path.split(model_path)[0]):</span><br><span class="line">    #     #     os.makedirs(os.path.split(model_path)[0])</span><br><span class="line">    #     # joblib.dump(clf, model_path)</span><br><span class="line">    #     # clf &#x3D; joblib.load(model_path)</span><br><span class="line">    #     print(&quot;Classifier saved to &#123;&#125;&quot;.format(model_path))</span><br><span class="line">    #     for feat_path in glob.glob(os.path.join(test_feat_path, &#39;*.feat&#39;)):</span><br><span class="line">    #         total +&#x3D; 1</span><br><span class="line">    #         data_test &#x3D; joblib.load(feat_path)</span><br><span class="line">    #         data_test_feat &#x3D; data_test[:-1].reshape((1, -1))</span><br><span class="line">    #         result &#x3D; clf.predict(data_test_feat)</span><br><span class="line">    #         if int(result) &#x3D;&#x3D; int(data_test[-1]):</span><br><span class="line">    #             num +&#x3D; 1</span><br><span class="line">    #     rate &#x3D; float(num)&#x2F;total</span><br><span class="line">    #     t1 &#x3D; time.time()</span><br><span class="line">    #     print(&#39;The classification accuracy is %f&#39;%rate)</span><br><span class="line">    #     print(&#39;The cast of time is :%f&#39;%(t1-t0))</span><br><span class="line">    #</span><br></pre></td></tr></table></figure>
<p><strong>extract_feature_from_fasterrcnn_labeled.py</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">extract image hog feature from image labeled  for fasterrcnn</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">from skimage import feature,exposure</span><br><span class="line">from config import *</span><br><span class="line">import imutils</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def read_label_info_from_file(txt_file):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    read label information from txt file.</span><br><span class="line">    content of file should looks like:</span><br><span class="line">            9,3</span><br><span class="line">            56,103,261,317,0,yinliao</span><br><span class="line">            261,0,442,120,2,kele</span><br><span class="line">            ...</span><br><span class="line">    :param txt_file:</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    id_and_rect &#x3D; &#123;&#125;</span><br><span class="line">    with open(txt_file,&#39;r&#39;) as label_info:</span><br><span class="line">        lines &#x3D; label_info.readlines()</span><br><span class="line">        for line in lines[1:]:</span><br><span class="line">            infos &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">            xmin,ymin,xmax,ymax,class_id &#x3D; int(infos[0].strip()),int(infos[1].strip()),int(infos[2].strip()),int(infos[3].strip()),str(infos[4].strip())</span><br><span class="line">            if not class_id  in id_and_rect:</span><br><span class="line">                id_and_rect[class_id] &#x3D; [(xmin,ymin,xmax,ymax)]</span><br><span class="line">            else:</span><br><span class="line">                id_and_rect[class_id].append([xmin, ymin, xmax, ymax])</span><br><span class="line">    return id_and_rect</span><br><span class="line"></span><br><span class="line">def generate_neg_box(im_width,im_height,pos_boxes,times &#x3D; 5):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">        generate negative roi from image. negative roi should not in pos_boxes</span><br><span class="line"></span><br><span class="line">    :param im_height:   height of original image</span><br><span class="line">    :param im_width:    width of original image</span><br><span class="line">    :param pos_boxes:   positive roi boxes(xmin,ymin,xmax,ymax)</span><br><span class="line">    :param times:       times of negative vs positive boxes</span><br><span class="line">    :return:            a list of negative roi [(xmin,ymin,xmax,ymax),...]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    neg_boxes &#x3D; []</span><br><span class="line">    min_size &#x3D; min(im_width,im_height)</span><br><span class="line">    mask &#x3D; np.ones((im_width,im_height))</span><br><span class="line">    for (xmin,ymin,xmax,ymax) in pos_boxes:</span><br><span class="line">        mask[xmin:xmax,ymin:ymax] &#x3D; 0</span><br><span class="line"></span><br><span class="line">    for (xmin,ymin,xmax,ymax) in pos_boxes:</span><br><span class="line">        tmp_width &#x3D;  xmax-xmin</span><br><span class="line">        tmp_height &#x3D; ymax-ymin</span><br><span class="line">        # get $times times of negative boxes for every positive box</span><br><span class="line">        for _ in range(times):</span><br><span class="line">            flag &#x3D; True</span><br><span class="line">            neg_x_min &#x3D; 0</span><br><span class="line">            neg_y_min &#x3D; 0</span><br><span class="line">            start_time &#x3D; time.time()</span><br><span class="line">            while flag:</span><br><span class="line">                neg_x_min &#x3D; random.randint(0,min_size-tmp_width)</span><br><span class="line">                neg_y_min &#x3D; random.randint(0,min_size-tmp_height)</span><br><span class="line">                tmp_rect &#x3D; mask[neg_x_min:(neg_x_min+tmp_width),neg_y_min:(neg_y_min+tmp_height)]</span><br><span class="line">                flag &#x3D; np.any(tmp_rect &#x3D;&#x3D; 0)   # overlap with positive boxes</span><br><span class="line">                end_time &#x3D; time.time()</span><br><span class="line">                #print(end_time-start_time)</span><br><span class="line">                if (end_time-start_time)&gt;5:        # if takes more than 10 seconds,this should be stop</span><br><span class="line">                    print(&quot;time takes more than 10 secs&quot;)</span><br><span class="line">                    flag &#x3D; False</span><br><span class="line"></span><br><span class="line">            neg_boxes.append((neg_x_min,neg_y_min,(neg_x_min+tmp_width),(neg_y_min+tmp_height)))</span><br><span class="line"></span><br><span class="line">    return neg_boxes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def extract_hog_feature(img_file,label_info_file,feat_save_path,with_neg &#x3D; False):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    extract hog feature from one image</span><br><span class="line">    attention: one image may contains several classes of object</span><br><span class="line"></span><br><span class="line">    :param img_file:        image to be extracted</span><br><span class="line">    :param label_info_file: object label information file</span><br><span class="line">    :param feat_save_path: where to save the hog feature file</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    id_and_rect &#x3D; read_label_info_from_file(label_info_file)</span><br><span class="line">    im &#x3D; cv2.imread(img_file)</span><br><span class="line">    gray &#x3D; cv2.cvtColor(im,cv2.COLOR_RGB2GRAY)</span><br><span class="line">    print(gray.shape)</span><br><span class="line">    i &#x3D; 0</span><br><span class="line">    basename &#x3D; os.path.basename(img_file).split(&quot;.&quot;)[0]</span><br><span class="line">    for (id,rect) in id_and_rect.items():</span><br><span class="line">        print(&quot;id&#x3D; &quot;+ str(id))</span><br><span class="line">        target_path &#x3D; os.path.join(feat_save_path,str(id))</span><br><span class="line">        if not os.path.exists(target_path):</span><br><span class="line">            os.mkdir(target_path)</span><br><span class="line">        for rec in rect:</span><br><span class="line">            #print(rec[0],rec[1],rec[2],rec[3])</span><br><span class="line">            roi &#x3D; gray[int(rec[1]):int(rec[3]),int(rec[0]):int(rec[2])]  # caution : the array sequence</span><br><span class="line">            if with_neg and int(id)&lt;3:</span><br><span class="line">                neg_target_path &#x3D; os.path.join(feat_save_path, str(id) + &quot;_neg&quot;)</span><br><span class="line">                if not os.path.exists(neg_target_path):</span><br><span class="line">                    os.mkdir(neg_target_path)</span><br><span class="line">                neg_boxes &#x3D; generate_neg_box(gray.shape[0],gray.shape[1],rect)</span><br><span class="line">                for neg_box in neg_boxes:</span><br><span class="line">                    (xmin, ymin, xmax, ymax) &#x3D; neg_box</span><br><span class="line">                    neg_roi &#x3D; gray[ymin:ymax,xmin:xmax]   # cautious sequence of x and y</span><br><span class="line">                    # print(xmin, ymin, xmax, ymax)</span><br><span class="line">                    # cv2.imshow(&quot;neg_roi&quot;,neg_roi)</span><br><span class="line">                    # cv2.waitKey(0)</span><br><span class="line">                    neg_roi &#x3D; cv2.resize(neg_roi, (200, 250))</span><br><span class="line">                    feat &#x3D; feature.hog(neg_roi, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),</span><br><span class="line">                                       cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">                    joblib.dump(feat, os.path.join(neg_target_path, basename + str(i) + &quot;.feat&quot;))</span><br><span class="line">                    cv2.imwrite(os.path.join(neg_target_path, basename + str(i) + &quot;.jpg&quot;),neg_roi)</span><br><span class="line">                    i &#x3D; i + 1</span><br><span class="line">            # for neg_box in neg_boxes:</span><br><span class="line">            #     (xmin,ymin,xmax,ymax) &#x3D; neg_box</span><br><span class="line">            #     cv2.rectangle(gray,(xmin,ymin),(xmax,ymax),(0,255,0))</span><br><span class="line">            cv2.rectangle(gray,(int(rec[0]),int(rec[1])),(int(rec[2]),int(rec[3])),(255,255,120))</span><br><span class="line">            cv2.imshow(&quot;samples &quot;,gray)</span><br><span class="line">            cv2.imshow(&quot;roi&quot;,roi)</span><br><span class="line">            cv2.waitKey(0)</span><br><span class="line">            roi &#x3D; cv2.resize(roi,(200,250))</span><br><span class="line">            print(&quot;lable is:\t&quot;,id)</span><br><span class="line">            #feat &#x3D; get_hog(roi)</span><br><span class="line">            feat &#x3D; feature.hog(roi, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),</span><br><span class="line">                        cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">            joblib.dump(feat, os.path.join(target_path,basename+str(i)+&quot;.feat&quot;))</span><br><span class="line">            i &#x3D; i+1</span><br><span class="line"></span><br><span class="line">        # cv2.imshow(&quot;samples &quot;,gray)</span><br><span class="line">        # #cv2.imshow(&quot;roi&quot;,roi)</span><br><span class="line">        # cv2.waitKey(0)</span><br><span class="line"></span><br><span class="line">def get_hog(image):</span><br><span class="line">    winSize &#x3D; (64,64)</span><br><span class="line">    image &#x3D; cv2.resize(image,(200,250))</span><br><span class="line">    #winSize &#x3D; (image.shape[1], image.shape[0])</span><br><span class="line">    blockSize &#x3D; (8,8)</span><br><span class="line">    # blockSize &#x3D; (16,16)</span><br><span class="line">    blockStride &#x3D; (8,8)</span><br><span class="line">    cellSize &#x3D; (8,8)</span><br><span class="line">    nbins &#x3D; 9</span><br><span class="line">    derivAperture &#x3D; 1</span><br><span class="line">    winSigma &#x3D; 4.</span><br><span class="line">    histogramNormType &#x3D; 0</span><br><span class="line">    L2HysThreshold &#x3D; 2.0000000000000001e-01</span><br><span class="line">    gammaCorrection &#x3D; 0</span><br><span class="line">    nlevels &#x3D; 64</span><br><span class="line">    hog &#x3D; cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,</span><br><span class="line">                            histogramNormType,L2HysThreshold,gammaCorrection,nlevels)</span><br><span class="line">    winStride &#x3D; (8,8)</span><br><span class="line">    padding &#x3D; (8,8)</span><br><span class="line">    locations &#x3D; [] # (10, 10)# ((10,20),)</span><br><span class="line">    hist &#x3D; hog.compute(image,winStride,padding,locations)</span><br><span class="line">    return hist</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # img_dir &#x3D; r&quot;D:\data\imgs\images&quot;</span><br><span class="line">    # feat_dir &#x3D; r&quot;D:\data\imgs\hog_feats&quot;</span><br><span class="line">    img_dir &#x3D; r&quot;D:\data\test_carmera\svm\images&quot;</span><br><span class="line">    feat_dir &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats&quot;</span><br><span class="line">    for file in os.listdir(img_dir):</span><br><span class="line">        image &#x3D; os.path.join(img_dir,file)</span><br><span class="line">        txt_file &#x3D; image.replace(&quot;images&quot;,&quot;labels&quot;).replace(&quot;.jpg&quot;,&quot;.xml.txt&quot;)</span><br><span class="line">        extract_hog_feature(image,txt_file,feat_dir,with_neg &#x3D;True)</span><br><span class="line">        print(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;    extract hog feature from file %s  done..&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;%image)</span><br><span class="line">    print(&quot;&#x3D;&#x3D;&#x3D;&#x3D;all file have extract done...&quot;)</span><br></pre></td></tr></table></figure>
<p>参考:</p>
<p><a href="http://blog.csdn.net/yjl9122/article/details/72765959" target="_blank" rel="noopener">http://blog.csdn.net/yjl9122/article/details/72765959</a></p>
<p><a href="https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/</a></p>
<p><a href="https://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>merlin语音合成中文前端处理2-实践</title>
    <url>/2018/09/26/merlin-mandarin-text-process-work/</url>
    <content><![CDATA[<h2 id="0-数据示例"><a href="#0-数据示例" class="headerlink" title="0  数据示例"></a>0  数据示例</h2><p>以 THSCH-30数据集为例子。THSCH-30数据集分为两部分<strong>音频</strong>和<strong>文本</strong>。音频文件列表如下:</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_1.png" alt="mtts_mandarin_text">  </p>
<p>文本内容，全部文本存放在一个文件内。内容如下：</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_2.png" alt="mtts_mandarin_text">  </p>
<h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1 数据预处理"></a>1 数据预处理</h2><p>我们先处理文本内容，第一步是数据预处理。主要完成以下操作：</p>
<ol>
<li>去除所有标点符号。</li>
<li>去掉所有数字和字母</li>
<li>替换所有句子结束的标点符号为 <code>#4</code>，即 <code>re.sub(&#39;[,.，。]&#39;, &#39;#4&#39;, txt)</code>。其中的<code>逗号</code>,<code>点号</code>,<code>句号</code>,都替换为符号<code>#4</code>，此处的<code>#4</code>代表了不同的韵律标注层次。具体的不同层次，参考下面的说明</li>
</ol>
<ul>
<li><strong>#0</strong>:  stands for word segment</li>
<li><strong>#1</strong> : stands for prosodic word</li>
<li><strong>#2</strong>:  stands for stressful word (actually in this project we regrad it as #1)</li>
<li><strong>#3</strong>:  stands for prosodic phrase</li>
<li><strong>#4</strong>:   stands for intonational phrase</li>
</ul>
<p>以 下面这句为示例<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;A11_0 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然&#39;</span><br></pre></td></tr></table></figure><br>经过第一步的处理，其中<code>A11_0</code>是句子编号，不会被处理。以空格分割之后会处理后面的中文文本，经过处理，后面的文本没有变化。没有数字和字母，也没有标点符号。</p>
<h2 id="2-添加拼音"><a href="#2-添加拼音" class="headerlink" title="2 添加拼音"></a>2 添加拼音</h2><p>第二步是给每个中文添加拼音标注。使用的是<code>pypinyin</code>。示例代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">numstr, txt &#x3D; line.split(&#39; &#39;)</span><br><span class="line">       txt &#x3D; re.sub(&#39;#\d&#39;, &#39;&#39;, txt)</span><br><span class="line">       pinyin_list &#x3D; pinyin(txt, style&#x3D;Style.TONE3)</span><br><span class="line">       new_pinyin_list &#x3D; []</span><br><span class="line">       for item in pinyin_list:</span><br><span class="line">           if not item:</span><br><span class="line">               logger.warning(</span><br><span class="line">                   &#39;&#123;file_num&#125; do not generate right pinyin&#39;.format(numstr))</span><br><span class="line">           if not item[0][-1].isdigit(): # 对于没有 声调的拼音，统一添加声调5</span><br><span class="line">               phone &#x3D; item[0] + &#39;5&#39;</span><br><span class="line">           else:</span><br><span class="line">               phone &#x3D; item[0]</span><br><span class="line">           new_pinyin_list.append(phone)</span><br><span class="line">       lab_file &#x3D; os.path.join(wav_dir_path, numstr + &#39;.lab&#39;)</span><br><span class="line">       with open(lab_file, &#39;w&#39;) as oid:</span><br><span class="line">           oid.write(&#39; &#39;.join(new_pinyin_list))</span><br></pre></td></tr></table></figure><br>这样，上面的一句中文对应的拼音内容为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2</span><br></pre></td></tr></table></figure>
<p>可以看到，每个字都有对应的拼音。其中，有一步处理是给没有音调的字统一添加为音调5。有些词比如<code>的</code>是没有音调的，统一被添加音调5。这一步会将拼音内容写入一个标注文本<code>A11_0.lab</code></p>
<h2 id="3-强制对齐"><a href="#3-强制对齐" class="headerlink" title="3 强制对齐"></a>3 强制对齐</h2><p>这一步使用了语音识别模型，对语音进行强制对齐。语音识别模型的作用是识别语音中每个字的发音起止时间，并存储为TextGrid格式，这个格式是语音标注软件Praat的标注格式。<br>此步骤依赖以下内容：</p>
<h3 id="3-1-拼音词典"><a href="#3-1-拼音词典" class="headerlink" title="3.1 拼音词典"></a>3.1 拼音词典</h3><p>此步骤依赖的拼音词典是 <code>mandarin_mtts.lexicon</code>，其中的内容如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a1 a1</span><br><span class="line">a2 a2</span><br><span class="line">a3 a3</span><br><span class="line">a4 a4</span><br><span class="line">a5 a5</span><br><span class="line">ai1 ai1</span><br><span class="line">ai2 ai2</span><br><span class="line">ai3 ai3</span><br><span class="line">ai4 ai4</span><br><span class="line">ai5 ai5</span><br><span class="line">an1 an1</span><br><span class="line">an2 an2</span><br><span class="line">an3 an3</span><br><span class="line">an4 an4</span><br><span class="line">an5 an5</span><br><span class="line">ang1 ang1</span><br><span class="line">ang2 ang2</span><br><span class="line">ang3 ang3</span><br><span class="line">ang4 ang4</span><br><span class="line">ang5 ang5</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="3-2-强制对齐工具"><a href="#3-2-强制对齐工具" class="headerlink" title="3.2 强制对齐工具"></a>3.2 强制对齐工具</h3><p>MTTS所使用的强制对齐工具为 <a href="https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/releases/download/v1.0.0/montreal-forced-aligner_linux.tar.gz" target="_blank" rel="noopener">Montreal-Forced-Aligner</a></p>
<h3 id="3-3-强制对齐模型"><a href="#3-3-强制对齐模型" class="headerlink" title="3.3 强制对齐模型"></a>3.3 强制对齐模型</h3><p><strong>预备知识</strong></p>
<p>汉字按照长度可以划分为：<strong>句子</strong>，<strong>短语</strong>，<strong>汉字（音节）</strong>，<strong>音素</strong>。而音素由<strong>声母</strong>，<strong>韵母</strong>，<strong>元音</strong>，<strong>静音</strong>组成。</p>
<p>对齐模型使用的是由THSCH-30数据集所训练的中文语音识别模型，下载地址为 <a href="https://github.com/Jackiexiao/MTTS/releases/download/v0.1/thchs30.zip" target="_blank" rel="noopener">THSCH-30语音识别模型</a></p>
<h3 id="3-4-输出结果"><a href="#3-4-输出结果" class="headerlink" title="3.4 输出结果"></a>3.4 输出结果</h3><p>这一步主要是对语音音频文件处理，并得到识别结果。识别的结果是<strong>直接到音素</strong>，存储为TextGrid格式，示例文本对应的音频文件 <code>A11_0.wav</code>所得到的TextGrid标注文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File type &#x3D; &quot;ooTextFile&quot;</span><br><span class="line">Object class &#x3D; &quot;TextGrid&quot;</span><br><span class="line"></span><br><span class="line">xmin &#x3D; 0.0</span><br><span class="line">xmax &#x3D; 7.8</span><br><span class="line">tiers? &lt;exists&gt;</span><br><span class="line">size &#x3D; 2</span><br><span class="line">item []:</span><br><span class="line">    item [1]:</span><br><span class="line">        class &#x3D; &quot;IntervalTier&quot;</span><br><span class="line">        name &#x3D; &quot;words&quot;</span><br><span class="line">        xmin &#x3D; 0.0</span><br><span class="line">        xmax &#x3D; 7.8</span><br><span class="line">        intervals: size &#x3D; 33</span><br><span class="line">            intervals [1]:</span><br><span class="line">                xmin &#x3D; 0.0</span><br><span class="line">                xmax &#x3D; 1.100</span><br><span class="line">                text &#x3D; &quot;&quot;</span><br><span class="line">            intervals [2]:</span><br><span class="line">                xmin &#x3D; 1.100</span><br><span class="line">                xmax &#x3D; 1.350</span><br><span class="line">                text &#x3D; &quot;lv4&quot;</span><br><span class="line">            intervals [3]:</span><br><span class="line">                xmin &#x3D; 1.350</span><br><span class="line">                xmax &#x3D; 1.460</span><br><span class="line">                text &#x3D; &quot;shi4&quot;</span><br><span class="line">         ....</span><br><span class="line">            intervals [59]:</span><br><span class="line">                xmin &#x3D; 6.980</span><br><span class="line">                xmax &#x3D; 7.150</span><br><span class="line">                text &#x3D; &quot;ang4&quot;</span><br><span class="line">            intervals [60]:</span><br><span class="line">                xmin &#x3D; 7.150</span><br><span class="line">                xmax &#x3D; 7.280</span><br><span class="line">                text &#x3D; &quot;r&quot;</span><br><span class="line">            intervals [61]:</span><br><span class="line">                xmin &#x3D; 7.280</span><br><span class="line">                xmax &#x3D; 7.430</span><br><span class="line">                text &#x3D; &quot;an2&quot;</span><br><span class="line">            intervals [62]:</span><br><span class="line">                xmin &#x3D; 7.430</span><br><span class="line">                xmax &#x3D; 7.780</span><br><span class="line">                text &#x3D; &quot;sp&quot;</span><br><span class="line">            intervals [63]:</span><br><span class="line">                xmin &#x3D; 7.780</span><br><span class="line">                xmax &#x3D; 7.8</span><br><span class="line">                text &#x3D; &quot;&quot;</span><br></pre></td></tr></table></figure>
<p>可以看到，一共识别到了64个字。其中解析如下：</p>
<ul>
<li><strong>xmin </strong>:当前字的开始时间，单位为秒</li>
<li><strong>xmax</strong>：当前字的结束时间，单位为秒</li>
<li><strong>text</strong>：当前字的拼音和音调。</li>
</ul>
<h2 id="4-TextGrid标注格式转换为SFS格式"><a href="#4-TextGrid标注格式转换为SFS格式" class="headerlink" title="4 TextGrid标注格式转换为SFS格式"></a>4 TextGrid标注格式转换为SFS格式</h2><p>SFS即为声韵母标注，主要将每个字的音素标注为以下三类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>s</strong></td>
<td>时长超过100ms的静音</td>
<td>sil,sp</td>
</tr>
<tr>
<td><strong>d</strong></td>
<td>时长短于100ms的静音</td>
<td>-</td>
</tr>
<tr>
<td><strong>a</strong></td>
<td>辅音</td>
<td>包含```b’, ‘p’, ‘m’, ‘f’, ‘d’, ‘t’, ‘n’, ‘l’, ‘g’, ‘k’, ‘h’, ‘j’, ‘q’, ‘x’, ‘zh’, ‘ch’, ‘sh’, ‘r’, ‘z’, ‘c’, ‘s’, ‘y’, ‘w’</td>
</tr>
<tr>
<td><strong>b</strong></td>
<td>元音</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>同时将发音起止时间的单位从秒更改为 纳秒，即乘以10的6次方。上一步得到的TextGrid格式的标注转换为sfs格式之后，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11000000 s</span><br><span class="line">12400000 a</span><br><span class="line">13500000 b</span><br><span class="line">....</span><br><span class="line">67500000 b</span><br><span class="line">68500000 a</span><br><span class="line">69800000 b</span><br><span class="line">71500000 b</span><br><span class="line">72800000 a</span><br><span class="line">74300000 b</span><br><span class="line">77800000 s</span><br></pre></td></tr></table></figure>
<h2 id="5-sfs到真实标注文件"><a href="#5-sfs到真实标注文件" class="headerlink" title="5 sfs到真实标注文件"></a>5 sfs到真实标注文件</h2><p>此步骤依赖于<strong>sfs标注文件</strong>和<strong>原始中文文本</strong>。上面的步骤示例中的A11_0文本和sfs标注即可。得到的标注结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">12400000 13500000 sil^l-v4+sh&#x3D;ih4@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">13500000 14300000 l^v4-sh+ih4&#x3D;y@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">14300000 14600000 v4^sh-ih4+y&#x3D;iang2@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">...</span><br><span class="line">66900000 67500000 ei4^sh-ih1+y&#x3D;i4@ih@&#x2F;A:4-1^4@&#x2F;B:26+3@1^2^27+4#27-4-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">67500000 68500000 sh^ih1-y+i4&#x3D;ang4@i@&#x2F;A:1-4^4@&#x2F;B:27+2@2^1^28+3#28-3-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">68500000 69800000 ih1^y-i4+ang4&#x3D;r@i@&#x2F;A:1-4^4@&#x2F;B:27+2@2^1^28+3#28-3-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">69800000 71500000 y^i4-ang4+r&#x3D;an2@ang@&#x2F;A:4-4^2@&#x2F;B:28+1@1^2^29+2#29-2-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">71500000 72800000 i4^ang4-r+an2&#x3D;sil@an@&#x2F;A:4-2^xx@&#x2F;B:29+0@2^1^30+1#30-1-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">72800000 74300000 ang4^r-an2+sil&#x3D;xx@an@&#x2F;A:4-2^xx@&#x2F;B:29+0@2^1^30+1#30-1-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">74300000 77800000 r^an2-sil+xx&#x3D;xx@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br></pre></td></tr></table></figure>
<h3 id="5-1-中文分词，词性标注，韵律标注"><a href="#5-1-中文分词，词性标注，韵律标注" class="headerlink" title="5.1 中文分词，词性标注，韵律标注"></a>5.1 中文分词，词性标注，韵律标注</h3><p>如果原中文内容里没有进行韵律标注，韵律标注以<strong>#</strong>分割。就会默认所有的分词结果里的每个词都是<strong>#0</strong>，但是最后一个是<code>#4</code>,即最后一个代表当前语句结束。参照第数据预处理部分的韵律不同值代表的不同意义。<br>第一步是对输入的原始中文进行分词，还是以上面步骤的示例文本为例，使用jieba分词的posseg分词和词性标注得到的结果如下：</p>
<ul>
<li><strong>分词结果</strong>：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;绿&#39;, &#39;是&#39;, &#39;阳春&#39;, &#39;烟景&#39;, &#39;大块文章&#39;, &#39;的&#39;, &#39;底色&#39;, &#39;四月&#39;, &#39;的&#39;, &#39;林峦&#39;, &#39;更是&#39;, &#39;绿&#39;, &#39;得&#39;, &#39;鲜活&#39;, &#39;秀媚&#39;, &#39;诗意&#39;, &#39;盎然&#39;]</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>词性标注结果</strong>：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;a&#39;, &#39;v&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;u&#39;, &#39;n&#39;, &#39;m&#39;, &#39;u&#39;, &#39;n&#39;, &#39;d&#39;, &#39;a&#39;, &#39;u&#39;, &#39;a&#39;, &#39;a&#39;, &#39;n&#39;, &#39;z&#39;]</span><br></pre></td></tr></table></figure></li>
<li><p><strong>韵律标注结果</strong>： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#4&#39;] （当前没有预标注韵律，默认所有分词都是#0，且最后一个为#4）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>音节分解结果</strong>：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&#39;l&#39;, &#39;v4&#39;), (&#39;sh&#39;, &#39;ih4&#39;), (&#39;y&#39;, &#39;iang2&#39;), (&#39;ch&#39;, &#39;un1&#39;), (&#39;y&#39;, &#39;ian1&#39;), (&#39;j&#39;, &#39;ing3&#39;), (&#39;d&#39;, &#39;a4&#39;), (&#39;k&#39;, &#39;uai4&#39;), (&#39;w&#39;, &#39;uen2&#39;), (&#39;zh&#39;, &#39;ang1&#39;), (&#39;d&#39;, &#39;e5&#39;), (&#39;d&#39;, &#39;i3&#39;), (&#39;s&#39;, &#39;e4&#39;), (&#39;s&#39;, &#39;ic4&#39;), (&#39;y&#39;, &#39;ve4&#39;), (&#39;d&#39;, &#39;e5&#39;), (&#39;l&#39;, &#39;in2&#39;), (&#39;l&#39;, &#39;uan2&#39;), (&#39;g&#39;, &#39;eng4&#39;), (&#39;sh&#39;, &#39;ih4&#39;), (&#39;l&#39;, &#39;v4&#39;), (&#39;d&#39;, &#39;e2&#39;), (&#39;x&#39;, &#39;ian1&#39;), (&#39;h&#39;, &#39;uo2&#39;), (&#39;x&#39;, &#39;iu4&#39;), (&#39;m&#39;, &#39;ei4&#39;), (&#39;sh&#39;, &#39;ih1&#39;), (&#39;y&#39;, &#39;i4&#39;), (&#39;ang4&#39;,), (&#39;r&#39;, &#39;an2&#39;)]</span><br></pre></td></tr></table></figure>
<h3 id="5-2-获取音素类型和时间"><a href="#5-2-获取音素类型和时间" class="headerlink" title="5.2 获取音素类型和时间"></a>5.2 获取音素类型和时间</h3><p>分为两种情况，有sfs标注文件的和没有的。</p>
<p><strong>使用sfs标注文件</strong><br>sfs标注文件中每一行都是如下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11000000 s</span><br><span class="line">12400000 a</span><br><span class="line">13500000 b</span><br><span class="line">..</span><br></pre></td></tr></table></figure>
<p>每一行以空格分割，分别代表了当前第i个音素的<strong>开始时间</strong>和<strong>音素类型</strong>。音素类型参考第4节的表。分别读取并保存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">音素类型列表：phs_type &#x3D;   [&#39;s&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;s&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;s&#39;]</span><br><span class="line">音素起止时间列表: times &#x3D; [&#39;0&#39;, 11000000, 12400000, 13500000, 14300000, 14600000, 15100000, 16500000, 17600000, 18300000, 19000000, 20400000, 21400000, 23100000, 24100000, 25200000, 26200000, 26900000, 27799999, 28700000, 29600000, 30299999, 31000000, 31500000, 32200000, 33200000, 34800000, 37400000, 37700000, 39100000, 39900000, 40700000, 41500000, 42200000, 42699999, 43500000, 44300000, 45500000, 47300000, 48300000, 49100000, 50900000, 52000000, 53000000, 53700000, 54300000, 54600000, 56000000, 57100000, 58700000, 59900000, 61500000, 62699999, 63900000, 65199999, 66900000, 67500000, 68500000, 69800000, 71500000, 72800000, 74300000, 77800000]</span><br></pre></td></tr></table></figure>
<p>音素起止时间列表(63)比音素类型列表(62)多一个，开始时间0。</p>
<p><strong>没有sfs标注文件</strong></p>
<p>如果没有sfs标注的时间，程序可以自动生成,方法是计算得到语句的所有因素长度，并将默认起止时间都设置为0.，音素类型都默认设置为<code>a</code>。</p>
<p>参考韵律列表。韵律列表其实就是当前语句被分词之后，每个词的停顿间隙。以<strong>#0,#1,#2,#3,#4</strong>标识，分别代表了不同层次的韵律。当前示例语句被分词为如下</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_3.png" alt="mtts_mandarin_text">  </p>
<p>原始语句如下，没有带任何标点符号：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然</span><br></pre></td></tr></table></figure><br>分词结果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">语句分词结果：  [&#39;绿&#39;, &#39;是&#39;, &#39;阳春&#39;, &#39;烟景&#39;, &#39;大块文章&#39;, &#39;的&#39;, &#39;底色&#39;, &#39;四月&#39;, &#39;的&#39;, &#39;林峦&#39;, &#39;更是&#39;, &#39;绿&#39;, &#39;得&#39;, &#39;鲜活&#39;, &#39;秀媚&#39;, &#39;诗意&#39;, &#39;盎然&#39;]</span><br></pre></td></tr></table></figure><br>对应的韵律列表如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#4&#39;]</span><br></pre></td></tr></table></figure><br>比如对分词结果 <code>阳春</code>得到的音素为列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&#39;y&#39;, &#39;iang2&#39;), (&#39;ch&#39;, &#39;un1&#39;)]</span><br></pre></td></tr></table></figure>
<p>长度为4，向音素类型列表中添加对应长度的默认值<code>a</code>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">phs_type &#x3D;  [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;] (后面四个是当前词&#96;阳春&#96;的音素对应的音素类型)</span><br></pre></td></tr></table></figure>
<p>最后得到的音素起止时间列表和音素类型列表分别为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">time&#x3D;  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">phs_type&#x3D;[&#39;s&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;s&#39;]</span><br></pre></td></tr></table></figure>
<p>可以看到，与有sfs对比，长度一致。都是对应音素数目的长度，只不过没有sfs文件的都是默认值。</p>
<h2 id="6-音素状态决策树"><a href="#6-音素状态决策树" class="headerlink" title="6 音素状态决策树"></a>6 音素状态决策树</h2><p>上面已经得到了音素对应的起止时间和音素类型，下一步是构建音素状态决策树。以英文为示意图，如下：</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_4.png" alt="mtts_mandarin_text">  </p>
<ol>
<li>首先，一整句话是有前后承接关系的，当<strong><em>绿是阳春</em></strong>这几个词出现时，<code>绿</code>字是<code>是</code>前缀，<code>阳</code>字是<code>是</code>字的后缀。这只是字面上的上下文关系，当前需要构建音素级别的上下文承接关系。所以需要进一步细化。<br>2.韵律标注层次有 ,由小到大（以<a href="https://github.com/Jackiexiao/MTTS" target="_blank" rel="noopener">MTTS</a>前端为例，其他标注格式不一定，<a href="http://www.data-baker.com/open_source.html" target="_blank" rel="noopener">标贝数据</a>只有,#1,#2,#3,#4）。<ul>
<li>音素: phn</li>
<li>音节:syllables</li>
<li>词:   #0</li>
<li>短语:  #1</li>
<li>句子: #3</li>
<li>句子结束: #4 </li>
</ul>
</li>
</ol>
<h3 id="6-1-MTTS的上下文标注"><a href="#6-1-MTTS的上下文标注" class="headerlink" title="6.1 MTTS的上下文标注"></a>6.1 MTTS的上下文标注</h3><p><strong>说明</strong></p>
<ul>
<li>没有设计语调短语层和段落层</li>
<li>也没有设置重音标注</li>
<li>@&amp;#$!^-+=以及/A:/B:…的使用主要是为了正则表达式匹配方便，10个符号(@&amp;#$!^-+=)共有100个匹配组合，即可以匹配100个属性</li>
<li>如果前后位置的基元不存在的话，用xx代替，例如 xx^sil-w+o=sh </li>
</ul>
<p>标注文件会标记不同韵律层次所有的上下文信息，详细可以参考下面的两张表：</p>
<p><strong>不同层级和对应的标注格式</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层级</th>
<th>标注格式  </th>
</tr>
</thead>
<tbody>
<tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6@  </td>
</tr>
<tr>
<td>声调层</td>
<td>/A:a1-a2^a3@  </td>
</tr>
<tr>
<td>字/音节层</td>
<td>/B:b1+b2@b3^b4^b5+b6#b7-b8-  </td>
</tr>
<tr>
<td>词层</td>
<td>/C:c1_c2^c3#c4+c5+c6&amp;  </td>
</tr>
<tr>
<td>韵律词层</td>
<td>/D:d1=d2!d3@d4-d5&amp;  </td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/E:e1</td>
<td>e2-e3@e4#e5&amp;e6!e7-e8#  </td>
</tr>
<tr>
<td>语句层</td>
<td>/F:f1^f2=f3_f4-f5!  </td>
</tr>
</tbody>
</table>
</div>
<p>更加细致的划分（基元代表了不同层次的单元，可以是音素，也可以是音节，声调等）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前音节的元音</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>a1</td>
<td>前一音节/字的声调</td>
</tr>
<tr>
<td>a2</td>
<td>当前音节/字的声调</td>
</tr>
<tr>
<td>a3</td>
<td>后一音节/字的声调</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节/字到语句开始字的距离</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节/字到语句结束字的距离</td>
</tr>
<tr>
<td>b3</td>
<td>当前音节/字在词中的位置（正序）</td>
</tr>
<tr>
<td>b4</td>
<td>当前音节/字在词中的位置（倒序）</td>
</tr>
<tr>
<td>b5</td>
<td>当前音节/字在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节/字在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节/字在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节/字在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>c1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>c2</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>c3</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>c4</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>c5</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>c6</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>d1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>d2</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>d3</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>d4</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>d5</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>e1</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>e2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>e4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e5</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e6</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e7</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>e8</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>f1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>f2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>f3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>f4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>f5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody>
</table>
</div>
<p>以英文单词 author为例</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_5.png" alt="mtts_mandarin_text">  </p>
<p>体现在代码里面的标准公式化字符串如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">formation&#x3D;[</span><br><span class="line">    &#39; &#39;, &#39; &#39;,                                      #  开始时间，结束时间（上图中没有出现）</span><br><span class="line">    &#39;^&#39;, &#39;-&#39;, &#39;+&#39;, &#39;&#x3D;&#39;, &#39;@&#39;, &#39;@&#x2F;A:&#39;,                 # 连接符</span><br><span class="line">    &#39;-&#39;, &#39;^&#39;, &#39;@&#x2F;B:&#39;, </span><br><span class="line">    &#39;+&#39;, &#39;@&#39;, &#39;^&#39;, &#39;^&#39;, &#39;+&#39;, &#39;#&#39;, &#39;-&#39;, &#39;-&#x2F;C:&#39;, </span><br><span class="line">    &#39;_&#39;, &#39;^&#39;, &#39;#&#39;, &#39;+&#39;, &#39;+&#39;, &#39;&amp;&#x2F;D:&#39;, </span><br><span class="line">    &#39;&#x3D;&#39;, &#39;!&#39;, &#39;@&#39;, &#39;-&#39;, &#39;&amp;&#x2F;E:&#39;, </span><br><span class="line">    &#39;|&#39;, &#39;-&#39;, &#39;@&#39;, &#39;#&#39;, &#39;&amp;&#39;, &#39;!&#39;, &#39;-&#39;, &#39;#&#x2F;F:&#39;, </span><br><span class="line">    &#39;^&#39;, &#39;&#x3D;&#39;, &#39;_&#39;, &#39;-&#39;, &#39;!&#39;]</span><br></pre></td></tr></table></figure>
<p>示例中文的标注结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">12400000 13500000 sil^l-v4+sh&#x3D;ih4@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!&#39;</span><br><span class="line">13500000 14300000 l^v4-sh+ih4&#x3D;y@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!&#39;</span><br><span class="line">...</span><br><span class="line">74300000 77800000 r^an2-sil+xx&#x3D;xx@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!&#39;</span><br></pre></td></tr></table></figure>
<p>以第一行和第二行为例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br></pre></td></tr></table></figure></p>
<ul>
<li>注意：第一行其实是静音。因为任何一句话的开头都是静音，所以第一行的所有标注基元都是<strong>xx</strong>，代表了不存在的基元。</li>
<li>第二行才是字<strong>绿</strong>的开始，首先<strong>绿</strong>字被拆分为音素<strong>l</strong>和<strong>v4</strong></li>
<li>11000000和12400000代表了该字的开始和结束时间。</li>
<li><p><strong>xx^sil-l+v4=sh@v</strong>,是声母韵母层级的标注，(上表中的p)依次是：</p>
<ul>
<li><code>前前基元</code>（不存在，以xx代表）</li>
<li><code>连接符</code> :<strong>^</strong></li>
<li><code>前一基元</code>:<strong>sil</strong>（一句话的开始都是sil）</li>
<li><code>连接符</code>:<strong>-</strong></li>
<li><code>当前基元</code>:<strong>l</strong></li>
<li><code>连接符</code> :<em>*+</em></li>
<li><code>后一基元</code>: <strong>v4</strong></li>
<li><code>连接符</code>:<strong>=</strong></li>
<li><code>后后基元</code>:<strong>sh)</strong></li>
<li><code>连接符</code>:<strong>@</strong></li>
<li><code>当前音节的元音</code>:<strong>v</strong></li>
</ul>
</li>
<li><p><strong>@/A:xx-4^4</strong>，是声调层级标注(上标中的a): 依次是</p>
<ul>
<li><code>字调标注的开始</code>:<strong>@/A</strong></li>
<li><code>前一音节的字调</code>: <strong>xx</strong> (不存在的xx)</li>
<li><code>连接符</code>:<strong>-</strong></li>
<li><code>当前音节的字调</code>:<strong>4</strong>((绿)4)</li>
<li><code>连接符</code>:<strong>^</strong></li>
<li><code>后一个音节的字调</code>:<strong>4</strong>((是)4)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
      <tags>
        <tag>语音合成</tag>
        <tag>中文前端</tag>
      </tags>
  </entry>
  <entry>
    <title>merlin语音合成中文前端处理1-理论</title>
    <url>/2018/09/25/merlin-mandarin-text-process/</url>
    <content><![CDATA[<h2 id="0-概述"><a href="#0-概述" class="headerlink" title="0 概述"></a>0 概述</h2><p>语音合成过程，需要处理两部分内容，分别是：</p>
<ul>
<li><p>文本(Text)处理： 假设我们的输入是<code>你好看啊</code></p>
</li>
<li><p>音频(speech)处理： 对应<code>你好看啊.wav</code></p>
</li>
</ul>
<h2 id="1-文本处理"><a href="#1-文本处理" class="headerlink" title="1 文本处理"></a>1 文本处理</h2><h3 id="1-1-规范化"><a href="#1-1-规范化" class="headerlink" title="1.1 规范化"></a>1.1 规范化</h3><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等</p>
<p>有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音[7]。例如</p>
<ul>
<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”；</li>
<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>
</ul>
<p>对于英文而言，如：</p>
<ul>
<li><strong>类别为年份（NYER）</strong>： 2011 $\rightarrow$ twenty eleven</li>
<li><strong>类别为货币(MONEY)</strong>: £100 $\rightarrow$  one hundred pounds</li>
<li><strong>类别为非单词，需要拟音(ASWD)</strong>:  IKEA $\rightarrow$  apply letter-to-sound</li>
<li><strong>类别为数字(NUM)</strong> : 100 NUM $\rightarrow$ one hundred</li>
<li><strong>类别为字母(LSEQ)</strong> :  DVD  $\rightarrow$ dee vee dee</li>
</ul>
<h3 id="1-2-转化为拼音"><a href="#1-2-转化为拼音" class="headerlink" title="1.2 转化为拼音"></a>1.2 转化为拼音</h3><p>参考<a href="http://www.moe.edu.cn/s78/A19/yxs_left/moe_810/s230/195802/t19580201_186000.html" target="_blank" rel="noopener">国家汉语拼音方案</a></p>
<p>使用一个汉语拼音词典，将<code>你好看啊</code>转换为： <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>。此过程需要注意有些多音词需要处理，可以只是使用python的<strong>pypinyin</strong></p>
<h3 id="1-3-拼音转换为音调表示"><a href="#1-3-拼音转换为音调表示" class="headerlink" title="1.3 拼音转换为音调表示"></a>1.3 拼音转换为音调表示</h3><p>目前支持将汉语拼音中的<code>一</code>,<code>二</code>,<code>三</code>,<code>四</code>声转换为 <code>1</code>,<code>2</code>,<code>3</code>,<code>4</code>,<code>5</code>（5代表轻声）</p>
<p> <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>$\rightarrow$ <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<p>事实上<strong>pypinyin</strong>可以一步从<code>你好看啊</code>转换为 <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<h3 id="1-4-将音节分解为音素"><a href="#1-4-将音节分解为音素" class="headerlink" title="1.4  将音节分解为音素"></a>1.4  将音节分解为音素</h3><p>音素为汉语拼音的最小单元。包括<code>声母</code>,<code>韵母</code>,但是其中还会有一些整体认读音节。(注意：下面所列并非官方标准版，不同情形可以采取不同取舍，参考<a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">MTTS文本分析</a>)</p>
<p><strong>整体认读音节</strong></p>
<p>16个整体认读音节分别是：<code>zhi 、chi、shi、ri、zi、ci、si、yi、wu、yu、ye、yue、yuan、yin 、yun、ying</code>，但是要注意没有yan，因为yan并不发作an音</p>
<p><strong>声母（23个）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b p m f d t n l g k h j q x zh ch sh r z c s y w</span><br></pre></td></tr></table></figure>
<p><strong>韵母（39个）</strong></p>
<ul>
<li>单韵母 a、o、e、 ê、i、u、ü、-i（前）、-i（后）、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 üe、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 üan 、en、in、uen、 ün 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<p><strong>韵母（39个）（转换标注后）</strong></p>
<ul>
<li>单韵母 a、o、e、ea、i、u、v、ic、ih、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 ve、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 van 、en、in、uen、 vn 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<h3 id="1-5-结果"><a href="#1-5-结果" class="headerlink" title="1.5 结果"></a>1.5 结果</h3><p>此步骤的结果为 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&#39;n&#39;, &#39;i3&#39;), (&#39;h&#39;, &#39;ao3&#39;), (&#39;k&#39;, &#39;an4&#39;), (&#39;a5&#39;,)]</span><br></pre></td></tr></table></figure>
<h2 id="2-合成基元选取"><a href="#2-合成基元选取" class="headerlink" title="2 合成基元选取"></a>2 合成基元选取</h2><p>合成基元就是合成语音所需的最小单元。由大到小来说：</p>
<ol>
<li>可以选择每个汉字，一共有6万多，会导致需要很大的训练集</li>
<li>可以选择所有拼音，数量会比汉字少很多</li>
<li>也可以选择声韵母，声韵母是组成音节的单元，21个声母+39个韵母，数据量大幅度减少。</li>
</ol>
<p>在实际语音中除了这些文本上的内容之外，还会存在开始和结束的<strong>静音</strong>，标点符号之间存在的<strong>短暂停顿</strong>。所以我们可以采取以下这套合成基元方案。</p>
<ul>
<li><strong>声母</strong>：  21个声母+wy（共23个）</li>
<li><strong>韵母</strong>： 39个韵母</li>
<li><strong>静音</strong>：<code>sil</code>, <code>pau</code>, <code>sp</code>。sil(silence) 表示句首和句尾的静音，pau(pause) 表示由逗号，顿号造成的停顿，句中其他的短停顿为sp(short pause)</li>
</ul>
<h2 id="3-上下文相关标注"><a href="#3-上下文相关标注" class="headerlink" title="3  上下文相关标注"></a>3  上下文相关标注</h2><p>上下文相关标注的规则要综合考虑有哪些上下文对当前音素发音的影响，总的来说，需要考虑发音基元及其前后基元的信息，以及发音基元所在的音节、词、韵律词、韵律短语、语句相关的信息。</p>
<p>此类标注对于不同任务可以自由设计，一种参考是<a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/mandarin_example_label.md" target="_blank" rel="noopener">MTTS普通话标注示例</a>。这里将参考中的一些内容作出一些解释：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层级（由小到达）</th>
<th>标注格式</th>
</tr>
</thead>
<tbody>
<tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6_p7</td>
</tr>
<tr>
<td>.</td>
<td>/A:a1_a2-a3_a4#a5</td>
</tr>
<tr>
<td>音节层</td>
<td>/B:b1_b2!b3_b4#b5@b6!b7+b8@b9#b10_b11</td>
</tr>
<tr>
<td>.</td>
<td>/C:c1+c2-c3=c4#c5</td>
</tr>
<tr>
<td>词层</td>
<td>/D:d1-d2 /E:e1&amp;e2^e3_e4 /F:f1-f2</td>
</tr>
<tr>
<td>韵律层</td>
<td>/G:g1-g2 /H:h1-h2@h3+h4 /I:i1-i2</td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/J:j1^j2=j3-j4 /K:k1=k2_k3^k4&amp;k5_k6 /L:l1^l2#l3-l4</td>
</tr>
<tr>
<td>语句层</td>
<td>/M:m1#m2+m3+m4!m5</td>
</tr>
</tbody>
</table>
</div>
<p>下面的（发音）基元指的是声韵母，HMM建模选用的单元是音节</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前基元在当前音节的位置（正序）</td>
</tr>
<tr>
<td>p7</td>
<td>当前基元在当前音节的位置（倒序） </td>
</tr>
<tr>
<td>a1</td>
<td>前一音节的首基元</td>
</tr>
<tr>
<td>a2</td>
<td>前一音节的末基元</td>
</tr>
<tr>
<td>a3，a4</td>
<td>前一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>前一音节的基元数目</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节的首基元</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节的末基元</td>
</tr>
<tr>
<td>b3，b4</td>
<td>当前音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>当前音节的基元数目</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节在词中的位置（正序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节在词中的位置（倒序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b9</td>
<td>当前音节在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b10</td>
<td>当前音节在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b11</td>
<td>当前音节在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>c1</td>
<td>后一音节的首基元</td>
</tr>
<tr>
<td>c2</td>
<td>后一音节的末基元</td>
</tr>
<tr>
<td>c3，c4</td>
<td>后一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>c5</td>
<td>后一音节的基元数目</td>
</tr>
<tr>
<td>d1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>d2</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>e1</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>e2</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>当前词在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>e4</td>
<td>当前词在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>f1</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>f2</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>g1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>g2</td>
<td>前一个韵律词的词数目</td>
</tr>
<tr>
<td> —-</td>
<td>——</td>
</tr>
<tr>
<td>h1</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>h2</td>
<td>当前韵律词的词数目</td>
</tr>
<tr>
<td>h3</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>h4</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td> —-</td>
<td>—</td>
</tr>
<tr>
<td>i1</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>i2</td>
<td>后一个韵律词的词数目</td>
</tr>
<tr>
<td> —</td>
<td>—-</td>
</tr>
<tr>
<td>j1</td>
<td>前一韵律短语的语调类型</td>
</tr>
<tr>
<td>j2</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>j3</td>
<td>前一韵律短语的词数目</td>
</tr>
<tr>
<td>j4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td> —-</td>
<td>——</td>
</tr>
<tr>
<td>k1</td>
<td>当前韵律短语的语调类型</td>
</tr>
<tr>
<td>k2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>k3</td>
<td>当前韵律短语的词数目</td>
</tr>
<tr>
<td>k4</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>k5</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>k6</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td> —-</td>
<td>—-</td>
</tr>
<tr>
<td>l1</td>
<td>后一韵律短语的语调类型</td>
</tr>
<tr>
<td>l2</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>l3</td>
<td>后一韵律短语的词数目</td>
</tr>
<tr>
<td>l4</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td> —-</td>
<td>—-</td>
</tr>
<tr>
<td>m1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>m2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>m3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>m4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>m5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody>
</table>
</div>
<h2 id="4-问题集设计"><a href="#4-问题集设计" class="headerlink" title="4 问题集设计"></a>4 问题集设计</h2><p>问题集(Question Set)即是决策树中条件判断的设计。问题集通常很大，由几百个判断条件组成。</p>
<p>问题集的设计依赖于不同语言的语言学知识，而且<strong>与上下文标注文件相匹配，改变上下文标注方法也需要相应地改变问题集</strong>，对于中文语音合成而言，问题集的设计的规则有:</p>
<ul>
<li><strong>前前个，前个，当前，下个，下下个声韵母分别是某个合成基元吗</strong>，合成基元共有65个(23声母+39韵母+3静音)，例如判断是否是元音a QS “LL-a” QS “L-a” QS “C-a” QS “R-a” QS “RR-a”</li>
<li><strong>声母特征划分</strong>，例如声母可以划分成塞音，擦音，鼻音，唇音等，声母特征划分24个</li>
<li><strong>韵母特征划分</strong>，例如韵母可以划分成单韵母，复合韵母，分别包含aeiouv的韵母，韵母特征划分8个</li>
<li><strong>其他信息划分</strong>，词性划分，26个词性; 声调类型，5个; 是否是声母或者韵母或者静音，3个</li>
<li><strong>韵律特征划分</strong>，如是否是重音，重音和韵律词/短语的位置数量</li>
<li><strong>位置和数量特征划分</strong></li>
</ul>
<p>对于三音素模型而言，对于每个划分的特征，都会产生3个判断条件，该音素是否满足条件，它的左音素（声韵母）和右音素（声韵母）是否满足条件，有时会扩展到左左音素和右右音素的情况，这样就有5个问题。其中，每个问题都是以 QS 命令开头，问题集的答案可以有多个，中间以逗号隔开，答案是一个包含通配符的字符串。当问题表达式为真时，该字符串成功匹配标注文件中的某一行标注。格式如：</p>
<p>QS 问题表达式 {答案 1，答案 2，答案 3，……}</p>
<p>QS “LL==Fricative” {f^<em>,s^</em>,sh^<em>,x^</em>,h^<em>,lh^</em>,hy^<em>,hh^</em>}</p>
<p>对于3音素上下文相关的基元模型的3个问题，例如： <em> 判断当前，前接，后接音素/单元是否为擦音 </em> QS ‘C_Fricative’ <em> QS ‘L_Fricative’ </em> QS ‘R_Fricative’</p>
<p>问题集示例参考 <a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/question.md" target="_blank" rel="noopener">MTTS问题集设计参考</a></p>
<p>值得注意的是，merlin中使用的问题集和HTS中有所不同，Merlin中新增加了CQS问题，Merlin处理Questions Set 的模块在merlin/src/frontend/label_normalisation 中的Class HTSLabelNormalisation</p>
<p><strong>Question Set 的格式是</strong></p>
<p>QS + 一个空格 + “question<em>name” + 任意空格+ {Answer1, answer2, answer3…} # 无论是QS还是CQS的answer中，前后的**不用加，加了也会被去掉 CQS + 一个空格 + “question_name” + 任意空格+ {Answer} #对于CQS，这里只能有一个answer 比如 CQS C-Syl-Tone {</em>(d+)+} merlin也支持浮点数类型，只需改为CQS C-Syl-Tone {_([d.]+)+}</p>
<p>参考 ：  <a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html</a></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>merlin语音合成方案mandarin_voice操作步骤</title>
    <url>/2018/09/23/merlin-mandain-voice-op/</url>
    <content><![CDATA[<h3 id="0-概览"><a href="#0-概览" class="headerlink" title="0 概览"></a>0 概览</h3><p>本文详细解释Merlin Mandarin_voice下脚本一步一步所做的事。</p>
<h3 id="01-setup"><a href="#01-setup" class="headerlink" title="01_setup"></a>01_setup</h3><p>脚本<code>merlin/egs/mandarin_voice/s1/01_setup.sh</code></p>
<p>主要工作是创建一个目录，做好准备工作。主要创建了如下文件夹:</p>
<ul>
<li>experiments</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">─ mandarin_voice(voice name)</span><br><span class="line">    ├── acoustic_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    ├── duration_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    └── test_synthesis</span><br><span class="line">        ├── gen-lab</span><br><span class="line">        ├── prompt-lab</span><br><span class="line">        ├── test_id_list.scp</span><br><span class="line">        └── wav</span><br></pre></td></tr></table></figure>
<ul>
<li>database</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> feats</span><br><span class="line">│ ├── bap</span><br><span class="line">│ ├── lf0</span><br><span class="line">│ └── mgc</span><br><span class="line">├── labels</span><br><span class="line">│ └── label_phone_align</span><br><span class="line">├── prompt-lab</span><br><span class="line">│ ├── A11_0.lab</span><br><span class="line">│ ├── A11_1.lab</span><br><span class="line">│ ├── A11_2.lab</span><br><span class="line">└── wav</span><br><span class="line">    ├── A11_0.wav</span><br><span class="line">    ├── A11_100.wav</span><br><span class="line">    ├── A11_101.wav</span><br></pre></td></tr></table></figure>
<p>将一些基本参数写入到<code>conf/global_setting.cfg</code>文件中</p>
<p><img src="/images/blog/merlin_mandarin_voice_op1.jpg" alt="merlinmandarin voice操作"></p>
<p><strong>注意：一定要在setup.sh里面定义好train,valid,test的数量，不然修改global_config.cfg里面的值也没用。这三者相加的值要等于（duration_model/FileIdList下）file_id_list.scp总行数</strong></p>
<h3 id="02-prepare-lab"><a href="#02-prepare-lab" class="headerlink" title="02_prepare_lab"></a>02_prepare_lab</h3><p>需要两个参数：</p>
<ul>
<li>lab_dir: 第一步中的标注目录 <code>database/labels</code></li>
<li>prompt_lab_dir :第一步中生成的<code>database/prompt-lab</code></li>
</ul>
<h4 id="2-1-准备文件夹"><a href="#2-1-准备文件夹" class="headerlink" title="2.1 准备文件夹"></a>2.1 准备文件夹</h4><ul>
<li><p>将 <code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件分别复制到<code>experiments/mandarin_voice/duration_model/data</code>（时域模型）和<code>experiments/mandarin_voice/acoustic_model/data</code>（声学模型）下。【用于训练】</p>
</li>
<li><p>将<code>database/prompt-lab</code>下的lab文件复制到<code>experiments/mandarin_voice/test_synthesis</code>下【用于测试（合成）】</p>
</li>
</ul>
<h4 id="2-2-生成文件列表"><a href="#2-2-生成文件列表" class="headerlink" title="2.2 生成文件列表"></a>2.2 生成文件列表</h4><ul>
<li><p>将<code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件列表写入到<code>experiments/mandarin_voice/duration_model/FileIdList&#39;和</code>experiments/mandarin_voice/acoustic_model/FileIdList’。并移除文件后缀【训练集文件列表】</p>
</li>
<li><p>将<code>database/prompt-lab</code>下的lab文件列表写入到<code>experiments/mandarin_voice/test_synthesis/test_id_list.scp</code>文件中，并移除文件后缀【用于合成语音的文本列表】</p>
</li>
</ul>
<h3 id="03-prepare-acoustic-feature"><a href="#03-prepare-acoustic-feature" class="headerlink" title="03_prepare_acoustic_feature"></a>03_prepare_acoustic_feature</h3><p>需要两个参数</p>
<ul>
<li><strong>wav_dir</strong>: 使用的是第一步中的<code>database/wav</code>，下面存放的是所有的wav音频文件</li>
<li><strong>feat_dir</strong>:输出文件目录<code>database/feats</code>，是当前脚本输出的特征存放文件目录<h4 id="3-1-使用声码器抽取声学特征"><a href="#3-1-使用声码器抽取声学特征" class="headerlink" title="3.1 使用声码器抽取声学特征"></a>3.1 使用声码器抽取声学特征</h4></li>
</ul>
<p>使用<code>merlin/misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本抽取，注意，其中的声码器可以是<code>WORLD</code>也可以是其他的，比如<code>straight</code>,<code>WORLD_2</code>。其实依然是在python中调用以下脚本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">world &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;WORLD&quot;)</span><br><span class="line">sptk &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;SPTK-3.9&quot;)</span><br><span class="line">reaper &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;REAPER&quot;)</span><br></pre></td></tr></table></figure>
<p>生成的特征目录如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sp_dir &#x3D; os.path.join(feat_dir, &#39;sp&#39; )</span><br><span class="line">mgc_dir &#x3D; os.path.join(feat_dir, &#39;mgc&#39;)</span><br><span class="line">ap_dir &#x3D; os.path.join(feat_dir, &#39;ap&#39; )</span><br><span class="line">bap_dir &#x3D; os.path.join(feat_dir, &#39;bap&#39;)</span><br><span class="line">f0_dir &#x3D; os.path.join(feat_dir, &#39;f0&#39; )</span><br><span class="line">lf0_dir &#x3D; os.path.join(feat_dir, &#39;lf0&#39;)</span><br></pre></td></tr></table></figure>
<p>如果我们使用world作为vocoder的话，会使用<code>misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本，生成步骤其实是：</p>
<ol>
<li>直接从原始wav文件，使用<code>world analysis</code>抽取 <code>sp</code>,<code>bapd</code>特征。<code>straight</code>vocoder 会产生 <code>ap</code>,如果使用reaper会产生<code>f0</code>特征。</li>
<li><code>f0</code>$\rightarrow$ <code>lf0</code>,<code>bapd</code>$\rightarrow$ <code>bap</code>,<code>sp</code>$\rightarrow$ <code>mgc</code></li>
</ol>
<h4 id="3-2-复制特征到声学特征目录下"><a href="#3-2-复制特征到声学特征目录下" class="headerlink" title="3.2 复制特征到声学特征目录下"></a>3.2 复制特征到声学特征目录下</h4><p>将所有<code>feat_dir</code>下的所有文件,包括<code>sp</code>,<code>mgc</code>,<code>ap</code>,<code>bap</code>,<code>f0</code>,<code>lf0</code>复制到<code>experiments/mandarin_voice/acoustic_model/data</code>下。</p>
<h2 id="04-prepare-conf-files"><a href="#04-prepare-conf-files" class="headerlink" title="04_prepare_conf_files"></a>04_prepare_conf_files</h2><p>执行<code>./scripts/prepare_config_files.sh</code></p>
<p><strong>duration相关配置</strong></p>
<ul>
<li><p>先从<code>merlin/misc/recipes/duration_demo.conf</code>复制一份到<code>conf/duration_mandarin_voice.conf</code>，并修改<code>conf/duration_mandarin_voice.conf</code>中的一些目录</p>
<ul>
<li>MerlinDir</li>
<li>WorkDir</li>
<li>TOPLEVEL</li>
<li>FileIdList</li>
</ul>
</li>
<li><p>修改Label相关的配置项【Labels】</p>
<ul>
<li>silence_pattern：修改为 <code>[&#39;*-sil+*&#39;]</code></li>
<li>label_type:<code>state_align</code> 或 <code>phone_align</code>，修改之后为<code>phone_align</code></li>
<li>label_align: 即配置音素对齐文件的目录<code>/experiments/mandarin_voice/duration_model/data/label_phone_align</code></li>
<li>question_file_name:<code>/misc/questions/questions-mandarin.hed</code>问题集</li>
</ul>
</li>
<li><p>修改输出配置【Outputs】，label_type有<code>state_align</code> 或 <code>phone_align</code>，如果是<code>state_align</code>会在【outputs】处指定<code>dur=5</code>,如果是<code>phone_align</code>则指定<code>dur=1</code></p>
</li>
<li>神经网络的架构配置，如果当前声音文件是<code>demo</code>则修改<code>hidden_layer_size</code> 【architechture】</li>
<li>修改训练、验证、测试数据数量。【data】<ul>
<li>train_file_number: 200</li>
<li>valid_file_number: 25</li>
<li>test_file_number: 25</li>
</ul>
</li>
</ul>
<p><strong>acoustic相关配置</strong></p>
<ul>
<li>复制文件<code>conf/acoustic_mandarin_voice.conf</code>，修改变量，label配置都和duration相关配置一样。</li>
<li>修改输出配置【outputs】<ul>
<li>mgc</li>
<li>dmgc</li>
<li>bap</li>
<li>dbap</li>
<li>lf0</li>
<li>dlf0</li>
</ul>
</li>
<li>波形文件设置【waveform】<ul>
<li>framelength</li>
<li>minimum_phase_order</li>
<li>fw_alpha</li>
</ul>
</li>
<li>其他的【architechture】和【data】都和duration相关配置一样。</li>
</ul>
<p>执行<code>./scripts/prepare_config_files_for_synthesis.sh</code>配置测试（或合成）语音相关的参数。基本和上面的<code>./scripts/prepare_config_files.sh</code>一样，需要配置<code>duration</code>和<code>ascoustic</code>参数。新增了【Processes】</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DurationModel: True</span><br><span class="line">GenTestList: True</span><br><span class="line"># sub-processes</span><br><span class="line">NORMLAB: True</span><br><span class="line">MAKEDUR: False</span><br><span class="line">MAKECMP: False</span><br><span class="line">NORMCMP: False</span><br><span class="line">TRAINDNN: False</span><br><span class="line">DNNGEN: True</span><br><span class="line">CALMCD: False</span><br></pre></td></tr></table></figure>
<h3 id="05-train-duration-model"><a href="#05-train-duration-model" class="headerlink" title="05_train_duration_model"></a>05_train_duration_model</h3><p>实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/duration_mandarin_voice.conf</code></p>
<p>其中<code>./scripts/submit.sh</code>是theano相关参数的配置。</p>
<h3 id="06-train-acoustic-model"><a href="#06-train-acoustic-model" class="headerlink" title="06_train_acoustic_model"></a>06_train_acoustic_model</h3><p>训练声学模型，实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/acoustic_mandarin_voice.conf</code></p>
<h3 id="07-run-merlin"><a href="#07-run-merlin" class="headerlink" title="07_run_merlin"></a>07_run_merlin</h3><p>需要两个参数</p>
<ul>
<li>test_dur_config_file: 语音合成的时域配置文件</li>
<li>test_synth_config_file:语音合成的</li>
</ul>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>merlin语音合成讲义三：系统回归</title>
    <url>/2018/08/13/merlin-tts-techmap3/</url>
    <content><![CDATA[<h2 id="1-概览"><a href="#1-概览" class="headerlink" title="1 概览"></a>1 概览</h2><p>前馈神经网络</p>
<ul>
<li>概念上直白的</li>
<li>对每个输入帧frame<ul>
<li>执行回归得到对应的输出特征</li>
</ul>
</li>
<li>为避免更广(wider)的输入上下文，可以简单的将几个frame堆叠</li>
<li>需要注意的是：语言特征已经跨越(span)了几个时间尺度(timescale)</li>
</ul>
<h3 id="1-1-方向"><a href="#1-1-方向" class="headerlink" title="1.1 方向"></a>1.1 方向</h3><ul>
<li>前馈架构<ul>
<li>没有记忆</li>
</ul>
</li>
<li>简单的循环神经网络</li>
<li>梯度消失现象</li>
<li>LSTM神经元解决了梯度消失现象（其他类型的可能存在）</li>
</ul>
<p><strong>但是</strong></p>
<ul>
<li>输入和输出有相同的帧率(frame rate)</li>
<li>需要一个额外的时钟或者对齐机制来对输入做上采样</li>
</ul>
<h3 id="1-2-sequence-to-sequence"><a href="#1-2-sequence-to-sequence" class="headerlink" title="1.2 sequence-to-sequence"></a>1.2 sequence-to-sequence</h3><ul>
<li>下一步是，集成对齐机制到网络内部</li>
<li><p>当前：输入序列长度可能与输出序列长度不一致</p>
</li>
<li><p>例如：</p>
<ul>
<li>输入：上下文依赖的音素序列<ul>
<li>输出：声学帧(对于声码器vocoder)</li>
</ul>
</li>
</ul>
</li>
<li><p>概念上</p>
<ul>
<li>读取整个输入序列；使用一个固定长度的表征来记忆</li>
<li>给定表征，写输出序列</li>
</ul>
</li>
<li><p>encoder（编码器）</p>
</li>
<li>是一个循环神经网络，读入整个输入序列，然后用固定长度表征来summarises或者memorises他们。</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_1.png" alt="TTS merlin技术路线"></p>
<ul>
<li>encoder和decoder</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_2.png" alt="TTS merlin技术路线"></p>
<h3 id="1-3-sequence-to-sequence中的对齐"><a href="#1-3-sequence-to-sequence中的对齐" class="headerlink" title="1.3 sequence-to-sequence中的对齐"></a>1.3 sequence-to-sequence中的对齐</h3><ul>
<li>基本模型，输入和输出之间没有对齐</li>
<li>通过加入注意力模型来获得更好结果<ul>
<li>decoder可以接近输入序列<ul>
<li>decoder也可以在前一个时间步(time step)接近其输出</li>
</ul>
</li>
</ul>
</li>
<li>对齐像ASR模型。但是用声码器(vocoder)来做ASR效果不好<ul>
<li>因而我们期望通过使用ASR样式的声学特征(仅仅是模型的对齐部分)来获得更好效果</li>
</ul>
</li>
</ul>
<p><code>04_prepare_conf_files.sh</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;preparing config files for acoustic, duration models...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_config_files.sh $global_config_file</span><br><span class="line">echo &quot;preparing config files for synthesis...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_config_files_for_synthesis.sh $global_config_file</span><br></pre></td></tr></table></figure></p>
<p><code>05_train_duration_model.sh</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $duration_conf_file</span><br></pre></td></tr></table></figure><br><code>config files</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">Merlin: &lt;path to Merlin root directory&gt;</span><br><span class="line">TOPLEVEL: &lt;path where experiments are created&gt;</span><br><span class="line">[Paths]</span><br><span class="line"># where to place work files</span><br><span class="line">work: &lt;path where data, log, models and generated data are stored and created&gt;</span><br><span class="line"># where to find the data</span><br><span class="line">data: %(work)s&#x2F;data</span><br><span class="line"># where to find intermediate directories</span><br><span class="line">inter_data: %(work)s&#x2F;inter_module</span><br><span class="line"># list of file basenames, training and validation in a single list</span><br><span class="line">file_id_list: %(data)s&#x2F;file_id_list.scp</span><br><span class="line">test_id_list: %(data)s&#x2F;test_id_list.scp</span><br><span class="line">in_mgc_dir: %(data)s&#x2F;mgc</span><br><span class="line">in_bap_dir : %(data)s&#x2F;bap</span><br><span class="line">[Labels]</span><br><span class="line">enforce_silence: False</span><br><span class="line">silence_pattern: [&#39;*-sil+*&#39;]</span><br><span class="line"># options: state_align or phone_align</span><br><span class="line">label_type: state_align</span><br><span class="line">label_align: &lt;path to labels&gt;</span><br><span class="line">question_file_name: &lt;path to questions set&gt;</span><br><span class="line">add_frame_features: True</span><br><span class="line"># options: full, coarse_coding, minimal_frame, state_only, frame_only, none</span><br><span class="line">subphone_feats: full</span><br><span class="line">[Outputs]</span><br><span class="line"># dX should be 3 times X</span><br><span class="line">mgc : 60</span><br><span class="line">dmgc : 180</span><br><span class="line">bap : 1</span><br><span class="line">dbap : 3</span><br><span class="line">lf0 : 1</span><br><span class="line">dlf0 : 3</span><br><span class="line">[Waveform]</span><br><span class="line">[Outputs]</span><br><span class="line"># dX should be 3 times X</span><br><span class="line">mgc : 60</span><br><span class="line">dmgc : 180</span><br><span class="line">bap : 1</span><br><span class="line">dbap : 3</span><br><span class="line">lf0 : 1</span><br><span class="line">dlf0 : 3</span><br><span class="line">[Waveform]</span><br><span class="line">test_synth_dir: None</span><br><span class="line"># options: WORLD or STRAIGHT</span><br><span class="line">vocoder_type: WORLD</span><br><span class="line">samplerate: 16000</span><br><span class="line">framelength: 1024</span><br><span class="line"># Frequency warping coefficient used to compress the spectral envelope into MGC (or MCEP)</span><br><span class="line">fw_alpha: 0.58</span><br><span class="line">minimum_phase_order: 511</span><br><span class="line">use_cep_ap: True</span><br><span class="line">[Architecture]</span><br><span class="line">switch_to_keras: False</span><br><span class="line">hidden_layer_size : [1024, 1024, 1024, 1024, 1024, 1024]</span><br><span class="line">hidden_layer_type : [&#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;]</span><br><span class="line">model_file_name: feed_forward_6_tanh</span><br><span class="line">#if RNN or sequential training is used, please set sequential_training to True.</span><br><span class="line">sequential_training : False</span><br><span class="line">dropout_rate : 0.0</span><br><span class="line">batch_size : 256</span><br><span class="line"># options: -1 for exponential decay, 0 for constant learning rate, 1 for linear decay</span><br><span class="line">lr_decay : -1</span><br><span class="line">learning_rate : 0.002</span><br><span class="line"># options: sgd, adam, rprop</span><br><span class="line">optimizer : sgd</span><br><span class="line">warmup_epoch : 10</span><br><span class="line">training_epochs : 25</span><br><span class="line">[Processes]</span><br><span class="line"># Main processes</span><br><span class="line">AcousticModel : True</span><br><span class="line">GenTestList : False</span><br><span class="line"># sub-processes</span><br><span class="line">NORMLAB : True</span><br><span class="line">MAKECMP : True</span><br><span class="line">NORMCMP : True</span><br><span class="line">TRAINDNN : True</span><br><span class="line">DNNGEN : True</span><br><span class="line">GENWAV : True</span><br><span class="line">CALMCD : True</span><br></pre></td></tr></table></figure></p>
<p><code>06_train_acoustic_model.sh</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $acoustic_conf_file</span><br></pre></td></tr></table></figure></p>
<p><code>07_run_merlin.sh</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inp_txt&#x3D;$1</span><br><span class="line">test_dur_config_file&#x3D;$2</span><br><span class="line">test_synth_config_file&#x3D;$3</span><br><span class="line">echo &quot;preparing full-contextual labels using Festival frontend...&quot;</span><br><span class="line">lab_dir&#x3D;$(dirname $inp_txt)</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_labels_from_txt.sh $inp_txt $lab_dir $global_config_file</span><br><span class="line">echo &quot;synthesizing durations...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $test_dur_config_file</span><br><span class="line">echo &quot;synthesizing speech...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $test_synth_config_file</span><br></pre></td></tr></table></figure></p>
<h2 id="2-设计选择：声学模型"><a href="#2-设计选择：声学模型" class="headerlink" title="2 设计选择：声学模型"></a>2 设计选择：声学模型</h2><ul>
<li>直白的方式：如果输入和输出<strong>有相同长度并且是对齐的</strong></li>
<li>前馈神经网络</li>
<li>循环神经网络层</li>
<li>不那么直白的方式：对非对齐输入和输出序列<ul>
<li>使用sequence-to-sequence</li>
</ul>
</li>
<li>唯一的实践限制是，是使用什么技术，比如Theano,Tensorflow</li>
</ul>
<h3 id="2-1-方向"><a href="#2-1-方向" class="headerlink" title="2.1 方向"></a>2.1 方向</h3><ul>
<li>回归的输出是什么<ul>
<li>声学特征<ul>
<li>不是语音波形<br>所以还需要进一步</li>
</ul>
</li>
</ul>
</li>
<li>生成波形<ul>
<li>输入时声学特征</li>
<li>输出是语音波形</li>
</ul>
</li>
</ul>
<h2 id="3-波形生成-waveform-generator"><a href="#3-波形生成-waveform-generator" class="headerlink" title="3 波形生成(waveform generator)"></a>3 波形生成(waveform generator)</h2><h3 id="3-1-从声学-acoustic-特征回到原始声码器-vocoder-特征"><a href="#3-1-从声学-acoustic-特征回到原始声码器-vocoder-特征" class="headerlink" title="3.1 从声学(acoustic)特征回到原始声码器(vocoder)特征"></a>3.1 从声学(acoustic)特征回到原始声码器(vocoder)特征</h3><p><img src="/images/blog/merlin_tts_tch3_3.png" alt="TTS merlin技术路线"></p>
<h3 id="3-2-WORLD：periodic-excitation-using-a-pulse-train"><a href="#3-2-WORLD：periodic-excitation-using-a-pulse-train" class="headerlink" title="3.2 WORLD：periodic excitation using a pulse train"></a>3.2 WORLD：periodic excitation using a pulse train</h3><ul>
<li>脉冲位置的计算<ul>
<li>声音分割：每一个fundamental period(基本周期)创建一个脉冲，T0。从F0计算T0，其中F0之前被声学模型预测得到</li>
<li>非声音分割：固定频率 $T0=5ms$<h3 id="3-3-WORLD-obtain-spectral-envelope-at-exact-pulse-locations-by-interpolation-插值法在每个确定的脉冲位置获取频谱包络"><a href="#3-3-WORLD-obtain-spectral-envelope-at-exact-pulse-locations-by-interpolation-插值法在每个确定的脉冲位置获取频谱包络" class="headerlink" title="3.3 WORLD:obtain spectral envelope at exact pulse locations, by interpolation(插值法在每个确定的脉冲位置获取频谱包络)"></a>3.3 WORLD:obtain spectral envelope at exact pulse locations, by interpolation(插值法在每个确定的脉冲位置获取频谱包络)</h3></li>
</ul>
</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_4.png" alt="TTS merlin技术路线"></p>
<h3 id="3-4-WORLD：重构周期性和非周期性的幅度频谱-magnitude-spectra"><a href="#3-4-WORLD：重构周期性和非周期性的幅度频谱-magnitude-spectra" class="headerlink" title="3.4 WORLD：重构周期性和非周期性的幅度频谱(magnitude spectra)"></a>3.4 WORLD：重构周期性和非周期性的幅度频谱(magnitude spectra)</h3><p><img src="/images/blog/merlin_tts_tch3_5.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_6.png" alt="TTS merlin技术路线"></p>
<h3 id="3-5-WORLD-生成波形"><a href="#3-5-WORLD-生成波形" class="headerlink" title="3.5 WORLD:生成波形"></a>3.5 WORLD:生成波形</h3><p><img src="/images/blog/merlin_tts_tch3_7.png" alt="TTS merlin技术路线"></p>
<h2 id="4-拓展"><a href="#4-拓展" class="headerlink" title="4 拓展"></a>4 拓展</h2><ul>
<li>混合语音合成<ul>
<li>使用Merlin来预测声学特征，使用Festival来做单元选取(select unit)</li>
</ul>
</li>
<li>声音转换<ul>
<li>输入语音而非文本</li>
<li>训练数据是对齐的输入和输出语音（而不是音素标签和语音）</li>
</ul>
</li>
<li>讲话人调整<ul>
<li>增强输入</li>
<li>调整隐藏层</li>
<li>转换输出</li>
</ul>
</li>
</ul>
<h3 id="4-1-经典单元选取"><a href="#4-1-经典单元选取" class="headerlink" title="4.1 经典单元选取"></a>4.1 经典单元选取</h3><p>此处以音素单元为例，目标和join cost</p>
<p><img src="/images/blog/merlin_tts_tch3_8.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_9.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_10.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-独立特征形式-Independent-Feature-Formulation-IFF-目标损失"><a href="#4-2-独立特征形式-Independent-Feature-Formulation-IFF-目标损失" class="headerlink" title="4.2 独立特征形式(Independent Feature Formulation(IFF))目标损失"></a>4.2 独立特征形式(Independent Feature Formulation(IFF))目标损失</h3><p><img src="/images/blog/merlin_tts_tch3_11.png" alt="TTS merlin技术路线"></p>
<h3 id="4-3-声学空间形式-Acoustic-Space-Formulation-目标损失"><a href="#4-3-声学空间形式-Acoustic-Space-Formulation-目标损失" class="headerlink" title="4.3 声学空间形式(Acoustic Space Formulation)目标损失"></a>4.3 声学空间形式(Acoustic Space Formulation)目标损失</h3><p><img src="/images/blog/merlin_tts_tch3_12.png" alt="TTS merlin技术路线"></p>
<h3 id="4-4-混合语音合成就像使用Acoustic-Space-Formulation目标损失的单元选取"><a href="#4-4-混合语音合成就像使用Acoustic-Space-Formulation目标损失的单元选取" class="headerlink" title="4.4  混合语音合成就像使用Acoustic Space Formulation目标损失的单元选取"></a>4.4  混合语音合成就像使用Acoustic Space Formulation目标损失的单元选取</h3><p><img src="/images/blog/merlin_tts_tch3_13.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_14.png" alt="TTS merlin技术路线"></p>
<h3 id="4-5-混合语音合成就像：统计参数语音合成，使用声码器-vocoder-的替换"><a href="#4-5-混合语音合成就像：统计参数语音合成，使用声码器-vocoder-的替换" class="headerlink" title="4.5 混合语音合成就像：统计参数语音合成，使用声码器(vocoder)的替换"></a>4.5 混合语音合成就像：统计参数语音合成，使用声码器(vocoder)的替换</h3><p><img src="/images/blog/merlin_tts_tch3_15.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_16.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_17.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_18.png" alt="TTS merlin技术路线"></p>
<h3 id="4-6-混合语音合成就像：同时对目标和join-cost使用混合密度网络"><a href="#4-6-混合语音合成就像：同时对目标和join-cost使用混合密度网络" class="headerlink" title="4.6 混合语音合成就像：同时对目标和join cost使用混合密度网络"></a>4.6 混合语音合成就像：同时对目标和join cost使用混合密度网络</h3><p><img src="/images/blog/merlin_tts_tch3_19.png" alt="TTS merlin技术路线"></p>
<h3 id="7-声音转换"><a href="#7-声音转换" class="headerlink" title="7 声音转换"></a>7 声音转换</h3><p>将源声转换为另外一个人的声音，而不改变声音内容</p>
<p><img src="/images/blog/merlin_tts_tch3_20.png" alt="TTS merlin技术路线"></p>
<p>使用神经网络完成</p>
<p><img src="/images/blog/merlin_tts_tch3_21.png" alt="TTS merlin技术路线"></p>
<h3 id="7-1-输入和输出的声学特征的抽取和工程"><a href="#7-1-输入和输出的声学特征的抽取和工程" class="headerlink" title="7.1 输入和输出的声学特征的抽取和工程"></a>7.1 输入和输出的声学特征的抽取和工程</h3><p><img src="/images/blog/merlin_tts_tch3_22.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_23.png" alt="TTS merlin技术路线"></p>
<h3 id="7-2-输入输出的对齐"><a href="#7-2-输入输出的对齐" class="headerlink" title="7.2 输入输出的对齐"></a>7.2 输入输出的对齐</h3><ul>
<li>从波形waveform中声学特征</li>
<li>使用动态时间封装(Dynamic Time Wraping(DTW))</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_24.png" alt="TTS merlin技术路线"></p>
<h3 id="7-3-最简单的方法：对齐输入和输出特征-逐帧回归"><a href="#7-3-最简单的方法：对齐输入和输出特征-逐帧回归" class="headerlink" title="7.3 最简单的方法：对齐输入和输出特征+逐帧回归"></a>7.3 最简单的方法：对齐输入和输出特征+逐帧回归</h3><p><img src="/images/blog/merlin_tts_tch3_25.png" alt="TTS merlin技术路线"></p>
<h3 id="7-4-当然，我们也可以用前馈神经网络做得更好"><a href="#7-4-当然，我们也可以用前馈神经网络做得更好" class="headerlink" title="7.4 当然，我们也可以用前馈神经网络做得更好"></a>7.4 当然，我们也可以用前馈神经网络做得更好</h3><p><img src="/images/blog/merlin_tts_tch3_26.png" alt="TTS merlin技术路线"></p>
<p>我们可以使用<code>Merlin/egs/voice_conversion/s1/</code>目录下的脚本完成这个工作</p>
<p><code>03_align_src_with_target.sh</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">src_feat_dir&#x3D;$1</span><br><span class="line">tgt_feat_dir&#x3D;$2</span><br><span class="line">src_aligned_feat_dir&#x3D;$3</span><br><span class="line">src_mgc_dir&#x3D;$src_feat_dir&#x2F;mgc</span><br><span class="line">tgt_mgc_dir&#x3D;$tgt_feat_dir&#x2F;mgc</span><br><span class="line">echo &quot;Align source acoustic features with target acoustic features...&quot;</span><br><span class="line">python $&#123;MerlinDir&#125;&#x2F;misc&#x2F;scripts&#x2F;voice_conversion&#x2F;dtw_aligner_festvox.py $&#123;MerlinDir&#125;&#x2F;tools</span><br><span class="line">$&#123;src_feat_dir&#125; $&#123;tgt_feat_dir&#125; $&#123;src_aligned_feat_dir&#125; $&#123;bap_dim&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/merlin_tts_tch3_26.png" alt="TTS merlin技术路线"></p>
<h2 id="8-讲话人调整-Speaker-Adaptation"><a href="#8-讲话人调整-Speaker-Adaptation" class="headerlink" title="8 讲话人调整(Speaker Adaptation)"></a>8 讲话人调整(Speaker Adaptation)</h2><ul>
<li>只使用了目标讲话人一小段录音来创造一个新的声音</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_28.png" alt="TTS merlin技术路线"></p>
<h3 id="8-1-使用DNN方法的讲话人调整"><a href="#8-1-使用DNN方法的讲话人调整" class="headerlink" title="8.1 使用DNN方法的讲话人调整"></a>8.1 使用DNN方法的讲话人调整</h3><ul>
<li>需要额外的输入特征</li>
<li>应用转换（声音转换）到输出特征</li>
<li>学习一个模型参数的修改(LHUC)</li>
<li>共享层(hat swapping)</li>
<li>在目标讲述人数据上fine-tuning整个模型</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_29.png" alt="TTS merlin技术路线"></p>
<p>共享层和hot swapping</p>
<p><img src="/images/blog/merlin_tts_tch3_30.png" alt="TTS merlin技术路线"></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>merlin语音合成讲义二：如何构建系统之数据准备</title>
    <url>/2018/08/12/merlin-tts-techmap2/</url>
    <content><![CDATA[<p>一 前端部分</p>
<p><img src="/images/blog/merlin_tts_tch2_1.png" alt="TTS merlin技术路线"></p>
<p>如果存在已经训练好的前端工具，我们可以使用已经有的工具<code>Festival</code>,<code>MaryTTS</code>,<code>sSpeak</code><br>如果我们没有标注数据，我们可以使用<code>Ossian</code></p>
<h3 id="1-1-Ossian-Toolkit"><a href="#1-1-Ossian-Toolkit" class="headerlink" title="1.1 Ossian Toolkit"></a>1.1 Ossian Toolkit</h3><ul>
<li>使用训练数据，可以使用最少的speech+text<ul>
<li>句子或段落对齐</li>
</ul>
</li>
<li>可以利用用户的任何额外数据</li>
<li>提供<code>前端模块</code>和<code>胶水</code>来组合，Merlin DNNs</li>
</ul>
<p>我们将展示</p>
<ul>
<li>Ossian如何与Merlin结合来构建一个<code>Swahili</code>声音而不需要任何语言专家，只需要转录语音</li>
<li>引入Ossian的某些思路来管理，而不需要标注</li>
</ul>
<h4 id="1-1-1-Ossian-：Training-Data"><a href="#1-1-1-Ossian-：Training-Data" class="headerlink" title="1.1.1  Ossian ：Training Data"></a>1.1.1  Ossian ：Training Data</h4><p>我们仅需要UTF-8格式的文本和语音，同时匹配 <code>句子 除以 段落</code> 尺寸的chunks</p>
<p><img src="/images/blog/merlin_tts_tch2_2.png" alt="TTS merlin技术路线"></p>
<p><img src="/images/blog/merlin_tts_tch2_3.png" alt="TTS merlin技术路线"></p>
<p><strong>utt文件</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;utt text &#x3D;&quot;Khartoum imejitenga na mzozo huo.&quot; waveform&#x3D;&quot;.&#x2F;wav&#x2F;pm_n2336.wav&quot;  utterance_name&#x3D;&quot;pm_n2236&quot;&gt;</span><br></pre></td></tr></table></figure>
<p>其中<code>utterance_name=&quot;pm_n2236&quot;</code>是一个XML格式的发生结构，在训练集语料库为每个句子构建的。<br>其内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;utt text&#x3D;&quot;Khartoum imejitenga na mzozo huo.&quot; waveform&#x3D;&quot;.&#x2F;wav&#x2F;pm_n2236.wav&quot;</span><br><span class="line">utterance_name&#x3D;&quot;pm_n2236&quot; processors_used&#x3D;&quot;,word_splitter&quot;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;_END_&quot; token_class&#x3D;&quot;_END_&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;Khartoum&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;imejitenga&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;na&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;mzozo&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;huo&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;.&quot; token_class&#x3D;&quot;punctuation&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;_END_&quot; token_class&#x3D;&quot;_END_&quot;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;utt&gt;</span><br></pre></td></tr></table></figure><br>unicode字符属性用以无关语言的正则表达式来tokenise文本<br>正则表达式</p>
<p><img src="/images/blog/merlin_tts_tch2_4.png" alt="TTS merlin技术路线"></p>
<p>同时unicode用来将tokens分类为单词，空格和标点。</p>
<h3 id="1-1-2-词性标注-POS-Tagging"><a href="#1-1-2-词性标注-POS-Tagging" class="headerlink" title="1.1.2 词性标注(POS Tagging)"></a>1.1.2 词性标注(POS Tagging)</h3><p><img src="/images/blog/merlin_tts_tch2_5.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_6.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_7.png" alt="TTS merlin技术路线"></p>
<h4 id="4-1-3-分布式词向量作为POS-Part-Of-Speech-tag-替代品"><a href="#4-1-3-分布式词向量作为POS-Part-Of-Speech-tag-替代品" class="headerlink" title="4.1.3 分布式词向量作为POS(Part Of Speech) tag 替代品"></a>4.1.3 分布式词向量作为POS(Part Of Speech) tag 替代品</h4><p><img src="/images/blog/merlin_tts_tch2_8.png" alt="TTS merlin技术路线"><br>分别来看<br><img src="/images/blog/merlin_tts_tch2_9.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_10.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_11.png" alt="TTS merlin技术路线"></p>
<p>将所有词映射到词向量空间<br><img src="/images/blog/merlin_tts_tch2_12.png" alt="TTS merlin技术路线"></p>
<p>将词向量替代POS<br><img src="/images/blog/merlin_tts_tch2_13.png" alt="TTS merlin技术路线"></p>
<p>以字母替代音素的标注文件<br><img src="/images/blog/merlin_tts_tch2_14.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_15.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-强制对齐和-静音检测"><a href="#4-2-强制对齐和-静音检测" class="headerlink" title="4.2 强制对齐和 静音检测"></a>4.2 强制对齐和 静音检测</h3><p><img src="/images/blog/merlin_tts_tch2_16.png" alt="TTS merlin技术路线"></p>
<h3 id="4-3-phrasing-短语"><a href="#4-3-phrasing-短语" class="headerlink" title="4.3 phrasing(短语)"></a>4.3 phrasing(短语)</h3><p><img src="/images/blog/merlin_tts_tch2_17.png" alt="TTS merlin技术路线"></p>
<h2 id="5-语言特征工程：使用XPATHS-做flatten"><a href="#5-语言特征工程：使用XPATHS-做flatten" class="headerlink" title="5 语言特征工程：使用XPATHS 做flatten"></a>5 语言特征工程：使用XPATHS 做flatten</h2><p><img src="/images/blog/merlin_tts_tch2_18.png" alt="TTS merlin技术路线"><br>对应的详细标注<br><img src="/images/blog/merlin_tts_tch2_19.png" alt="TTS merlin技术路线"></p>
<h2 id="6-语言特征和工程"><a href="#6-语言特征和工程" class="headerlink" title="6 语言特征和工程"></a>6 语言特征和工程</h2><p><img src="/images/blog/merlin_tts_tch2_20.png" alt="TTS merlin技术路线"></p>
<h3 id="6-1-语言特征工程：flatten到上下文依赖的音素"><a href="#6-1-语言特征工程：flatten到上下文依赖的音素" class="headerlink" title="6.1 语言特征工程：flatten到上下文依赖的音素"></a>6.1 语言特征工程：flatten到上下文依赖的音素</h3><p><img src="/images/blog/merlin_tts_tch2_21.png" alt="TTS merlin技术路线"></p>
<p>注意看左下角，<code>ao-th+er</code>，当前音素为<code>th</code>，其前缀为<code>ao</code>,后缀为<code>er</code>。进一步</p>
<p><img src="/images/blog/merlin_tts_tch2_22.png" alt="TTS merlin技术路线"></p>
<p>得到一个完整的音素标注。再进一步</p>
<p><img src="/images/blog/merlin_tts_tch2_23.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_24.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_25.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_26.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_27.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_28.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_29.png" alt="TTS merlin技术路线"></p>
<p>详细解释上面的标注文件</p>
<p><img src="/images/blog/merlin_tts_tch2_30.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_31.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_32.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_33.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_34.png" alt="TTS merlin技术路线"></p>
<p>一个句子的完整标注文件<br><img src="/images/blog/merlin_tts_tch2_35.png" alt="TTS merlin技术路线"></p>
<h3 id="6-3-将每个上下文依赖的音素编码为向量"><a href="#6-3-将每个上下文依赖的音素编码为向量" class="headerlink" title="6.3 将每个上下文依赖的音素编码为向量"></a>6.3 将每个上下文依赖的音素编码为向量</h3><p><strong>示例</strong>：使用一个长度为1-40二进制码来编码quinphone</p>
<p><img src="/images/blog/merlin_tts_tch2_36.png" alt="TTS merlin技术路线"></p>
<p>对应的二进制码</p>
<p><img src="/images/blog/merlin_tts_tch2_37.png" alt="TTS merlin技术路线"></p>
<p>开始编码，以头<code>sil</code>开始：</p>
<p><img src="/images/blog/merlin_tts_tch2_38.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_39.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_40.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_41.png" alt="TTS merlin技术路线"></p>
<h3 id="6-4-语言特征工程：上采样到声学帧率-framerate"><a href="#6-4-语言特征工程：上采样到声学帧率-framerate" class="headerlink" title="6.4 语言特征工程：上采样到声学帧率(framerate)"></a>6.4 语言特征工程：上采样到声学帧率(framerate)</h3><p><img src="/images/blog/merlin_tts_tch2_42.png" alt="TTS merlin技术路线"></p>
<h3 id="6-5-语言特征工程：添加音素内-within-phone-位置特征"><a href="#6-5-语言特征工程：添加音素内-within-phone-位置特征" class="headerlink" title="6.5 语言特征工程：添加音素内(within-phone)位置特征"></a>6.5 语言特征工程：添加音素内(within-phone)位置特征</h3><p><img src="/images/blog/merlin_tts_tch2_43.png" alt="TTS merlin技术路线"></p>
<h2 id="7-时域到底来源于哪里"><a href="#7-时域到底来源于哪里" class="headerlink" title="7 时域到底来源于哪里"></a>7 时域到底来源于哪里</h2><p>先看脚本<br>02_prepare_labels.sh<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># alignments can be state-level (like HTS) or phone-level</span><br><span class="line">if [ &quot;$Labels&quot; &#x3D;&#x3D; &quot;state_align&quot; ]</span><br><span class="line">.&#x2F;scripts&#x2F;run_state_aligner.sh $wav_dir $inp_txt $lab_dir $global_config_file</span><br><span class="line">elif [ &quot;$Labels&quot; &#x3D;&#x3D; &quot;phone_align&quot; ]</span><br><span class="line">.&#x2F;scripts&#x2F;run_phone_aligner.sh $wav_dir $inp_txt $lab_dir $global_config_file</span><br><span class="line"># the alignments will be used to train the duration model later</span><br><span class="line">cp -r $lab_dir&#x2F;label_$Labels $duration_data_dir</span><br><span class="line"># and to upsample the linguistic features to acoustic frame rate</span><br><span class="line"># when training the acoustic model</span><br><span class="line">cp -r $lab_dir&#x2F;label_$Labels $acoustic_data_dir</span><br></pre></td></tr></table></figure><br>run_state_aligner.sh<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#do prepare full-contextual labels without timestamps</span><br><span class="line">echo &quot;preparing full-contextual labels using Festival frontend...&quot;</span><br><span class="line">bash $&#123;WorkDir&#125;&#x2F;scripts&#x2F;prepare_labels_from_txt.sh $inp_txt $lab_dir $global_config_file $train</span><br><span class="line"># do forced alignment using HVite from HTK</span><br><span class="line">python $aligner&#x2F;forced_alignment.py</span><br></pre></td></tr></table></figure><br>forced_alignment.py<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aligner &#x3D; ForcedAlignment()</span><br><span class="line">aligner.prepare_training(file_id_list_name, wav_dir, lab_dir, work_dir, multiple_speaker)</span><br><span class="line">aligner.train_hmm(7, 32)</span><br><span class="line">aligner.align(work_dir, lab_align_dir)</span><br></pre></td></tr></table></figure></p>
<h2 id="8-声学特征抽取和工程"><a href="#8-声学特征抽取和工程" class="headerlink" title="8 声学特征抽取和工程"></a>8 声学特征抽取和工程</h2><h3 id="8-1-为何我们使用声学特征抽取-波形waveform"><a href="#8-1-为何我们使用声学特征抽取-波形waveform" class="headerlink" title="8.1 为何我们使用声学特征抽取-波形waveform"></a>8.1 为何我们使用声学特征抽取-波形waveform</h3><ul>
<li>音素 a: 的波形</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_44.png" alt="TTS merlin技术路线"></p>
<ul>
<li>音素a:的magnitude spectrum（幅度频谱）</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_45.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_46.png" alt="TTS merlin技术路线"></p>
<h3 id="8-2-术语"><a href="#8-2-术语" class="headerlink" title="8.2 术语"></a>8.2 术语</h3><ul>
<li>Spectral Envelope：频谱封装</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_47.png" alt="TTS merlin技术路线"></p>
<ul>
<li>F0</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_48.png" alt="TTS merlin技术路线"></p>
<ul>
<li>Aperiodic energy：非周期能量</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_49.png" alt="TTS merlin技术路线"></p>
<h3 id="8-5-典型声码器-WORLD"><a href="#8-5-典型声码器-WORLD" class="headerlink" title="8.5 典型声码器 WORLD"></a>8.5 典型声码器 WORLD</h3><ul>
<li>语音特征<ul>
<li>Spectral Envelope(使用CheapTrick评估)</li>
<li>F0：使用DIO评估</li>
<li>Band aperiodicties：使用D4C评估</li>
</ul>
</li>
</ul>
<h4 id="8-5-1-spectral-envelope-评估"><a href="#8-5-1-spectral-envelope-评估" class="headerlink" title="8.5.1 spectral envelope 评估"></a>8.5.1 spectral envelope 评估</h4><ul>
<li>Hanning窗长度3T0</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_50.png" alt="TTS merlin技术路线"></p>
<ul>
<li>使用一个<strong>长度为 2/3 F0</strong>移动平均过滤器</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_51.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_52.png" alt="TTS merlin技术路线"></p>
<ul>
<li>使用一个<strong>长度为 2 F0</strong>移动平均过滤器</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_53.png" alt="TTS merlin技术路线"></p>
<ul>
<li>$SpEnv= q_0logSp(F)+q1logSp(F+F0)+q1logSp(F-F0)$</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_54.png" alt="TTS merlin技术路线"></p>
<h4 id="8-5-3-F0-评估"><a href="#8-5-3-F0-评估" class="headerlink" title="8.5.3 F0 评估"></a>8.5.3 F0 评估</h4><p><img src="/images/blog/merlin_tts_tch2_55.png" alt="TTS merlin技术路线"></p>
<h4 id="8-5-4-band-aperiodicities"><a href="#8-5-4-band-aperiodicities" class="headerlink" title="8.5.4 band aperiodicities"></a>8.5.4 band aperiodicities</h4><ul>
<li>能量和非能量之间的比率，对固定频率bands取平均</li>
<li>比如： 总功率/sine 波形功率(total power /sine wave power)</li>
<li>示例中，此比例为<ul>
<li>最低band： a</li>
<li>更多band： b</li>
<li>最高band： c</li>
</ul>
</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_56.png" alt="TTS merlin技术路线"></p>
<h3 id="8-6-声学特征工程"><a href="#8-6-声学特征工程" class="headerlink" title="8.6 声学特征工程"></a>8.6 声学特征工程</h3><p><img src="/images/blog/merlin_tts_tch2_57.png" alt="TTS merlin技术路线"></p>
<p>原始声学特征与实际使用的声学特征</p>
<p><img src="/images/blog/merlin_tts_tch2_58.png" alt="TTS merlin技术路线"></p>
<p>抽取一部分来分析</p>
<p><img src="/images/blog/merlin_tts_tch2_59.png" alt="TTS merlin技术路线"></p>
<p>再细致来看</p>
<p><img src="/images/blog/merlin_tts_tch2_60.png" alt="TTS merlin技术路线"></p>
<p>处理步骤如下</p>
<p><img src="/images/blog/merlin_tts_tch2_61.png" alt="TTS merlin技术路线"></p>
<p>可以直接运行脚本<code>03_prepare_acoustic_features.sh</code>得到<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python $&#123;MerlinDir&#125;&#x2F;misc&#x2F;scripts&#x2F;vocoder&#x2F;$&#123;Vocoder,,&#125;&#x2F;</span><br><span class="line">extract_features_for_merlin.py $&#123;MerlinDir&#125; $&#123;wav_dir&#125; $&#123;feat_dir&#125; $SamplingFreq</span><br></pre></td></tr></table></figure><br><code>extract_features_for_merlin.py</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tools directory</span><br><span class="line">world &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;WORLD&quot;)</span><br><span class="line">sptk &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;SPTK-3.9&quot;)</span><br><span class="line">if fs &#x3D;&#x3D; 16000:</span><br><span class="line">nFFTHalf &#x3D; 1024</span><br><span class="line">alpha &#x3D; 0.58</span><br><span class="line">elif fs &#x3D;&#x3D; 48000:</span><br><span class="line">nFFTHalf &#x3D; 2048</span><br><span class="line">alpha &#x3D; 0.77</span><br><span class="line">mcsize&#x3D;59</span><br><span class="line">world_analysis_cmd &#x3D; &quot;%s %s %s %s %s&quot; % (os.path.join(world, &#39;analysis&#39;), \</span><br><span class="line">filename,</span><br><span class="line">os.path.join(f0_dir, file_id + &#39;.f0&#39;), \</span><br><span class="line">os.path.join(sp_dir, file_id + &#39;.sp&#39;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &#39;.bapd&#39;))</span><br><span class="line">os.system(world_analysis_cmd)</span><br><span class="line">### convert f0 to lf0 ###</span><br><span class="line">sptk_x2x_da_cmd &#x3D; &quot;%s +da %s &gt; %s&quot; % (os.path.join(sptk, &#39;x2x&#39;), \</span><br><span class="line">extract_features_for_merlin.py</span><br><span class="line">os.path.join(f0_dir, file_id + &#39;.f0a&#39;), \</span><br><span class="line">os.path.join(sptk, &#39;sopr&#39;) + &#39; -magic 0.0 -LN -MAGIC</span><br><span class="line">-1.0E+10&#39;, \</span><br><span class="line">os.path.join(lf0_dir, file_id + &#39;.lf0&#39;))</span><br><span class="line">os.system(sptk_x2x_af_cmd)</span><br><span class="line">### convert sp to mgc ###</span><br><span class="line">sptk_x2x_df_cmd1 &#x3D; &quot;%s +df %s | %s | %s &gt;%s&quot; % (os.path.join(sptk, &#39;x2x&#39;), \</span><br><span class="line">os.path.join(sp_dir, file_id + &#39;.sp&#39;), \</span><br><span class="line">os.path.join(sptk, &#39;sopr&#39;) + &#39; -R -m 32768.0&#39;, \</span><br><span class="line">os.path.join(sptk, &#39;mcep&#39;) + &#39; -a &#39; + str(alpha</span><br><span class="line">&#39; -m &#39; + str(</span><br><span class="line">mcsize) + &#39; -l &#39; + str(</span><br><span class="line">nFFTHalf) + &#39; -e 1.0E-8 -j 0 -f 0.0 -q 3 &#39;,</span><br><span class="line">os.path.join(mgc_dir, file_id + &#39;.mgc&#39;))</span><br><span class="line">os.system(sptk_x2x_df_cmd1)</span><br><span class="line">### convert bapd to bap ###</span><br><span class="line">sptk_x2x_df_cmd2 &#x3D; &quot;%s +df %s &gt; %s &quot; % (os.path.join(sptk, &quot;x2x&quot;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &quot;.bapd&quot;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &#39;.bap&#39;))</span><br><span class="line">os.system(sptk_x2x_df_cmd2)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>merlin语音合成讲义一：技术路线概览</title>
    <url>/2018/08/11/merlin-tts-techmap1/</url>
    <content><![CDATA[<h2 id="一-概览"><a href="#一-概览" class="headerlink" title="一 概览"></a>一 概览</h2><p>经典的统计参数语音合成方法的三步</p>
<p><img src="/images/blog/merlin_tts_tch1.png" alt="TTS merlin技术路线"></p>
<p>单独看前端和后端</p>
<p><img src="/images/blog/merlin_tts_tch2.png" alt="TTS merlin技术路线"></p>
<p>那么统计模型的任务是</p>
<p><img src="/images/blog/merlin_tts_tch3.png" alt="TTS merlin技术路线"></p>
<p>可以看到其实统计模型的任务就是做一个sequence-to-sequence的回归</p>
<p><img src="/images/blog/merlin_tts_tch4.png" alt="TTS merlin技术路线"></p>
<p>即：输入序列(语义特征)回归到输出序列的声学特征。但是由于二者之间不同的声学始终频率而导致长度不一。</p>
<p><img src="/images/blog/merlin_tts_tch5.png" alt="TTS merlin技术路线"></p>
<h2 id="三-TTS的三个方向"><a href="#三-TTS的三个方向" class="headerlink" title="三 TTS的三个方向"></a>三 TTS的三个方向</h2><ul>
<li><p>目前为止<br>将TTS问题设置为一个<strong>sequence-tosequence</strong>的回归问题。这是一个有意为之的通用方法，这样易于理解</p>
<ul>
<li>用不同的方法来做回归，神经网络或者机器学习方法</li>
<li>选取不同的输入输出特征</li>
</ul>
</li>
<li><p>接下来<br>TTS是如何完成的，使用一个pre-built系统。可以快速完成整个pipeline，从文本到波形输出</p>
</li>
<li><p>进一步<br>如何构建上面说的pre-built系统。一个缓慢的，一步一步的运行整个pipeline，关注在如何创造一个新系统（对任何语言）</p>
</li>
</ul>
<h2 id="4-术语"><a href="#4-术语" class="headerlink" title="4 术语"></a>4 术语</h2><ol>
<li>前端<br>即<code>text</code>$\rightarrow$ <code>linguistic specification</code></li>
<li>统计模型回归<br><code>linguistic specification$$\rightarrow$$acoustic features</code></li>
<li>waveform geneator（波形语音生成）<br><code>acoustic features$$\rightarrow$$waveform</code><br>4.语言规范(Linguistic specification)<br>完整的事物</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch6.png" alt="TTS merlin技术路线"></p>
<ol>
<li>语言特征<br>独立的元素。</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch7.png" alt="TTS merlin技术路线"></p>
<ol>
<li>声学特征<br>帧序列<br><img src="/images/blog/merlin_tts_tch8.png" alt="TTS merlin技术路线"></li>
</ol>
<h2 id="4-从文本到语音"><a href="#4-从文本到语音" class="headerlink" title="4 从文本到语音"></a>4 从文本到语音</h2><ul>
<li>文本处理<ul>
<li>pipeline 架构</li>
<li>语言规范</li>
</ul>
</li>
<li>回归<ul>
<li>时域模型</li>
<li>声学特征</li>
</ul>
</li>
<li>波形生成<ul>
<li>声学特征</li>
<li>信号处理</li>
</ul>
</li>
</ul>
<h3 id="4-1-语言规范"><a href="#4-1-语言规范" class="headerlink" title="4.1 语言规范"></a>4.1 语言规范</h3><p><img src="/images/blog/merlin_tts_tch9.png" alt="TTS merlin技术路线"></p>
<p>使用前端工具从文本中抽取特征</p>
<p><img src="/images/blog/merlin_tts_tch10.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-文本预处理"><a href="#4-2-文本预处理" class="headerlink" title="4.2 文本预处理"></a>4.2 文本预处理</h3><p>对应的文本处理pipeline为<br><img src="/images/blog/merlin_tts_tch11.png" alt="TTS merlin技术路线"></p>
<p>而前端之中的文本预处理详细的划分为：<br><img src="/images/blog/merlin_tts_tch12.png" alt="TTS merlin技术路线"></p>
<p>需要注意的是，<code>tokenize</code>,<code>POS tag</code>,<code>LTS</code>,<code>Phrase breaks</code>,<code>intonation</code>等都是从标记数据中独立学习得到的。</p>
<h4 id="4-2-1-Tokenize-amp-Normalize"><a href="#4-2-1-Tokenize-amp-Normalize" class="headerlink" title="4.2.1 Tokenize &amp; Normalize"></a>4.2.1 Tokenize &amp; Normalize</h4><p><img src="/images/blog/merlin_tts_tch13.png" alt="TTS merlin技术路线"></p>
<ol>
<li><p>第一步：将输入流划分为token，即潜在的单词</p>
<ul>
<li>对于英语和其他语言</li>
</ul>
<ul>
<li>基于规则</li>
<li>空格和标点都是很好的特征</li>
</ul>
<ul>
<li>对于许多其他语言，特别是没有使用空格的</li>
</ul>
<ul>
<li>可能i更困难</li>
<li>需要其他技术</li>
</ul>
</li>
<li>第二步：对每个token分类，找到非标注词(Non-Standard Words)，需要做进一步预处理</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch14.png" alt="TTS merlin技术路线"></p>
<ol>
<li>第三步： 对每一类非标准词(NSW)，使用一些特殊模块来处理。<br><img src="/images/blog/merlin_tts_tch15.png" alt="TTS merlin技术路线"></li>
</ol>
<h4 id="4-2-2-POS-tagging-词性标注"><a href="#4-2-2-POS-tagging-词性标注" class="headerlink" title="4.2.2 POS tagging 词性标注"></a>4.2.2 POS tagging 词性标注</h4><p><img src="/images/blog/merlin_tts_tch16.png" alt="TTS merlin技术路线"></p>
<ul>
<li>Part-Of-Speech tagger</li>
<li>准确率可能很高</li>
<li>在标注过的数据集上训练</li>
<li>类别是为文本设计的，而非语音</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch16_1.png" alt="TTS merlin技术路线"></p>
<h4 id="4-2-3-Pronuncication-LTS"><a href="#4-2-3-Pronuncication-LTS" class="headerlink" title="4.2.3  Pronuncication /LTS"></a>4.2.3  Pronuncication /LTS</h4><p><img src="/images/blog/merlin_tts_tch17.png" alt="TTS merlin技术路线"></p>
<ul>
<li>发音模型：<ul>
<li>查找词典，等等</li>
<li>单词到声音的模型</li>
</ul>
</li>
<li><p>但是：</p>
<ul>
<li>需要深层次的语言知识来设计发音集合</li>
<li>需要人类专家来撰写词典</li>
</ul>
<p>发音词典示例</p>
<p><img src="/images/blog/merlin_tts_tch18.png" alt="TTS merlin技术路线"></p>
</li>
</ul>
<h4 id="4-2-4-语言规范"><a href="#4-2-4-语言规范" class="headerlink" title="4.2.4 语言规范"></a>4.2.4 语言规范</h4><p>得到语言规范如下</p>
<p><img src="/images/blog/merlin_tts_tch19.png" alt="TTS merlin技术路线"></p>
<h2 id="5-语言特征工程"><a href="#5-语言特征工程" class="headerlink" title="5 语言特征工程"></a>5 语言特征工程</h2><p><img src="/images/blog/merlin_tts_tch20.png" alt="TTS merlin技术路线"></p>
<h3 id="5-1-术语"><a href="#5-1-术语" class="headerlink" title="5.1 术语"></a>5.1 术语</h3><ul>
<li>Flatten：<code>语言规范</code>$\rightarrow$ <code>上下文依赖的音素序列</code></li>
<li>Encode：<code>上下文依赖的音素序列$$\rightarrow$$向量序列</code></li>
<li>Upsample： <code>向量序列$$\rightarrow$$在声学特征framerate帧率上的向量序列</code></li>
</ul>
<h3 id="5-2-Flatten-amp-encode-将语言规范转换为向量序列"><a href="#5-2-Flatten-amp-encode-将语言规范转换为向量序列" class="headerlink" title="5.2 Flatten &amp; encode:将语言规范转换为向量序列"></a>5.2 Flatten &amp; encode:将语言规范转换为向量序列</h3><p><img src="/images/blog/merlin_tts_tch21.png" alt="TTS merlin技术路线"></p>
<h3 id="5-3-Upsample：添加时域信息"><a href="#5-3-Upsample：添加时域信息" class="headerlink" title="5.3 Upsample：添加时域信息"></a>5.3 Upsample：添加时域信息</h3><p><img src="/images/blog/merlin_tts_tch22.png" alt="TTS merlin技术路线"></p>
<h2 id="6-统计模型"><a href="#6-统计模型" class="headerlink" title="6 统计模型"></a>6 统计模型</h2><h3 id="6-1-声学模型：一个简单的前馈神经网络"><a href="#6-1-声学模型：一个简单的前馈神经网络" class="headerlink" title="6.1  声学模型：一个简单的前馈神经网络"></a>6.1  声学模型：一个简单的前馈神经网络</h3><p><img src="/images/blog/merlin_tts_tch23.png" alt="TTS merlin技术路线"></p>
<p>有向权重连接</p>
<p>这些网络层的不同作用：</p>
<p><img src="/images/blog/merlin_tts_tch24.png" alt="TTS merlin技术路线"></p>
<h3 id="6-2-用神经网络来合成"><a href="#6-2-用神经网络来合成" class="headerlink" title="6.2 用神经网络来合成"></a>6.2 用神经网络来合成</h3><p><img src="/images/blog/merlin_tts_tch25.png" alt="TTS merlin技术路线"></p>
<h2 id="7-波形生成（waveform-generator）"><a href="#7-波形生成（waveform-generator）" class="headerlink" title="7 波形生成（waveform generator）"></a>7 波形生成（waveform generator）</h2><h3 id="7-1-声学特征是什么"><a href="#7-1-声学特征是什么" class="headerlink" title="7.1 声学特征是什么"></a>7.1 声学特征是什么</h3><p><img src="/images/blog/merlin_tts_tch26.png" alt="TTS merlin技术路线"></p>
<h2 id="8-使用神经网络的TTS系统"><a href="#8-使用神经网络的TTS系统" class="headerlink" title="8 使用神经网络的TTS系统"></a>8 使用神经网络的TTS系统</h2><p>如果我们把所有的这一切综合起来的示意图如下：</p>
<p>第一步：<br><img src="/images/blog/merlin_tts_tch27.png" alt="TTS merlin技术路线"></p>
<p>第二步：<br><img src="/images/blog/merlin_tts_tch28.png" alt="TTS merlin技术路线"><br>第三步：<br><img src="/images/blog/merlin_tts_tch29.png" alt="TTS merlin技术路线"><br>第四步<br><img src="/images/blog/merlin_tts_tch30.png" alt="TTS merlin技术路线"><br>第五步<br><img src="/images/blog/merlin_tts_tch31.png" alt="TTS merlin技术路线"></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>语音合成：MSD-HMM多空间概率分布HMM</title>
    <url>/2018/08/10/TTS_MSD_HMM/</url>
    <content><![CDATA[<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一  概念"></a>一  概念</h2><p>从数据中抽取音高(Pitch)之后，我们发现无法对音高建模，因为数据中同时存在离散和连续的值。</p>
<h3 id="1-1-多空间概率分布"><a href="#1-1-多空间概率分布" class="headerlink" title="1.1 多空间概率分布"></a>1.1 多空间概率分布</h3><p>多空间概率分布的示意图如下:</p>
<p><img src="/images/blog/multi_space_pdf_2.png" alt="多空间概率分布示意图"></p>
<p>如上图M的一个样本空间由G个空间构成:，其中$\Omega <em>g^{g=1}$是一个 $n_g$维的空间$R^{n_g}$。每个空间都有自己的维度，其中一些可能有着相同的维度。每个空间$\Omega _g$都有其出现的概率$w_g$,即$P(\Omega)=w_g$,其中$\sum </em>{g=1} ^Gw_g=1$如果$n_g&gt;0$则每个空间都有一个概率密度函数$N_g(x),x\epsilon R^{n_g}$ 。如果$n_g$=0则认为$\Omega _g$仅包括一个样点,且概率$P(\Omega)$被定义为$P(\Omega)=1$。</p>
<p>一个具体问题来解释这个过程。</p>
<p><img src="/images/blog/multi_space_sample.png" alt="假设问题"></p>
<p> 一个人在池塘钓鱼。池塘中有红色的鱼,蓝色的鱼以及乌龟,此外,水中还有一些垃圾。当这个人钓到一条鱼的时候,他会观察鱼的种类和大小,如长度和重量当他钓到一只乌龟,则会观察乌龟壳的直径这里假设乌龟壳是圆形的而当他钓上一些垃圾的时候,则不会关心垃圾的任何属性。在这个例子中,我们可以看到全部的样本空间由四个空间组成。</p>
<ul>
<li>$\Omega _1$ ,二维的空间,代表着红色的鱼的长度及重量。</li>
<li>$\Omega _2$ ,二维的空间,代表着蓝色的鱼的长度及重量。</li>
<li>$\Omega _3$ ,一维的空间,代表着乌龟壳的直径。</li>
<li>$\Omega _4$ ,零维的空间,代表垃圾</li>
</ul>
<p>权重 $w_1,w_2,w_3,w_4$ 由池塘中红鱼蓝鱼,乌龟和垃圾所占的比例所决定。函数$N_1(\dot)$ 和 $N_2(\dot)$ 分别是关于红鱼和蓝鱼大小的二维概率密度函数长度和重量。函数 $N_3(\dot)$ 则是关于乌龟的一维的概率密度函数。例如这个人钓到一条红鱼,则观察向量 $o=(\lbrace 1\rbrace,x)$ 。其中 $x$ 为一个两维的向量,代表红鱼的长度和重量。假设这个人日夜不停的钓鱼,如果在夜晚,他无法区分鱼的颜色,只能丈量鱼的长度和重量,在这种情况下,鱼的观测向量$o=(\lbrace 1,2 \rbrace,x)$</p>
<h3 id="1-2-MSD-HMM"><a href="#1-2-MSD-HMM" class="headerlink" title="1.2 MSD-HMM"></a>1.2 MSD-HMM</h3><p>根据上述多空间概率分布的定义,人们提出了一MSD-HMM结构如下图所示。</p>
<p><img src="/images/blog/msd-hmm.png" alt="MSD_HMM示意图"></p>
<p> MSD-HMM的初始分布 $\pi$ 二及转移矩阵A都与其他的HMM相同,此处仅给出状态输出分布概率:</p>
<script type="math/tex; mode=display">
 b_i(o)=\sum _{g\belong S(o)}w_{ig}N_{ig}(V(o))</script><p>实际上,MSD-HMM同时包含了连续HMM和离散HMM。当 $n_g=0$ 时MSD-HMM与离散HMM是完全相同的。当 $S(o_t)$ 代表且仅代表一个特定的空间时,MSD-HMM则与连续HMM相同。由于MSD-HMM的这个特性,它很适合用于描述语音信号中基音周期。</p>
<h2 id="二-上下文相关的声学建模技术"><a href="#二-上下文相关的声学建模技术" class="headerlink" title="二 上下文相关的声学建模技术"></a>二 上下文相关的声学建模技术</h2><p>在连续的语音中,人们的发音普遍会受到上下文的影响而发生变化,这就是连续语音之间的协同发音现象。上下文无关的建模方法对每个识别基元分别独立建模,忽略了这种协同发音的现象,因而采用上下文无关模型的语音合成系统,其合成语音会出现不连贯或一字一顿等现象,所以无法取得较高的自然度。解决这个问题的方法是使用上下文相关的建模方法。上下文相关的建模方法在语音识别中有很多的应用,在语音合成中,我们也可以采用这个方法。与上下文无关的建模方法相比,上下文相关建模方法需要考虑如下的几个问题：</p>
<ol>
<li><p>如何选取基本识别基元。对于汉语语音系统而言,常用的基本基元有音节、声韵母和音素。由于汉语有个无调音节,如考虑语调则超过个音节,如果考虑上下文相关的变化,则会由于基元数目太多而导致模型无法实现。而声韵母与音素的数目都相对很少,因此可以用来作为上下文相关模型的基元</p>
</li>
<li><p>如何降低模型的规模。即使采用声韵母或音素作为上下文相关模型的基元,模型的规模仍然非常巨大。假设基元的个数为,则有,个可能的上下文相关基元。如果每个基元分为五个状态,每个状态采用单个高斯分布来描述,系统中仍然有,个高斯分布,如此大规模的模型会导致系统的速度下降,模型存储空间占用巨大,而且在训练数据库不是足够大的情况下,很多基元会存在训练不充分的问题。解决的办法是采用参数共享的技术。例如进行状态共享建模,或者混合密度共享建模。</p>
</li>
<li><p>如何预测在训练数据中没有出现的基元。在上下文相关的声学模型中,由于训练数据的限制,有些基元可能在训练数据中完全不出现,但是可能出现在待合成的数据中。为了保证合成过程的顺利进行,我们必须采取的补救措施保证每个基元都能找到模型描述。通常使用的方法是基于决策树的策略,使用那些可见基元的分布来合成在训练数据中不可见的基元。</p>
</li>
</ol>
<p>在实际中的上下文相关声学建模技术中,通常采用决策树与状态共享相结合的策略,这样既可以降低模型规模,避免训练不充分问题,还可以有效合成那些训练数据中不可见的基元。</p>
<h3 id="2-1-合成基元的选择"><a href="#2-1-合成基元的选择" class="headerlink" title="2.1 合成基元的选择"></a>2.1 合成基元的选择</h3><p><strong>音节</strong>： 汉语约有个无调音节和多个有调音节。在进行上下文无关的声学建模时,选用音节作为基元可以取得比较好的性能。但如果使用上下文相关的建模,由前接一当前一后续所组成的元组的数目将过于庞大,故采用音节作为基元并不合适。</p>
<p><strong>音素</strong>：汉语有大约个音素,但音素并没有反映出汉语语音的特点,而且,相对于声韵母,音素显得十分不稳定,这就给标注带来了困难,进而影响声学建模。</p>
<p><strong>声韵母</strong></p>
<ul>
<li><p>声韵结构是汉语音节特有的结构,使用声韵母基元,可以利用已有的语言学知识,进而提高声学模型的性能</p>
</li>
<li><p>使用声韵母作为识别基元,上下文相关信息也变得比较确定</p>
</li>
<li><p>选择声韵母作为基元,使得语音段的长度和基元数目比较适当</p>
</li>
</ul>
<h3 id="声韵母作为基元"><a href="#声韵母作为基元" class="headerlink" title="声韵母作为基元"></a>声韵母作为基元</h3><p><strong>基元的定义</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基元</th>
<th>元素</th>
</tr>
</thead>
<tbody>
<tr>
<td>声母</td>
<td>b、p、m、f、d、t、n、l、g、k、h、j、q、x、zh、ch、sh、z、c、s 、 y、w、r，_A,_E,_I,_O,_U</td>
</tr>
<tr>
<td>韵母</td>
<td>ai 、ei、 ui 、ao、 ou、 iu 、ie 、ve、 er、 an 、en 、in、 un 、vn 、ang 、eng、 ing 、ong ，an 、en 、in、 un 、vn，ang 、eng、 ing 、ong</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三-基于决策树的状态共享"><a href="#三-基于决策树的状态共享" class="headerlink" title="三 基于决策树的状态共享"></a>三 基于决策树的状态共享</h2><p>根据上面定义的基本基元,以这些基本基元为中心,考虑它们上下文相关的情况,我们可以将每个上下文相关的基元表示为 $l-c+r/env$ 的方式,其中 $c$ 为中心基元,$l$ 为左相关信息,$r$ 为右相关信息,$env$ 则表示该基元所在位置的一些环境特征。本系统中,环境特征包括前接音节字调,当前音节字调,后续音节字调,当前音节到前一自然停处的字数,当前音节到后一自然停顿处的字数,前接词的词性,当前词的词性,后续词的词性,当前音节在当前词中的位置,当前词的音节数,音节所在句的长度。</p>
<script type="math/tex; mode=display">
 L-C+R/A:a1\_a2\_a3/B:b1\_b2/C:c1\_c2\_c3/D:d1\_d2/E:e</script><p>其中 $C$ 代表当前元,$L$ 代表前接基元,$R$ 代表后接基元,$ABCDE$ 几项代表当前基元的上下文相关的一些特征。$a1$ 为前接字字调,$a2$ 为当前字字调,$a3$ 为后接字字调,$b?$ 为基元所在的字在当前停顿段落短语或短句中的位置,$b1$ 为到段落开始字的距离,$b2$ 为到段落结束字的距离,$c1$ 为前接词的词性, $c2$ 为当前词的词性,$c3$ 为后接词的词性,$d1$ 为当前字在当前词中的位置,为 $d2$ 当前词的字数, $e$ 为句子的总字数。为了将发音相似的基元共享到一起。本系统中使用决策树来实现参数共享的策略。这样做的好处是</p>
<ul>
<li><p>一是降低模型的规模</p>
</li>
<li><p>二是避免由于训练数据的稀疏性而造成训练不充分的问题</p>
</li>
<li><p>三是可以近似合成那些在训练数据中不存在的基元。</p>
</li>
</ul>
<p><strong>示例</strong></p>
<p>例如“他见了人就甜牙吠咬,咬住就不撒嘴”一句中的“见”字的韵母的标注为:</p>
<script type="math/tex; mode=display">
  j-ian4+1/A:1\_4\_5/B:2\_3/C:r\_v\_u/D:1\_1/E:15</script><h3 id="3-1-决策树划分特征的确定"><a href="#3-1-决策树划分特征的确定" class="headerlink" title="3.1 决策树划分特征的确定"></a>3.1 决策树划分特征的确定</h3><p>决策树的分裂依赖于问题集的设计。为了定义问题集,我们首先来确认划分特征。划分特征包括两大类,发音相似性和基元的上下文相关信息。</p>
<p>其中发音相似性的特征有以下几类,韵母划分特征,声母划分特征,单音划分特征。<strong>此划分，每个人不一样</strong></p>
<p><strong>声母的特征划分</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>划分特征</th>
<th>描述</th>
<th>基元列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stop</td>
<td>塞音</td>
<td>b,d,g,p,t,k</td>
</tr>
<tr>
<td>Aspirated Stop</td>
<td>塞送气音</td>
<td>b,d,g</td>
</tr>
<tr>
<td>Unaspirated Stop</td>
<td>非塞送气音</td>
<td>p,t,k</td>
</tr>
<tr>
<td>Affricate</td>
<td>塞擦音</td>
<td>z,zh,j,c,ch,q</td>
</tr>
<tr>
<td>Aspirated Affricate</td>
<td>塞擦送气音</td>
<td>z,zh,j</td>
</tr>
<tr>
<td>Unaspirated  Affricate</td>
<td>非塞擦送气音</td>
<td>C,zh,q</td>
</tr>
<tr>
<td>Fricative</td>
<td>擦音</td>
<td>f,s,sh,x,h,r</td>
</tr>
<tr>
<td>Fricative 2</td>
<td>擦音2</td>
<td>f,s,sh,x,h,r,k</td>
</tr>
<tr>
<td>Voiceless Fricative</td>
<td>擦清音</td>
<td>f,s,sh,x,h</td>
</tr>
<tr>
<td>Voice Fricative</td>
<td>浊清音</td>
<td>r,k</td>
</tr>
<tr>
<td>Nasal</td>
<td>鼻音</td>
<td>m,n,</td>
</tr>
<tr>
<td>Nasal 2</td>
<td>鼻音2</td>
<td>m,n,l</td>
</tr>
<tr>
<td>Labial</td>
<td>唇音</td>
<td>B,p,m</td>
</tr>
<tr>
<td>Labial 2</td>
<td>唇音2</td>
<td>B,p,m,f</td>
</tr>
<tr>
<td>Apical</td>
<td>顶音</td>
<td>Z,c,s,d,t,n,l,zh,sh,r</td>
</tr>
<tr>
<td>Apical Front</td>
<td>顶前音</td>
<td>Z,c,s</td>
</tr>
<tr>
<td>Apical 1</td>
<td>顶音1</td>
<td>D,t,n,l</td>
</tr>
<tr>
<td>Apical 2</td>
<td>顶音2</td>
<td>D,t</td>
</tr>
<tr>
<td>Apical 3</td>
<td>顶音3</td>
<td>N,l</td>
</tr>
<tr>
<td>Apical End</td>
<td>顶后音1</td>
<td>Zh,ch,sh</td>
</tr>
<tr>
<td>Apical End 2</td>
<td>顶后音2</td>
<td>Zh,ch,sh</td>
</tr>
<tr>
<td>Tongue Top</td>
<td>舌前音</td>
<td>J,q,x</td>
</tr>
<tr>
<td>Tongue Root</td>
<td>舌根音</td>
<td>G,k,h</td>
</tr>
<tr>
<td>Zero</td>
<td>零声母</td>
<td>_A,_E,_I,_O,_U,_V</td>
</tr>
<tr>
<td>XFuyin</td>
<td>全部声母（包含零声母）</td>
<td>全部</td>
</tr>
<tr>
<td>Fuyin</td>
<td>全部声母（不包含零声母）</td>
<td>不包含零声母</td>
</tr>
</tbody>
</table>
</div>
<p><strong>韵母的划分特征</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>划分特征</th>
<th>描述</th>
<th>基元特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single Yun</td>
<td>单韵母</td>
<td>A,I,u,e,v,ic,ih</td>
</tr>
<tr>
<td>Com Yun</td>
<td>复合韵母</td>
<td>An,ai,ang,…vn</td>
</tr>
<tr>
<td>Type A</td>
<td>含 a的韵母</td>
<td>A,ia,an,ang,ai,ua,ao</td>
</tr>
<tr>
<td>Type E</td>
<td>含e的韵母</td>
<td>E,ie,ve,ei,uei</td>
</tr>
<tr>
<td>Type I</td>
<td>含I的韵母</td>
<td>I,ai,ei,uei,ia,ian,iang,iao,ie,in,ing,iong,iou</td>
</tr>
<tr>
<td>Type O</td>
<td>含o的韵母</td>
<td>O,ao,uo,ou,ong,iou</td>
</tr>
<tr>
<td>Type U</td>
<td>含u的韵母</td>
<td>U,ua,uen,u,ueng,uo,iou</td>
</tr>
<tr>
<td>Type V</td>
<td>含v的韵母</td>
<td>V,vn,ve</td>
</tr>
</tbody>
</table>
</div>
<p>为了使得决策树的分裂更加细致,我们将每个声韵母作为一个划分特征,这就是单基元划分特征。最后再加上句首尾静音SIL,句中的由逗号和顿号造成的停顿PAU,句中其他的短停顿sp。</p>
<p><strong>上下文相关信息划分特征</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>上下文相关信息划分特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>基基元所在音节的前接音节的声调调</td>
</tr>
<tr>
<td>基基元所在的音节为的声调调</td>
</tr>
<tr>
<td>基基元所在音节的后接音节的声调调</td>
</tr>
<tr>
<td>基基元所在的音节在韵律短语中的位置正向</td>
</tr>
<tr>
<td>基基元所在的音节在韵律短语中的位置反向</td>
</tr>
<tr>
<td>基基元所在词的前接词的词性性</td>
</tr>
<tr>
<td>基基元所在词的词性性</td>
</tr>
<tr>
<td>基基元所在词的后接词的词性性</td>
</tr>
<tr>
<td>基基元在其所在词中的位置置</td>
</tr>
<tr>
<td>基基元所在词的音节数字数</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-2-决策树问题集的定义"><a href="#3-2-决策树问题集的定义" class="headerlink" title="3.2 决策树问题集的定义"></a>3.2 决策树问题集的定义</h3><p>在确定了划分特征后,我们根据划分特征来定义决策树的问题集。对于发音相似性的特征,每个特征都会对应三个问题左问题,中心问题和右问题。其中,对于单基元特征和声母的划分特征,其对应问题的答案是对称的。例如塞音(Stop)对应的三个问题为：</p>
<p><strong>发音相似的特征</strong></p>
<script type="math/tex; mode=display">
QS'L\_Stop' \quad \lbrace b- ^{\star} ,d- ^{\star},g- ^{\star},p- ^{\star},t- ^{\star},k- ^{\star} \rbrace  \\
QS'R\_Stop'\quad \lbrace ^{\star}+b/ ^{\star} ,^{\star}+d/ ^{\star},^{\star}g/^{\star},^{\star}p+/ ^{\star},^{\star}t+/ ^{\star},^{\star}k+/ ^{\star} \rbrace  \\
QS'C\_Stop'\quad \lbrace b- ^{\star} ,d- ^{\star},g- ^{\star},p- ^{\star},t- ^{\star},k- ^{\star} \rbrace</script><p>其中单引号中的部分为问题的标识,而大括号内的部分为问题的答案’,<strong>$\star$</strong>和”’<strong>?</strong>‘为通配符,如<strong>“$b-\star$”</strong>则代表所有以<strong>“$b-$”</strong>开头的上下文相关基元</p>
<p><strong>部分韵母的划分特征</strong></p>
<p>而对于部分韵母的划分特征,其问题的答案是非对称的。如</p>
<script type="math/tex; mode=display">
 QS'L\_Type\_A'\quad \lbrace a?- ^{\star},ia?-^{\star},ua?-^{\star},A-^{\star}\rbrace \\
QS'R\_Type\_A'\quad \lbrace ^{\star}+a?/^{\star},^{\star}+ai?/^{\star},^{\star}+an?/^{\star},^{\star}+ang/^{\star},^{\star}+ao?/^{\star},^{\star}+_A/^{\star}</script><p>因为这类问题的意思是左邻的发音是“$a$”,在这类问题中,复合韵母一般是不对称的。</p>
<p><strong>上下文相关信息的划分特征</strong></p>
<p>对于上下文相关信息的划分特征,问题的设计方式为首先对每个单独的划分特征建立各自的问题,然后,对关系密切的划分特征建立联合的问题。如</p>
<script type="math/tex; mode=display">
 QS'C\_tone1'\quad \lbrace ^{\star}A:?\_1\_?/B^{\star} \rbrace</script><p>代表所有当前音节为一声的基元</p>
<script type="math/tex; mode=display">
 QS'C\_tone3\_3'\quad \lbrace ^{\star}A:?\_3\_3?/B^{\star} \rbrace</script><p>则代表当前音节为三声而后续音节也为三声的基元。这样设计的好处是可以把汉语中一些变调的规则加入问题集中,经过训练,上下文相关的基元中可以包含变调的声音,最终提高合成语音的自然度。</p>
<h3 id="3-3-决策树的构建"><a href="#3-3-决策树的构建" class="headerlink" title="3.3 决策树的构建"></a>3.3 决策树的构建</h3><p>问题集建立后,则开始构造决策树。考虑到合成基元的拓扑结构其第一个状态和最后一个状态分别为起始状态和结束状态,他们不能驻留,只在模型中起辅助作用。其余的状态可以驻留或者转移到下一个状态。因此,真正起作用的是中间的几个状态。因此在构造决策树的时候,我们只考虑中间的几个状态。</p>
<p>决策树的的构造有两种方法</p>
<ol>
<li>方法对每个中心基元的每个状态分别构造决策树。这种方法假设当基元的中心音素不同时,基元之间相互独立,因此首先根据中心音素对所有的基元进行分类,然后在利用决策树来进行状态共享。图一给出了中心基元为的所有基元的状态组成的决策树示意图。</li>
</ol>
<p><img src="/images/blog/method1_descion_tree1.png" alt="决策树构建"></p>
<ol>
<li>方法对所有基元的同一个状态构造决策树。这种方法假设当中心音素不同时,基元之间仍然有一定的重叠。即使基元的中心音素不同,它们之间的状态仍然有可能共享。基元之间的状态共享情况完全依靠基于决策树的分类策略。图一给出了所有基元的状态组成的决策树示意图。</li>
</ol>
<p><img src="/images/blog/method1_descion_tree2.png" alt="决策树构建"></p>
<blockquote>
<p>这个树结构的意思是静音是第一个问题，就是对这个树分类影响最大的一个问题，后面的问题依次减弱，然后可能会一直分叉，直到决策树判断截止为止，所以的单元走到那个叶节点就截止了，能走到那个叶节点的单元都共享一个状态了，这个状态用高斯分布描述</p>
</blockquote>
<p>于方法1共需要构造基元总数有效状态数棵不同的决策树,这样,只有相同基元的状态才会被共享,这样对保证最后合成语音的单音清晰度有所帮助。而对于方法工,决策树的数量与基元的有效状态数相同,在这里,所有基元的状态进行共享,不同基元中一些发音相似的状态亦被共享到一起,有助于减小最终模型的规模,并且可以在一定程度上提高对训练集中未出现基元的鲁棒性。实验表明,当训练数据较少时句,方法的清晰度明显高于方法工,但在训练数据增加后句,两种方法合成语音的质量十分接近。考虑到方法近似合成未知基元的能力较好,本系统最终选择使用方法2进行决策树的构建。</p>
<p>决策树由自顶而下的顺序生成,首先,将所有的状态放入根节点中,然后进行节点分裂。节点分裂依赖于评估函数。决策树评估函数用来估计决策树的节点上的样本相似性。这里,我们定义对数似然概率作为为节点分裂的评估函数。在每个节点进行分裂的时候,我们从问题集中选择一个问题,然后根据此问题把节点分成两个子节点并且计算评估函数的增量,我们选择具有最大增量的问题,并且根据此问题把节点划分成两部分。当所有问题的增量都低于某个闭值的时候,节点上的分裂过程将停止。最终,同一个叶子节点中的状态将被共享到一起。</p>
<h2 id="四-模型的训练流程"><a href="#四-模型的训练流程" class="headerlink" title="四 模型的训练流程"></a>四 模型的训练流程</h2><p>训练部分主要依靠HTK工具包(加入HTS1.1补丁)加入补丁和SPTK3.0工具包完成。</p>
<ol>
<li>从语音文件中提取基音周期参数（使用auto-corelation方法）。</li>
<li>使用SPTK工具提取语音文件的MFCC及能量</li>
<li>将上述两步中得到的数据组合,计算差分,最终加入HTK文件头,得到格式的训练文件</li>
<li>使用上下文无关标注文件和训练文件进行上下文无关基元的HMM训练(使用HTK工具包中的HInit和HRest工具)</li>
<li>使用上下文相关标注文件和训练文件进行5次嵌入式训练。使用HeRest工具</li>
<li>引入问题集,对频谱HMM和基音周期HMM进行基于决策树的状态共享。使用HHed工具</li>
<li>使用上下文相关标注文件和训练文件进行5次嵌入式训练,在最后一次<br>迭代中,根据HMM的状态转移矩阵得到基元状态时长的HMM。使用HeRest工具</li>
<li>引入问题集,对状态时长HMM进行基于决策树的状态共享。使用HHed工具</li>
<li>输出二进制的模型文件</li>
</ol>
<h2 id="五-语音合成流程"><a href="#五-语音合成流程" class="headerlink" title="五 语音合成流程"></a>五 语音合成流程</h2><ol>
<li>使用进行分词,标注。</li>
<li>音字转换,同时获得声韵母基元的上下文环境信息</li>
<li>生成合成模块所需的格式文件</li>
<li>解析文字处理模块的文件,将其转换为带有环境信息的上下文相关基元序列</li>
<li>根据每个带有环境信息的上下文相关基元搜索并得到其相应的状态时长、基音周期和频谱的HMM。</li>
<li>由状态时长HMM得到基元个状态的持续时长。</li>
<li>根据状态的时长、基音周期HMM和频谱HMM,进行参数生成,得到每祯的基音周期、对数能量和MFCC参数。</li>
<li><p>将每祯的基音周期、对数能量和MFCC参数送入基于MSLA滤波器的合成器,得到合成语音</p>
<p><img src="/images/blog/tts_chn_paper.png" alt="合成流程"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>迁移学习实践-Tensorflow分类任务</title>
    <url>/2018/06/02/transfer-learning-pratice/</url>
    <content><![CDATA[<p>摘自 : <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" target="_blank" rel="noopener">medium transfer learning</a></p>
<h2 id="1-说明和准备"><a href="#1-说明和准备" class="headerlink" title="1 说明和准备"></a>1 说明和准备</h2><h3 id="1-1-任务问题"><a href="#1-1-任务问题" class="headerlink" title="1.1 任务问题"></a>1.1 任务问题</h3><p>我们要对只有4000张图片(3000张训练，1000张验证)的数据集做图像分类，分为<code>猫</code>和<code>狗</code>两类。图片数据可以从<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">kaggle 猫狗分类挑战</a>上下载到25000张，不过为了演示迁移学习，假定只有4000张图片。</p>
<h3 id="1-2-数据准备"><a href="#1-2-数据准备" class="headerlink" title="1.2 数据准备"></a>1.2 数据准备</h3><p>首先下载全部的数据集，然后筛选出其中的4000张。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">np.random.seed(42)</span><br><span class="line">files &#x3D; glob.glob(&#39;train&#x2F;*&#39;)</span><br><span class="line"># 载入全部的猫狗图片</span><br><span class="line">cat_files &#x3D; [fn for fn in files if &#39;cat&#39; in fn]</span><br><span class="line">dog_files &#x3D; [fn for fn in files if &#39;dog&#39; in fn]</span><br><span class="line">print(len(cat_files), len(dog_files))</span><br><span class="line"># (12500, 12500)</span><br><span class="line"></span><br><span class="line"># 筛选出其中的4000张图片</span><br><span class="line">cat_train &#x3D; np.random.choice(cat_files, size&#x3D;1500, replace&#x3D;False)</span><br><span class="line">dog_train &#x3D; np.random.choice(dog_files, size&#x3D;1500, replace&#x3D;False)</span><br><span class="line">cat_files &#x3D; list(set(cat_files) - set(cat_train))</span><br><span class="line">dog_files &#x3D; list(set(dog_files) - set(dog_train))</span><br><span class="line"></span><br><span class="line">cat_val &#x3D; np.random.choice(cat_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">dog_val &#x3D; np.random.choice(dog_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">cat_files &#x3D; list(set(cat_files) - set(cat_val))</span><br><span class="line">dog_files &#x3D; list(set(dog_files) - set(dog_val))</span><br><span class="line"></span><br><span class="line">cat_test &#x3D; np.random.choice(cat_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">dog_test &#x3D; np.random.choice(dog_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line"></span><br><span class="line">print(&#39;Cat datasets:&#39;, cat_train.shape, cat_val.shape, cat_test.shape)</span><br><span class="line">print(&#39;Dog datasets:&#39;, dog_train.shape, dog_val.shape, dog_test.shape)</span><br><span class="line"># Cat datasets: (1500,) (500,) (500,)</span><br><span class="line"># Dog datasets: (1500,) (500,) (500,)</span><br></pre></td></tr></table></figure>
<p>将数据子集单独放到其他文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_dir &#x3D; &#39;training_data&#39;</span><br><span class="line">val_dir &#x3D; &#39;validation_data&#39;</span><br><span class="line">test_dir &#x3D; &#39;test_data&#39;</span><br><span class="line"></span><br><span class="line">train_files &#x3D; np.concatenate([cat_train, dog_train])</span><br><span class="line">validate_files &#x3D; np.concatenate([cat_val, dog_val])</span><br><span class="line">test_files &#x3D; np.concatenate([cat_test, dog_test])</span><br><span class="line"></span><br><span class="line">os.mkdir(train_dir) if not os.path.isdir(train_dir) else None</span><br><span class="line">os.mkdir(val_dir) if not os.path.isdir(val_dir) else None</span><br><span class="line">os.mkdir(test_dir) if not os.path.isdir(test_dir) else None</span><br><span class="line"></span><br><span class="line">for fn in train_files:</span><br><span class="line">    shutil.copy(fn, train_dir)</span><br><span class="line"></span><br><span class="line">for fn in validate_files:</span><br><span class="line">    shutil.copy(fn, val_dir)</span><br><span class="line">    </span><br><span class="line">for fn in test_files:</span><br><span class="line">    shutil.copy(fn, test_dir)</span><br></pre></td></tr></table></figure>
<p>为模型准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img</span><br><span class="line"></span><br><span class="line">IMG_DIM &#x3D; (150, 150)</span><br><span class="line"></span><br><span class="line">train_files &#x3D; glob.glob(&#39;training_data&#x2F;*&#39;)</span><br><span class="line">train_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in train_files]</span><br><span class="line">train_imgs &#x3D; np.array(train_imgs)</span><br><span class="line">train_labels &#x3D; [fn.split(&#39;\\&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in train_files]</span><br><span class="line"></span><br><span class="line">validation_files &#x3D; glob.glob(&#39;validation_data&#x2F;*&#39;)</span><br><span class="line">validation_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in validation_files]</span><br><span class="line">validation_imgs &#x3D; np.array(validation_imgs)</span><br><span class="line">validation_labels &#x3D; [fn.split(&#39;\\&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in validation_files]</span><br><span class="line"></span><br><span class="line">print(&#39;Train dataset shape:&#39;, train_imgs.shape, </span><br><span class="line">      &#39;\tValidation dataset shape:&#39;, validation_imgs.shape)</span><br><span class="line"># Train dataset shape: (3000, 150, 150, 3)  </span><br><span class="line"># Validation dataset shape: (1000, 150, 150, 3)</span><br></pre></td></tr></table></figure>
<p>现在，我们得到了3000张训练集和1000张验证集，图像长宽为$150\times 150$，接下来，我们要将图片像素矩阵值取值范围缩放到0到1.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_imgs_scaled &#x3D; train_imgs.astype(&#39;float32&#39;)</span><br><span class="line">validation_imgs_scaled &#x3D; validation_imgs.astype(&#39;float32&#39;)</span><br><span class="line">train_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line">validation_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line"></span><br><span class="line">print(train_imgs[0].shape)</span><br><span class="line">array_to_img(train_imgs[0])</span><br></pre></td></tr></table></figure>
<p>一些基本参数，同时将字符型的分类类别改为数值型。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input_shape &#x3D; (150, 150, 3)</span><br><span class="line"></span><br><span class="line"># encode text category labels</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line"></span><br><span class="line">le &#x3D; LabelEncoder()</span><br><span class="line">le.fit(train_labels)</span><br><span class="line">train_labels_enc &#x3D; le.transform(train_labels)</span><br><span class="line">validation_labels_enc &#x3D; le.transform(validation_labels)</span><br><span class="line"></span><br><span class="line">print(train_labels[1495:1505], train_labels_enc[1495:1505])</span><br><span class="line"># [&#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;] [0 0 0 0 0 1 1 1 1 1]</span><br></pre></td></tr></table></figure>
<h2 id="2-基准模型"><a href="#2-基准模型" class="headerlink" title="2 基准模型"></a>2 基准模型</h2><p>先手写个基准的CNN模型，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Conv2D(16, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;, </span><br><span class="line">                 input_shape&#x3D;input_shape))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(64, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>模型架构如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Layer (type) Output Shape Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_1 (Conv2D) (None, 148, 148, 16) 448       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 (None, 74, 74, 16) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D) (None, 72, 72, 64) 9280      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D) (None, 34, 34, 128) 73856     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten) (None, 36992) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense) (None, 512) 18940416  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense) (None, 1) 513       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 19,024,513</span><br><span class="line">Trainable params: 19,024,513</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>
<p>我们设置<code>batch_size=30</code>，总共有3000张图片，也就是一个epoch需要100次迭代。我们训练30个epoch，然后验证模型。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">batch_size &#x3D; 30</span><br><span class="line">num_classes &#x3D; 2</span><br><span class="line">epochs &#x3D; 30</span><br><span class="line">history &#x3D; model.fit(x&#x3D;train_imgs_scaled, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_imgs_scaled, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>训练过程的输出如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Train on 3000 samples, validate on 1000 samples</span><br><span class="line">Epoch 1&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 10s - loss: 0.7583 - acc: 0.5627 - val_loss: 0.7182 - val_acc: 0.5520</span><br><span class="line">Epoch 2&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.6343 - acc: 0.6533 - val_loss: 0.5891 - val_acc: 0.7190</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 29&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.0314 - acc: 0.9950 - val_loss: 2.7014 - val_acc: 0.7140</span><br><span class="line">Epoch 30&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.0147 - acc: 0.9967 - val_loss: 2.4963 - val_acc: 0.7220</span><br></pre></td></tr></table></figure>
<p>训练集都接近100%准确率了，但是验证集准确率还只有72%。模型可能存在过拟合。可以使用如下的代码画出训练和验证过程的loss下降和准确率变化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f, (ax1, ax2) &#x3D; plt.subplots(1, 2, figsize&#x3D;(12, 4))</span><br><span class="line">t &#x3D; f.suptitle(&#39;Basic CNN Performance&#39;, fontsize&#x3D;12)</span><br><span class="line">f.subplots_adjust(top&#x3D;0.85, wspace&#x3D;0.3)</span><br><span class="line"></span><br><span class="line">epoch_list &#x3D; list(range(1,31))</span><br><span class="line">ax1.plot(epoch_list, history.history[&#39;acc&#39;], label&#x3D;&#39;Train Accuracy&#39;)</span><br><span class="line">ax1.plot(epoch_list, history.history[&#39;val_acc&#39;], label&#x3D;&#39;Validation Accuracy&#39;)</span><br><span class="line">ax1.set_xticks(np.arange(0, 31, 5))</span><br><span class="line">ax1.set_ylabel(&#39;Accuracy Value&#39;)</span><br><span class="line">ax1.set_xlabel(&#39;Epoch&#39;)</span><br><span class="line">ax1.set_title(&#39;Accuracy&#39;)</span><br><span class="line">l1 &#x3D; ax1.legend(loc&#x3D;&quot;best&quot;)</span><br><span class="line"></span><br><span class="line">ax2.plot(epoch_list, history.history[&#39;loss&#39;], label&#x3D;&#39;Train Loss&#39;)</span><br><span class="line">ax2.plot(epoch_list, history.history[&#39;val_loss&#39;], label&#x3D;&#39;Validation Loss&#39;)</span><br><span class="line">ax2.set_xticks(np.arange(0, 31, 5))</span><br><span class="line">ax2.set_ylabel(&#39;Loss Value&#39;)</span><br><span class="line">ax2.set_xlabel(&#39;Epoch&#39;)</span><br><span class="line">ax2.set_title(&#39;Loss&#39;)</span><br><span class="line">l2 &#x3D; ax2.legend(loc&#x3D;&quot;best&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_1.png" alt=""></p>
<p>上图左可以看到，3个epoch之后就开始过拟合了，训练准确率一直上升，但是验证准确率保持不变了。</p>
<h3 id="2-1-简单的优化模型"><a href="#2-1-简单的优化模型" class="headerlink" title="2.1 简单的优化模型"></a>2.1 简单的优化模型</h3><p>上面的CNN是个基本的架构，接下来我们做一些优化策略，网络架构上加入正则化，使用一定概率的dropout。只修改网络结构部分，其他训练代码不变。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Conv2D(16, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;, </span><br><span class="line">                 input_shape&#x3D;input_shape))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(64, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])          </span><br><span class="line">history &#x3D; model.fit(x&#x3D;train_imgs_scaled, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_imgs_scaled, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>使用上面的画图代码，画出训练曲线。</p>
<p><img src="/images/blog/transfer_learning_pratice_2.png" alt=""></p>
<p>有所改善，但是效果不明显。依然是过拟合，究其原因，数据量太少，可以使用部分的数据集增强策略增加数据多样性。</p>
<h3 id="2-2-使用数据增强策略"><a href="#2-2-使用数据增强策略" class="headerlink" title="2.2 使用数据增强策略"></a>2.2 使用数据增强策略</h3><p>keras的<code>ImageDataGenerator</code>自带了一些数据增强方法，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br></pre></td></tr></table></figure>
<p>当然，我们还可以使用 <a href="https://github.com/albu/albumentations" target="_blank" rel="noopener">Albumentations</a>来做更多的增强策略。我们先来看看<code>ImageDataGenerator</code>增强之后的图片效果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mg_id &#x3D; 2595</span><br><span class="line">cat_generator &#x3D; train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],</span><br><span class="line">                                   batch_size&#x3D;1)</span><br><span class="line">cat &#x3D; [next(cat_generator) for i in range(0,5)]</span><br><span class="line">fig, ax &#x3D; plt.subplots(1,5, figsize&#x3D;(16, 6))</span><br><span class="line">print(&#39;Labels:&#39;, [item[1][0] for item in cat])</span><br><span class="line">l &#x3D; [ax[i].imshow(cat[i][0][0]) for i in range(0,5)]</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_3.png" alt=""></p>
<p>再次使用上面的基准模型(加了dropout层的)，此次我们将学习率稍微改小点，将默认的<code>1e-3</code>改为<code>1e-4</code>，防止模型过拟合，此时的数据量增多了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-4),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])        </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>打印训练曲线<br><img src="/images/blog/transfer_learning_pratice_4.png" alt=""></p>
<p>模型的准确率提升到<strong>82%</strong>，而且已经不再过拟合了。</p>
<h2 id="3-使用其他模型做迁移学习"><a href="#3-使用其他模型做迁移学习" class="headerlink" title="3 使用其他模型做迁移学习"></a>3 使用其他模型做迁移学习</h2><h3 id="3-1-VGG-16模型"><a href="#3-1-VGG-16模型" class="headerlink" title="3.1 VGG-16模型"></a>3.1 VGG-16模型</h3><p>分类模型，我们选用VGG16为例。首先，我们需要理解VGG16的模型架构，如下</p>
<p><img src="/images/blog/transfer_learning_pratice_5.png" alt=""></p>
<p>13个$3\times 3$的卷积，5个maxpooling缩减了网络输入尺寸。在两个全连接层之前的输出是4096个神经元，全连接都是1000个神经元(代表了1000个分类)。由于我们要做的是做猫狗分类，最后三层是不需要的。我们更关心前5个blocks(下图)，我们可以将VGG模型看做一个特征抽取器。下图是VGG模型的三种用法</p>
<p><img src="/images/blog/transfer_learning_pratice_6.png" alt=""></p>
<ul>
<li>如果我们只是作为特征抽取器，则按照图中中间的示例，冻结所有的blocks(5个)，在训练过程中，这些blocks中的所有参数都不会更新。</li>
<li>如果我们做fine-tuning，可以考虑按照图右冻结前3个blocks，更新后面两个blocks(4和5)的参数（每个训练epoch过程都会）。</li>
</ul>
<h3 id="3-2-将预训练模型作为特征抽取器"><a href="#3-2-将预训练模型作为特征抽取器" class="headerlink" title="3.2 将预训练模型作为特征抽取器"></a>3.2 将预训练模型作为特征抽取器</h3><p>3.1节中最后一张图的中间的架构，冻结所有blocks层的参数的用法。下面是对应的代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras.applications import vgg16</span><br><span class="line">from keras.models import Model</span><br><span class="line">import keras</span><br><span class="line"></span><br><span class="line">vgg &#x3D; vgg16.VGG16(include_top&#x3D;False, weights&#x3D;&#39;imagenet&#39;, </span><br><span class="line">                                     input_shape&#x3D;input_shape)</span><br><span class="line"></span><br><span class="line">output &#x3D; vgg.layers[-1].output</span><br><span class="line">output &#x3D; keras.layers.Flatten()(output)</span><br><span class="line">vgg_model &#x3D; Model(vgg.input, output)</span><br><span class="line"></span><br><span class="line">vgg_model.trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    layer.trainable &#x3D; False</span><br><span class="line">    </span><br><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&#39;max_colwidth&#39;, -1)</span><br><span class="line">layers &#x3D; [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]</span><br><span class="line">pd.DataFrame(layers, columns&#x3D;[&#39;Layer Type&#39;, &#39;Layer Name&#39;, &#39;Layer Trainable&#39;])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_7.png" alt=""></p>
<p>此处，将VGG模型看做SURF或者HOG特征之类的东西就可以，使用过程不更新参数，直接输入图片，预测得到特征。用法如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bottleneck_feature_example &#x3D; vgg.predict(train_imgs_scaled[0:1])</span><br><span class="line">print(bottleneck_feature_example.shape)</span><br><span class="line">plt.imshow(bottleneck_feature_example[0][:,:,0])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_8.png" alt=""></p>
<p>从训练数据和验证数据中使用VGG16抽取特征如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_bottleneck_features(model, input_imgs):</span><br><span class="line">    features &#x3D; model.predict(input_imgs, verbose&#x3D;0)</span><br><span class="line">    return features</span><br><span class="line"></span><br><span class="line">train_features_vgg &#x3D; get_bottleneck_features(vgg_model, train_imgs_scaled)</span><br><span class="line">validation_features_vgg &#x3D; get_bottleneck_features(vgg_model, validation_imgs_scaled)</span><br><span class="line">print(&#39;Train Bottleneck Features:&#39;, train_features_vgg.shape, </span><br><span class="line">      &#39;\tValidation Bottleneck Features:&#39;, validation_features_vgg.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Train Bottleneck Features: (3000, 8192)  </span><br><span class="line">Validation Bottleneck Features: (1000, 8192)</span><br></pre></td></tr></table></figure>
<p>接下来，我们可以以VGG作为特征抽取器重新构建一个训练模型。其实，在抽取特征之后直接接一个SVM或者KNN分类器也是一样的。下面以keras代码，重新构建CNN模型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"># 此时的vgg_model 已经设置了trainable&#x3D;False</span><br><span class="line">input_shape &#x3D; vgg_model.output_shape[1]</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(InputLayer(input_shape&#x3D;(input_shape,)))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-4),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>网络结构如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type) Output Shape Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">input_2 (InputLayer) (None, 8192) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense) (None, 512) 4194816   </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout) (None, 512) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense) (None, 512) 262656    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout) (None, 512) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense) (None, 1) 513       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 4,457,985</span><br><span class="line">Trainable params: 4,457,985</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>
<p><strong>注意，此时的训练代码中输入数据不再是图片，而是VGG抽取的特征了</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">history &#x3D; model.fit(x&#x3D;train_features_vgg, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_features_vgg, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_9.png" alt=""></p>
<p>验证准确率提升到<strong>88%</strong>，虽然看起来依然过拟合了。</p>
<h3 id="3-3-使用数据增强-VGG作为特征抽取器"><a href="#3-3-使用数据增强-VGG作为特征抽取器" class="headerlink" title="3.3 使用数据增强+VGG作为特征抽取器"></a>3.3 使用数据增强+VGG作为特征抽取器</h3><p>由于我们使用data generator，此处不再用VGG作为特征抽取器.此部分与上面的区别在于，上面的VGG模型不是网络的一部分，属于数据处理部分，用vgg将处理图片(特征抽取)之后的数据传入了新的小网络。而当前是将VGG作为网络的一部分，与新的网络层，构建了一个新模型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line"></span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br><span class="line">train_generator &#x3D; train_datagen.flow(train_imgs, train_labels_enc, batch_size&#x3D;30)</span><br><span class="line">val_generator &#x3D; val_datagen.flow(validation_imgs, validation_labels_enc, batch_size&#x3D;20)</span><br></pre></td></tr></table></figure>
<p>网络构建部分<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras.applications import vgg16</span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line">from keras.models import Model</span><br><span class="line">import keras</span><br><span class="line"></span><br><span class="line">vgg &#x3D; vgg16.VGG16(include_top&#x3D;False, weights&#x3D;&#39;imagenet&#39;, </span><br><span class="line">                                     input_shape&#x3D;input_shape)</span><br><span class="line"></span><br><span class="line">output &#x3D; vgg.layers[-1].output</span><br><span class="line">output &#x3D; keras.layers.Flatten()(output)</span><br><span class="line">vgg_model &#x3D; Model(vgg.input, output)</span><br><span class="line"></span><br><span class="line">vgg_model.trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    layer.trainable &#x3D; False</span><br><span class="line"># 下面是我们新加的网络层，将VGG放在前面</span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(vgg_model)</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"># 学习率变小了</span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;2e-5),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">              </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure><br>此时的学习曲线，如下</p>
<p><img src="/images/blog/transfer_learning_pratice_10.png" alt=""></p>
<p>此时的验证准确率提升到了90%，而且没有过拟合。</p>
<h3 id="3-5-fine-tuning-预训练的VGG模型-数据增强"><a href="#3-5-fine-tuning-预训练的VGG模型-数据增强" class="headerlink" title="3.5 fine-tuning 预训练的VGG模型+数据增强"></a>3.5 fine-tuning 预训练的VGG模型+数据增强</h3><p>此部分，可以参考3.1节部分VGG示意图的最右边那张图，此时vgg模型中某些blocks的参数也在训练过程中得到更新。如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vgg_model.trainable &#x3D; True</span><br><span class="line"></span><br><span class="line">set_trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    if layer.name in [&#39;block5_conv1&#39;, &#39;block4_conv1&#39;]:</span><br><span class="line">        set_trainable &#x3D; True</span><br><span class="line">    if set_trainable:</span><br><span class="line">        layer.trainable &#x3D; True</span><br><span class="line">    else:</span><br><span class="line">        layer.trainable &#x3D; False</span><br><span class="line">        </span><br><span class="line">layers &#x3D; [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]</span><br><span class="line">pd.DataFrame(layers, columns&#x3D;[&#39;Layer Type&#39;, &#39;Layer Name&#39;, &#39;Layer Trainable&#39;])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_11.png" alt=""></p>
<p>可以看到<code>block4</code>和<code>block5</code>已经变成可以训练了。此时再次减小学习率，同时使用了增强了的数据处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br><span class="line">train_generator &#x3D; train_datagen.flow(train_imgs, train_labels_enc, batch_size&#x3D;30)</span><br><span class="line">val_generator &#x3D; val_datagen.flow(validation_imgs, validation_labels_enc, batch_size&#x3D;20)</span><br><span class="line"></span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(vgg_model)</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-5),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">              </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_12.png" alt=""></p>
<p>可以看到，验证准确率已经提升到了<strong>96%</strong>，与基准模型相比，提升了24%。</p>
<h2 id="4-测试模型"><a href="#4-测试模型" class="headerlink" title="4 测试模型"></a>4 测试模型</h2><p>接下来在测试集上测试上面的5种模型</p>
<ol>
<li>基准CNN模型</li>
<li>使用了数据增强的基准CNN模型</li>
<li>迁移学习：使用VGG16作为特征抽取器【VGG只用在数据处理上】</li>
<li>迁移学习：使用VGG作为模型的一部分，并且使用了数据增强策略</li>
<li>迁移学习：对VGG模型微调，让其<code>block4</code>和<code>block5</code>参数可更新</li>
</ol>
<p>测试代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IMG_DIM &#x3D; (150, 150)</span><br><span class="line"></span><br><span class="line">test_files &#x3D; glob.glob(&#39;test_data&#x2F;*&#39;)</span><br><span class="line">test_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in test_files]</span><br><span class="line">test_imgs &#x3D; np.array(test_imgs)</span><br><span class="line">test_labels &#x3D; [fn.split(&#39;&#x2F;&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in test_files]</span><br><span class="line"></span><br><span class="line">test_imgs_scaled &#x3D; test_imgs.astype(&#39;float32&#39;)</span><br><span class="line">test_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line">test_labels_enc &#x3D; class2num_label_transformer(test_labels)</span><br><span class="line"></span><br><span class="line">print(&#39;Test dataset shape:&#39;, test_imgs.shape)</span><br><span class="line">print(test_labels[0:5], test_labels_enc[0:5])</span><br></pre></td></tr></table></figure>
<p>测试基准模型的代码如下（其他模型类似）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">predictions &#x3D; basic_cnn.predict_classes(test_imgs_scaled, verbose&#x3D;0)</span><br><span class="line">predictions &#x3D; num2class_label_transformer(predictions)</span><br><span class="line">meu.display_model_performance_metrics(true_labels&#x3D;test_labels, predicted_labels&#x3D;predictions, </span><br><span class="line">                                      classes&#x3D;list(set(test_labels)))</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_13.png" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习：迁移学习</title>
    <url>/2018/05/11/LHY_transferlearning/</url>
    <content><![CDATA[<p>笔记视频：<a href="https://www.bilibili.com/video/av15889450/#page=26" target="_blank" rel="noopener">https://www.bilibili.com/video/av15889450/#page=26</a></p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1 基本概念"></a>1 基本概念</h2><p>迁移学习：当前训练的数据集与目标任务没有直接相关。此处的不直接相关特指以下情形：</p>
<ul>
<li>相同领域，但是不同任务。比如都是对动物分类的，但是分类的目标不同，比如只有猫和狗的图片数据，但是分类任务是对大象和老虎分类。</li>
</ul>
<p><img src="/images/blog/transfer_learning1.jpg" alt=""><br><img src="/images/blog/transfer_learning2.jpg" alt=""></p>
<ul>
<li>领域不同，但是任务相同。 比如同样是对猫和狗做分类，但是数据是真是相机拍摄的图像，而目标是招财猫和动漫狗，它们的数据分布不一致</li>
</ul>
<p><img src="/images/blog/transfer_learning3.jpg" alt=""></p>
<h3 id="1-1-为何会考虑迁移学习"><a href="#1-1-为何会考虑迁移学习" class="headerlink" title="1.1 为何会考虑迁移学习"></a>1.1 为何会考虑迁移学习</h3><p>数据不充足的情况下，可能会考虑使用迁移学习，比如以下情况。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>目标领域</th>
<th>目标任务</th>
<th>不相关的数据</th>
</tr>
</thead>
<tbody>
<tr>
<td>语音识别</td>
<td>对台湾语做识别</td>
<td>从youtube上爬取英文、中文语音数据训练模型来迁移学习</td>
</tr>
<tr>
<td>图像分类</td>
<td>医疗数据极度缺乏，做医疗诊断时</td>
<td>使用已有的海量图像数据(coco,imagenet等)</td>
</tr>
<tr>
<td>文本分析</td>
<td>特定领域，比如法律文件分析</td>
<td>可以从其他领域的文本分析迁移</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-可以用迁移学习做什么"><a href="#2-可以用迁移学习做什么" class="headerlink" title="2 可以用迁移学习做什么"></a>2 可以用迁移学习做什么</h2><h3 id="2-1-模型fine-tuning"><a href="#2-1-模型fine-tuning" class="headerlink" title="2.1 模型fine-tuning"></a>2.1 模型fine-tuning</h3><p>当我们的特定任务所拥有的数据集非常少(比如识别某个人的声音，但是那个人的声音数据很少)，但是非相关的数据集很多(比如来自很多人的很多语音数据)，我们无法用某一个人的声音数据来训练一个语音识别模型，这种情况要做迁移学习，可以称之为<strong>one-shot</strong>。用许多人的语音数据训练模型，再来某个人的语音数据来fine-tuning。</p>
<p><strong>问题</strong></p>
<p>目标数据集太少，即便是用非直接相关数据训练出了一个初始模型，然后用目标数据集做迁移学习，很容易会导致<strong>过拟合</strong>。</p>
<h2 id="3-迁移学习技巧"><a href="#3-迁移学习技巧" class="headerlink" title="3 迁移学习技巧"></a>3 迁移学习技巧</h2><h3 id="3-1-Conservation-Training"><a href="#3-1-Conservation-Training" class="headerlink" title="3.1 Conservation Training"></a>3.1 Conservation Training</h3><p>例如，已经有大量的source data数据（比如语音识别中大量的不同speaker的语音数据），以及target data(某个speaker的语音数据)。此时如果直接用source data训练出来的模型，再用target data做迁移学习，模型可能就会坏掉。 </p>
<p>可以在training的时候，加一些限制(就是加一些非L1,L2的正则化)，使得训练完成之后，前后两次模型效果差不太多。</p>
<p><img src="/images/blog/transfer_learning4.jpg" alt=""></p>
<h3 id="3-2-Layer-transfer"><a href="#3-2-Layer-transfer" class="headerlink" title="3.2 Layer transfer"></a>3.2 Layer transfer</h3><p>先用源数据训练出一个模型，然后将这个模型的某些层网络直接复制到新的网络中，然后只用新数据训练网络的余下层网络。这样训练时只需要训练很少的参数。</p>
<p><img src="/images/blog/transfer_learning5.jpg" alt=""></p>
<p>但是，哪些层应该被transfer，哪些不应该被transfer? 不同的任务之中，需要transfer的网络层不同。</p>
<ul>
<li><p>语音识别中，通常只复制最后几层网络。然后重新训练输入层网络。(同样的发音方式，得到的结果不同)语音识别的结果，应该跟发音者没有关系的，所以最后几层是可以被复制的。而不同的地方在于，从声音信号到发音方式，每个人都不一样。</p>
</li>
<li><p>在图像任务中。通常只复制前面几层，而训练最后几层。通常前几层做的就是检测图像中有没有简单的几层图形，而这些是可以迁移到其他任务中。而通常最后几层通常是比较特异化的，这些是需要训练的。</p>
</li>
</ul>
<p><img src="/images/blog/transfer_learning6.jpg" alt=""></p>
<p><strong>网络层迁移学习的实验结果(图像任务)</strong></p>
<p>ImageNet的数据120万图像分为source和target，按照分类数量划分，其中500个分类作为source，另外500个为target。其中横轴为transfer learning复制的网络层，其中0代表没有复制网络层，纵轴为分类准确率。可以发现，当我们只复制前面几个网络层时，效果有提升，但是复制得太多效果就开始变差。</p>
<p><img src="/images/blog/transfer_learning7.jpg" alt=""></p>
<p>上图中</p>
<ul>
<li><p>5 黄色线：代表在做了复制网络前几层之后，做了fine tuning之后的结果</p>
</li>
<li><p>3蓝色线：对照组。在目标领域上训练出一个模型，然后复制此模型的前面几层，然后固定住这几层，接着继续用<strong>目标数据</strong>训练剩下的几层。</p>
</li>
<li><p>2蓝色线：对照组。在目标领域上训练出一个模型，然后复制此模型的前面几层，然后固定住这几层，接着继续用<em>*新数据</em>训练剩下的几层。结果，有时候很差。在训练时，前面的层和后面的层其实是需要相互搭配的，否则后面的层的结果就很差。</p>
</li>
<li><p>4红色线：</p>
</li>
</ul>
<p><strong>source和target不是同种分类数据时</strong></p>
<p>如果source和target是不同的分类数据，比如source数据是自然风光，而target是人造物体，那么做transfer learning时，其准确率会大幅度降低。</p>
<p><img src="/images/blog/transfer_learning8.jpg" alt=""></p>
<p>如果只复制前面几层时，与没有复制没有太多区别。</p>
<h3 id="3-3-多任务学习"><a href="#3-3-多任务学习" class="headerlink" title="3.3 多任务学习"></a>3.3 多任务学习</h3><p>一个成功的实例是，多语言语音识别。输入是不同语言的语音，前面的几层公用参数，后面的几层不同参数。</p>
<p><img src="/images/blog/transfer_learning9.jpg" alt=""></p>
<h2 id="4-progressive-neural-network"><a href="#4-progressive-neural-network" class="headerlink" title="4 progressive neural network"></a>4 progressive neural network</h2><p><img src="/images/blog/transfer_learning10.jpg" alt=""></p>
<p>先训练一个Task1的网络，训练完成之后，固定其参数。再去训练一个Task2，它的每个隐藏层的都会去接 Task1的隐藏层输出。它的好处是，即便Task1和Task2完全不像，Task2的数据不会影响到Task1的模型参数，所以迁移的结果一定不会更差，最糟糕的情况就是直接训练Task2模型(此时Task1的输出设置为0)</p>
<h2 id="5-labeled-source-data-amp-unlabeled-target-data"><a href="#5-labeled-source-data-amp-unlabeled-target-data" class="headerlink" title="5 labeled source data  &amp; unlabeled target data"></a>5 labeled source data  &amp; unlabeled target data</h2><p>源数据为标记数据$(x^s,y^s)$ 作为训练集，而目标数据为非标记数据$x^t$为测试集。比如下图的<code>MNIST</code>数据集为训练集，而<code>MNIST-M</code>为测试集，其中<code>MNIST-M</code>同样为手写字，不过其背景变为风景和彩色的。</p>
<p><img src="/images/blog/transfer_learning11.jpg" alt=""></p>
<p>我们分析下领域对抗训练，把CNN作为特征抽取工具，会发现source data有很明显的分类现象，而target data却没有。</p>
<p><img src="/images/blog/transfer_learning12.jpg" alt=""></p>
<p>如上图中，MNIST数据集很明显的分为10个团，而MNIST-M没有。此时对于MNIST-M无能为力。</p>
<p>所以，我们希望CNN的feature extractor能够消除领域特性，就需要使用 <code>domain-adversarial training</code></p>
<h3 id="5-1-domain-adversarial-training"><a href="#5-1-domain-adversarial-training" class="headerlink" title="5.1 domain-adversarial training"></a>5.1 domain-adversarial training</h3><p><img src="/images/blog/transfer_learning13.jpg" alt=""></p>
<p>feature extractor与domain classifer做相反的事，domain classifer 极力区分当前数据的来源，而feature extractor希望domain classifer能够无视domain 的差异。</p>
<p><img src="/images/blog/transfer_learning14.jpg" alt=""></p>
<p>其实际做法是，在计算BP时，feature extractor 将domain classifer的梯度乘以负1，然后传给 domain classifer。</p>
<p>以下是这种训练出来的实验结果</p>
<p><img src="/images/blog/transfer_learning15.jpg" alt=""></p>
<h2 id="6-zero-shot"><a href="#6-zero-shot" class="headerlink" title="6  zero-shot"></a>6  zero-shot</h2><p>即：测试集里面的分类数据是训练集中从未出现过的，比如训练集的分类是毛和狗，而测试集里面却有草泥马</p>
<p><img src="/images/blog/transfer_learning16.jpg" alt=""></p>
<p>这种任务，在语音识别中很常见，训练集中不可能出现所有的语音和词汇。在语音上的做法是，不去直接辨别一段声音属于哪个word，而是去辨别一段声音属于哪个音标，然后做一个音标和tab之间的对应关系表，即lexics。所以，即便某些词汇没有出现在训练集，也可以从音标和lexics表得到。</p>
<p>那么，这个操作应用到图像中就是以每种分类的特定属性替代分类。比如狗这个分类，以<code>furry</code>,<code>4 legs</code>,<code>tail</code>这些属性来表示，切记这些属性必须足够丰富才可以。那么在训练时，就不直接识别分类，而是识别图像具备哪些属性。</p>
<p><img src="/images/blog/transfer_learning17.jpg" alt=""></p>
<p>测试集的时候，即便来了一个未出现的动物，也可以使用这些属性描述。</p>
<p><img src="/images/blog/transfer_learning18.jpg" alt=""></p>
<p>如果属性集太大，还可以做 attribute embedding。</p>
<p><img src="/images/blog/transfer_learning19.jpg" alt=""></p>
<p>其中$f(x^2)和g(y^2)$ 都可能是神经网络。</p>
<h3 id="6-1-zero-shot的成功应用"><a href="#6-1-zero-shot的成功应用" class="headerlink" title="6.1 zero-shot的成功应用"></a>6.1 zero-shot的成功应用</h3><p>机器翻译</p>
<p><img src="/images/blog/transfer_learning20.jpg" alt=""></p>
<p>在没有日语翻译成韩文的数据集，由于有韩语翻译到英文、日文和英文翻译到日文，所以可以完成从日语翻译成韩文。根据学习好的encoder，把各种语言的词汇映射到空间中的向量，会出现下图的结果</p>
<p><img src="/images/blog/transfer_learning21.jpg" alt=""></p>
<p>上图中，不同颜色代表不同语言，处于相同位置的代表意义相同。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习-15-生成网络</title>
    <url>/2018/04/02/LHY_GAN/</url>
    <content><![CDATA[<p>视频来源：<a href="https://www.bilibili.com/video/av9770302/?p=15" target="_blank" rel="noopener">https://www.bilibili.com/video/av9770302/?p=15</a></p>
<h2 id="1-前提概览"><a href="#1-前提概览" class="headerlink" title="1 前提概览"></a>1 前提概览</h2><p>生成网络可以做什么？ 写诗，画动漫头像。</p>
<h3 id="1-1-Auto-encoder"><a href="#1-1-Auto-encoder" class="headerlink" title="1.1 Auto-encoder"></a>1.1 Auto-encoder</h3><p><img src="/images/blog/gan1.jpg" alt=""></p>
<p>通过encoder将一张图片变成一个 code vector，然后用一个decoder将此code vector 生成一张图像。它们训练时时联合训练，训练完成之后，可以用decoder来生成一张图片。</p>
<p><img src="/images/blog/gan2.jpg" alt=""></p>
<h4 id="1-1-1-实例"><a href="#1-1-1-实例" class="headerlink" title="1.1.1  实例"></a>1.1.1  实例</h4><p>生成一个手写字生成的decoder，其生成的coder vector假设会2维的，如下：</p>
<p><img src="/images/blog/gan3.jpg" alt=""></p>
<p>接着，我们输入一个二维向量，假设为 $ [-1.5,0] $，可能生成的手写字是$0$。假设我们输入一个二维向量 $[1.5,0]$生成的手写字可能是$1$。如下图：</p>
<p><img src="/images/blog/gan4.jpg" alt=""></p>
<p>如果，二维向量的值在$ [-1.5,0] $和$ [1.5,0] $之间等距离取值的话，可能得到如下的结果</p>
<p><img src="/images/blog/gan5.jpg" alt=""></p>
<p><strong>但是auto-encoder无法生成state of art的结果。</strong></p>
<h3 id="1-2-VAE-Variational-Auto-Encoder"><a href="#1-2-VAE-Variational-Auto-Encoder" class="headerlink" title="1.2  VAE(Variational Auto-Encoder)"></a>1.2  VAE(Variational Auto-Encoder)</h3><p>VAE是一个进阶版的auto-encoder，训练的时候，输入一张图片，但是<strong>它同时输出三个vector</strong>，假设这三个vector都是3维的，如下图：</p>
<p><img src="/images/blog/gan6.jpg" alt=""></p>
<p>其中的$m_1,m_2,m_3$代表VAE的encoder的输出code，只不过是个三维vector。同时还会生成另外一个三维的vector $\sigma _1,\sigma _2,\sigma _3$,同时会随机从一个符合正态分布的数据集中sample一个三维vector $e_1,e_2,e_3$ （称为noise）。接下来做如下操作：</p>
<ol>
<li>将vector $\sigma _1,\sigma _2,\sigma _3$ 指数化</li>
<li>将第一步指数化之后的值与noise vector  $e_1,e_2,e_3$ 相乘</li>
<li>将生成的code vector $m_1,m_2,m_3$与第二步的结果相加，得到结果 $c_1,c_2,c_3$</li>
</ol>
<p>再将最后的结果 $c_1,c_2,c_3$ 输入到<strong>decoder</strong>网络训练，整个过程如下：</p>
<p><img src="/images/blog/gan7.jpg" alt=""></p>
<h4 id="1-2-1-VAE的受限条件"><a href="#1-2-1-VAE的受限条件" class="headerlink" title="1.2.1 VAE的受限条件"></a>1.2.1 VAE的受限条件</h4><p>在训练VAE时由于添加了额外的项 $\sigma$，则需要添加一个受限条件（假设），即需要最小化：</p>
<script type="math/tex; mode=display">
\sum _{i=1} ^3(exp(\sigma _i)-(1+\sigma _i)+(m_i)^2)</script><p>其中 $(m_i)^2$ 可以看做L2 正则，而最小化 $exp(\sigma _i)-(1+\sigma _i)$ 部分即最小化 $\sigma _i$，当它为0时，这部分的值最小。</p>
<h4 id="1-2-2-VAE的问题"><a href="#1-2-2-VAE的问题" class="headerlink" title="1.2.2  VAE的问题"></a>1.2.2  VAE的问题</h4><p>我们期望的VAE是它能生成与真实图像越接近越好的图像</p>
<p><img src="/images/blog/gan8.jpg" alt=""></p>
<p>但实际上VAE实际模拟过程与人类的有出入，下图蓝色框代表了两种可能出现的情况。很显然，人类可以分辨出左边是比较接近真实的，右边的不那么接近的（黄色框），但是对于VAE（蓝色框）来说它们在损失函数面前是等价的。</p>
<p><img src="/images/blog/gan9.jpg" alt=""></p>
<h3 id="1-3-evolution-of-generation"><a href="#1-3-evolution-of-generation" class="headerlink" title="1.3 evolution of generation"></a>1.3 evolution of generation</h3><p><img src="/images/blog/gan10.jpg" alt=""></p>
<p>上图是一个示例，分别迭代多次，每次是一对 <code>generator</code> 和 <code>discriminator</code>，不断演化，最后得到较好结果。其中的<code>discriminator</code>是一个二分类器，如果来自真实图像，则输出1，如果来自生成网络则输出0.</p>
<h4 id="1-3-1-GAN中的Discriminator"><a href="#1-3-1-GAN中的Discriminator" class="headerlink" title="1.3.1  GAN中的Discriminator"></a>1.3.1  GAN中的Discriminator</h4><p>Discriminatory本质上是一个二分类分类器，输入一张图片，它会判断该图片是real(1)还是fake(0)。</p>
<p><img src="/images/blog/gan11.jpg" alt=""></p>
<h4 id="1-3-2-GAN中的Generator"><a href="#1-3-2-GAN中的Generator" class="headerlink" title="1.3.2 GAN中的Generator"></a>1.3.2 GAN中的Generator</h4><p><img src="/images/blog/gan12.jpg" alt=""></p>
<p>GAN中的Generator与VAE中的decoder类似，输入一个随机的vector，输出一些图片。与VAE不同的是，在训练VAE的时候需要最小化一个重构误差</p>
<p>此处GAN中Generator的架构与VAE一样，只是在训练时方法不一。</p>
<p>首先，我们有个所有参数都是随机产生的generator。此时输入一组参数随机的向量，generator会产生一组<strong>假的</strong>图像；同时从训练数据集中随机抽取一组<strong>真的</strong>图像，然后将所有假的图像标签标记为0(negative sample)，所有真的图像标记为1(positive sample)</p>
<h4 id="1-3-3-GAN-过程"><a href="#1-3-3-GAN-过程" class="headerlink" title="1.3.3 GAN 过程"></a>1.3.3 GAN 过程</h4><p>首先随机输入一组向量给Generator，产生一组图像，Discriminator知道这个是假的图像，会输出一个很低的置信度。</p>
<p><img src="/images/blog/gan13.jpg" alt=""></p>
<p>接下来，需要更新generator参数，它会产生的图像让第一代的Discrimintor觉得它是真的图像，输出1。</p>
<p><strong>注意，我们在训练过程中会固定 Discriminator，使用随机梯度更新Generator</strong></p>
<h2 id="二-GAN的核心思路"><a href="#二-GAN的核心思路" class="headerlink" title="二 GAN的核心思路"></a>二 GAN的核心思路</h2><h3 id="2-1-最大似然估计"><a href="#2-1-最大似然估计" class="headerlink" title="2.1 最大似然估计"></a>2.1 最大似然估计</h3><ul>
<li>给定数据分布 $P_{data}(x)$，此处的$x$就想象成一张图片的所有的像素值串起来。</li>
<li>现在我们要找到一个数据分布$P_G(x;\theta)$，它受控于一组参数$\theta$的。<ul>
<li>其中$P_G(x;\theta)$是一种数据分布，比如可以是高斯混合模型。其中$\theta$代表了高斯分布的期望和方差这两个参数。只不过在GAN中$P_G(x;\theta)$ 是一个 神经网络。</li>
<li>那么我们要做的事情就是，找到一个一组参数 $\theta$，使得 $P<em>G(x;\theta)$的分布与$P</em>{data}(x)$ 的分布越接近越好。</li>
</ul>
</li>
</ul>
<p>从 $P_{data}(x)$中抽样${x^1,x^2,…x^m}$。</p>
<p>如果给定参数$\theta$ 那么我们可以计算 $P_G(x^i;\theta)$的值。</p>
<p><strong>似然度</strong>：即给定参数$\theta$时，从$P<em>G(x^i;\theta)$ 中抽样产生 $x^1,x^2,x^3…x^n$的概率。似然度为 $L=\prod ^m </em>{i=1}P_G(x^i;\theta)$。</p>
<p>我们要做的其实就找一组参数 $\theta ^*$使得最大化 $L$的似然度。对于高斯混合 模型，参数就是均值、方差，以及混合权重。比如有下图的高斯混合模型，数据有三个高斯分布混合而成，如下：</p>
<p>该分布中均值即上图中三个黄色中心点，方差即三个圆形半径。</p>
<p><img src="/images/blog/gan14.jpg" alt=""></p>
<script type="math/tex; mode=display">
\theta ^* = arg \quad max_{\theta} \prod ^m _{i=1}P_G(x^i;\theta)=arg\quad max_{\theta}\quad log \prod ^m _{i=1}P_G(x^i;\theta)\quad 等同于求对数极大值\\
= arg \quad max_{\theta}\sum ^m_{i=1}logP_G(x^i;\theta)\quad\quad 其中{x^1,x^2,...x^m}都是从  P_{data}(x)中抽样得到的 \\
 \approx arg\quad max_{\theta}\quad E_{x~P_{data}}[logP_G(x;\theta)] \quad\quad 等同于从 P_{x~{data}}分布中抽样 x^1,x^2,..x^n 然后计算每个 x^1,x^2,..x^n 使得 log P_G(x;\theta) 最大这件事 \\
=arg \quad max_{\theta} \int _x P_{data}(x)logP_G(x;\theta)dx \\
等同于  arg \quad max_{\theta} \int P_{data}(x)logP_G(x;\theta)dx-\int _xP_{data}(x)log P_{data}dx \\
=arg\quad min_{\theta}\quad KL(P_{data}(x)\|\|P_G(x;\theta)) \quad 【KL散度】</script><p>在GAN之前，高斯混合模型生成的图像非常模糊，因为高斯混合模型无法真正模拟图像数据分布。</p>
<h3 id="2-2-将-P-G-x-theta-换成一个神经网络"><a href="#2-2-将-P-G-x-theta-换成一个神经网络" class="headerlink" title="2.2 将 $P_G(x;\theta)$ 换成一个神经网络"></a>2.2 将 $P_G(x;\theta)$ 换成一个神经网络</h3><p>此时的GAN结构如下，输入通常为一个简单的 高斯分布的向量。经过神经网络 $G(z)$ 之后输出x</p>
<p><img src="/images/blog/gan15.jpg" alt=""></p>
<p>关于神经网络 $G(z)$ 的函数表达式可以表示为： $P<em>G(x)=\int _xP</em>{prior}(z)I<em>{[G(z)=x]}dz$ 。该公式的通俗理解是，假设$G(z)$参数已经固定(即网络参数固定)，从该网络中取样得到x的概率等于，对所有可能的z取积分，乘以z出现的概率($P</em>{prior}(z)$)，同时每个z经过函数$G(z)$之后生成x，该x是否即为当前正在考量的x，此处由函数$I_{G(z)=x}$判定，如果等同则为1，否则为0。</p>
<p>当前问题是，如果以这种方式计算。难以计算，给定x，即便我们知道输入分布z的参数，但是由于神经网络极其复杂，要想计算由网络生成x的概率会很困难。 在无法计算似然度的情况下，无法调整参数$\theta$使得网络输出x接近真实数据分布。这个就是GAN的共享。</p>
<h3 id="2-3-GAN的基本介绍"><a href="#2-3-GAN的基本介绍" class="headerlink" title="2.3 GAN的基本介绍"></a>2.3 GAN的基本介绍</h3><ul>
<li>Generator G<ul>
<li>G是一个函数，输入为Z，输出为x</li>
<li>给定先验分布 $P_{prior}(z)$ ，又得知函数G，我们可以定义一个概率分布 $P_G(x)$</li>
</ul>
</li>
<li>Discriminator D<ul>
<li>D是一个函数，输入为x，输出为标量。</li>
<li>Discriminator D的作用就是衡量 $P<em>G(x)$和 $P</em>{data}(x)$的差异</li>
</ul>
</li>
<li>有一个函数 $V(G,D)$，我们要找的最好的G。 $G^* = arg\quad min_G\quad max_D\quad V(G,D)$</li>
</ul>
<h4 id="2-3-1-如何理解-G-arg-quad-min-G-quad-max-D-quad-V-G-D"><a href="#2-3-1-如何理解-G-arg-quad-min-G-quad-max-D-quad-V-G-D" class="headerlink" title="2.3.1 如何理解 $G^* = arg\quad min_G\quad max_D\quad V(G,D)$"></a>2.3.1 如何理解 $G^* = arg\quad min_G\quad max_D\quad V(G,D)$</h4><p>我们先看最右边的 $max_D\quad V(G,D)$ 部分。它的意思是选择使得 $V(G,D)$最大的 D，假设我们只有三个可能的G($G_1,G_2,G_3$，如下图)，实际上由于G是一个神经网络，所以它有无数种可能。</p>
<p>下图中，分别对于不同可能的G，改变D，可以得到不同的 $V(G,D)$。对于$G_1,G_2,G_3$，$max_DV(G,D)$(最大值)就是下图中，红色点的值。</p>
<p><img src="/images/blog/gan16.jpg" alt=""></p>
<p>接下来再去寻找一个$G^* $使得 $max_DV(G,D)$最小的G，可以从上图（红色点）中看到，对于 $G_1,G_2,G_3$其最大值，在为$G_3$时它的最大值最小。</p>
<h4 id="2-3-2-关于函数-V的定义"><a href="#2-3-2-关于函数-V的定义" class="headerlink" title="2.3.2 关于函数 V的定义"></a>2.3.2 关于函数 V的定义</h4><p>$V= E<em>{x~P</em>{data}}[logD(x)]+E_{x~P_G}[log(1-D(x))]$ ,先不用考虑此公式如何得来。 </p>
<p>对于给定的G，$max<em>DV(G,D)$评估的是 $P_G$和$P</em>{data}$之间的差异，所以我们要寻找的是那个能使得  $P<em>G$和$P</em>{data}$差异最小的 $P<em>G$（$P</em>{data}$固定）。</p>
<ul>
<li><p>对于给定G，最优的$D^*$是可以最大化V的。其中V的形式如下：</p>
<script type="math/tex; mode=display">
V=E_{x~P_{data}}[logD(x)]+E_{x~P_G}[log(1-D(x))]\\
=\int _xP_{data}(x)logD(x)dx+\int _xP_G(x)log(1-D(x))dx \quad \quad 期望等于概率的积分\\
=\int _x[P_{data}logD(x)+P_G(x)log(1-D(x))]dx\quad\quad 都是对x的积分，相同部分放一起</script></li>
<li><p>对于给定$x$，最优化的V等价于最大化上式中括号中的 </p>
<script type="math/tex; mode=display">
P_{data}logD(x)+P_G(x)log(1-D(x)) \\
a\quad\quad\quad D\quad\quad b\quad\quad\quad D\quad\quad \\
给定x，P_{data}和P_G都是常量</script></li>
<li><p>找到$D^*$能够最大化： $f(D)=alog(D)+blog(1-D)$,对该式子求极值的方法就是下面求导，取0得到。</p>
</li>
</ul>
<script type="math/tex; mode=display">
\frac{df(D)}{dD}=a\times \frac{1}{D}+b\times \frac{1}{1-D}\times (-1)=0 \\
\rightarrow a\times \frac{1}{D}=b\times \frac{1}{1-D} \\
\rightarrow a\times (1-D^*)=b\times D* \\
\rightarrow D^* = \frac{a}{a+b} \quad\quad 再把a,b代回来得到\\
\rightarrow D^*(x)=\frac{P_{data(x)}}{P_{data}(x)+P_G(x)}</script><p>将各个$D^*$ 显现在图中，如下：</p>
<p><img src="/images/blog/gan17.jpg" alt=""></p>
<p>红色顶点处即，不同的$G$，取得最大D的值。该点到水平轴(D)的距离就是$V(G,D)$的值，也即$P<em>{G_1}$和$P</em>{data}$的差异。</p>
<p>由上面的推导可知 $D^*(x)=\frac{P<em>{data(x)}}{P</em>{data}(x)+P<em>G(x)}$。而$V(G,D)=E</em>{x~P<em>{data}}[logD(x)]+E</em>{x~P_G[log(1-D(x))]}$。那么，其实我们带入得到:</p>
<script type="math/tex; mode=display">
D^*(x)=\frac{P_{data(x)}}{P_{data}(x)+P_G(x)}  \\
=E_{x~P_{data}}[log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x~P_G}[log\frac{P_G(x)}{P_{data}(x)+P_G}]  \quad 将求期望转换为求积分\\
\rightarrow \int _x log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx +\int _x P_G(x)log \frac{P_G(x)}{P_{data}(x)+P_G(x)}dx \\
下面就开始推导 KL散度了，这里就不推导了。推导完也记不住，也看不懂</script><ul>
<li><p>那么对于给定G， $max <em>DV(G,D)$可以看做计算  $-2log2+2JSD(P</em>{data}(x)||P<em>G(x))$(其中$JSD(P</em>{data}(x)||P<em>G(x))$用以衡量$P_G和P</em>{data}$之间的差异度，是从上面的推导推导而来)。$JSD(P<em>{data}(x)||P_G(x))$的取值范围，最小为 $0$,即$P_G$和$P</em>{data}$完全重合，最小值为$log2$即$P<em>G$和$P</em>{data}$完全不存在交集。所以$max _DV(G,D)$的取值范围为 $[-2log2,0]$</p>
</li>
<li><p>那么，那个G才是使得$max <em>DV(G,D)$ 最小的值呢？  只有当 $P_G=P</em>{data}$</p>
</li>
</ul>
<h4 id="2-3-3-具体算法"><a href="#2-3-3-具体算法" class="headerlink" title="2.3.3 具体算法"></a>2.3.3 具体算法</h4><p>算法可以按照如下步骤循环：</p>
<ol>
<li><p>给定一个初始的 $G_0$</p>
</li>
<li><p>根据$G_0$ 找到一个 $D^* _0$使得它可以最大化 $V(G_0,D)$</p>
<ul>
<li>其中 $V(G<em>0,D^* _0)$ 是$P</em>{data}$和 $P_{G_0}(x)$之间的JS差异。</li>
</ul>
</li>
<li><p>下一步，我们需要找一个新的 $G$，假若为 $G<em>1$。它必须使得$P</em>{data}$和 $P_{G_0}(x)$之间的JS差异减小。可以通过求梯度的方法，</p>
<ul>
<li><p>$ \theta _G\leftarrow \theta_G -\lambda \frac{\partial V(G,D^* _0)}{\partial \theta _G}$ 。可以通过此公式计算得到新的 $G_1$</p>
</li>
<li><p>用新的 $G<em>1$计算$P</em>{data}$和 $P_{G_1}(x)$之间的JS差异</p>
</li>
</ul>
</li>
<li><p>再找下一个$G_2$，使用同样的方式…</p>
</li>
<li><p>重复，不断去寻找新的G</p>
</li>
</ol>
<h4 id="2-3-4-实际如何操作"><a href="#2-3-4-实际如何操作" class="headerlink" title="2.3.4 实际如何操作"></a>2.3.4 实际如何操作</h4><p>我们的loss函数是 $V=E<em>{x~P</em>{data}}[logD(x)]+E<em>{x~P_G}[log(1-D(x))]$。在上面的推导过程中，我们是假定可以对$P</em>{data}$求积分的，但是实际情况是，{P_{data}}是所有可能图像的分布，是不可积分的。所以，我们做如下逼近。</p>
<ul>
<li><p>通过从 $P<em>{data}(x)$中抽样 ${x^1,x^2,x^3,…x^m}$来毕竟 $P</em>{data}$可能的数据分布，同时从generator $P_G(x)$中也抽样 $\tilde x^1,\tilde x^2,..\tilde x^m$。</p>
</li>
<li><p>那么我们求上面的$V=E<em>{x~P</em>{data}}[logD(x)]+E<em>{x~P_G}[log(1-D(x))]$也等同于求一个 $\tilde V=\frac{1}{m}\sum ^m</em>{i=1}logD(x^i)+\frac{1}{m}\sum ^m_{i=1}log(1-D(\tilde x^i))$ 。此式可以看做一个对二分类分类器的交叉熵损失函数。</p>
<ul>
<li>比如一个二分类分类器，假若其输出为$D(x)$，那么我们就需要最小化其交叉熵，我们会这么做</li>
<li>如果$x$是正样本，那么就需要最小化 $-logD(x)$</li>
<li>如果$x$是负样本，那么就需要最小化 $-log(1-D(x))$</li>
</ul>
</li>
<li><p>再回过来，D是一个参数为$\theta$的二分类的分类器。我们从 $P<em>{data}(x)$中抽取 $x^1,x^2,…x^m$作为正样本，从$P_G$中抽样 $\tilde x^1,\tilde x^2,…\tilde x^m$作为负样本。以上面讨论的结论可以将最大化$V$变成最小化 $L=-\frac{1}{m}\sum^m </em>{i=1}logD(x^i)-\frac{1}{m}\sum ^m_{i=1}log(1-D(\tilde x^i))$</p>
</li>
</ul>
<h3 id="2-4-GAN完整算法"><a href="#2-4-GAN完整算法" class="headerlink" title="2.4 GAN完整算法"></a>2.4 GAN完整算法</h3><ul>
<li>在每次算法迭代过程中，都会更新Discriminator和Generator</li>
</ul>
<p>我们先看看学习<strong>Discriminator</strong> 部分，一般会<strong>重复K次</strong>，一次无法找到全局最优参数。</p>
<ul>
<li>从数据分布 $P_{data}(x)$中抽样 ${x^1,x^2,…x^m}$</li>
<li>从先验分布$P_{prior}(z)$中随机抽取噪声数据${z^1,z^2,…z^m}$。注意此处的先验分布只是个普通的正态分布</li>
<li>将先验分布抽样得到的${z^1,z^2,…z^m}$喂入$\tilde x^i=G(z^1)$，获取一批生成数据 ${\tilde x^1,\tilde x^2,…\tilde x^m}$</li>
<li>更新discriminator的参数 $\theta _d$，可以使得下式<strong>最大</strong><ul>
<li>$\tilde V = \frac{1}{m}\sum ^m<em>{i=1}logD(x^1)+\frac{1}{m}\sum ^m </em>{i=1}log(1-D(\tilde x^1))$</li>
<li>使用梯度下降方法计算 $\theta _d\leftarrow \theta _d+\lambda \nabla \tilde V(\theta _d)$</li>
</ul>
</li>
</ul>
<p>再来看训练<strong>Generator</strong>部分，下面的部分通常只会<strong>更新一次</strong>。generator不能更新太多，否则会导致JS差异度无法下降（generator已经以假乱真了）。</p>
<ul>
<li><p>从先验分布$P_{prior}(z)$中随机抽取噪声数据${z^1,z^2,…z^m}$。此处的随机噪声数据可以与上面训练Discriminator部分的随机样本值一样，也可以不一样</p>
</li>
<li><p>更新Generator的参数 $\theta _g$使得下式<strong>最小</strong></p>
<ul>
<li>$\tilde V = \frac{1}{m}\sum^m <em>{i=1}logD(x^i)+\frac{1}{m}\sum ^m</em>{i=1}(1-D(G(z^i)))$ 。可以看到此式，前半部分跟Generator无关</li>
<li>再用梯度下降法去更新 Generator的参数：$\theta _g-leftarrow \theta _g+\lambda \nabla \tilde V(\theta _g)$</li>
</ul>
</li>
</ul>
<h2 id="3-实际如何实现GAN"><a href="#3-实际如何实现GAN" class="headerlink" title="3 实际如何实现GAN"></a>3 实际如何实现GAN</h2><h3 id="3-1-真实实现中，Generator的目标函数"><a href="#3-1-真实实现中，Generator的目标函数" class="headerlink" title="3.1 真实实现中，Generator的目标函数"></a>3.1 真实实现中，Generator的目标函数</h3><p>从上面的讨论中，我们可以看到Generator会使得式子 $V= E<em>{x~P</em>{data}}[logD(x)]+E<em>{x~-P_G}[log(1-D(x))]$的值最小。省略前面的(与generator无关)，只看$E</em>{x~-P_G}[log(1-D(x))]$这部分，目标函数理论上应该是最小化此式，但是我们可以分别看看 $-log(D(x))$和 $log(1-D(x))$曲线，如下图（上面蓝色的为$-log(D(x))$，下面红色的为$log(1-D(x))$）：</p>
<p><img src="/images/blog/gan18.jpg" alt=""></p>
<p>观察需要最小化的 $log(1-D(x))$，在$D(x)$很小时，该曲线很平滑，在$D(x)$很大时该曲线很陡峭。 $D(x)$很小意味着，由Generator产生出来的x无法骗过Discriminator，Discriminator可以很容易认出。也即在训练的初始步骤，由generator产生的样本都集中在平滑部分，此时的$log(1-D(x))$微分值很小，训练变得缓慢。此时，我们可以修改目标函数为 </p>
<script type="math/tex; mode=display">
  v= E_{x~P_G}[-log(D(x))]</script><p>此式子效果等同于$log(1-D(x))$，同时可以快速训练，在初始步骤微分值很大，在后续步骤变得很小，比较符合训练期待。</p>
<h3 id="3-2-如何评估JS-divergence-差异"><a href="#3-2-如何评估JS-divergence-差异" class="headerlink" title="3.2 如何评估JS divergence(差异)"></a>3.2 如何评估JS divergence(差异)</h3><p>我们将discriminator的loss就是来衡量JS divegence，loss越大，divergence越大</p>
<p><img src="/images/blog/gan19.jpg" alt=""></p>
<p>图中分别衡量的三个<strong>Generator</strong>，分别训练了1个epoches，10个epoches，25个epoches。其中训练了25个epoches的generator已经几乎可以state of art了，但是用这些Generator去训练discriminator时，discriminator依然有十分高的准确率。</p>
<p>我们先看看目标损失函数 $max <em>DV(G,D)=-2log2+2JSD(P</em>{data}(x)||P_G(x))$ 导致这个问题的主要原因有以下几点</p>
<ul>
<li>我们在训练和调整到的时候，不是真正用积分去计算，而是通过抽样来拟合。现在假设我们有红色和蓝色两个椭圆的数据点分布，如下，但是因为我们是使用抽样的方式来代表数据分布：</li>
</ul>
<p><img src="/images/blog/gan20.jpg" alt=""></p>
<p>即便Generator产生的数据样本与真实样本之间有重叠，但是由于Discriminator比较强，所以它依然能找到一条曲线将红色点和蓝色点区分开。如何解决这个问题？</p>
<ul>
<li>使得discriminator 变弱一点，少更新，加dropout。但是一个弱discriminator将导致JS divergence无法计算。</li>
<li>$P<em>G$和$P</em>{data}$都是高维空间数据，现在假设它们都是二维空间的，那么 $P<em>G$和$P</em>{data}$可以看做二维空间里面的两条直线，那么这两条之间的交集非常小，几乎趋近于零（如下两条直线）。</li>
</ul>
<p><img src="/images/blog/gan21.jpg" alt=""></p>
<p>所以真实$P<em>G$和$P</em>{data}$的情况可能像下面这样演化：</p>
<p><img src="/images/blog/gan22.jpg" alt=""></p>
<p>可以看到在$P<em>G_0$和$P_G</em>{50}$…到$P<em>G</em>{100}$之前，JS divergence都是log2，GAN没有演化的动力。 </p>
<h3 id="3-3-如何解决GAN无法优化的问题"><a href="#3-3-如何解决GAN无法优化的问题" class="headerlink" title="3.3 如何解决GAN无法优化的问题"></a>3.3 如何解决GAN无法优化的问题</h3><ul>
<li>加入噪音数据。在discriminator的输入中加入一些人工噪音数据</li>
<li>训练Discriminator时，将其label加噪音。比如有张图片是positive，现在随机替换图像的部分内容为噪音</li>
</ul>
<p>加入噪音数据之后，原本交集非常少$P<em>G$和$P</em>{data}$就可能会拓宽。如下：</p>
<p><img src="/images/blog/gan23.jpg" alt=""></p>
<p><strong>注意：噪音数据要随着训练的推荐，逐步减小</strong></p>
<h2 id="4-mode-collapse"><a href="#4-mode-collapse" class="headerlink" title="4 mode collapse"></a>4 mode collapse</h2><p>比如有真实的数据分布为蓝色，而generator生成的数据分布为红色。如下左图，右边是对应生成的图像。</p>
<p><img src="/images/blog/gan24.jpg" alt=""></p>
<p>现在问题是，我们只知道GAN生成了的数据，无法知道GAN没有生成的数据。 </p>
<p>假设当前$P_{data}$的数据分布如下，为8个黑点。    </p>
<p><img src="/images/blog/gan25.jpg" alt=""></p>
<p>但是，我们训练过程中会出现不一致的情况。比如，我们期望$P<em>G$可以慢慢去覆盖$P</em>{data}$,但是实际训练时$P<em>G$一直只产生一个数据分布，不断去调整，但始终无法覆盖所有的$P</em>{data}$</p>
<p><img src="/images/blog/gan26.jpg" alt=""></p>
<p>可能的原因是之前的损失函数定义，即KL divergence定义有误。下图左边代表了原始的损失函数定义</p>
<p><img src="/images/blog/gan27.jpg" alt=""></p>
<p>其中 $KL= \int P<em>{data}log\frac{P</em>{data}}{P<em>G}dx$，当$P</em>{data}$有值，而$P<em>G$没有值的时候，该函数将取无穷大的值。所以此时GAN会尽力去覆盖尽可能多的$P</em>{data}$的数据。</p>
<p>而看上图右边，KL divergence的倒数，$Reverse KL= \int P<em>{data}log\frac{P_G}{P</em>{data}}dx$。此时当$P<em>G$有值，而$P</em>{data}$没有值得时候函数取值会趋近无穷大，此时为了避免出现这种情况,$P<em>G$会尽可能拟合一个数据分布(假设真实的$P</em>{data}$由多个分布组成的话)。</p>
<h2 id="5-condintional-GAN"><a href="#5-condintional-GAN" class="headerlink" title="5 condintional GAN"></a>5 condintional GAN</h2><p>与GAN不同的时，我们想生成制定的东西，此时的<strong>Generator</strong>输入就不止一个先验分布（正态分布）了。如下：</p>
<p><img src="/images/blog/gan28.jpg" alt=""></p>
<p>但此时可能会出现一个问题，generator可能会无视先验分布($P_Z$)，generator会觉得先验分布只是个噪音数据，解决办法是在generator里面添加 dropout。</p>
<p>此时训练<strong>Discriminator</strong>也不一样，它的输入不再是一张图片，而是一张图片以及对应的描述，而对应的label则根据正负样本区别对待。</p>
<p><img src="/images/blog/gan29.jpg" alt=""></p>
<ul>
<li>正样本： $(\hat c,\hat x)$,其中$\hat c$为图像真实描述，$\hat x$为真实图像。</li>
<li>负样本： $(\hat c,G(\hat c)),(\hat c’,x)$。其中$\hat c$为真正的图像描述，而$G(\hat c)为对generator输入$\hat c$时生成的图像$。同时要有另外一种fake sample，给discriminator真实的图像，但是给错误的描述。比如此处的$\hat x$为真实图像，但是$\hat c’$为错误描述。 </li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习-八-RNN和GAN</title>
    <url>/2018/03/18/LHY_RNN_and_GAN/</url>
    <content><![CDATA[<h2 id="1-生成网络"><a href="#1-生成网络" class="headerlink" title="1 生成网络"></a>1 生成网络</h2><h3 id="1-1-生成文本"><a href="#1-1-生成文本" class="headerlink" title="1.1 生成文本"></a>1.1 生成文本</h3><p>我们需要知道的是，一个句子由字符或者单词组成。但是中文里面一个单词是一个有意义的单位。使用RNN生成句子时，每次由RNN生成一个字符或单词。</p>
<p>假若我们要基于RNN生成一个句子，生成过程如下：</p>
<ul>
<li>向RNN中输入 <code>&lt;BOS&gt;</code>(begin of sentence)</li>
<li>RNN会给出某个字符(或单词)的分布概率，根据这个概率分布抽样，可以得到一个词。比如说<code>床</code></li>
<li>继续把<code>床</code>这个词输入到RNN，RNN继续输出字符（或词）的分布概率，根据其概率分布抽样得到下一个词，以此类推，下一个词比如为<code>前</code></li>
</ul>
<p><img src="/images/rnn_and_gan1.jpg" alt=""></p>
<h3 id="1-2-生成图像"><a href="#1-2-生成图像" class="headerlink" title="1.2 生成图像"></a>1.2 生成图像</h3><p>图像都是由像素组成，可以由RNN每次生成一个像素。图像的二维结构转换为一个像素序列，如下图：</p>
<p><img src="/images/rnn_and_gan2.jpg" alt=""></p>
<p>序列处理过程如下：</p>
<p><img src="/images/rnn_and_gan3.jpg" alt=""></p>
<p>但是这种方式只有像素值的连续性，没有考虑图像中像素的局部关联性。一种更贴近的方式是下面的图，中间黑色像素块同时受上面的红色块以及左边的粉红色块的影响。</p>
<p><img src="/images/rnn_and_gan4.jpg" alt=""></p>
<p>但是要生成这种考虑空间特征的图像，可以使用Grid LSTM。如下图</p>
<p><img src="/images/rnn_and_gan5.jpg" alt=""></p>
<p>左下角黑框为一个filter。最开始时输入一个类似<code>BOS</code>的字符，由Grid LSTM生成第一个像素蓝色块，再将蓝色块输入到Grid LSTM(中间图第二个黑色箭头)，Grid LSTM会生成第二个红色块，第二个红色像素块在生成时会考虑输入<code>蓝色块</code>和之前的输入信息。</p>
<p>如何产生空间信息，如下图</p>
<p><img src="/images/rnn_and_gan6.jpg" alt=""></p>
<p>上图中，注意红色箭头的位置，是一个空间中的第二层。</p>
<p><strong>此方法可以生成 state of art 的图像。</strong></p>
<h2 id="2-基于条件的生成"><a href="#2-基于条件的生成" class="headerlink" title="2  基于条件的生成"></a>2  基于条件的生成</h2><p>RNN只能生成一些随机的句子，我们想要基于条件生成想要的文本。比如在 image caption中，看到特定图像能生成对应的文本描述，在chatbot对话中会根据对话者所说的话来生成回应。</p>
<h3 id="2-1-看图说话-image-caption"><a href="#2-1-看图说话-image-caption" class="headerlink" title="2.1 看图说话 image caption"></a>2.1 看图说话 image caption</h3><p>一般使用 <code>CNN+RNN</code>的方式生成，使用cnn来抽取图像特征，再将特征传入RNN即可生成对应描述，如下图示例。</p>
<p><img src="/images/rnn_and_gan7.jpg" alt=""></p>
<p>左边的CNN会将抽取出的特征vector传给右边的RNN，如果觉得只在开始的时候传入会导致RNN后续遗忘，可以每次都传入图像特征vector。</p>
<h3 id="2-2-机器翻译"><a href="#2-2-机器翻译" class="headerlink" title="2.2 机器翻译"></a>2.2 机器翻译</h3><p>比如，我们想把中文<code>机器学习</code>翻译成对应的英文<code>machine learning</code>，即 <code>机器学习$$\Rightarrow$$machine learning</code>。此时二者之间毫无关联，但是我们把中文变成一个vector然后传入RNN。</p>
<p><strong>将中文变成vector</strong></p>
<p>同样可以使用RNN来完成，下图将<code>机器学习</code>这四个字用RNN抽取出一个vector代表整个句子的信息。</p>
<p><img src="/images/rnn_and_gan8.jpg" alt=""></p>
<p>然后再将抽取出的vector传入给另外一个RNN，如下图：</p>
<p><img src="/images/rnn_and_gan9.jpg" alt=""></p>
<p>其中红色的矩形块代表了中文部分抽取出的特征，可以三次重复传入右边的RNN，让右边的RNN分别输出对应的<code>machine</code>,<code>learning</code>以及句号(代表结束)。</p>
<p>比如可以同样将类似的方法来做chatbot，比如我输入一句<code>你好吗</code>，用RNN生成一个vector然后传入右边的RNN，让其生成<code>我很好</code>，类似的对话。</p>
<p>类似的设计在深度学习里面称之为<strong>Encoder-Decoder</strong></p>
<p><img src="/images/rnn_and_gan9.jpg" alt=""></p>
<p>它们二者可以联合训练。至于左边的encoder和右边的decoder是否一样，可以视情况而定，<strong>encoder和decoder可以一样，也可以不一</strong>。如果二者一样，可能容易导致过拟合。</p>
<h2 id="3-Attention-Dynamic-Conditional-Generation"><a href="#3-Attention-Dynamic-Conditional-Generation" class="headerlink" title="3 Attention(Dynamic  Conditional Generation)"></a>3 Attention(Dynamic  Conditional Generation)</h2><p>在上一节里面的<code>机器学习$$\Rightarrow$$machine learning</code>时，左边的encoder每次传入到右边的decoder都是<strong>同样的vector</strong>。其实，我们可以使得每次传给右边的vector不一样。比如，我们想要右边的输出为<code>machine</code>时，关注左边的<code>机器</code>即可，此时RNN应该可以更好的掌握</p>
<p><img src="/images/rnn_and_gan11.jpg" alt=""></p>
<h3 id="3-1-机器翻译"><a href="#3-1-机器翻译" class="headerlink" title="3.1  机器翻译"></a>3.1  机器翻译</h3><p><strong>基于注意力的模型</strong></p>
<ul>
<li><strong>输入</strong>： 每个时间点的每次词汇都可以用一个vector来表示，这个vector是RNN hidden layer的输出</li>
<li><strong>初始参数$z^0$</strong>： 有一个初始的vector $z^0$，可以当做RNN的一个参数，它可以根据训练数据集学习得到</li>
<li><strong>匹配函数match</strong>: 假设有这样一个匹配函数<code>match</code>，此函数由自己设计。用来计算每个timestep，输入词汇与$z^n$的匹配程度。match的示例<ul>
<li>余弦相似度: match函数可以是余弦相似度，来计算$z$和$h$ 的相似度。</li>
<li>更小的神经网络：match函数可以是另外一个神经网络，输入是$z$和$h$,输出是一个标量（衡量匹配度）</li>
<li>参数式: match函数里面有个矩阵参数$w$，此参数可以被学习。可以用$\alpha = h^TWz$来衡量匹配度。</li>
</ul>
</li>
</ul>
<p>例如：</p>
<p><img src="/images/rnn_and_gan12.jpg" alt=""></p>
<p>现在对所有的RNN的隐藏层输出$h^t$计算与$z^0$的匹配度，然后加个softmax（非必要），得到所有输入词汇的匹配程度，如下：</p>
<p><img src="/images/rnn_and_gan13.jpg" alt=""></p>
<p>经过匹配函数match之后，各个输入词汇的在$c^0$中所占比最发生变化，再将输出的$c^0$作为输入传给右边（decoder）的RNN，会使得decoder更加关注此示例中的<code>机器</code>这个单词，更容易学习输出对应的<code>machine</code>，如下图</p>
<p><img src="/images/rnn_and_gan14.jpg" alt=""></p>
<p>可以将此步骤右边decoder的rnn的hidden layer的输出$z^1$作为左边encoder的新的匹配函数（当然可以是其他各式各样的方法），来继续下一步的输出。</p>
<p><img src="/images/rnn_and_gan15.jpg" alt=""></p>
<h3 id="3-2-语音识别"><a href="#3-2-语音识别" class="headerlink" title="3.2 语音识别"></a>3.2 语音识别</h3><p>输入一段音频信号，可以将其抽取为一排vector，每个时间点大概0.01秒用一个vector来表示。神经网络会对所有的vector先计算匹配度，下图的下半部分黑色方格代表匹配度，颜色越深代表越匹配。</p>
<p><img src="/images/rnn_and_gan16.jpg" alt=""></p>
<p>如图，使用第一个红色方格标记的部分黑色方格代表此时RNN需要注意的输入，将此输入传入给一个decoder，会得到对应的输出，即左边横轴的音素<code>h</code>，不仅如此，decoder还会产生空白。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>SSD深入理解</title>
    <url>/2018/03/09/SSD_detail/</url>
    <content><![CDATA[<h2 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1  网络结构"></a>1  网络结构</h2><p> <img src="/images/blog/ssd_structure1.png" alt="网络结构图"><br>加的卷积层的 feature map 的大小变化比较大，允许能够检测出不同尺度下的物体： 在低层的feature map,感受野比较小，高层的感受野比较大，在不同的feature map进行卷积，可以达到多尺度的目的。</p>
<p><strong>SSD去掉了全连接层</strong>，每一个输出只会感受到目标周围的信息，包括上下文。这样来做就增加了合理性。并且不同的feature map,预测不同宽高比的图像，这样比YOLO增加了预测更多的比例的box</p>
<p><strong>横向流程图</strong></p>
<p> <img src="/images/blog/ssd_structure2.jpg" alt="网络横向结构图"></p>
<h3 id="1-1-网络结构-代码"><a href="#1-1-网络结构-代码" class="headerlink" title="1.1 网络结构(代码)"></a>1.1 网络结构(代码)</h3><p>basenet 以VGG-19为例。</p>
<p>代码如下:</p>
<p>第一段是 VGG-19</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Input image format</span><br><span class="line">  img_height, img_width, img_channels &#x3D; image_size[0], image_size[1], image_size[2]</span><br><span class="line"></span><br><span class="line">  ### Design the actual network</span><br><span class="line">  ###############################  这一段是basenet网络结构  用的是VGG-19   ######################################</span><br><span class="line">  x &#x3D; Input(shape&#x3D;(img_height, img_width, img_channels))</span><br><span class="line">  normed &#x3D; Lambda(lambda z: z&#x2F;127.5 - 1.0, # Convert input feature range to [-1,1]</span><br><span class="line">                  output_shape&#x3D;(img_height, img_width, img_channels),</span><br><span class="line">                  name&#x3D;&#39;lambda1&#39;)(x)</span><br><span class="line"></span><br><span class="line">  conv1_1 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv1_1&#39;)(normed)</span><br><span class="line">  conv1_2 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv1_2&#39;)(conv1_1)</span><br><span class="line">  pool1 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2), strides&#x3D;(2, 2), padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;pool1&#39;)(conv1_2)</span><br><span class="line"></span><br><span class="line">  conv2_1 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv2_1&#39;)(pool1)</span><br><span class="line">  conv2_2 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv2_2&#39;)(conv2_1)</span><br><span class="line">  pool2 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2), strides&#x3D;(2, 2), padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;pool2&#39;)(conv2_2)</span><br><span class="line"></span><br><span class="line">  conv3_1 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv3_1&#39;)(pool2)</span><br><span class="line">  conv3_2 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv3_2&#39;)(conv3_1)</span><br><span class="line">  conv3_3 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv3_3&#39;)(conv3_2)</span><br><span class="line">  pool3 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2), strides&#x3D;(2, 2), padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;pool3&#39;)(conv3_3)</span><br><span class="line"></span><br><span class="line">  conv4_1 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv4_1&#39;)(pool3)</span><br><span class="line">  conv4_2 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv4_2&#39;)(conv4_1)</span><br><span class="line">  conv4_3 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv4_3&#39;)(conv4_2)</span><br><span class="line">  pool4 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2), strides&#x3D;(2, 2), padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;pool4&#39;)(conv4_3)</span><br><span class="line"></span><br><span class="line">  conv5_1 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv5_1&#39;)(pool4)</span><br><span class="line">  conv5_2 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv5_2&#39;)(conv5_1)</span><br><span class="line">  conv5_3 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv5_3&#39;)(conv5_2)</span><br><span class="line">  pool5 &#x3D; MaxPooling2D(pool_size&#x3D;(3, 3), strides&#x3D;(1, 1), padding&#x3D;&#39;same&#39;, name&#x3D;&#39;pool5&#39;)(conv5_3)</span><br><span class="line">   ###############################  这一段是basenet网络结束      ######################################</span><br></pre></td></tr></table></figure>
<p>第二段为SSD使用的6个额外的特征层(接上面的)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fc6 &#x3D; Conv2D(1024, (3, 3), dilation_rate&#x3D;(6, 6), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;fc6&#39;)(pool5)</span><br><span class="line">fc7 &#x3D; Conv2D(1024, (1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;fc7&#39;)(fc6)</span><br><span class="line"></span><br><span class="line">conv6_1 &#x3D; Conv2D(256, (1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv6_1&#39;)(fc7)</span><br><span class="line">conv6_2 &#x3D; Conv2D(512, (3, 3), strides&#x3D;(2, 2), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv6_2&#39;)(conv6_1)</span><br><span class="line"></span><br><span class="line">conv7_1 &#x3D; Conv2D(128, (1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv7_1&#39;)(conv6_2)</span><br><span class="line">conv7_2 &#x3D; Conv2D(256, (3, 3), strides&#x3D;(2, 2), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv7_2&#39;)(conv7_1)</span><br><span class="line"></span><br><span class="line">conv8_1 &#x3D; Conv2D(128, (1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv8_1&#39;)(conv7_2)</span><br><span class="line">conv8_2 &#x3D; Conv2D(256, (3, 3), strides&#x3D;(1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;valid&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv8_2&#39;)(conv8_1)</span><br><span class="line"></span><br><span class="line">conv9_1 &#x3D; Conv2D(128, (1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv9_1&#39;)(conv8_2)</span><br><span class="line">conv9_2 &#x3D; Conv2D(256, (3, 3), strides&#x3D;(1, 1), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;valid&#39;, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;conv9_2&#39;)(conv9_1)</span><br></pre></td></tr></table></figure>
<p>对conv4_3的输出做正则化处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Feed conv4_3 into the L2 normalization layer</span><br><span class="line">conv4_3_norm &#x3D; L2Normalization(gamma_init&#x3D;20, name&#x3D;&#39;conv4_3_norm&#39;)(conv4_3)</span><br></pre></td></tr></table></figure>
<p>接下来的步骤是基于basenet的结果做多层输出。 包含以下几个特征层</p>
<ul>
<li>conv4_3_norm</li>
<li>fc7</li>
<li>conv6_2</li>
<li>conv7_2</li>
<li>conv8_2</li>
<li>conv9_2</li>
</ul>
<h2 id="2-分类和回归"><a href="#2-分类和回归" class="headerlink" title="2  分类和回归"></a>2  分类和回归</h2><p>顺着代码继续走。接下来是解析 上图中 <code>Detector &amp; classifier</code> 这部分的代码。</p>
<p>需要了解的是上面的<code>Detector &amp; classifier</code> 这部分操作其实由三部分组成。以<code>Detector &amp; classifier 4</code>为例，如下图：</p>
<p><img src="/images/blog/ssd_3_clas_loc.png" alt="网络横向结构图"></p>
<p>做了 三个操作：</p>
<ul>
<li>生成 anchor box</li>
<li>做卷积-&gt;定位(localization)</li>
<li>做卷积-&gt;分类(confidence)</li>
</ul>
<p>注意上图默认是每个feature map上每个点生成3个 priorbox，所以一共生成了75个。</p>
<h3 id="2-1-卷积-gt-分类"><a href="#2-1-卷积-gt-分类" class="headerlink" title="2.1 卷积-&gt;分类"></a>2.1 卷积-&gt;分类</h3><p>直接看源码如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># we predict &#39;n_classes&#39; confidence values for each box,hence the confidence predictors have depth &#39;n_boxes*n_classes&#39;</span><br><span class="line"># Output shape of confidence layers : &#39; (batch,height,width,n_boxes*n_classes)</span><br><span class="line">conv4_3_mbox_conf &#x3D; Conv2D(n_boxes_fc7*n_classes,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name &#x3D; &#39;conv4_3_norm_mbox_conf&#39;)(conv4_3)</span><br><span class="line">fc7_mbox_conf &#x3D; Conv2D(n_boxes_fc7*n_classes,(3,3),padding &#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;fc7_mbox_conf&#39;)(fc7)</span><br><span class="line">conv8_2_mbox_conf &#x3D; Conv2D(n_boxes_conv6_2*n_classes,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv8_2_mbox_conf&#39;)(conv8_2)</span><br><span class="line">conv9_2_mbox_conf &#x3D; Conv2D(n_boxes_conv7_2*n_classes,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv9_2_mbox_conf&#39;)(conv9_2)</span><br><span class="line">conv10_2_mbox_conf &#x3D; Conv2D(n_boxes_conv9_2*n_classes,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv9_2_mbox_conf&#39;)(conv10_2)</span><br></pre></td></tr></table></figure>
<p>需要注意的是<strong>卷积核数目是跟分类数目相关</strong>。假设某一层feature map的size是 $m\times n$，通道数是 $p$。例如上面展示的 <code>Detector &amp; classifier4</code>就是  $m=5,n=5,p=256$。做分类时<strong>所有的卷积核都是3x3xp</strong>(上面的代码没有体现出p),而输出通道数是 $n<em>{boxes}\times n</em>{classes}$ （代码中的n_boxes和n_classes）<br>n_boxes代表的是default box(从feature map上自动生成的方框)。不同feautre map层的n_boxes不同，一般是4或6.</p>
<h3 id="2-2-卷积-gt-回归-其实还是卷积"><a href="#2-2-卷积-gt-回归-其实还是卷积" class="headerlink" title="2.2 卷积-&gt;回归(其实还是卷积)"></a>2.2 卷积-&gt;回归(其实还是卷积)</h3><p>从feature map中回归得到 每个预测框的 $x(中心点x坐标),y(中心点y坐标),w(预测框的宽度),h(预测框的高度)$ 。同样使用 $3\times 3$的卷积核(理论上应该是 $3\times3\times p$)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## predict 4 boxes for coordinates for each box,hence the localization predictors have depth &#39;n_boxes*4&#39;</span><br><span class="line">conv4_3_mbox_loc &#x3D; Conv2D(n_boxes_conv6_2*4,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv4_3_mbox_loc&#39;)(conv4_3_norm)</span><br><span class="line">fc7_mbox_loc &#x3D; Conv2D(n_boxes_fc7*4,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;fc7_mbox_loc&#39;)(fc7)</span><br><span class="line">conv8_2_mbox_loc &#x3D; Conv2D(n_boxes_conv7_2*4,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv8_2_mbox_loc&#39;)(conv8_2)</span><br><span class="line">conv9_2_mbox_loc &#x3D; Conv2D(n_boxes_conv8_2*4,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv9_2_mbox_loc&#39;)(conv9_2)</span><br><span class="line">conv10_2_mbox_loc &#x3D; Conv2D(n_boxes_conv9_2*4,(3,3),padding&#x3D;&#39;same&#39;,kernel_initializer&#x3D;&#39;he_normal&#39;,name&#x3D;&#39;conv10_2_mbox_loc&#39;)(conv10_2)</span><br></pre></td></tr></table></figure>
<p>与上面的一致，只不过输出通道数变为 $n_{boxes}\times 4$，最后乘以4，代表的是对每个default box(从feature map上自动生成的方框)的位置信息。</p>
<h3 id="2-4-生成prior-box-default-box"><a href="#2-4-生成prior-box-default-box" class="headerlink" title="2.4 生成prior box(default box)"></a>2.4 生成prior box(default box)</h3><p><strong>注意，此时已经有两个地方生成box了。一个来自2.2步的卷积，一个是这一步由新的keras层生成。这一步生成的box是模板形式的，而且最后一个维度是8（2.2步生成的是4）是4个location维度+4个偏置(回归所需的参数)。</strong></p>
<p>论文中并没有提到prior box是基于什么生成的，看图的话会以为是直接从feature map中生成，从代码来看，<strong>prior box是从位置回归的feature map中生成</strong>，这一点与第二节开始的那个图(生成75个box)不太一致，此处暂时按照代码的思路走。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Generate the anchor box(called &quot;priors&quot; in the original caffe&#x2F;c++ implemention )</span><br><span class="line"># output shape of anchor &#39;(batch,height,width,n_boxes,8)&#39;</span><br><span class="line">conv4_3_mbox_priorbox &#x3D; AnchorBoxes(img_height,img_width,this_scale &#x3D; scales[0],next_scale &#x3D; scales[1],</span><br><span class="line">                                        aspect_ratios &#x3D; aspect_ratios_conv4_3,two_boxes_for_ar1 &#x3D; two_boxes_for_ar1,</span><br><span class="line">                                        limit_boxes&#x3D; limit_boxes,variances&#x3D;variances,coords &#x3D; coords,normalize_coords&#x3D; normalize_coords,</span><br><span class="line">                                        name&#x3D;&#39;conv4_3_mbox_priorbox&#39;)(conv4_3_mbox_loc)</span><br><span class="line">fc7_mbox_priorbox &#x3D; AnchorBoxes(img_height, img_width, this_scale&#x3D;scales[1], next_scale&#x3D;scales[2],</span><br><span class="line">                                    aspect_ratios&#x3D;aspect_ratios_fc7,</span><br><span class="line">                                    two_boxes_for_ar1&#x3D;two_boxes_for_ar1, limit_boxes&#x3D;limit_boxes, variances&#x3D;variances,</span><br><span class="line">                                    coords&#x3D;coords, normalize_coords&#x3D;normalize_coords, name&#x3D;&#39;fc7_mbox_priorbox&#39;)(fc7_mbox_loc)</span><br><span class="line">conv8_2_mbox_priorbox &#x3D; AnchorBoxes(img_height, img_width, this_scale&#x3D;scales[3], next_scale&#x3D;scales[4],</span><br><span class="line">                                        aspect_ratios&#x3D;aspect_ratios_conv7_2,</span><br><span class="line">                                        two_boxes_for_ar1&#x3D;two_boxes_for_ar1, limit_boxes&#x3D;limit_boxes,</span><br><span class="line">                                        variances&#x3D;variances, coords&#x3D;coords, normalize_coords&#x3D;normalize_coords,</span><br><span class="line">                                        name&#x3D;&#39;conv7_2_mbox_priorbox&#39;)(conv8_2_mbox_loc)</span><br><span class="line">conv9_2_mbox_priorbox &#x3D; AnchorBoxes(img_height, img_width, this_scale&#x3D;scales[4], next_scale&#x3D;scales[5],</span><br><span class="line">                                        aspect_ratios&#x3D;aspect_ratios_conv8_2,</span><br><span class="line">                                        two_boxes_for_ar1&#x3D;two_boxes_for_ar1, limit_boxes&#x3D;limit_boxes,</span><br><span class="line">                                        variances&#x3D;variances, coords&#x3D;coords, normalize_coords&#x3D;normalize_coords,</span><br><span class="line">                                        name&#x3D;&#39;conv8_2_mbox_priorbox&#39;)(conv9_2_mbox_loc)</span><br><span class="line">conv10_2_mbox_priorbox &#x3D; AnchorBoxes(img_height, img_width, this_scale&#x3D;scales[5], next_scale&#x3D;scales[6],</span><br><span class="line">                                        aspect_ratios&#x3D;aspect_ratios_conv9_2,</span><br><span class="line">                                        two_boxes_for_ar1&#x3D;two_boxes_for_ar1, limit_boxes&#x3D;limit_boxes,</span><br><span class="line">                                        variances&#x3D;variances, coords&#x3D;coords, normalize_coords&#x3D;normalize_coords,</span><br><span class="line">                                        name&#x3D;&#39;conv9_2_mbox_priorbox&#39;)(conv10_2_mbox_loc)&#96;</span><br></pre></td></tr></table></figure>
<p>注意 priorbox的输入是 box_loc。上面的 AnchorBoxes是重写了一个Keras的网络层。</p>
<h3 id="2-5-如何生成prior-box"><a href="#2-5-如何生成prior-box" class="headerlink" title="2.5 如何生成prior box"></a>2.5 如何生成prior box</h3><h4 id="2-5-1-理论"><a href="#2-5-1-理论" class="headerlink" title="2.5.1 理论"></a>2.5.1 理论</h4><p>prior box是按照不同的 scale 和 ratio 生成，m(默认是6，但是有的层不一定，比如conv4_3层的是3(实际上因为对于ratio=1的会多生成一个，所以是4个))个 default boxes，这种结构有点类似于 Faster R-CNN 中的 Anchor。(此处m=6所以：$5\times 5\times 6$ = 150 boxes)。</p>
<p><img src="/images/blog/ssd_4_map.png" alt="网络横向结构图"></p>
<p>上图中从左到右依次是：原图，以特征图中一个像素点为中心生成的3个priorbox（不同宽和高），特征图(256x5x5)。</p>
<ul>
<li><p><strong>scale</strong>: 假定使用N个不同层的feature map 来做预测。最底层的 feature map 的 scale 值为 $s<em>{min}=0.2$，最高层的为$s</em>{max} = 0.9$ ，其他层通过下面公式计算得到 $s<em>k = s</em>{min}+\frac{s<em>{max}-s</em>{min}}{m-1}(k-1), k\in [1,N]$ (低层检测小目标，高层检测大目标)。当前$300\times3\times3$网络一共使用了6(N=6)个feature map，即网络结构图中的detector1..detector6。比如第一层<strong>detector1</strong>的$s_k=0.2$，第二层的<strong>detector2</strong>的$s_k=0.2+\frac{0.9-0.2}{6-1}(2-1)=0.34$,…第五层<strong>detector5</strong>的$s_k=0.2+\frac{0.9-0.2}{6-1}(5-1)=0.76$</p>
</li>
<li><p><strong>ratio</strong>: 使用不同的 ratio值 $a<em>r\in \lbrace 1,2,\frac{1}{2},3,\frac{1}{3}\rbrace$ 计算 default box 的宽度和高度： $w_K^{a} = s_k \sqrt{a_r} , h_k^{a} =s_k/\sqrt{a_r}$ 。另外对于 ratio = 1 的情况，额外再指定 scale 为 $s_k{`}=\sqrt{s_ks</em>{k+1}}$ 也就是总共有 6 中不同的 default box。比如示意图中的为<strong>detector4</strong>，其$s_k=0.62$,依据公式 $w_K^{a} = s_k \sqrt{a_r}$ 按照 $\lbrace 1,2,\frac{1}{2},3,\frac{1}{3}\rbrace$ 顺序可以有 $w_k^a$ : $[0.62\times300,0.62\times1.414\times300,0.62\times0.707\times300,0.62\times1.732\times300,0.62\times0.577\times300]$ 。<strong>与图中的168不一致</strong></p>
</li>
<li><p><strong>default box中心</strong>：上每个 default box的中心位置设置成 $(\frac{i+0.5}{\vert f_k \vert},\frac{j+0.5}{\vert f_k\vert})$ ，其中 $\vert f_k \vert$ 表示第k个特征图的大小 $i,j\in [0,\vert f_k\vert]$  。</p>
</li>
</ul>
<p>注意：每一层的scale参数是</p>
<p><strong>注意这些参数都是相对于原图的参数，不是最终值</strong></p>
<h4 id="2-5-2-代码解析"><a href="#2-5-2-代码解析" class="headerlink" title="2.5.2 代码解析"></a>2.5.2 代码解析</h4><p>我把<code>ssd_box_encode_decode_utils.py</code>代码里面关于如何生成prior box的部分精简部分提取出来如下,注意生成prior box的代码是一个类<code>AnchorBoxes</code>：</p>
<p>先看构造方法里面的参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def __init__(self,</span><br><span class="line">                img_height,</span><br><span class="line">                img_width,</span><br><span class="line">                this_scale,</span><br><span class="line">                next_scale,</span><br><span class="line">                aspect_ratios&#x3D;[0.5, 1.0, 2.0],</span><br><span class="line">                two_boxes_for_ar1&#x3D;True,</span><br><span class="line">                limit_boxes&#x3D;True,</span><br><span class="line">                variances&#x3D;[1.0, 1.0, 1.0, 1.0],</span><br><span class="line">                coords&#x3D;&#39;centroids&#39;,</span><br><span class="line">                normalize_coords&#x3D;False,</span><br><span class="line">                **kwargs)</span><br></pre></td></tr></table></figure>
<p>依次解析参数。</p>
<ul>
<li>img_height：原始输入图像的尺寸</li>
<li>img_width：</li>
<li>this_scale：当前feature map的scale</li>
<li>next_scale：下一个feature map的scale。至于用处，下面的代码会说明</li>
<li>aspect_ratios=[0.5, 1.0, 2.0] :当前feature map即将生成的<strong>每个</strong>prior box的ratios，它的长度即当前feature map上<strong>每个特征点</strong>会生成的prior box数目。</li>
<li>two_boxes_for_ar1=True：对于ratios=1的特征层是否多生成一个 prior box</li>
<li>limit_boxes=True :是否限制boxes的数目</li>
<li>variances=[1.0, 1.0, 1.0, 1.0]： 这个参数是用来和 two_boxes_for_ar1配合使用，用来处理如何多生成一个prior box的</li>
<li>coords=’centroids’：坐标体系，是$(x,y,w,h)$还是$(x<em>{min},y</em>{min},x<em>{max},y</em>{max})$</li>
<li>normalize_coords=False:是否归一化</li>
</ul>
<p>接下来看<code>call(self,x)函数</code>，该函数里面写明了如何处理数据，如何生成priorbox。</p>
<h4 id="2-5-3-获取每个cell的尺寸"><a href="#2-5-3-获取每个cell的尺寸" class="headerlink" title="2.5.3 获取每个cell的尺寸"></a>2.5.3 获取每个cell的尺寸</h4><p>cell代表的是将<strong>原图</strong>切割成 <strong>feature_map_width * feature_map_height</strong>个小矩形格。代码<code>keras_layer_AnchorBoxes</code>的<code>call</code>方法中演示了如何根据每个特征层生成priorbox。代码做了两个操作</p>
<ul>
<li><p>获取每个cell的宽和高</p>
</li>
<li><p>获取每个cell的 起始坐标(左上角的x,y)</p>
</li>
</ul>
<p>为了演示如何处理，我单独测试这个代码。假设测试的特征层为上图的 $5\times5\times5\times256$ ,让所有的值为1.</p>
<p><img src="/images/blog/ssd_5_code1.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input &#x3D; np.ones([16,5,5,512],dtype&#x3D;np.int16)</span><br></pre></td></tr></table></figure>
<p>当前层feature map的ratios = [0.5,1,2]，根据公式$w_K^{a} = s_k \sqrt{a_r} , h_k^{a} =s_k/\sqrt{a_r}$。计算 priorbox的宽和高，注意中间都会乘以size(原图尺寸参考)。</p>
<p>以下图的168为例，</p>
<p><img src="/images/blog/ssd_6_map.png" alt=""></p>
<p>然后将<strong>原图划分cell</strong>，依据是当前feature map大小。比如下面的代码中，feature map大小是 $5\times 5$，原图大小是 $300\times300$，那么每个cell尺寸是 $\frac{300}{5}\times \frac{300}{5}=60\times60$</p>
<p><img src="/images/blog/ssd_7_code2.png" alt=""></p>
<p>上面这一步做的其实是下图</p>
<p><img src="/images/blog/ssd_8_bbox.png" alt="网络横向结构图"></p>
<p>不同的feature map的cell宽和高不同。依据feature map将原图划分为等额的cell，<strong>红框部分是获取每个cell在原图里的起始坐标点(x,y)</strong>。</p>
<p>注意boxes是如何产生的 <code>boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))</code> 创建了一个  <strong><em>size= [feature_map_height,feature_map_width,n_boxes,4]</em></strong> 的四维矩阵。代表的是每个feature map的每个特征点有n_boxes个priorbox，而每个priorbox有<code>x</code>,<code>y</code>,<code>w</code>，<code>h</code>四个参数来定义一个priorbox。</p>
<p>接下来是把priorbox超出原图边界的修正下。</p>
<p>然后再创建一个<code>variances_tensor</code>，它和上面的<code>boxes_tensor</code>维度一样，只不过它的值都为0加上variance(尺寸和n_boxes一样).然后将<code>variances_tensor</code>和<code>boxes_tensor</code>做连接（concatenate）操作。所以生成的priorbox 会变成 <strong><em>size= [feature_map_height,feature_map_width,n_boxes,8]</em></strong> (论文里面不会说得这么具体)</p>
<p>下图可以看出，原图中两个动物分别在不同层次的<code>detector &amp; classifier</code> 被检测出来。<br><img src="/images/blog/ssd_9_code0.png" alt=""></p>
<h3 id="2-6-Reshape"><a href="#2-6-Reshape" class="headerlink" title="2.6 Reshape"></a>2.6 Reshape</h3><p>接下来变换特征矩阵便于做统一处理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># reshape the predict class predictoins,yield 3D tensor of shape &#39;(batch,height*width*n_boxes,n_classes)&#39;</span><br><span class="line"># we want the classes isolated in the last axis to perform softmax on the them</span><br><span class="line">conv4_3_mbox_conf_reshape &#x3D; Reshape((-1,n_classes),name &#x3D; &#39;conv4_3_mbox_conf_reshape&#39;)(conv4_3_mbox_conf)</span><br><span class="line">fc7_mbox_conf_reshape &#x3D; Reshape((-1,n_classes),name&#x3D; &#39;fc7_mbox_conf_reshape&#39;)(fc7_mbox_conf)</span><br><span class="line">conv8_2_mbox_conf_reshape &#x3D; Reshape((-1,n_classes),name &#x3D; &#39;conv8_2_mbox_conf_reshape&#39;)(conv8_2_mbox_conf)</span><br><span class="line">conv9_2_mbox_conf_reshape &#x3D; Reshape((-1,n_classes),name&#x3D; &#39;conv9_2_mbox_conf_reshape&#39;)(conv9_2_mbox_conf)</span><br><span class="line">conv10_2_mbox_conf_reshpe &#x3D; Reshape((-1,n_classes),name &#x3D; &#39;conv10_2_mbox_conf_reshape&#39;)(conv10_2_mbox_conf)</span><br><span class="line"></span><br><span class="line">conv4_3_mbox_loc_reshape &#x3D; Reshape((-1,4),name &#x3D; &#39;conv4_3_mbox_loc_reshape&#39;)(conv4_3_mbox_loc)</span><br><span class="line">fc7_mbox_loc_reshape &#x3D; Reshape((-1, 4), name&#x3D;&#39;fc7_mbox_loc_reshape&#39;)(fc7_mbox_loc)</span><br><span class="line">conv8_2_mbox_loc_reshape &#x3D; Reshape((-1, 4), name&#x3D;&#39;conv8_2_mbox_loc_reshape&#39;)(conv8_2_mbox_loc)</span><br><span class="line">conv9_2_mbox_loc_reshape &#x3D; Reshape((-1, 4), name&#x3D;&#39;conv9_2_mbox_loc_reshape&#39;)(conv9_2_mbox_loc)</span><br><span class="line">conv10_2_mbox_loc_reshpe &#x3D; Reshape((-1, 4), name&#x3D;&#39;conv10_2_mbox_loc_reshape&#39;)(conv10_2_mbox_loc)</span><br><span class="line"></span><br><span class="line">## Reshape the anchor box tensors ,yield 3D tensors of shape &#96;(batch,height*width*n_boxes,8)&#96;</span><br><span class="line">conv4_3_mbox_priorbox_conf_reshape &#x3D; Reshape((-1,8),name&#x3D;&#39;conv4_3_mbox_priorbox_conf_reshape&#39;)(conv4_3_mbox_priorbox)</span><br><span class="line">fc7_mbox_priorbox_conf_reshappe &#x3D; Reshape((-1,8),name&#x3D;&#39;fc7_mbox_priorbox_conf_reshappe&#39;)(fc7_mbox_priorbox)</span><br><span class="line">conv8_2_priorbox_conf_reshape &#x3D; Reshape((-1,8),name&#x3D; &#39;conv8_2_priorbox_conf_reshape&#39;)(conv8_2_mbox_priorbox)</span><br><span class="line">conv9_2_mbox_priorbox_reshape &#x3D; Reshape((-1, 8), name&#x3D;&#39;conv9_2_mbox_priorbox_reshape&#39;)(conv9_2_mbox_priorbox)</span><br><span class="line">conv10_2_mbox_priorbox_reshape &#x3D; Reshape((-1, 8), name&#x3D;&#39;conv10_2_mbox_priorbox_reshape&#39;)(conv10_2_mbox_priorbox)</span><br></pre></td></tr></table></figure>
<p>如何理解这一步的操作？</p>
<p>比如feature map为 $5\times 5\times 256$ (对应的是<code>conv8_2_mbox_conf</code>)这一层，如何运算到当前步骤(不考虑batch)。</p>
<ol>
<li>【分类】做$3\times3$卷积运算,输入通道数是 256，卷积数目是 <strong>n_boxes_conv6_2*n_classes</strong>(注意不是n_boxes_conv8_2<em>n_classes)【见2.1节，没有改变feature map大小】，那么输出矩阵是[n_boxes_conv6_2</em>n_classes,5,5] 。n_boxes_conf6_2 = 4，假设是20个分类(要加一个背景分类)，那么产生新的feature map尺寸为[21x4,5,5]。对应的会生成一共 $21\times4\times5\times5=2100$个priorbox</li>
<li>【回归】做$3\times3$卷积运算,输入通道数是 256，卷积数目是 <strong>n_boxes_conv6_2*4</strong>(注意乘以的是4，不是分类数)【见<strong>2.2</strong>节，没有改变feature map大小】，那么输出矩阵是[n_boxes_conv6_2*4,5,5] 。n_boxes_conf6_2 = 4)，那么产生新的feature map尺寸为[4x4,5,5]。对应的会生成一共 $4\times4\times5\times5=400$个priorbox</li>
<li>【生成priorbox】，从上一步【回归】的矩阵输出 $4\times4\times5\times5$,feature map大小是 $5\times5$，当前层每个特征点生成4个priorbox，每个priorbox有<code>x</code>,<code>y</code>,<code>w</code>,<code>h</code>四个参数。这一步才是真的填补priorbox的四个参数，并且添加了每个参数的偏置variance，变成8.(即$8\times4\times5\times5$)</li>
<li>【reshape】<ul>
<li>对【分类】步骤的结果reshape：[n_boxes_conv6_2*n_classes,5,5]（即[21x4,5,5]）—&gt;[-1,n_classes]（即[100,21]）</li>
<li>对【回归】步骤的结果reshape: [n_boxes_conv6_2*4,5,5] （即[4x4,5,5])—&gt;[-1,4]（即[100,4]）</li>
<li>对【priorbox】步骤的结果reshape:[n_boxes_conv6_2*8,5,5]（即[4x8,5,5]）—&gt;[-1,8]（即[100,8]）</li>
</ul>
</li>
</ol>
<h3 id="2-8-连接concatenate"><a href="#2-8-连接concatenate" class="headerlink" title="2.8 连接concatenate"></a>2.8 连接concatenate</h3><p>连接所有的分类，回归，priorbox</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Concatenate the prediction from different layers</span><br><span class="line"># Axis 0 (batch)  and axis 2 (n_classes or 4)  are identical for all layer predictions</span><br><span class="line"># so we want to concatenate along axis 1, the number of box per layer</span><br><span class="line"># Output shape of &#96;mbox_conf&#96;  :(batch,n_boxes_total,n_classes)</span><br><span class="line">mbox_conf &#x3D; Concatenate(axis&#x3D;1,name&#x3D;&#39;mbox_conf&#39;)([conv4_3_mbox_conf,fc7_mbox_conf_reshape,conv8_2_mbox_conf_reshape,conv9_2_mbox_conf_reshape,conv10_2_mbox_conf_reshpe])</span><br><span class="line"></span><br><span class="line"># output shape of mbox_loc (batch,n_boxes_total,4)</span><br><span class="line">mbox_loc &#x3D; Concatenate(axis&#x3D;1,name&#x3D;&#39;mbox_loc&#39;)([conv4_3_mbox_loc_reshape,fc7_mbox_loc_reshape,conv8_2_mbox_loc_reshape,conv9_2_mbox_loc_reshape,conv10_2_mbox_loc_reshpe])</span><br><span class="line"></span><br><span class="line"># Output shape of &#39;mbox_prior &#39;: (batch,n_boxes_total,8)</span><br><span class="line">mbox_priorbox &#x3D; Concatenate(axis&#x3D;1,name&#x3D;&#39;mbox_priorbox&#39;)([conv4_3_mbox_priorbox_conf_reshape,fc7_mbox_priorbox_conf_reshappe,conv8_2_priorbox_conf_reshape,conv9_2_mbox_priorbox_reshape,conv10_2_mbox_priorbox_reshape])</span><br></pre></td></tr></table></figure>
<p>所以从代码上来看，所有的分类走一条线，回归走一条线，生成priorbox走一条线（中间是从回归那边过来）。一条线的意思是，从basenet开始到最后添加的所有的feature map层处理这一段流程。<strong>从论文来看回归即priorbox，但是代码上来看是分开的</strong></p>
<p>回归<code>loc</code>和<code>priorbox</code>所生成的结果是相互独立的，而分类的结果之间是相互影响的(每个分类都有个单独的结果)，需要做一个softmax实现多分类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mbox_conf_softmax &#x3D; Activation(&#39;softmax&#39;,name&#x3D;&#39;mbox_conf_softmax&#39;)(mbox_conf)</span><br></pre></td></tr></table></figure>
<p>最后做个汇总，把分类、回归、priorbox连接起来。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># concatenate the class and box predictions and the anchor box</span><br><span class="line"># output shape is (batch,n_boxes_total,n_classes+8+4)</span><br><span class="line">prediction &#x3D; Concatenate(axis &#x3D; 1,name&#x3D;&#39;all_prediction&#39;)([mbox_conf_softmax,mbox_loc,mbox_priorbox])</span><br></pre></td></tr></table></figure>
<p>注意是在最后一个维度连接，最后的维度是 <strong>n_classes+4+8</strong></p>
<h2 id="3-数据生成generator"><a href="#3-数据生成generator" class="headerlink" title="3 数据生成generator"></a>3 数据生成generator</h2><p>从源代码来看，generator相当复杂。我们可以只关注<code>ssd_batch_generator.py</code>中的<code>generator</code>方法，可以看到里面做了大量的数据增强。我们顺序来看</p>
<p><strong>数据混排</strong></p>
<p><img src="/images/blog/ssd_9_datashuffle.png" alt=""></p>
<p><strong>等值变换</strong>（增强对比度）</p>
<p><img src="/images/blog/ssd_10_equal.png" alt=""></p>
<p><strong>明暗度变换</strong></p>
<p><img src="/images/blog/ssd_11_brightness.png" alt=""></p>
<p><strong>水平翻转</strong></p>
<p><img src="/images/blog/ssd_12_flip.png" alt=""></p>
<p>等等。。</p>
<h2 id="4-如何生成训练样本-正-负Box"><a href="#4-如何生成训练样本-正-负Box" class="headerlink" title="4 如何生成训练样本(正/负Box)"></a>4 如何生成训练样本(正/负Box)</h2><p>AnchorBox是FasterRCNN的叫法，SSD的是PriorBox。下面的代码是<code>ssd_box_encode_decode_utils</code>的<code>encode_y</code>方法。通过这个方法可以知道代码里面是如何生成正/负样本的。</p>
<p>方法传入的是一张图片的所有真实bbox,即[(分类1，xmin,ymin,xmax,ymax),(分类2,xmin,ymin,xmax,ymax),…]。注意，从下面这段代码可以看出，<strong>没有直接使用真实的标注bbox，而是使用与真实bbox重叠超过一定比率的预设priorbox作为正样本，小于一定比率的为负样本</strong></p>
<p>大概过程如下：</p>
<ol>
<li>先收集整个网络的PriorBox。包含了根据SSD所有特征层生成的PriorBox。作为全部正样本候选</li>
<li>拷贝一份正样本，作为负样本的候选。</li>
<li>计算每个正样本与全部真实标记框的IOU<ul>
<li>1 如果所有的PriorBox与真实标记得IOU都没有高于阈值的，则将有最高IOU的PriorBox作为正样本。同时从负样本中剔除该PriorBox</li>
<li>2 IOU高于阈值的PriorBox会作为正样本保留，同时将对应的priorbox从负样本中剔除</li>
</ul>
</li>
</ol>
<p><img src="/images/blog/ssd_13_bbox.png" alt=""></p>
<h3 id="4-1-如何在矩阵中做变换的"><a href="#4-1-如何在矩阵中做变换的" class="headerlink" title="4.1 如何在矩阵中做变换的"></a>4.1 如何在矩阵中做变换的</h3><p>回顾2.8节，SSD网络的最后输出是  <strong>[box_feature,n_classes+4+8]</strong>。</p>
<p>我们考虑下矩阵是如何变换的，下面的列表是依次说明每一列所代表的意义。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>index</th>
<th>标记</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>[0,..]</td>
<td>box_feature</td>
<td>所有的box</td>
</tr>
<tr>
<td>1</td>
<td>if_class</td>
<td>背景分类的概率</td>
</tr>
<tr>
<td>2</td>
<td>if_class</td>
<td>分类1的概率</td>
</tr>
<tr>
<td>3</td>
<td>if_class</td>
<td>分类2的概率</td>
</tr>
<tr>
<td>4</td>
<td>if_class</td>
<td>分类3的概率</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>分类n的概率</td>
</tr>
<tr>
<td>n+1</td>
<td>xmin</td>
<td>SSD网络预测的可能的box的坐标xmin)</td>
</tr>
<tr>
<td>n+2</td>
<td>xmin</td>
<td>SSD网络预测的可能的box的坐标ymin)</td>
</tr>
<tr>
<td>n+3</td>
<td>xmin</td>
<td>SSD网络预测的可能的box的坐标xmax</td>
</tr>
<tr>
<td>n+4</td>
<td>xmin</td>
<td>SSD网络预测的可能的box的坐标ymax</td>
</tr>
<tr>
<td>n+5</td>
<td>box_xmin</td>
<td>生成的PriorBox的坐标xmin</td>
</tr>
<tr>
<td>n+6</td>
<td>box_ymin</td>
<td>生成的PriorBox的坐标ymin</td>
</tr>
<tr>
<td>n+7</td>
<td>box_xmax</td>
<td>生成的PriorBox的坐标xmax</td>
</tr>
<tr>
<td>n+8</td>
<td>box_ymax</td>
<td>生成的PriorBox的坐标ymax</td>
</tr>
<tr>
<td>n+9</td>
<td>box_x_var</td>
<td>将网络预测的xmin调整到真实xmin所需的参数</td>
</tr>
<tr>
<td>n+10</td>
<td>box_y_var</td>
<td>将网络预测的ymin调整到真实ymin所需的参数</td>
</tr>
<tr>
<td>n+11</td>
<td>box_wth_var</td>
<td>将网络预测的box的<strong>宽度</strong>调整到真实box<strong>宽度</strong>所需的参数</td>
</tr>
<tr>
<td>n+12</td>
<td>box_hgt_var</td>
<td>将网络预测的box的<strong>高度</strong>调整到真实box<strong>高度</strong>所需的参数</td>
</tr>
</tbody>
</table>
</div>
<p>注意：</p>
<ul>
<li><code>SSD网络预测的可能的box的坐标</code>: 这个结果你可以当做普通卷积的一个输出结果，跟PriorBox无关</li>
<li><code>生成的PriorBox的坐标</code>:指的是在feature map参照下生成的各个priorbox坐标。这个是模板形式，任意图片进来都是相同的值。它的作用是产生正/负样本，真实坐标是没有直接参与训练的，priorbox坐标与真实坐标iou大于阈值的为正，小于另外一个阈值的为负。</li>
</ul>
<p>添加测试代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aspect_ratios_per_layer &#x3D; [[0.5, 1.0, 2.0],</span><br><span class="line">                          [1.0 &#x2F; 3.0, 0.5, 1.0, 2.0, 3.0],</span><br><span class="line">                          [1.0 &#x2F; 3.0, 0.5, 1.0, 2.0, 3.0],</span><br><span class="line">                          [1.0 &#x2F; 3.0, 0.5, 1.0, 2.0, 3.0],</span><br><span class="line">                          [0.5, 1.0, 2.0],</span><br><span class="line">                          [0.5, 1.0, 2.0]]</span><br><span class="line">encoder &#x3D; SSDBoxEncoder(300,300,21,predictor_sizes &#x3D; [(20,50,120,150),(20,50,120,150),(20,50,120,150),(20,50,120,150)])</span><br><span class="line">ground_label &#x3D; [[np.array([1,20,50,120,150]),np.array([2,220,150,70,80])]]</span><br><span class="line">encoder.encode_y(ground_label)</span><br></pre></td></tr></table></figure>
<p>我们先分析生成生成Box的数量问题。通过调试上面的测试代码，可以看到</p>
<p><img src="/images/blog/ssd_14_box.png" alt=""></p>
<p>下面再对shape的后一个size 33做出解释。</p>
<p><img src="/images/blog/ssd_15_boxes.png" alt=""></p>
<h2 id="4-损失函数"><a href="#4-损失函数" class="headerlink" title="4 损失函数"></a>4 损失函数</h2><p>损失函数的代码在<code>keras_ssd_loss.py</code>这个类中。</p>
<h3 id="4-1-理论"><a href="#4-1-理论" class="headerlink" title="4.1 理论"></a>4.1 理论</h3><p>目标函数，和常见的 Object Detection 的方法目标函数相同，分为两部分：计算相应的 default box 与目标类别的 score(置信度)以及相应的回归结果（位置回归）。置信度是采用 Softmax Loss（Faster R-CNN是log loss），位置回归则是采用 Smooth L1 loss （与Faster R-CNN一样采用 offset_PTDF靠近 offset_GTDF的策略）。</p>
<script type="math/tex; mode=display">
 L(x,c,l,g) = \frac{1}{n}(L_{cof}(x,c)+\alpha L_{loc}(x,l,g))</script><p>其中N代表正样本数目。回归损失函数如下：</p>
<script type="math/tex; mode=display">
L_{loc}(x,l,g) =\sum ^N_{i\in Pos}\sum_{m\in \lbrace cx,cy,w,h\rbrace}x_{i,j}^k smooth_{L_1}(l_i^m-\hat g_j^m) \\
\hat g_j^{cx}= \frac{(g_j^{cx}-d_i^{cx})}{d_i^w} \\
\hat g_j^{cy}= \frac{(g_j^{cy}-d_i^{cy})}{d_i^h} \\
\hat g_j^w= \frac{(g_j^w-d_i^w)}{d_i^w} \\
\hat g_j^h= \frac{(g_j^h-d_i^h)}{d_i^h}</script><p>分类损失函数如下：</p>
<script type="math/tex; mode=display">
 L_{conf}(x,c) = \sum _{i\in Pos}^Nx_{ij}^plog(\hat c_i^p)-\sum_{i\in Neg}log(\hat c_i^0) \quad\quad 其中 \hat c_i^p = \frac{exp(c_i^p)}{\sum_pexp(c_i^p)}</script><h3 id="4-2-代码中的详细计算"><a href="#4-2-代码中的详细计算" class="headerlink" title="4.2 代码中的详细计算"></a>4.2 代码中的详细计算</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 1: Compute the losses for class and box predictions for every box</span><br><span class="line">classification_loss &#x3D; tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)</span><br><span class="line">localization_loss &#x3D; tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)</span><br></pre></td></tr></table></figure>
<p>可以看到计算loss的时候是分别取出对应部分值的。注意<strong>2.8节</strong>最后的维度是 <strong>n_classes+4+8</strong>,上面计算classification_loss的时候是取得<strong>n_classes</strong>部分，localization_loss取的是<code>4</code>(回归得到的priorbox的四个参数)。<strong>此处最后的<code>8</code>没有使用，这个<code>8</code>是生成的priorbox的4个参数和4个参数的偏置，只有在inference的时候需要使用</strong>。</p>
<p><strong>生成模板</strong></p>
<p><code>generate_encode_template</code>主要做了一下操作：</p>
<ol>
<li>给所有特征层生成box。包括宽、高、坐标、尺寸等。<strong>[batch_size,len(box),4]</strong> （这一步使用的是<code>generate_anchor_boxes</code>方法，不是keras新层AnchorBox，AnchorBox生成的box的最后一个维度是8，已经带了variance）</li>
<li>生成与box同等数量的分类(one-hot形式)，初始都是0。 <strong>[batch_size,len(box),n_classes]</strong></li>
<li>生成与box同等数量的variance。<strong>[batch_size,len(box),4]</strong><br>4.连接1+2+3步骤生成的矩阵，其中第一步生成的box重复一次(原本只是模板，只有初始值（为了保证与ssd网络的输出维度一致）)，所以尺寸是<strong>[batch_size,len(box),n_classes+4+4+4]</strong></li>
</ol>
<p><strong>匹配模板</strong></p>
<p><code>encode_y</code>对传入的<code>ground_truth_labels</code></p>
<h4 id="3-3-如何卷积"><a href="#3-3-如何卷积" class="headerlink" title="3.3 如何卷积"></a>3.3 如何卷积</h4><p>feature map 都会通过一些小的卷积核操作，得到每一个 default boxes 关于物体类别的21个置信度 $(c_1,c_2 ,\cdots, c_p$ 20个类别和1个背景) 和4偏移 (shape offsets) 。</p>
<ul>
<li><p>假设feature map 通道数为 p 卷积核大小统一为 3<em>3</em>p （此处p=256）。个人猜想作者为了使得卷积后的feature map与输入尺度保持一致必然有 padding = 1， stride = 1 。  $ \frac{inputFieldSize-kernelSize+2\times padding}{stride}+1 = \frac{5-3+2\times 1 }{1}+1 = 5$</p>
</li>
<li><p>假如feature map 的size 为 m<em>n, 通道数为 p，使用的卷积核大小为 3</em>3<em>p。每个 feature map 上的每个特征点对应 k 个 default boxes，物体的类别数为 c，那么一个feature map就需要使用 k(c+4)个这样的卷积滤波器，最后有 (m</em>n) <em>k</em> (c+4)个输出</p>
</li>
</ul>
<p>参考 </p>
<p><a href="https://zhuanlan.zhihu.com/p/24954433" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24954433</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习-七-RNN</title>
    <url>/2018/03/03/LHY_RNN/</url>
    <content><![CDATA[<h2 id="1-什么是循环神经网络"><a href="#1-什么是循环神经网络" class="headerlink" title="1  什么是循环神经网络"></a>1  什么是循环神经网络</h2><p>以NLP中的语义分析为例：输入一个词序列，经过神经网络分析，输出此词序列是正面/负面情绪。</p>
<p>首先，<strong>词序列</strong>会被表示为一个<strong>词向量</strong>，接着 我们查看循环神经网络和递归神经网络的处理方式。</p>
<p><strong>循环结构</strong></p>
<p><img src="/images/blog/LHY7_RNN.jpg" alt="前馈网络示意图"></p>
<p>上图是一个典型的循环结构，输入为$x^1,x^2,x^3,x^4$ 这些词向量，$f$为循环神经网络的神经元，每个$f$都是一样的，最后输出的$f$经过另外一个激活函数$g$之后即可输出语音分析结果。</p>
<p><strong>递归网络</strong><br>如果采取的是递归网络，我们需要先决定好输入为$x^1,x^2,x^3,x^4$ 这些词向量之间的先后依赖关系。如下图，$x^1,x^2$一起输入到函数$f$以及$x^3,x^4$一起输入到函数$f$做输出这个是需要自己事先分析并决定好的。</p>
<p><img src="/images/blog/LHY7_RNN2.jpg" alt="前馈网络示意图"></p>
<h3 id="1-1-递归结构"><a href="#1-1-递归结构" class="headerlink" title="1.1 递归结构"></a>1.1 递归结构</h3><p>我们先了解下递归结构，假说需要分析<code>not very good</code>这句话的词性。会分别拆分为<code>not</code>,<code>very</code>,<code>good</code>，这三个单词需要按照<code>very</code>,<code>good</code>先结合之后再和<code>not</code>结合（这个需要我们事先决定）。对应的形式如下，至于函数$f$的形式，需要使用训练数据集学习出来：</p>
<p><img src="/images/blog/LHY7_RNN3.jpg" alt="前馈网络示意图"></p>
<p>完整的学习构建过程如下，假设我们最后需要输出5个分类，从非常负面到非常正面：</p>
<p><img src="/images/blog/LHY7_RNN4.jpg" alt="前馈网络示意图"></p>
<h3 id="1-2-递归网络中函数f的结构"><a href="#1-2-递归网络中函数f的结构" class="headerlink" title="1.2 递归网络中函数f的结构"></a>1.2 递归网络中函数f的结构</h3><h4 id="1-2-1-简单结构"><a href="#1-2-1-简单结构" class="headerlink" title="1.2.1 简单结构"></a>1.2.1 简单结构</h4><p>最简单的结构，如下图：</p>
<p><img src="/images/blog/LHY7_RNN5.jpg" alt="前馈网络示意图"></p>
<p>左边是计算式，右边是结构式。向量<code>a</code>和<code>b</code>串接在一起，乘以参数<code>w</code>(其中<code>w</code>是通过学习得到)，得到最左边的绿色向量结果。</p>
<p>但是这种方式得到的结果一般并不理想，因为有些<strong>矩阵之间存在相互影响关系</strong>,比如上面演示的<code>very</code>和<code>good</code>的结合就会强化正面，<code>very</code>和<code>bad</code>结合就会强化负面。<strong>直接串接在一起只有组合和累计效果，没有相乘关系</strong>。</p>
<h4 id="1-2-2-递归的Neural-Tensor-Network"><a href="#1-2-2-递归的Neural-Tensor-Network" class="headerlink" title="1.2.2 递归的Neural Tensor Network"></a>1.2.2 递归的Neural Tensor Network</h4><p>递归的Neural Tensor Network可以产生相乘效果，如下图，除了包含上面的简单结构之外，它还做了其他组合。</p>
<p><img src="/images/blog/LHY7_RNN6.jpg" alt="前馈网络示意图"></p>
<p>上图中，左侧黑色<code>w</code>左乘了<code>x</code>的转置并右乘了<code>x</code>（其中<code>x</code>是<code>a</code>和<code>b</code>的串接），此处他们之间运算公式为$\sum <em>{i,j} W</em>{i,j}x_ix_j$，注意此处出现的相乘关系。其中$x_i,x_j$分别来自蓝色和黄色矩阵。但是上图虚线框内的相乘结果是一个标量，无法直接与右侧相加的，右侧的相乘结果是$2\times 4$乘以$4\times 1$得到$2\times 1$。需要额外添加其他项，需要重复虚线框内的操作，不过将黑色<code>w</code>矩阵替换为一个新的矩阵，这样的一个组合就会得到一个$2\times 1$的矩阵。</p>
<p><img src="/images/blog/LHY7_RNN7.jpg" alt="前馈网络示意图"></p>
<h3 id="1-2-矩阵-向量-Matrix-Vector-递归网络"><a href="#1-2-矩阵-向量-Matrix-Vector-递归网络" class="headerlink" title="1.2  矩阵-向量(Matrix-Vector)递归网络"></a>1.2  矩阵-向量(Matrix-Vector)递归网络</h3><p>该网络以每个词都由两部分组成，即<code>单词本身的含义</code>和<code>对其他词的影响</code>，如下。</p>
<p><img src="/images/blog/LHY7_RNN8.jpg" alt="前馈网络示意图"></p>
<p>一个实例如下，比如<code>not</code>这个否定词的作用是对词性取反。它本身的含义可以认为是空白，对其他词的影响都是取反，所以可以分解为两部分一部分是空白向量(<code>a</code>)，一部分是对角值为-1的向量(<code>A</code>)。</p>
<p><img src="/images/blog/LHY7_RNN9.jpg" alt="前馈网络示意图"></p>
<p>而<code>good</code>这个词除了表示了一种积极的信息(<code>b</code>)外，对其他词几乎没有影响，所以对其他词的影响部分向量可以看做一个全为1的单位向量(<code>B</code>)，如下：</p>
<p><img src="/images/blog/LHY7_RNN10.jpg" alt="前馈网络示意图"></p>
<p>一个完整的计算过程如下，右上角展开之后的计算过程是中间的黑框内：</p>
<p><img src="/images/blog/LHY7_RNN11.jpg" alt="前馈网络示意图"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习-六-HighwayNetwork和LSTM</title>
    <url>/2018/02/03/LHYDeepLearning_highway/</url>
    <content><![CDATA[<h2 id="1-前馈网络和循环神经网络-RNN"><a href="#1-前馈网络和循环神经网络-RNN" class="headerlink" title="1   前馈网络和循环神经网络(RNN)"></a>1   前馈网络和循环神经网络(RNN)</h2><p><strong>前馈网络示意图</strong></p>
<p><img src="/images/blog/LHY_RNN1.jpg" alt="前馈网络示意图">    </p>
<p>上图中，$f_1,f_2,f_3,f_4..$表示的是前馈网络的网络层，一个输入$x$，一个输出$y$。</p>
<p><strong>循环神经网络</strong></p>
<p><img src="/images/blog/LHY_RNN2.jpg" alt="RNN示意图">    </p>
<p>对比可知，两者十分相似。不同之处是，</p>
<ol>
<li>循环神经网络每个网络层都有输入，而前馈网络只有一个输入</li>
<li>循环神经网络的每一层的激活函数是同一个，而前馈网络的每一层的激活函数都不同。</li>
</ol>
<h2 id="2-如何把GRU改成Highway-Network"><a href="#2-如何把GRU改成Highway-Network" class="headerlink" title="2 如何把GRU改成Highway Network"></a>2 如何把GRU改成Highway Network</h2><p>GRU网络示意图如下：</p>
<p><img src="/images/blog/LHY_GRU.jpg" alt="GRU示意图">   </p>
<p>变成  </p>
<p><img src="/images/blog/LHY_GRU2.jpg" alt="GRU示意图">    </p>
<ol>
<li>拿掉每个时间步的输入$x^t$,</li>
<li>拿掉$y^t$,RNN每个时间步都会输出一个$y^t$,Highway network只有一个最后的输出$y^t$.</li>
<li>把 $h^{t-1}$ 改成 $a^{t-1}$。其中$a^{t-1}$ 是第$t$层的输出</li>
<li>拿掉<code>reset gate</code>。它的作用是让GRU忘记之前发生过的事情，但是Highway不应该忘记，它只有一个开始的。</li>
</ol>
<h2 id="3-Highway-Network"><a href="#3-Highway-Network" class="headerlink" title="3 Highway Network"></a>3 Highway Network</h2><p>一个常见的Highway Network的示意图如下：</p>
<p><img src="/images/blog/LHY_Highway.jpg" alt="Highway示意图">   </p>
<p>注意，它由两部分组成：Gate controller 部分和copy部分。其中的$z,h^{‘},a^{t-1}$（可以通过第二节看到这些详细的）分别如下计算：</p>
<script type="math/tex; mode=display">
 h^{'}=\sigma (Wa^{t-1}) \\
z=\sigma(W^{'}a^{t-1}) \quad\quad  蓝色部分\\
a^t = z\bigodot a^{t-1}+(1-z)\bigodot h  \quad\quad  黑色部分</script><p>一个较深的Highway Network示意图</p>
<p><img src="/images/blog/LHY_Highway2.jpg" alt="Highway示意图">   </p>
<p>它在训练过程中会自动给连接层之间的gate 赋予权重，会自动丢弃某些不重要的层，会自动决定需要多少层。</p>
<p>事实上Highway Network的论文中有论证，通过不断丢弃某些层来评估对网络loss的影响。下图是在<code>MNIST</code>数据集上评测<code>ResNet</code>网络，下图中横轴代表的是网络层，纵轴代表的是网络loss。可以看到$15~45$层 这些层被丢弃之后对网络loss几乎没有影响。</p>
<p><img src="/images/blog/LHY_Highway_loss1.jpg" alt="Highway示意图"> </p>
<p>另外一张图是评测<code>ResNet</code>在CIFAR-10数据集上的结果，可以看到拿掉某些层对网络性能的影响非常大。CIFAR-10是个比较复杂的数据集。</p>
<p><img src="/images/blog/LHY_Highway_loss2.jpg" alt="Highway示意图"> </p>
<h2 id="4-Grid-LSTM"><a href="#4-Grid-LSTM" class="headerlink" title="4 Grid LSTM"></a>4 Grid LSTM</h2><p>它是一种既横着，又竖着的LSTM。它既有时间方向的记忆，又有深度方向的记忆（左边是<strong>LSTM</strong>，右边是<strong>Grid LSTM</strong>）：</p>
<p><img src="/images/blog/LHY_GRIDLSTM1.jpg" alt="GridLSTM示意图"> </p>
<p>原来的LSTM的输入是 $c$ 和 $h$，输出是$c’$和$h^t$，这些都是时间方向上的</p>
<p>Grid LSTM时间方向上与传统的LSTM一致，<strong>多出了一个深度方向的输入输出.输入是$a$,$b$输出是$a’$,$b’$</strong></p>
<h3 id="4-1-Grid-LSTM如何连接"><a href="#4-1-Grid-LSTM如何连接" class="headerlink" title="4.1 Grid LSTM如何连接"></a>4.1 Grid LSTM如何连接</h3><p><img src="/images/blog/LHY_GRIDLSTM2.jpg" alt="GridLSTM示意图"> </p>
<h3 id="4-2-Grid-LSTM内部结构"><a href="#4-2-Grid-LSTM内部结构" class="headerlink" title="4.2  Grid LSTM内部结构"></a>4.2  Grid LSTM内部结构</h3><p><img src="/images/blog/LHY_GRIDLSTM3.jpg" alt="GridLSTM示意图"> </p>
<p>其中</p>
<ul>
<li>$h$ 是<strong>输入</strong></li>
<li>$z^f$ 是 <strong>遗忘门</strong></li>
<li>$z^i$ 是<strong>输入门</strong></li>
<li>$z$  是 <strong>输入信息</strong></li>
<li>$z^o$ 是<strong>输出门</strong></li>
<li>$c$  是<strong>记忆</strong></li>
</ul>
<p>我们 可以将上图右边做一个切分，分别是<strong>历史记忆</strong>，<strong>当前输入</strong>，<strong>准备输出</strong></p>
<p><img src="/images/blog/LHY_GRIDLSTM4.jpg" alt="GridLSTM示意图"> </p>
<h3 id="4-3-Grid-LSTM的输入输出"><a href="#4-3-Grid-LSTM的输入输出" class="headerlink" title="4.3 Grid LSTM的输入输出"></a>4.3 Grid LSTM的输入输出</h3><p>Grid LSTM 有<strong>两套记忆</strong>以及<strong>两套隐藏层输出</strong> ，如何结合并表现出来？</p>
<p><img src="/images/blog/LHY_GRIDLSTM5.jpg" alt="GridLSTM示意图"> </p>
<p>其中 $h$和$b$一起产生各类门(<code>遗忘门</code>,<code>输出门</code>,<code>输入门</code>,<code>输入信息</code>)， $c$和$a$组合成一串较长向量作为历史记忆。</p>
<h3 id="4-4-3D-Grid-LSTM"><a href="#4-4-3D-Grid-LSTM" class="headerlink" title="4.4 3D Grid LSTM"></a>4.4 3D Grid LSTM</h3><p><img src="/images/blog/LHY_3DGRIDLSTM.jpg" alt="GridLSTM示意图"> </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>darknet在nvidia tx2上的训练自己的数据</title>
    <url>/2017/11/14/darknet_on_tx2/</url>
    <content><![CDATA[<h2 id="一-准备数据"><a href="#一-准备数据" class="headerlink" title="一 准备数据"></a>一 准备数据</h2><p><strong>注意</strong>：所有的文件最好在linux下通过代码或者vi的方式来创建，如果从window下创建再拷贝过去的话，很容易出现各种找不到文件的错误。</p>
<p>首先知道yolo需要的几个数据</p>
<ul>
<li>cfg</li>
<li>data</li>
<li>names</li>
<li>weights</li>
</ul>
<p>其中前三个内容分别可以为:</p>
<h3 id="1-1-obj-cfg"><a href="#1-1-obj-cfg" class="headerlink" title="1.1 obj.cfg"></a>1.1 obj.cfg</h3><p>yolo的网络配置文件，一般可以从其官网 <a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg" target="_blank" rel="noopener">darknet-yolo-cfg文件</a>下载一份，然后修改。不要自己手动创建，容易因为编码问题导致程序无法运行。</p>
<p>内容为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[net]</span><br><span class="line"># Testing</span><br><span class="line">batch&#x3D;1</span><br><span class="line">subdivisions&#x3D;1</span><br><span class="line"># Training</span><br><span class="line"># batch&#x3D;64</span><br><span class="line"># subdivisions&#x3D;8</span><br><span class="line">height&#x3D;416</span><br><span class="line">width&#x3D;416</span><br><span class="line">channels&#x3D;3</span><br><span class="line">momentum&#x3D;0.9</span><br><span class="line">decay&#x3D;0.0005</span><br><span class="line">angle&#x3D;0</span><br><span class="line">saturation &#x3D; 1.5</span><br><span class="line">exposure &#x3D; 1.5</span><br><span class="line">hue&#x3D;.1</span><br><span class="line"></span><br><span class="line">learning_rate&#x3D;0.001</span><br><span class="line">burn_in&#x3D;1000</span><br><span class="line">max_batches &#x3D; 80200</span><br><span class="line">policy&#x3D;steps</span><br><span class="line">steps&#x3D;40000,60000</span><br><span class="line">scales&#x3D;.1,.1</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;32</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;64</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;64</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;128</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;256</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size&#x3D;2</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;512</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#######</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers&#x3D;-9</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;64</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[reorg]</span><br><span class="line">stride&#x3D;2</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers&#x3D;-1,-4</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize&#x3D;1</span><br><span class="line">size&#x3D;3</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;1024</span><br><span class="line">activation&#x3D;leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size&#x3D;1</span><br><span class="line">stride&#x3D;1</span><br><span class="line">pad&#x3D;1</span><br><span class="line">filters&#x3D;125</span><br><span class="line">activation&#x3D;linear</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[region]</span><br><span class="line">anchors &#x3D;  1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071</span><br><span class="line">bias_match&#x3D;1</span><br><span class="line">classes&#x3D;20</span><br><span class="line">coords&#x3D;4</span><br><span class="line">num&#x3D;5</span><br><span class="line">softmax&#x3D;1</span><br><span class="line">jitter&#x3D;.3</span><br><span class="line">rescore&#x3D;1</span><br><span class="line"></span><br><span class="line">object_scale&#x3D;5</span><br><span class="line">noobject_scale&#x3D;1</span><br><span class="line">class_scale&#x3D;1</span><br><span class="line">coord_scale&#x3D;1</span><br><span class="line"></span><br><span class="line">absolute&#x3D;1</span><br><span class="line">thresh &#x3D; .6</span><br><span class="line">random&#x3D;1</span><br></pre></td></tr></table></figure>
<h3 id="1-2-obj-data"><a href="#1-2-obj-data" class="headerlink" title="1.2 obj.data"></a>1.2 obj.data</h3><p>个人的数据配置，内容如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes&#x3D; 1  </span><br><span class="line">train  &#x3D; train.txt  </span><br><span class="line">valid  &#x3D; test.txt  </span><br><span class="line">names &#x3D; obj.names  </span><br><span class="line">backup &#x3D; backup&#x2F;</span><br></pre></td></tr></table></figure>
<p>其中 </p>
<ul>
<li>classes:为训练数据的类别数目，比如4分类模型，则为4</li>
<li>train: 训练集图片路径。一般是相对于darknet根目录的路径。</li>
<li>valid: 测试集图片路径。与train相同<br>+names: 图片类别对应的名称。比如：0代表狗，1代表猫，那么第一行就是dog，第二行就是cat。。下面会示例这个文件内容</li>
<li>backup:训练模型过程中产生的权重文件保存路径。有点像tensorflow的checkpoint路径</li>
</ul>
<p>下面示例（<code>train</code>）train.txt所包含的内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;images&#x2F;ee0fb09cfb52bde5157debcdca252fc7.jpg</span><br><span class="line">.&#x2F;images&#x2F;98e053938a2113363003ebd1bf9f81fe.jpg</span><br><span class="line">.&#x2F;images&#x2F;33104814567100bbd1034ef68d0bd39a.jpg</span><br><span class="line">.&#x2F;images&#x2F;115ed44ee5f896674210900491085839.jpg</span><br></pre></td></tr></table></figure>
<h3 id="1-3-obj-names"><a href="#1-3-obj-names" class="headerlink" title="1.3 obj.names"></a>1.3 obj.names</h3><p>注意这个文件里面的内容顺序代表了标注文件中的分类名称：</p>
<h3 id="1-4-weights权重文件"><a href="#1-4-weights权重文件" class="headerlink" title="1.4 weights权重文件"></a>1.4 weights权重文件</h3><p>darknet可以基于其他预训练的权重文件再训练，重新训练时可能需要提供一个权重文件，可以比如<a href="https://pjreddie.com/media/files/darknet19_448.conv.23" target="_blank" rel="noopener">ImageNet</a>的预训练权重开始。</p>
<h2 id="二-训练"><a href="#二-训练" class="headerlink" title="二 训练"></a>二 训练</h2><p>训练脚本为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;darknet detector train cfg&#x2F;obj.data cfg&#x2F;yolo-obj.cfg darknet19_448.conv.23</span><br></pre></td></tr></table></figure>
<h2 id="三-错误排查"><a href="#三-错误排查" class="headerlink" title="三 错误排查"></a>三 错误排查</h2><h3 id="3-1-error-1"><a href="#3-1-error-1" class="headerlink" title="3.1 error 1:"></a>3.1 error 1:</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia@tegra-ubuntu:~&#x2F;workspace&#x2F;cpp&#x2F;darknet$ sudo .&#x2F;darknet detector train ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;test.data ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;test.cfg .&#x2F;weights&#x2F;tiny-yolo-voc.weights </span><br><span class="line">[sudo] password for nvidia: </span><br><span class="line">test</span><br><span class="line">First section must be [net] or [network]: No such file or directory</span><br><span class="line">darknet: .&#x2F;src&#x2F;utils.c:253: error: Assertion &#96;0&#39; failed.</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure>
<p>将 darknet源代码 <code>cfg/voc-yolo.cfg</code>拷贝一份再修改参数。修改如下参数</p>
<ul>
<li>第三行修改为：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">batch&#x3D;64</span><br></pre></td></tr></table></figure>
<ul>
<li>第四行修改为,注意这个地方可能会导致 <code>could&#39;t open file train.txt</code>问题，可以尝试修改为其他，比如16,32</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">subdivisions&#x3D;8</span><br></pre></td></tr></table></figure>
<ul>
<li>第244行修改为</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes&#x3D;4</span><br></pre></td></tr></table></figure>
<ul>
<li>237行修改为。修改规则为 (classes+5)<em>5，当前有4个分类，所以是 (4+5)</em>5=45</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filters&#x3D;45</span><br></pre></td></tr></table></figure>
<h3 id="3-2-error-2"><a href="#3-2-error-2" class="headerlink" title="3.2 error 2"></a>3.2 error 2</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia@tegra-ubuntu:~&#x2F;workspace&#x2F;cpp&#x2F;darknet$ sudo .&#x2F;darknet detector train ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;test.data ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;yolo-test.cfg ~&#x2F;data&#x2F;test_yolo_data&#x2F;darknet19_448.conv.23 </span><br><span class="line">yolo-test</span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 &#x2F; 1   416 x 416 x   3   -&gt;   416 x 416 x  32</span><br><span class="line">    1 max          2 x 2 &#x2F; 2   416 x 416 x  32   -&gt;   208 x 208 x  32</span><br><span class="line">    2 conv     64  3 x 3 &#x2F; 1   208 x 208 x  32   -&gt;   208 x 208 x  64</span><br><span class="line">    3 max          2 x 2 &#x2F; 2   208 x 208 x  64   -&gt;   104 x 104 x  64</span><br><span class="line">    4 conv    128  3 x 3 &#x2F; 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    5 conv     64  1 x 1 &#x2F; 1   104 x 104 x 128   -&gt;   104 x 104 x  64</span><br><span class="line">    6 conv    128  3 x 3 &#x2F; 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    7 max          2 x 2 &#x2F; 2   104 x 104 x 128   -&gt;    52 x  52 x 128</span><br><span class="line">    8 conv    256  3 x 3 &#x2F; 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">    9 conv    128  1 x 1 &#x2F; 1    52 x  52 x 256   -&gt;    52 x  52 x 128</span><br><span class="line">   10 conv    256  3 x 3 &#x2F; 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">   11 max          2 x 2 &#x2F; 2    52 x  52 x 256   -&gt;    26 x  26 x 256</span><br><span class="line">   12 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   13 conv    256  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   14 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   15 conv    256  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   16 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   17 max          2 x 2 &#x2F; 2    26 x  26 x 512   -&gt;    13 x  13 x 512</span><br><span class="line">   18 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   19 conv    512  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   20 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   21 conv    512  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   22 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   23 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   24 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   25 route  16</span><br><span class="line">   26 conv     64  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x  64</span><br><span class="line">   27 reorg              &#x2F; 2    26 x  26 x  64   -&gt;    13 x  13 x 256</span><br><span class="line">   28 route  27 24</span><br><span class="line">   29 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1280   -&gt;    13 x  13 x1024</span><br><span class="line">   30 conv    125  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x 125</span><br><span class="line">   31 detection</span><br><span class="line">darknet: .&#x2F;src&#x2F;parser.c:281: parse_region: Assertion &#96;l.outputs &#x3D;&#x3D; params.inputs&#39; failed.</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure>
<p>参考:<a href="https://groups.google.com/forum/#!topic/darknet/4_RNBWVEOnQ" target="_blank" rel="noopener">https://groups.google.com/forum/#!topic/darknet/4_RNBWVEOnQ</a></p>
<p>解决方案就是修改上一步的 <code>filters=45</code>。</p>
<h3 id="3-3-error-3"><a href="#3-3-error-3" class="headerlink" title="3.3 error 3"></a>3.3 error 3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia@tegra-ubuntu:~&#x2F;workspace&#x2F;cpp&#x2F;darknet$ sudo .&#x2F;darknet detector train ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;test.data ~&#x2F;data&#x2F;test_yolo_data&#x2F;cfg&#x2F;yolo-test.cfg ~&#x2F;data&#x2F;test_yolo_data&#x2F;darknet19_448.conv.23 </span><br><span class="line">yolo-test</span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 &#x2F; 1   416 x 416 x   3   -&gt;   416 x 416 x  32</span><br><span class="line">    1 max          2 x 2 &#x2F; 2   416 x 416 x  32   -&gt;   208 x 208 x  32</span><br><span class="line">    2 conv     64  3 x 3 &#x2F; 1   208 x 208 x  32   -&gt;   208 x 208 x  64</span><br><span class="line">    3 max          2 x 2 &#x2F; 2   208 x 208 x  64   -&gt;   104 x 104 x  64</span><br><span class="line">    4 conv    128  3 x 3 &#x2F; 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    5 conv     64  1 x 1 &#x2F; 1   104 x 104 x 128   -&gt;   104 x 104 x  64</span><br><span class="line">    6 conv    128  3 x 3 &#x2F; 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    7 max          2 x 2 &#x2F; 2   104 x 104 x 128   -&gt;    52 x  52 x 128</span><br><span class="line">    8 conv    256  3 x 3 &#x2F; 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">    9 conv    128  1 x 1 &#x2F; 1    52 x  52 x 256   -&gt;    52 x  52 x 128</span><br><span class="line">   10 conv    256  3 x 3 &#x2F; 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">   11 max          2 x 2 &#x2F; 2    52 x  52 x 256   -&gt;    26 x  26 x 256</span><br><span class="line">   12 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   13 conv    256  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   14 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   15 conv    256  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   16 conv    512  3 x 3 &#x2F; 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   17 max          2 x 2 &#x2F; 2    26 x  26 x 512   -&gt;    13 x  13 x 512</span><br><span class="line">   18 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   19 conv    512  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   20 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   21 conv    512  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   22 conv   1024  3 x 3 &#x2F; 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   23 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   24 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   25 route  16</span><br><span class="line">   26 conv     64  1 x 1 &#x2F; 1    26 x  26 x 512   -&gt;    26 x  26 x  64</span><br><span class="line">   27 reorg              &#x2F; 2    26 x  26 x  64   -&gt;    13 x  13 x 256</span><br><span class="line">   28 route  27 24</span><br><span class="line">   29 conv   1024  3 x 3 &#x2F; 1    13 x  13 x1280   -&gt;    13 x  13 x1024</span><br><span class="line">   30 conv     45  1 x 1 &#x2F; 1    13 x  13 x1024   -&gt;    13 x  13 x  45</span><br><span class="line">   31 detection</span><br><span class="line">mask_scale: Using default &#39;1.000000&#39;</span><br><span class="line">Loading weights from &#x2F;home&#x2F;nvidia&#x2F;data&#x2F;test_yolo_data&#x2F;darknet19_448.conv.23...Done!</span><br><span class="line">Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005</span><br><span class="line">Couldn&#39;t open file: train.txt</span><br></pre></td></tr></table></figure>
<p>参考： <a href="https://groups.google.com/forum/#!msg/darknet/7JgHFfTyFHM/kPzfynNnAQAJ" target="_blank" rel="noopener">https://groups.google.com/forum/#!msg/darknet/7JgHFfTyFHM/kPzfynNnAQAJ</a><br>这个解决办法是将 <code>test.cfg</code>文件中的 <code>subdivisions=8</code>修改为 <code>subdivisions=16</code>或者其他32,64等。但是这个解决办法对我无效，我后来发现需要在linux下重新 编辑一个新的文件<code>test.data</code>(voc.data)。是由于之前的文件是在windows下生成的，与ubuntu系统的编码格式不同。</p>
<p>参考 </p>
<p><a href="https://timebutt.github.io/static/how-to-train-yolov2-to-detect-custom-objects/" target="_blank" rel="noopener">https://timebutt.github.io/static/how-to-train-yolov2-to-detect-custom-objects/</a></p>
<p><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>CNN+RNN来做口语识别</title>
    <url>/2017/10/22/CNN+RNN_Audio/</url>
    <content><![CDATA[<p>翻译自： <a href="https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/" target="_blank" rel="noopener">combining-cnn-and-rnn-for-spoken-language-identificatio</a></p>
<p>github：<a href="https://github.com/harvitronix/continuous-online-video-classification-blog" target="_blank" rel="noopener">源码</a></p>
<p><strong>翻译的原因是觉得示意图很好</strong></p>
<h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><p>正如以前一样，网络的输入是语音记录的图谱。图谱似乎是语音的作为深度学习系统的标准表征形式。</p>
<p>一些网络使用多达11khz的频率(858x256的图像)，而其他使用5.5khz的频率(858x128)。通常情况下，使用5.5khz的结果要相对好一点（可能是因为更高的频率没有包含太多有用的信息，反倒更容易过拟合）。</p>
<p>所有网络的输出层都是全连接的softmax层，176个神经元。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>我们测试了几个网络结构。第一个是纯粹的类似Alex-Net的卷积网络。第二个没有使用任何卷积层，并将（语音）图谱的列作为RNN的序列输入。第三个使用的是，将卷积神经网络抽取出的特征输入到RNN。所有的网络都用Theano和Lasagne。</p>
<p>几乎所有的网络都可以很轻易地在训练集上达到100%的准确率。下表描述的是在验证集上的准确率。</p>
<h2 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h2><p>网络结构由6块(block) 2D卷积组成，Relu激活函数，2D maxpooling和BatchNormalization。第一个卷积层的kernel尺寸是 $7\times 7$，第二个是 $5\times 5$,剩下的都是 $3\times 3$。Pooling的尺寸一直都是 $3\times 3$，步长为2.</p>
<p><strong>BatchNormalization</strong>可以显著提升训练速度。我们最后只在最后的一个Pooling层和softmax层之间使用了一个全连接层，并使用了50%的dropout。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络</th>
<th>准确率</th>
<th>注意</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net.py" target="_blank" rel="noopener">tc_net</a></td>
<td>&lt;80%</td>
<td>此网络与前面描述的CNN的区别在于，这个网络只有一个全连接层。我们并没有怎么训练这个网络，因为<code>ignore_border=False</code>，这个会拖慢训练过程</td>
</tr>
<tr>
<td><a href="https://github.com/YerevaNN/Spoken-language-identification/blob/master/theano/networks/tc_net_mod.py" target="_blank" rel="noopener">tc_net_mod</a></td>
<td>97.14%</td>
<td>与tc_net相同，只不过这里不是 <code>ignore_border=False</code>而是加入了<code>pad=2</code></td>
</tr>
<tr>
<td>tc_net_mod_5khz_small</td>
<td>96.49%</td>
<td>是tc_net_mod的较小副本，使用的是5.5khz</td>
</tr>
</tbody>
</table>
</div>
<p>Lasagne设置<code>ignore_border=False</code>  会使得Theano不使用CuDnn，将其设置为True，可以显著提升速度。</p>
<p>下面是<code>tc_net_mod</code>的详细网络结构：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Nr</th>
<th>Type</th>
<th>Channel</th>
<th>Width</th>
<th>Height</th>
<th>Kernel size/stride</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>input</td>
<td>1</td>
<td>858</td>
<td>256</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>Conv</td>
<td>16</td>
<td>852</td>
<td>250</td>
<td>7x7/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>16</td>
<td>852</td>
<td>250</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>16</td>
<td>427</td>
<td>126</td>
<td>3x3/,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>16</td>
<td>427</td>
<td>126</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>Conv</td>
<td>16</td>
<td>852</td>
<td>250</td>
<td>7x7/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>16</td>
<td>852</td>
<td>250</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>16</td>
<td>427</td>
<td>126</td>
<td>3x3/,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>16</td>
<td>427</td>
<td>126</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Conv</td>
<td>32</td>
<td>423</td>
<td>122</td>
<td>5x5/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>32</td>
<td>423</td>
<td>122</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>32</td>
<td>213</td>
<td>62</td>
<td>3x3/2,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>32</td>
<td>213</td>
<td>62</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Conv</td>
<td>64</td>
<td>211</td>
<td>60</td>
<td>3x3/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>64</td>
<td>211</td>
<td>60</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>64</td>
<td>107</td>
<td>31</td>
<td>3x3/2,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>64</td>
<td>107</td>
<td>31</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>Conv</td>
<td>128</td>
<td>105</td>
<td>29</td>
<td>3x3/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>128</td>
<td>105</td>
<td>29</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>128</td>
<td>54</td>
<td>16</td>
<td>3x3/,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>128</td>
<td>54</td>
<td>16</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>Conv</td>
<td>128</td>
<td>52</td>
<td>13</td>
<td>3x3/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>128</td>
<td>52</td>
<td>14</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>128</td>
<td>27</td>
<td>8</td>
<td>3x3/2,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>128</td>
<td>27</td>
<td>8</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>Conv</td>
<td>256</td>
<td>25</td>
<td>6</td>
<td>3x3/1</td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>256</td>
<td>25</td>
<td>6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>MaxPooling</td>
<td>256</td>
<td>14</td>
<td>3</td>
<td>3x3/2,pad=2</td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>256</td>
<td>14</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Fully connected</td>
<td>1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Relu</td>
<td>1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>BatchNorm</td>
<td>1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dropout</td>
<td>1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>Fully Connected</td>
<td>176</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Softmax Loss</td>
<td>176</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>图谱可以看做列向量序列，其中列向量由256（或者128，如果只使用&lt;5.5khz）个数字组成。我们使用了RNN，其中每一层500个GRU Cell，结构图如下：</p>
<p><img src="/images/blog/cnn+rnn_rnn1.png" alt="RNN"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络</th>
<th>准确率</th>
<th>注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td>rnn</td>
<td>93.27</td>
<td>在输入层上只有一个GRU层</td>
</tr>
<tr>
<td>rnn_2layers</td>
<td>95.66</td>
<td>输入层上两个GRU层</td>
</tr>
<tr>
<td>rnn_2layers_5khz</td>
<td>98.42</td>
<td>输入层上两个GRU层，最大频率是5.5khz</td>
</tr>
</tbody>
</table>
</div>
<p>CNN和RNN都在几个epoch中使用了$adadelta$ 参数，然后再使用冲量SGD（0.003或0.0003）。如果从一开始就使用带冲量的SGD，收敛得很慢。带$adadelta$ 的收敛速度会快一点，但是一般不会得到很高的准确率。</p>
<h2 id="结合CNN和RNN"><a href="#结合CNN和RNN" class="headerlink" title="结合CNN和RNN"></a>结合CNN和RNN</h2><p>CNN与RNN结合的框架一般是卷积抽取的特征作为输入，RNN作为输出，然后再在RNN的输出之后连接一个全连接层，最后是一个softmax层。</p>
<p>CNN的输出是几个channel（即feature map）的集合。我们可以在每个channel上使用几个独立的GRU(可以使用或者不适用权值共享)，如下图：</p>
<p><img src="/images/blog/cnn+rnn_cnn-multi-rnn.png" alt="CNN+RNN"></p>
<p>另外一种做法是，将CNN的输出作为一个3D-tensor，然后在那个tensor的2D slice上运行<strong>单个</strong>GRU。</p>
<p><img src="/images/blog/cnn+rnn_cnn-multi-rnn2.png" alt="CNN+RNN"></p>
<p>后一个做法需要更多的参数，但是<strong>不同channel的信息会在GRU中混淆，这看起来会提升一点性能</strong>。这种架构类似于<a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43455.pdf" target="_blank" rel="noopener">这篇语音识别论文</a>，除了他们会使用一些从输入到RNN和CNN到全连接层的残差(residual)连接。注意到类似的架构在<a href="http://arxiv.org/abs/1602.00367" target="_blank" rel="noopener">文本分类</a>上效果较好。</p>
<p><strong>下面的网络对应的代码位于<a href="https://github.com/YerevaNN/Spoken-language-identification/tree/master/theano/networks" target="_blank" rel="noopener">网络</a></strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络</th>
<th>准确率</th>
<th>注意</th>
</tr>
</thead>
<tbody>
<tr>
<td>tc_net_rnn</td>
<td>92.4</td>
<td>CNN由3个卷积块组成，输出32个channel，尺寸为104x13。每个channel以104个尺寸为13的向量序列输入喂入独立的GRU。GRU的输出会最终融合，然后输入到一个全连接层</td>
</tr>
<tr>
<td>tc_net_rnn_nodense</td>
<td>91.94</td>
<td>与上一个网络一样，只是GRU之后没有全连接层，GRU的输出直接喂入softmax层</td>
</tr>
<tr>
<td>fc_net_rnn_shared</td>
<td>96.96</td>
<td>与上一个网络一样。但是32个GRU单元之间共享权重，这可用于对抗过拟合</td>
</tr>
<tr>
<td>tc_net_rnn_shared_pad</td>
<td>98.11</td>
<td>4个卷积块使用<code>pad=2</code>，而不是<code>ignore_border=False</code>.CNN的输出是32个尺寸为 $54\times 8$的channels。使用32个GRU（每个channel与一个GRU对应），同时共享权重，同时不使用全连接层</td>
</tr>
<tr>
<td>tc_net_deeprnn_shared_pad</td>
<td>96.57</td>
<td>4个卷积块与上面的一样，但是在CNN的输出之后使用了2层共享权重的GRU。由于使用了2层，所以过拟合会严重一点</td>
</tr>
<tr>
<td>tc_net_shared_pad_agum</td>
<td>98.68</td>
<td>与tc_net_rnn_shared_pad一样，但是网络会在输入上做随机裁剪，并间隔9秒。性能提升了一点</td>
</tr>
<tr>
<td>tc_net_rnn_onernn</td>
<td>99.2</td>
<td>4个卷积块的输出被分组为一个 $32\time 54\times 8$ 的3D-tensor，单个GRU运行于54个尺寸为 $32\times 8$的序列上</td>
</tr>
<tr>
<td>tc_net_rnn_onernn_notimepool</td>
<td>99.24</td>
<td>与上面的网络类似，但是pool层在时间轴上的步长设为1。因为CNN的输出是32个尺寸为 $852\times 8$的channels</td>
</tr>
</tbody>
</table>
</div>
<p>第二层GRU并没有什么用，因为会产生过拟合。</p>
<p>看起来<strong>在时间维度的子抽样并不是什么好办法。在子抽样过程中丢失的信息，被RNN用起来效果更好</strong>。在论文<a href="http://arxiv.org/abs/1602.00367v1" target="_blank" rel="noopener">文本分类</a>中，作者直接建议所有的池化层/子抽样层都可以用RNN层来代替。本文没有尝试这种方法，不过应该是蛮有前景的。</p>
<p>这些网络都使用了带冲量的SGD。学习率在10个epoches左右时设置为0.003，然后手工缩减到0.001，然后到0.0003。平均大概需要35个epoches来训练这些网络。</p>
<h2 id="Ensembling（集成学习）"><a href="#Ensembling（集成学习）" class="headerlink" title="Ensembling（集成学习）"></a>Ensembling（集成学习）</h2><p>最好的单模型在验证集上取得了99.24%的准确率。所有的这些模型做了33个预测（不同的epoches之后，一些模型不止预测一次），我们只是简单的累加预测概率，并获得99.67%的准确率。出乎意料之外的是，其他集成学习尝试，（只是在所有模型的某些子集上集成）并没有获得更好的结果。</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这些CNN+RNN混合模型的超参数数量十分之多。受限于硬件，我们只覆盖了很少一部分可能的配置。</p>
<p>由于原始的<a href="https://apps.topcoder.com/forums/?module=Thread&amp;threadID=866217&amp;start=0&amp;mc=3" target="_blank" rel="noopener">竞赛</a>是非公开的数据集，所以我们没法发布全部的源代码在<a href="https://github.com/YerevaNN/Spoken-language-identification/tree/master/theano" target="_blank" rel="noopener">Github</a>。</p>
<p>参考：　<a href="http://blog.revolutionanalytics.com/2016/09/deep-learning-part-3.html" target="_blank" rel="noopener">http://blog.revolutionanalytics.com/2016/09/deep-learning-part-3.html</a></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>使用tensorflow object detection api训练自己的数据</title>
    <url>/2017/09/21/tf_obj_detect_api_train_owndata/</url>
    <content><![CDATA[<h2 id="一-数据准备"><a href="#一-数据准备" class="headerlink" title="一  数据准备"></a>一  数据准备</h2><p>首先，我们有如下数据结构如下：</p>
<ul>
<li>data<ul>
<li>annotations:标注文件<ul>
<li>txt：txt文本标注文件</li>
<li>xmls：xml格式标注文件</li>
</ul>
</li>
<li>images：图像文件</li>
<li>config: 配置文件目录，下面有个当前数据集的 <code>.config</code> 配置文件。</li>
<li>tf_records：需要创建的一个目录，用于存储tensorflow将images转换为tf_records。</li>
<li>xx_label_map.pbtxt:分类名称对应的整型分类</li>
</ul>
</li>
</ul>
<h3 id="1-1-images文件"><a href="#1-1-images文件" class="headerlink" title="1.1 images文件"></a>1.1 images文件</h3><p>images目录下的文件为：</p>
<p><img src="/images/blog/tf_obj_detect_own_iamges.jpg" alt="images目录"></p>
<h3 id="1-2-标注文件"><a href="#1-2-标注文件" class="headerlink" title="1.2  标注文件"></a>1.2  标注文件</h3><p>xml标注文件类似：</p>
<p><img src="/images/blog/tf_obj_detect_own_xml.jpg" alt="xml标注文件"></p>
<p> txt标注文件可以不需要。</p>
<h3 id="1-3-label-map-pbtxt文件"><a href="#1-3-label-map-pbtxt文件" class="headerlink" title="1.3  label_map.pbtxt文件"></a>1.3  label_map.pbtxt文件</h3><p><code>xx_label_map.pbtxt</code>文件中的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">item &#123;</span><br><span class="line">  id: 1</span><br><span class="line">  name: &#39;Abyssinian&#39;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 2</span><br><span class="line">  name: &#39;american_bulldog&#39;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 3</span><br><span class="line">  name: &#39;american_pit_bull_terrier&#39;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-创建tf-record文件"><a href="#1-4-创建tf-record文件" class="headerlink" title="1.4  创建tf_record文件"></a>1.4  创建tf_record文件</h3><p>先创建一个<code>create_xx_tf_record.py</code>文件，单独用来处理训练数据。可以直接从object_detection工程下的<code>create_pacal_tf_record.py</code>（如果是每个图片只有一个分类，可以使用<code>create_pet_tf_record.py</code>）复制而来。</p>
<p>修改起始参数配置：</p>
<ul>
<li>data_dir: 数据目录，包含了图片和标注的目录</li>
<li>output_dir:输出目录，图片转换为tf_record之后存储的位置</li>
<li>label_map_path:上面提到的xx_label_map.pbtxt</li>
</ul>
<p>修改<code>dict_to_tf_example</code></p>
<p> 参考你的标准xml文件，有些地方需要修改。</p>
<p> <img src="/images/blog/tf_obj_detect_own_dict.jpg" alt="dict_to_tf"></p>
<p>修改<code>main</code></p>
<p><img src="/images/blog/tf_obj_detect_own_main.jpg" alt="修改main"></p>
<p> 确保你的标注文件，图片目录对应的目录。标注文件目录下是否存在 <code>trainval.txt</code>文件是否存在，这个需要<strong>自己生成</strong>。我生成的列表（注意：没有带后缀）为：</p>
<p><img src="/images/blog/tf_obj_detect_own_trainval.jpg" alt="trainval文件"></p>
<p>执行完之后会在对应目录下生成 tf_record文件。</p>
<h3 id="1-5-创建-config-配置文件"><a href="#1-5-创建-config-配置文件" class="headerlink" title="1.5 创建 .config 配置文件"></a>1.5 创建 <code>.config</code> 配置文件</h3><p>目录<code>tensorflow\models\object_detection\samples\configs</code>下有各种配置文件，当前工程使用的是  <code>faster_rcnn_inception_resnet_v2_robot.config</code>，将其修改为适应当前数据的配置。</p>
<p>主要修改了这些参数：</p>
<ul>
<li>num_classes： 分类数目。视数据分类数目而定，当前数据集只有3个分类，修改为3</li>
<li>fine_tune_checkpoint：此处应该为空白，之前修改成github上下载的faster_rcnn的ckpt文件会导致无法训练的情况。</li>
<li>from_detection_checkpoint： 设置为true</li>
<li>num_steps: 训练步数。如果数据集较小，可以修改为较小。<code>pets</code>数据集包含7393张图片设置为20万次，当前数据集只有500张，设置为一万次应该差不多。可以在训练的时候查看loss增减情况来修改步数。</li>
</ul>
<h2 id="2-训练"><a href="#2-训练" class="headerlink" title="2 训练"></a>2 训练</h2><p>训练时执行<code>train.py</code>即可。不过需要传入一些参数，可以使用官网的指定方式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python object_detection&#x2F;train.py \</span><br><span class="line">    --logtostderr \</span><br><span class="line">    --pipeline_config_path&#x3D;$&#123;PATH_TO_YOUR_PIPELINE_CONFIG&#125; \</span><br><span class="line">    --train_dir&#x3D;$&#123;PATH_TO_TRAIN_DIR&#125;</span><br></pre></td></tr></table></figure>
<p>我在pycharm下运行，所以在Run-&gt;configigure里面加入参数即可。需要指定的参数是：</p>
<ul>
<li>pipeline_config_path:上面提到的<code>.config</code>配置文件</li>
<li>train_dir: 训练模型过程中保存的ckpt文件（tensorflow的权重文件）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--logtostderr --pipeline_config_path&#x3D;D:&#x2F;data&#x2F;robot_auto_seller&#x2F;config&#x2F;faster_rcnn_inception_resnet_v2_robot.config --train_dir&#x3D;D:&#x2F;data&#x2F;robot_auto_seller&#x2F;tf_ckpt</span><br></pre></td></tr></table></figure>
<p>训练完成之后，大概的效果如下：</p>
<p><img src="/images/blog/tf_obj_detect_own_train_result.jpg" alt="训练效果"></p>
<p>如果训练得当，应该可以用tensorboard查看训练参数变化：</p>
<p><img src="/images/blog/tf_obj_detect_own_tensorboard_cmd.jpg" alt="tensorboard"></p>
<p>打开浏览器中的： <a href="http://localhost:6006/#scalars" target="_blank" rel="noopener">http://localhost:6006/#scalars</a></p>
<p><img src="/images/blog/tf_obj_detect_own_tensorboard2.jpg" alt="tensorboard2"></p>
<h2 id="3-转换权重文件"><a href="#3-转换权重文件" class="headerlink" title="3 转换权重文件"></a>3 转换权重文件</h2><p>训练完成之后的权重文件大概是会包含如下文件:</p>
<ul>
<li>model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001,</li>
<li>model.ckpt-${CHECKPOINT_NUMBER}.index</li>
<li>model.ckpt-${CHECKPOINT_NUMBER}.meta</li>
</ul>
<p>我生成的大概为：</p>
<p><img src="/images/blog/tf_obj_detect_own_ckpt.jpg" alt="ckpt文件"></p>
<p> 这些文件无法直接使用，<code>eval.py</code> 所使用的权重文件是<code>.pb</code>。需要做一步转换，object_detection工程中已经包含了该工具<code>export_inference_graph.py</code>，运行指令为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python object_detection&#x2F;export_inference_graph.py \</span><br><span class="line">    --input_type image_tensor \</span><br><span class="line">    --pipeline_config_path $&#123;PIPELINE_CONFIG_PATH&#125; \</span><br><span class="line">    --trained_checkpoint_prefix $&#123;TRAIN_PATH&#125; \</span><br><span class="line">    --output_directory output_inference_graph.pb</span><br></pre></td></tr></table></figure>
<ul>
<li>pipeline_config_path :pipeline的配置路径，使用的是上面训练所使用的<code>.config</code>文件</li>
<li>trained_checkpoint_prefix :上一步保存tensorflow的权重文件ckpt的。精确到step数目，比如为<code>xxx/model.ckpt-8876</code></li>
<li>output_directory ：最终输出的可以用来做inference得文件（到具体文件名称）</li>
</ul>
<p>我的脚本为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--input_type image_tensor --pipeline_config_path D:&#x2F;data&#x2F;aa&#x2F;config&#x2F;faster_rcnn_inception_resnet_v2_robot.config --trained_checkpoint_prefix D:&#x2F;data&#x2F;aa&#x2F;tf_ckpt&#x2F;model.ckpt-6359  --output_directory  D:&#x2F;data&#x2F;aa&#x2F;robot_inference_graph</span><br></pre></td></tr></table></figure>
<p>生成的效果为：</p>
<p><img src="/images/blog/tf_obj_detect_own_graph.png" alt="pb文件"></p>
<h2 id="4-预测"><a href="#4-预测" class="headerlink" title="4  预测"></a>4  预测</h2><p>预测代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import six.moves.urllib as urllib</span><br><span class="line">import sys</span><br><span class="line">import tarfile</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line">from collections import defaultdict</span><br><span class="line">from io import StringIO</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from PIL import Image</span><br><span class="line">import cv2  </span><br><span class="line">from object_detection.utils import label_map_util</span><br><span class="line">from object_detection.utils import visualization_utils as vis_util</span><br><span class="line"></span><br><span class="line">cap &#x3D; cv2.VideoCapture(0)</span><br><span class="line">PATH_TO_CKPT &#x3D; &#39;D:&#x2F;data&#x2F;aa&#x2F;robot_inference_graph&#x2F;frozen_inference_graph.pb&#39;</span><br><span class="line">PATH_TO_LABELS &#x3D; os.path.join(&#39;D:&#x2F;data&#x2F;aa&#39;, &#39;robot_label_map.pbtxt&#39;)</span><br><span class="line">NUM_CLASSES &#x3D; 3</span><br><span class="line"></span><br><span class="line"># Load a (frozen) Tensorflow model into memory.</span><br><span class="line">detection_graph &#x3D; tf.Graph()</span><br><span class="line">with detection_graph.as_default():</span><br><span class="line">    od_graph_def &#x3D; tf.GraphDef()</span><br><span class="line">    with tf.gfile.GFile(PATH_TO_CKPT, &#39;rb&#39;) as fid:</span><br><span class="line">        serialized_graph &#x3D; fid.read()</span><br><span class="line">        od_graph_def.ParseFromString(serialized_graph)</span><br><span class="line">        tf.import_graph_def(od_graph_def, name&#x3D;&#39;&#39;)</span><br><span class="line"></span><br><span class="line">label_map &#x3D; label_map_util.load_labelmap(PATH_TO_LABELS)</span><br><span class="line">categories &#x3D; label_map_util.convert_label_map_to_categories(label_map, max_num_classes&#x3D;NUM_CLASSES,</span><br><span class="line">                                                            use_display_name&#x3D;True)</span><br><span class="line">category_index &#x3D; label_map_util.create_category_index(categories)</span><br><span class="line"></span><br><span class="line">def load_image_into_numpy_array(image):</span><br><span class="line">    (im_width, im_height) &#x3D; image.size</span><br><span class="line">    return np.array(image.getdata()).reshape(</span><br><span class="line">        (im_height, im_width, 3)).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># # Detection</span><br><span class="line">PATH_TO_TEST_IMAGES_DIR &#x3D; &#39;D:&#x2F;data&#x2F;aa&#x2F;images&#39;</span><br><span class="line">TEST_IMAGE_PATHS &#x3D; [os.path.join(PATH_TO_TEST_IMAGES_DIR, &#39;000&#123;&#125;.jpg&#39;.format(i)) for i in range(109, 115)]</span><br><span class="line"># Size, in inches, of the output images.</span><br><span class="line">IMAGE_SIZE &#x3D; (12, 8)</span><br><span class="line"></span><br><span class="line">with detection_graph.as_default():</span><br><span class="line">    with tf.Session(graph&#x3D;detection_graph) as sess:</span><br><span class="line">        #while True:  # for image_path in TEST_IMAGE_PATHS:    #changed 20170825</span><br><span class="line">        # Definite input and output Tensors for detection_graph</span><br><span class="line">        image_tensor &#x3D; detection_graph.get_tensor_by_name(&#39;image_tensor:0&#39;)</span><br><span class="line">        # Each box represents a part of the image where a particular object was detected.</span><br><span class="line">        detection_boxes &#x3D; detection_graph.get_tensor_by_name(&#39;detection_boxes:0&#39;)</span><br><span class="line">        # Each score represent how level of confidence for each of the objects.</span><br><span class="line">        # Score is shown on the result image, together with the class label.</span><br><span class="line">        detection_scores &#x3D; detection_graph.get_tensor_by_name(&#39;detection_scores:0&#39;)</span><br><span class="line">        detection_classes &#x3D; detection_graph.get_tensor_by_name(&#39;detection_classes:0&#39;)</span><br><span class="line">        num_detections &#x3D; detection_graph.get_tensor_by_name(&#39;num_detections:0&#39;)</span><br><span class="line">        for image_path in TEST_IMAGE_PATHS:</span><br><span class="line">            image &#x3D; Image.open(image_path)</span><br><span class="line">            # the array based representation of the image will be used later in order to prepare the</span><br><span class="line">            # result image with boxes and labels on it.</span><br><span class="line">            image_np &#x3D; load_image_into_numpy_array(image)</span><br><span class="line">            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]</span><br><span class="line">            image_np_expanded &#x3D; np.expand_dims(image_np, axis&#x3D;0)</span><br><span class="line">            # Actual detection.</span><br><span class="line">            (boxes, scores, classes, num) &#x3D; sess.run(</span><br><span class="line">                [detection_boxes, detection_scores, detection_classes, num_detections],</span><br><span class="line">                feed_dict&#x3D;&#123;image_tensor: image_np_expanded&#125;)</span><br><span class="line">            # Visualization of the results of a detection.</span><br><span class="line">            print(boxes)</span><br><span class="line">            vis_util.visualize_boxes_and_labels_on_image_array(</span><br><span class="line">                image_np,</span><br><span class="line">                np.squeeze(boxes),</span><br><span class="line">                np.squeeze(classes).astype(np.int32),</span><br><span class="line">                np.squeeze(scores),</span><br><span class="line">                category_index,</span><br><span class="line">                use_normalized_coordinates&#x3D;True,</span><br><span class="line">                line_thickness&#x3D;8)</span><br><span class="line">            plt.figure(figsize&#x3D;IMAGE_SIZE)</span><br><span class="line">            cv2.imwrite(&#39;D:&#x2F;data&#x2F;robot_auto_seller&#x2F;&#39;+os.path.basename(image_path),image_np)</span><br><span class="line">            plt.imshow(image_np)</span><br></pre></td></tr></table></figure>
<p>此检测过程有两个版本。一个版本是开启摄像头检测，一个版本是直接检测图片。上面这部分代码是检测图片的。修改部分为</p>
<ul>
<li>PATH_TO_CKPT ： 训练生成的<code>.pb</code>权重文件（上一步转换之后的结果）</li>
<li>PATH_TO_LABELS ：标签和分类(int)对应关系配置文件。第一步中设置的。</li>
<li>NUM_CLASSES ： 分类数。当前数据集是3个分类</li>
<li><p>PATH_TO_TEST_IMAGES_DIR ：需要检测的图片的路径。<br>TEST_IMAGE_PATHS ： 需要检测的图片列表。</p>
<p>使用摄像头检测的例子放在附件中了。</p>
</li>
</ul>
<p>参考：<br><a href="https://github.com/tensorflow/models/blob/master/object_detection/object_detection_tutorial.ipynb" target="_blank" rel="noopener">tensorflow 官方教程</a><br><a href="https://medium.com/towards-data-science/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9" target="_blank" rel="noopener">浣熊检测（英文）</a><br><a href="http://blog.csdn.net/qq_20373723/article/details/77838545" target="_blank" rel="noopener">tensorflow 生成pb文件</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>window测试tensorflow object detection api</title>
    <url>/2017/09/20/tensorflow_object_detection_api/</url>
    <content><![CDATA[<h2 id="0-注意"><a href="#0-注意" class="headerlink" title="0 注意"></a>0 注意</h2><p>安装window版本的tensorflow时，如果tensorflow版本是1.3。需要安装 cudnn 6.0,但是貌似官网不让看了，windows端安装地址为： <a href="http://developer.download.nvidia.com/compute/redist/cudnn/v6.0/cudnn-8.0-windows10-x64-v6.0.zip" target="_blank" rel="noopener">windows cudnn6.0</a><br>window 端的为 <a href="http://developer.download.nvidia.com/compute/redist/cudnn/v6.0/cudnn-8.0-linux-x64-v6.0.tgz" target="_blank" rel="noopener">linux cudnn 6.0</a></p>
<h2 id="1-预备"><a href="#1-预备" class="headerlink" title="1 预备"></a>1 预备</h2><h3 id="1-1-Tensorflow-Object-Detection-API-依赖："><a href="#1-1-Tensorflow-Object-Detection-API-依赖：" class="headerlink" title="1.1 Tensorflow Object Detection API 依赖："></a>1.1 Tensorflow Object Detection API 依赖：</h3><ul>
<li>Protobuf 2.6(最新版本是3.4，下面会提到如何安装)</li>
<li>Pillow 1.0 （从网站 <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy" target="_blank" rel="noopener">python第三方包下载网站</a>）</li>
<li>lxml（从网站 <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy" target="_blank" rel="noopener">python第三方包下载网站</a>）</li>
<li>tf Slim (<code>tensorflow/models</code>模块中包含了)</li>
<li>Jupyter notebook(如果不运行官网的网页测试就不需要)</li>
<li>Matplotlib</li>
<li>Tensorflow</li>
</ul>
<h3 id="1-2-下载model"><a href="#1-2-下载model" class="headerlink" title="1.2 下载model"></a>1.2 下载model</h3><p>下载链接： <a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">https://github.com/tensorflow/models</a></p>
<p>将<code>models/object_detection</code>拷贝到一个新工程目录<code>object_detection</code>下（工程名和代码目录都叫object_detection,工程名可以是其他）。我的目录结构如下：</p>
<p><img src="/images/blog/tf_obj_detect_struct1.jpg" alt="项目架构"></p>
<p>之所以在弄两个object_detection，是要保留代码的引用逻辑，否则你要改一堆import 错误。而单独把object_detection抽出来是方便集成到其他工程里。</p>
<h3 id="1-3-安装protoc"><a href="#1-3-安装protoc" class="headerlink" title="1.3 安装protoc"></a>1.3 安装protoc</h3><p>下载链接： <a href="https://github.com/google/protobuf/releases" target="_blank" rel="noopener">protoc</a></p>
<p><img src="/images/blog/tf_obj_detect_download1.jpg" alt="protoc下载"></p>
<p>我们在windows下使用，选择win32.下载后解压到某个目录下，解压 后的目录包含了<code>bin</code>目录：</p>
<p><img src="/images/blog/tf_obj_detect_win_proto_bin.jpg" alt="protoc"></p>
<p> 为了避免夜长梦多，我直接把这个路径加入到window环境变量</p>
<p><img src="/images/blog/tf_obj_detect_win_var.png" alt="protoc环境变量"></p>
<h3 id="1-4-将proto文件生成对应的代码"><a href="#1-4-将proto文件生成对应的代码" class="headerlink" title="1.4 将proto文件生成对应的代码"></a>1.4 将proto文件生成对应的代码</h3><p>虽然不太理解tensorflow的 <code>model/object_detection/protos/</code>目录下一堆<code>.proto</code>文件用处（proto貌似是谷歌文件传输数据格式），但是从安装过程大概可知，这些文件会被生成python文件。依赖的是一下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">protoc object_detection&#x2F;protos&#x2F;*.proto --python_out&#x3D;.</span><br></pre></td></tr></table></figure>
<p>注意该命令是在你的 <code>object_detection</code>文件夹的上一层目录下执行，默认是在<code>tensorflow/model</code>下。</p>
<p><img src="/images/blog/tf_obj_detect_proto_effect.png" alt="protoc前后"></p>
<h3 id="1-5-预知的错误"><a href="#1-5-预知的错误" class="headerlink" title="1.5 预知的错误"></a>1.5 预知的错误</h3><p>如果存在 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nets import xxx</span><br></pre></td></tr></table></figure>
<p>错误，是因为官网教程中提到的一句，将<code>slim</code>要加入到python环境变量中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># From tensorflow&#x2F;models&#x2F;</span><br><span class="line">export PYTHONPATH&#x3D;$PYTHONPATH:&#96;pwd&#96;:&#96;pwd&#96;&#x2F;slim</span><br></pre></td></tr></table></figure>
<p>windows下没法完成这句，所以我们在需要用到nets的时候，把对应的网络（位于<code>\models\slim\nets</code>）复制过去即可。比如 <code>faster_rcnn_inception_resnet_v2_feature_extractor</code>开始的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nets import inception_resnet_v2</span><br></pre></td></tr></table></figure>
<p>这一句显然无法执行，我们可以替换为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from object_detection.nets import inception_resnet_v2</span><br></pre></td></tr></table></figure>
<p>从<code>\models\slim\nets</code>下将<code>nets</code>文件夹拷贝到<code>object_detect/object_detection</code>工程下。</p>
<p><img src="/images/blog/tf_obj_detect_copy_net.jpg" alt="工程结构"></p>
<h2 id="2-编写测试代码"><a href="#2-编写测试代码" class="headerlink" title="2 编写测试代码"></a>2 编写测试代码</h2><p>测试代码基本复制自官方的 jupyter notebook脚本，名字为<code>object_detection_test.py</code> </p>
<p><img src="/images/blog/tf_obj_detect_test_code.png" alt="测试demo"></p>
<p>代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import six.moves.urllib as urllib</span><br><span class="line">import sys</span><br><span class="line">import tarfile</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line">from collections import defaultdict</span><br><span class="line">from io import StringIO</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">import cv2  # add 20170825</span><br><span class="line"></span><br><span class="line">cap &#x3D; cv2.VideoCapture(0)  # add 20170825</span><br><span class="line">sys.path.append(&quot;..&quot;)</span><br><span class="line"></span><br><span class="line">from object_detection.utils import label_map_util</span><br><span class="line"></span><br><span class="line">from object_detection.utils import visualization_utils as vis_util</span><br><span class="line"></span><br><span class="line">MODEL_NAME &#x3D; &#39;ssd_mobilenet_v1_coco_11_06_2017&#39;</span><br><span class="line">MODEL_FILE &#x3D; MODEL_NAME + &#39;.tar.gz&#39;</span><br><span class="line">DOWNLOAD_BASE &#x3D; &#39;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;models&#x2F;object_detection&#x2F;&#39;</span><br><span class="line">PATH_TO_CKPT &#x3D; MODEL_NAME + &#39;&#x2F;frozen_inference_graph.pb&#39;</span><br><span class="line"></span><br><span class="line"># List of the strings that is used to add correct label for each box.</span><br><span class="line">PATH_TO_LABELS &#x3D; os.path.join(&#39;data&#39;, &#39;mscoco_label_map.pbtxt&#39;)</span><br><span class="line"></span><br><span class="line">NUM_CLASSES &#x3D; 90</span><br><span class="line"></span><br><span class="line">opener &#x3D; urllib.request.URLopener()</span><br><span class="line">opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)</span><br><span class="line">tar_file &#x3D; tarfile.open(MODEL_FILE)</span><br><span class="line">for file in tar_file.getmembers():</span><br><span class="line">    file_name &#x3D; os.path.basename(file.name)</span><br><span class="line">    if &#39;frozen_inference_graph.pb&#39; in file_name:</span><br><span class="line">        tar_file.extract(file, os.getcwd())</span><br><span class="line"></span><br><span class="line">detection_graph &#x3D; tf.Graph()</span><br><span class="line">with detection_graph.as_default():</span><br><span class="line">    od_graph_def &#x3D; tf.GraphDef()</span><br><span class="line">    with tf.gfile.GFile(PATH_TO_CKPT, &#39;rb&#39;) as fid:</span><br><span class="line">        serialized_graph &#x3D; fid.read()</span><br><span class="line">        od_graph_def.ParseFromString(serialized_graph)</span><br><span class="line">        tf.import_graph_def(od_graph_def, name&#x3D;&#39;&#39;)</span><br><span class="line"></span><br><span class="line">label_map &#x3D; label_map_util.load_labelmap(PATH_TO_LABELS)</span><br><span class="line">categories &#x3D; label_map_util.convert_label_map_to_categories(label_map, max_num_classes&#x3D;NUM_CLASSES,</span><br><span class="line">                                                            use_display_name&#x3D;True)</span><br><span class="line">category_index &#x3D; label_map_util.create_category_index(categories)</span><br><span class="line"></span><br><span class="line">def load_image_into_numpy_array(image):</span><br><span class="line">    (im_width, im_height) &#x3D; image.size</span><br><span class="line">    return np.array(image.getdata()).reshape(</span><br><span class="line">        (im_height, im_width, 3)).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">PATH_TO_TEST_IMAGES_DIR &#x3D; &#39;test_images&#39;</span><br><span class="line">TEST_IMAGE_PATHS &#x3D; [os.path.join(PATH_TO_TEST_IMAGES_DIR, &#39;image&#123;&#125;.jpg&#39;.format(i)) for i in range(1, 3)]</span><br><span class="line"></span><br><span class="line"># Size, in inches, of the output images.</span><br><span class="line">IMAGE_SIZE &#x3D; (12, 8)</span><br><span class="line"></span><br><span class="line"># In[10]:</span><br><span class="line"></span><br><span class="line">with detection_graph.as_default():</span><br><span class="line">    with tf.Session(graph&#x3D;detection_graph) as sess:</span><br><span class="line">        while True:  # for image_path in TEST_IMAGE_PATHS:    #changed 20170825</span><br><span class="line">            ret, image_np &#x3D; cap.read()</span><br><span class="line"></span><br><span class="line">            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]</span><br><span class="line">            image_np_expanded &#x3D; np.expand_dims(image_np, axis&#x3D;0)</span><br><span class="line">            image_tensor &#x3D; detection_graph.get_tensor_by_name(&#39;image_tensor:0&#39;)</span><br><span class="line">            # Each box represents a part of the image where a particular object was detected.</span><br><span class="line">            boxes &#x3D; detection_graph.get_tensor_by_name(&#39;detection_boxes:0&#39;)</span><br><span class="line">            # Each score represent how level of confidence for each of the objects.</span><br><span class="line">            # Score is shown on the result image, together with the class label.</span><br><span class="line">            scores &#x3D; detection_graph.get_tensor_by_name(&#39;detection_scores:0&#39;)</span><br><span class="line">            classes &#x3D; detection_graph.get_tensor_by_name(&#39;detection_classes:0&#39;)</span><br><span class="line">            num_detections &#x3D; detection_graph.get_tensor_by_name(&#39;num_detections:0&#39;)</span><br><span class="line">            # Actual detection.</span><br><span class="line">            (boxes, scores, classes, num_detections) &#x3D; sess.run(</span><br><span class="line">                [boxes, scores, classes, num_detections],</span><br><span class="line">                feed_dict&#x3D;&#123;image_tensor: image_np_expanded&#125;)</span><br><span class="line">            # Visualization of the results of a detection.</span><br><span class="line">            vis_util.visualize_boxes_and_labels_on_image_array(</span><br><span class="line">                image_np,</span><br><span class="line">                np.squeeze(boxes),</span><br><span class="line">                np.squeeze(classes).astype(np.int32),</span><br><span class="line">                np.squeeze(scores),</span><br><span class="line">                category_index,</span><br><span class="line">                use_normalized_coordinates&#x3D;True,</span><br><span class="line">                line_thickness&#x3D;8)</span><br><span class="line">            cv2.imshow(&#39;object detection&#39;, cv2.resize(image_np, (1024, 800)))</span><br><span class="line">            if cv2.waitKey(25) &amp; 0xFF &#x3D;&#x3D; ord(&#39;q&#39;):</span><br><span class="line">                cv2.destroyAllWindows()</span><br><span class="line">                break</span><br></pre></td></tr></table></figure>
<p>注意，启动程序之前得准备个摄像头。</p>
<p>测试效果</p>
<p><img src="/images/blog/tf_obj_detect_test_result.png" alt="测试demo效果"></p>
<p>参考 :<a href="http://blog.csdn.net/c20081052/article/details/77608954" target="_blank" rel="noopener">http://blog.csdn.net/c20081052/article/details/77608954</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>Funnel-structured cascade for multi-view face detection with alignmentawareness 论文阅读笔记</title>
    <url>/2017/07/12/fust_multi_view_face_detection/</url>
    <content><![CDATA[<h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p>  目前主流的三类人脸识别方法。</p>
<ul>
<li>最经典的是增强级联框架（boosted cascade framework）。这些检测器(detector)计算高效，可快速抽取特征。</li>
<li>为了处理精确处理面部变化较大，DPM（deformable part models可变形部件模型）：用以同时抽取图像的全局和局部特征。它基于一种覆盖分类内部变化的启发式方法，因此对于图像中人物表情姿势的变化有较好鲁棒性。但是非常耗时。</li>
<li>最新的是使用CNN卷积神经网络的方法。缺点是计算代价高，因为网络得复杂性和许多复杂的非线性操作。</li>
</ul>
<p>以上工作都没有考虑特殊场景，比如<strong>多角度人脸识别</strong>。为了多角度识别人脸，一种直接的方法就是并行使用多个人脸检测器(detector)。并行架构需要所有候选窗口被所有模型分类，这导致计算成本和误报率的飙升。为缓解此类问题，所有模型需要精心地训练，使模型具有较好的区分能力去辨别人脸和非人脸。</p>
<p><img src="/images/blog/facedetect_model_2.jpg" alt="多模型"></p>
<p>多视角的多模型可以如上图这样组织成树形或金字塔形。这些结构中，根分类器都是区分是否为人脸，接下来的其他分类模型将人脸按照不同的精细粒度分为不同子分类，这里的每个模型都是独立的。金字塔模型实际是将共享了某些高层节点的模型压缩了，因此金字塔模型与并行模型有一样的问题。树形结构分类器不同之处在于，分支的动机是避免在同一层评估所有的分类器，但是这会导致检测错误分类分支。</p>
<p>为此我们提出了一种漏斗形级联的多视角人脸检测结构，获得较高准确率和较快速度。该结构上宽下窄，模型如下图。</p>
<p><img src="/images/blog/fust_arch.jpg" alt="漏斗级联架构"></p>
<p>模型的顶层是一些并行运行的，快速而粗粒度的分类器，用来快速地移除非人脸窗口。每个模型都是针对性地使用一小段区间范围的视角的人脸，因而可以保证多角度人脸的较高召回率。越往下，模型的区分能力越强，但是也越耗时，它们被用来筛选符合条件的窗口候选。模型的底部收集最后通过的窗口，最后一阶段是一个统一的多层干感知机。</p>
<h2 id="二-漏斗结构级联的多视角人脸检测器"><a href="#二-漏斗结构级联的多视角人脸检测器" class="headerlink" title="二 漏斗结构级联的多视角人脸检测器"></a>二 漏斗结构级联的多视角人脸检测器</h2><p> 输入图像根据滑动窗口树状图扫描，然后每个窗口依次分阶段地穿过探测器。</p>
<p> <strong>Fast LAB接连分类器</strong>用来快速移除大部分非人脸窗口，（LAB（Locally Assembled Binary））同时保证人脸窗口的较高召回率。<strong>Coarse MLP Cascade</strong>分类器以较低代价来进一步调整候选窗口。最后，统一<strong>Fine MLP Cascade</strong>分类器使用形状索引特征精确地区分人脸。</p>
<h3 id="2-1-Fast-LAB-cascade"><a href="#2-1-Fast-LAB-cascade" class="headerlink" title="2.1 Fast LAB cascade"></a>2.1 Fast LAB cascade</h3><p> 实时人脸识别时，最大的障碍在于需要检验的滑动窗口树状图的候选窗口太多。在一个640x480的图像上，要检测脸特征尺寸超过20x20的人脸，需要检查超过一百万个窗口。使用增强级联分类器，由Yan et al提出了一种有效的LAB((Locally Assembled Binary)，只需要考虑Haar 特征的相对关系，并使用look-up（查阅表）加速。一个窗口中抽取一个LAB特征仅需要访问内存一次。我们可以使用LAB 特征，可以在程序开始时快速地移除占比非常大的非人脸特征。</p>
<p> 尽管LAB 特征方法有速度，但是对于多角度人脸窗口的复杂变换表现较差。因此我们采取了一种分而治之的思路，将较难的多视角人脸问题分解为容易的单视角人脸检测问题。多个LAB 级联分类器，每个角度一个分类器，并行处理，然后最终的候选人脸窗口是所有经分类器筛选过后的结果合集。</p>
<p> <strong>公式：</strong>定义整个包含了多角度人脸的训练集为 <strong><em>S</em></strong>，根据角度划分为 <strong><em>v</em></strong> 个子集，<br> 定义为 $S_i,i=1,2,…v$ 。对每个训练集 $S_i$ ,一个LAB级联分类器 $c_i$ 被训练，它用于检测第 $i$ 个角度的人脸。对于输入图像中的窗口 $x$ ，它是否为人脸取决于如下所有的LAB 级联分类器：</p>
<script type="math/tex; mode=display">
  y=c_i(x)\vee c_2(x)...\vee c_v(x)</script><p> 其中 $y \epsilon \lbrace0,1\rbrace$ ，$c_i(x)\epsilon \lbrace0,1\rbrace$ 表明 $x$ 是否为人脸。使用多模型消耗更多时间，但是所有模型共享相同的LAB特征映射（用来特征抽取）。</p>
<h3 id="2-2-Coarse-MLP-cascade-粗粒度多层感知机级联"><a href="#2-2-Coarse-MLP-cascade-粗粒度多层感知机级联" class="headerlink" title="2.2  Coarse MLP cascade 粗粒度多层感知机级联"></a>2.2  Coarse MLP cascade 粗粒度多层感知机级联</h3><p>  LAB级联阶段之后，大部分非人脸窗口被抛弃，剩下的部分对于单个LAB 特征难以处理。因此，接下来，候选窗口将交给更复杂的分类器来处理，比如带 <strong>SURF（Speeded-up Robust Feature）</strong> 的MLP。为避免增加太多计算，小型网络被开发为更好，但是依旧粗粒度的校验。</p>
<p>  此外，使用SURF特征的MLP用于窗口分类，可以更好的建模非线性多角度人脸和带有等同的非线性激活函数的非人脸模式。</p>
<p>  MLP由输入层，输出层和一个或多个隐藏层组成。公式化n层的MLP如下:</p>
<script type="math/tex; mode=display">
  F(x)=f_{n-1}(f_{n-2}(...f_1(x)))\quad tag 2\\
  f_i(z)=\sigma(W_iz+b_i)</script><p>其中 $x$   是输入，比如候选窗口的SURF特征； $W_i$ 和 $b_i$ 分别为链接第 $i$ 层和第 $i+1$ 层的权重和偏置。激活函数 $\sigma$ 形如： $\sigma (x)=\frac{1}{1+e^{-x}}$ ，从上式可以看出，隐藏层和输出都做了非线性变换。MLP的训练目标是最小化预测值和实际值之间的均方误差</p>
<script type="math/tex; mode=display">
 min_F\sum_{i=1}^n \mid \mid F(x_i)-y_i \mid \mid ^2</script><p>其中 $x_i$ 是第 $i$ 个训练样本， $y_i$ 是对应的标签(0或1)。</p>
<p>由于MLP级联分类器有足够能力建模人脸和非人脸变换，穿过多个LAB级联分类器之间的窗口可以由同一个模型处理，也即MLP级联可以连接多个LAB级联分类器。</p>
<h3 id="2-3-带形状索引特征的细粒度MLP级联"><a href="#2-3-带形状索引特征的细粒度MLP级联" class="headerlink" title="2.3 带形状索引特征的细粒度MLP级联"></a>2.3 带形状索引特征的细粒度MLP级联</h3><p> 多视角人脸外貌之间存在一些冲突，主要源于非对齐特征，比如基于坐标抽取的特征存在语义不一致问题。比如，一个面向前方的人脸的中央区域包含了鼻子，但是面部外形也是脖子的一部分。为解决这个问题，我们采取了一种基于形状索引的方法在语义相同的位置上抽取特征作为细粒度MLP级联分类器的输入。如下图所示，选择了四个语义位置，分别对应的面部坐标是左、右眼中心，鼻尖和嘴中心。对于侧脸，不可见的眼部被视为与另外一只眼睛处于相同坐标。</p>
<p><img src="/images/blog/fust_land.jpg" alt="人脸关键点检测"></p>
<p>对于表情更丰富的基于形状索引的特征，更大、性能更强的非线性变换用来实现面部和非面部微调。与之前的不同的是，更大的MLPs同时预测标签，推测一个候选窗口是否为一张脸，推测其形状。一个额外的形状预测误差项加入到目标函数，新的优化问题变为如下：</p>
<script type="math/tex; mode=display">
min_F \sum_{i=1}^n \mid \mid F_c(\phi (x_i,\hat S_i))-y_i \mid \mid ^2+\lambda \sum_{i=1}^n \mid\mid F_s(\phi (x_i-\hat S_i))-s_i \mid\mid ^2_2</script><p>其中 $F_c$ 是面部分类输出， $F_s$ 是预测形状输出。 $\phi (x_i,\hat s_i)$ 代表的是基于形状索引的特征（比如SIFT），它是按照平均形状或预测形状为 $\hat s_i$ 从第 $i$ 个训练样本抽取的，其中 $s_i$ 是实际形状。 $\lambda$ 是平衡两类误差的权重因子，一般设置为 $\frac{1}{d}$，其中d为形状的维度。从上面的等式可以看出，可以获得一个比输入 $\hat s_i$更精确地外形 $F_s(\phi(x_i,\hat s_i))$ （注意看下标）。因此，多个级联的MLPs，用于特征抽取的形状越来越精确，这会获得更加有区分力的基于形状索引的特征，并且最后让多角度人脸与非人脸区域差异更大。下图展示了这一过程：</p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>近似推断</title>
    <url>/2017/06/12/sim_predict/</url>
    <content><![CDATA[<h2 id="一-问题描述"><a href="#一-问题描述" class="headerlink" title="一  问题描述"></a>一  问题描述</h2><p><strong>如果我们有一组可见变量v，如何推断产生这些数据的模型m </strong></p>
<p>难点：除了可见变量v，通常还有一系列的隐含变量h，模型由1）</p>
<ul>
<li><p>模型的类别ξ （如高斯分布，伽马分布，多项式分布等）与2）</p>
</li>
<li><p>模型的参数Θ 共同决定，即m (ξ, Θ ) 。</p>
<p>模型的选择 :假设 H为所有可能的模型集合（包括不同类别），那么选择m=argmax{p(m(ξ, Θ) |v ), m ∈ M}，<br>上述问题的主要挑战在于计算p(h| v)或者计算在p(h| v)下的期望。</p>
</li>
</ul>
<p><strong>为什么做近似推断</strong></p>
<p>概率推断的核心任务就是计算某分布下的某个函数的期望、或者计算边缘概率分布、条件概率分布等等。这些任务往往需要积分或求和操作，但在很多情况下，计算这些东西往往不那么容易。因为，</p>
<p>首先，积分中涉及的<strong>分布可能有很复杂的形式</strong>，这样就无法直接得到解析解；</p>
<p>其次，我们要积分的<strong>变量空间可能有很高的维度</strong>，这样就把我们做数值积分的路都给堵死了。</p>
<p>因此，进行精确计算往往是不可行的，需要引入一些近似计算方法。</p>
<h2 id="二-近似推断的方法"><a href="#二-近似推断的方法" class="headerlink" title="二 近似推断的方法"></a>二 近似推断的方法</h2><h3 id="2-1-随机方法"><a href="#2-1-随机方法" class="headerlink" title="2.1 随机方法"></a>2.1 随机方法</h3><p>Gibbs采样法，通过大量的样本估计真实的后验，以真实数据为基础来近似目标分布。优点如下：</p>
<p>更精确；而且采样过程相对简单；易于操作，有着良好的理论收敛性，并且实现更加简单。但是收敛速度较慢，难以判断收敛程度的问题</p>
<h3 id="2-2-确定近似法：变分法"><a href="#2-2-确定近似法：变分法" class="headerlink" title="2.2 确定近似法：变分法"></a>2.2 确定近似法：变分法</h3><p>用一些已知的简单的分布来近似后验分布。</p>
<p><strong>优点</strong>： 有解析解、计算开销较小、速度快、易于在大规模问题中应用。</p>
<p><strong>缺点</strong>：</p>
<ul>
<li><p>推导过程相对复杂，对人的要求高，</p>
</li>
<li><p>推导出想要的形式比较困难，也就是说，这些简单的分布到底能多大程度生近似目标分布呢？很难衡量。</p>
</li>
<li><p>只是优化对应分布之间的KL散度得到最终的结果，变分下界小于等于目标函数，所以在近似分布难以拟合的时候，其结果是严格小于目标函数的。容易造成结果的不精确</p>
</li>
</ul>
<h2 id="三-近似后验分布学习"><a href="#三-近似后验分布学习" class="headerlink" title="三 近似后验分布学习"></a>三 近似后验分布学习</h2><h3 id="3-1-最大期望算法（EM算法）"><a href="#3-1-最大期望算法（EM算法）" class="headerlink" title="3.1 最大期望算法（EM算法）"></a>3.1 最大期望算法（EM算法）</h3><p><strong>概念：</strong> 期望最大算法是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大估计方法。</p>
<p>我们都知道似然估计，主要是用来估计未知参数$\theta$ ，通常我们<strong>已知服从某种分布的样本</strong> $\lbrace x_1,x_2,…x_n\rbrace$。但是<strong>不知道样本参数$\theta$</strong> 。 我们估计 $\theta$ 的思想是，取得使似然函数最大的 $\theta$。所谓的似然函数，即出现这一样本集合的概率函数：</p>
<script type="math/tex; mode=display">
   L(\theta) = \prod _{i=1} ^{n} p(x_i;\theta)</script><p>其中的 $p_i$ 为每个样本出现的概率。累乘为同时出现的概率。</p>
<p>通常取对数，使连乘变累加。</p>
<script type="math/tex; mode=display">
   H(\theta) = ln(L(\theta)) =\sum _{i=1} ^n ln(p(x_i;\theta))</script><p>最后求使得$H(\theta)$ 最大的 $\theta$ 值。通常是求导数，令导数为0，得到似然方程，解似然方程，得到的参数即为所求。</p>
<p>EM算法实际上与似然估计很相似，但有一条，就是它不能确定样本来自哪个分布（可能有好几个分布，这些分布产生的样本混合在一起，不能确定样本来自哪个分布）</p>
<p><strong>所以EM算法比似然估计多了个过程就是，首先要估计样本来自哪个分布。</strong></p>
<p>假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。</p>
<p><strong>算法流程</strong>：</p>
<ol>
<li><p>第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其极大似然估计值；</p>
</li>
<li><p>第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。</p>
</li>
</ol>
<p><strong>示例</strong>：已知200人的身高数据，性别未知，求不同性别的身高分布。</p>
<blockquote>
<p>我们是先随便猜一下男生（身高）的正态分布的参数：如均值和方差是多少。例如男生的均值是1米7，方差是0.1米（当然了，刚开始肯定没那么准），然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布），这个是属于Expectation一步。有了每个人的归属，或者说我们已经大概地按上面的方法将这200个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整E步……如此往复，直到参数基本不再发生变化为止。</p>
</blockquote>
<h4 id="3-1-1-EM算法的推导过程"><a href="#3-1-1-EM算法的推导过程" class="headerlink" title="3.1.1 EM算法的推导过程"></a>3.1.1 EM算法的推导过程</h4><p>假设我们有一个样本集 $\lbrace x_1,x_2,…,x_m\rbrace。包含m个独立的样本。但是<strong>每个样本$i$对应的类别未知</strong>，也即隐含变量。故我们需要估计概率模型 $p(x,z)$ 的参数 $\theta$ ，但是由于里面包含了隐含变量z，所以很难用最大似然求解。但是如果知道Z，就很容易求解了。</p>
<p>对于参数估计，我们本质上还是想获得一个使似然函数最大化的那个参数$\theta$。现在与最大似然不同的只是似然函数式多了一个未知变量z。我们的目标变成找到合适的 $\theta$ 和 z，让$L(\theta)$ 最大。</p>
<script type="math/tex; mode=display">
 \sum _i log P(x^{(i)};\theta)= \sum _i log\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\theta) \\
 \sum _i log _{z^{(i)}}Q_i (z^{(i)})\frac{P(x^{(i)}),z^{(i)}\theta}{Q_i(z^{(i)})} \\
 \ge \sum _i\sum _{z^{(i)}} Q_i (z^{(i)}) log\frac{P(x^{(i)}),z^{(i)}\theta}{Q_i(z^{(i)})}</script><p>上式右边，第一行式子。这里z也是随机变量。对每一个样本i的所有可能类别z求等式右边的联合概率密度函数和，也就得到等式左边为随机变量x的边缘概率密度</p>
<p>上式右边，第二行式子。只是分子分母同乘以一个相等的函数，还是有“和的对数”，无法求解。</p>
<p>上式右边，第三行式子。变成了“对数的和”，那这样求导就容易，此处的等号变为不等号源于Jense不等式。</p>
<h4 id="3-1-2-Jensen不等式"><a href="#3-1-2-Jensen不等式" class="headerlink" title="3.1.2 Jensen不等式"></a>3.1.2 Jensen不等式</h4><p>如果f是凸函数，X是随机变量，那么：E[f(X)]&gt;=f(E[X])</p>
<p>特别地，如果f是严格凸函数，当且仅当X是常量时，上式取等号。</p>
<p><img src="/images/blog/jensen_fourma.jpg" alt="公式"></p>
<p>实线f是凸函数，X是随机变量，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。X的期望值就是a和b的中值了，图中可以看到E[f(X)]&gt;=f(E[X])成立。<strong>Jensen不等式应用于凹函数时，不等号方向反向。</strong></p>
<p>回到EM算法推导的第二个式子，因为$f(x)=logx$ 为凹函数（其二次导数为 $-\frac{1}{x^2}&lt;0$  。式子中 $log _{z^{(i)}}Q_i (z^{(i)})\frac{P(x^{(i)}),z^{(i)}\theta}{Q_i(z^{(i)})}$ 是 $frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$的期望（考虑到 $E(x)=\sum x\star p(x)）$ ,则$E(f(x))=\sum f(x)\starp(x)$ )。 而 $\sum_zQ_i(z^{(i)})=1 $</p>
<p>上式中，我们让 $Q_i$ 表示样本的隐含变量$z$ 的某种分布。$Q_i$满足的条件是 $\sum_z Q_i(z)=1,Q_i(z)\ge 0$。（如果z是连续性的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量z是身高，那么就是连续的高斯分布。如果按照隐藏变量是男女，那么就是伯努利分布了。</p>
<p>参考:</p>
<p><a href="http://blog.csdn.net/zouxy09/article/details/8537620/" target="_blank" rel="noopener">从最大似然到EM算法浅解</a></p>
<p><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html" target="_blank" rel="noopener">JerryLead The EM algorithm</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>语音合成步骤</title>
    <url>/2017/04/20/texttospeech/</url>
    <content><![CDATA[<p>译自：A beginners’ guide to statistical parametric speech synthesis</p>
<h2 id="一-语音合成-Text-To-Speech-TTS-概述"><a href="#一-语音合成-Text-To-Speech-TTS-概述" class="headerlink" title="一  语音合成(Text-To-Speech)TTS 概述"></a>一  语音合成(Text-To-Speech)TTS 概述</h2><p>TTS系统的输入是文本，输出为语音waveform。TTS一般分为两部分。第一部分将文本转换为语言规范，第二部分使用此规范来生成waveform。这种划分带来的好处是，系统前端基本是语言规范相关的，而waveform生成可以独立于语言。</p>
<p>文本转换为语言规范一般使用序列的分离处理和多种内部中间表征来完成。</p>
<p>本文主要讨论的是使用统计参数方法来合成语音。</p>
<h2 id="二-从声码到合成"><a href="#二-从声码到合成" class="headerlink" title="二  从声码到合成"></a>二  从声码到合成</h2><p>关于语音合成的描述一般是以一种程序式的眼光：通常将文本转换为语音转化为简单的pipeline结构。但是，其他方法认为语音合成是从声码器开始的，语音信号被转换为某些可以被传递的表征。声码器如下图</p>
<p><img src="/images/blog/voice_encoder1.png" alt="声码1"></p>
<p>我们可以将语音合成看做类似的架构，但是其中的参数化的语音的传递应该替换为存储。如下图:</p>
<p><img src="/images/blog/voice_encoder2.png" alt="声码1"></p>
<p> 后面再解释参数化和生成对应语音waveform。</p>
<p>此系统包含训练和合成两个阶段。训练阶段，存储的form由语音库(训练数据)获得。通过以语言规范索引这些存储的form，可以实现仅以语言规范作为输入，语音waveform为输出的合成系统。</p>
<p>存储的form可以是语音数据本身或者从数据中得到的统计模型。</p>
<h2 id="2-1-语言规范"><a href="#2-1-语言规范" class="headerlink" title="2.1 语言规范"></a>2.1 语言规范</h2><p>由上文可知，输入为语言规范。这可以很简单，比如音素序列，但是为了更好的结果，它需要包含超分段信息，比如产生语音的韵律模式。换句话说，语言规范包含了全部的影响声学模型实现的音素。</p>
<p>如何理解语言规范，我们可以以单词<code>speech</code>为例。语言规范需要涵盖可能影响这个原因声音的所有信息。即，它要包含出现此元音的全部上下文信息。此例子中，重要的情景因素包括前面的双边清音爆破（着会影响元音的共振峰轨迹）和此元音位于单音节词内（影响元音的存续时间）等等。</p>
<p>情景自然会包含相同单词相同发音内的因素，比如周围音素，单词和韵律模式，但是可能会拓展到周围发声，并进一步到协同因素如讲话者的心情或者听者的身份。对话语料中，上下文可能需要包含与其他讲话者的因素。实际上，大部分系统只考虑发声内部的因素。下表列出了在典型系统中会考虑的上下文因素:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>上下文因素</th>
</tr>
</thead>
<tbody>
<tr>
<td>Preceding and following phonemes</td>
</tr>
<tr>
<td>Position of segment in syllable</td>
</tr>
<tr>
<td>Position of syllable in word &amp; phrase</td>
</tr>
<tr>
<td>Position of word in phrase</td>
</tr>
<tr>
<td>Stress/accent/length features of current/preceding/following syllables</td>
</tr>
<tr>
<td>Distance from stressed/accented syllable</td>
</tr>
<tr>
<td>POS of current/preceding/following word</td>
</tr>
<tr>
<td>Length of current/preceding/following phrase</td>
</tr>
<tr>
<td>End tone of phrase</td>
</tr>
<tr>
<td>Length of utterance measured in syllables/words/phrases</td>
</tr>
</tbody>
</table>
</div>
<p>列出的因素对每个语音声音有潜在的影响。考虑到每个因素可能的取值数量（比如preceding phoneme可能有多达50个不同取值）以及排序的数量，很明显，即便只考虑语言学成立的组合，不同情景的数量巨大。但不是所有因素在所有时刻都有影响。实际上，我们希望少量因素在任意时刻都有显著影响。这可以显著减少情景。关键问题，我们会在第四节再看，它来决定哪个因素在何时比较重要。</p>
<p>对每个将要合成的句子，前端需要做的是从文本预测语言规范。需要任务都需要由前端完成（比如，从拼写来预测发音），这些都是与特定语言相关的。</p>
<h3 id="2-2-基于示例的模型"><a href="#2-2-基于示例的模型" class="headerlink" title="2.2 基于示例的模型"></a>2.2 基于示例的模型</h3><p>基于示例的语音合成系统简单的存储语音库，整个语料库或选择的一部分。使用语言规范来索引此类存储的form即给存储的语音数据打标签，使得其合适的部分得以被知晓，在合成阶段抽取、连接即可。<br>在典型的单元选取系统，打标签包含了对齐语音和韵律信息。恢复过程不是不重要，由于合成时所需的抽取规范在语料中不存在，所以需要在众多轻微的不匹配单元中做出选择。语音应该被存储为waveform或者其他适合拼接的表征形式，比如残差激活的LPC。</p>
<h3 id="2-3-基于模型的系统"><a href="#2-3-基于模型的系统" class="headerlink" title="2.3 基于模型的系统"></a>2.3 基于模型的系统</h3><p>基于模型的系统并不存储任何语音。相反，它在训练期间将模型适配语音库，并存储模型。模型将按照独立的语音单元构建，比如情景依赖的音素：这样模型就能被语言规范索引。在合成阶段，合适的情景依赖模型序列被检索到并用来生成语音。由于只有有限数量的训练数据，某些模型的缺失，这可能没法检索到。因而有可能对任意所需语言规范创建on-the-fly(直接使用的)模型。这可以通过在足够多的相似模型间共享参数完成。</p>
<h3 id="2-4-索引存储的form"><a href="#2-4-索引存储的form" class="headerlink" title="2.4 索引存储的form"></a>2.4 索引存储的form</h3><p>为了让存储的form，无论是语音或模型，能够被语言规范索引到，有必要为语音语料库中的每个发声产生语言规范。人工标签可以，但是不现实，也太费钱。常见的方法是，使用与合成句子语音时相同的前端，基于文本对应的语音语料库来预测语言规范。这可能与讲话者不是最佳匹配。</p>
<p>然而，一些从自动语音识别方法借鉴过来的基于强制对齐的技术，可以用来提高打标签的准确率，包括自动识别真的停顿位置和一些发音变化。</p>
<h2 id="三-语音合成的统计参数模型"><a href="#三-语音合成的统计参数模型" class="headerlink" title="三 语音合成的统计参数模型"></a>三 语音合成的统计参数模型</h2><p>我们谈及基于模型的语音合成时，尤其指从数据中学习模型时，我们通常指的是统计参数模型。模型的<code>参数化</code>是因为它使用参数来描述语音，而不是存储的模板。称为<code>统计</code>是因为使用统计项来描述这些参数(比如，概率密度函数的均值和方差)，这些统计项是从训练数据中的参数值分布习得的。</p>
<p>站在历史的角度上看，统计参数语音合成源于HMM在语音识别中的成功。没人可以说HMM就是语音的真实模型。但是其有效的学习算法(EM)，模型复杂度控制(parameter tying)的自动方法和高效计算的搜索算法(Viterbi search)使得HMM称为一个非常强力的算法。至于评估模型的性能，语音识别使用的是单词错误率，而在语音合成通过听力测试，这非常依赖于合适的配置。这个配置中两个重要的方面是语音信号的参数化(HMM术语中的模型的观察值)和建模单元的选取。由于建模单元基本是上下文依赖的音素，此选取即将哪些上下文因素考虑在内。下表概述了自动语音识别和语音合成的参数配置的差异:</p>
<p><strong>Comparison of Hidden (Semi) Markov Model configurations for recognition vs. synthesis</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>recognition</th>
<th>synthesis</th>
</tr>
</thead>
<tbody>
<tr>
<td>observations</td>
<td>spectral envelope represented using around 12 parameters</td>
<td>spectral envelope represented using 40-60 parameters, plus source features</td>
</tr>
<tr>
<td>modelling unit</td>
<td>triphone, considering preceding and following phoneme</td>
<td>full context, considering preceding two and succeeding two phonemes plus all other context features listed in Table 1</td>
</tr>
<tr>
<td>duration model</td>
<td>state self-transitions</td>
<td>explicit parametric model of state duration</td>
</tr>
<tr>
<td>parameter estimation</td>
<td>Baum-Welch</td>
<td>Baum-Welch, or Trajectory Training</td>
</tr>
<tr>
<td>decoding</td>
<td>Viterbi search</td>
<td>not usually required</td>
</tr>
<tr>
<td>generation</td>
<td>not required</td>
<td>Maximum-likelihood parameter generation</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-1-信号表征"><a href="#3-1-信号表征" class="headerlink" title="3.1 信号表征"></a>3.1 信号表征</h3><p>语音信号由在固定帧率(frame rate)的声码器参数集表征。典型的表征可能对每个帧使用40-60个参数来代表频谱包装(envelope),F0（基准频率）的值和5个描述非周期激发的频谱包装的参数。训练模型之前，声码器的编码阶段用来抽取向量，该向量包含了语音信号中的声码参数，5秒的帧率。在合成阶段，整个向量由模型生成，然后用于驱动声码器的输出。</p>
<p>从原理上讲，任何声码器都可以用于基于HMM的语音合成，只需要它能提供的参数足以高质量的重建语音信号并且这些参数可以在训练阶段自动抽取。这可能类似于一个共振峰。然而，由于参数可以被统计建模，一些声码器可以比其他的声码器表现的更好。出现在统计建模中的基本操作是平均训练阶段的声码器参数以及生成的新值（我们可以将其类比于在训练数据中获取的插值和外推法的值）。因此，在这种操作下声码器的参数值必须是表现较好并且不会导致不稳定的值。例如，线谱对可能比现行预测参数更好的表征，因为前者在插值下表现较好，而后者可能会导致不稳定的过滤。</p>
<p>一种流行的广泛应用于HMM合成的声码器是STRAIGHT (Speech Transformation and Representation using Adaptive Interpolation of weiGHTed spectrum)。我们可以说，STRAIGHT可以处理上述所需属性并且在实际应用中表现较好。</p>
<h3 id="3-2-术语"><a href="#3-2-术语" class="headerlink" title="3.2 术语"></a>3.2 术语</h3><h4 id="3-2-1-HSMMs而非HMMs"><a href="#3-2-1-HSMMs而非HMMs" class="headerlink" title="3.2.1 HSMMs而非HMMs"></a>3.2.1 HSMMs而非HMMs</h4><p>在统计参数合成语音中所使用的模型大部分其实完全不是HMMs。HMM中的持续时间模型(duration model,比如说自转换)相当简单，而且高质量的语音合成需要更好的持续时间模型。一旦加入一个明确的持续时间模型加入到HMM，它不再是一个马尔科夫模型了。模型现在是<strong>半马尔科夫</strong>—状态之间的转换依然存在，但是每个状态的明确的持续时间模型不是马尔科夫。此时模型为半隐马尔可夫模型(Hidden Semi-Markov Model)，或者说是HSMM。不过我们言及HMM语音合成时，一般实际指的是HSMM语音合成。</p>
<h4 id="3-2-2-标签和上下文"><a href="#3-2-2-标签和上下文" class="headerlink" title="3.2.2 标签和上下文"></a>3.2.2 标签和上下文</h4><p>前面描述的语言规范是一个复杂的、结构化的表征；它可能包含列表、树、和其他有用于语言学的结构。基于HMM的语音合成即从模型的线性序列中生成语音，其中每个模型对应了一个指定的语言单元类型。<br>因此，有必要将结构化的语言规范flatten到线性序列的标签。可以通过附加其他所有的语言信息(关于音节结构，韵律等)到语言规范中的音素上，其结果是线性的上下文依赖的音素序列。根据这些全上下文标签，可以挖掘对应的HMMs序列，从这里可以生成语音。</p>
<h4 id="3-2-3-Statics-deltas-and-delta-deltas"><a href="#3-2-3-Statics-deltas-and-delta-deltas" class="headerlink" title="3.2.3 Statics, deltas and delta-deltas"></a>3.2.3 Statics, deltas and delta-deltas</h4><p>声码器的输出阶段和产生语音仅需要声码器参数。然而，使用HMMs合成听起来自然的语音的关键取决于，不仅是给这些参数的统计分布建模，而且还有建模其变化频率，比如速度，声码器参数即static coefficients(静态系数)以及它们的一阶导数即delta系数。实际上，通过建模加速度(modelling acceleration)，可以获得delta-delta系数。</p>
<p>这三种类型的参数被堆叠在一个观察向量中。训练期间，模型学习这些参数的分布。合成阶段，模型生成有合适统计属性的参数的轨迹。</p>
<h3 id="3-3-训练"><a href="#3-3-训练" class="headerlink" title="3.3 训练"></a>3.3 训练</h3><p>如语音识别一样，HMMs合成必须在标签数据上训练。标签必须是如上文描述的全上下文标签，它们由2.4节所描述的方法产生。</p>
<h3 id="3-4-合成"><a href="#3-4-合成" class="headerlink" title="3.4 合成"></a>3.4 合成</h3><p>合成阶段只给文本作为输入，如下处理。</p>
<p>首先，输入文本被分析并产生全上下文标签的序列。模型的序列对应了此标签序列，然后连接成一个长的状态链。从这个模型，声码器参数使用下文算法生成。最终，生成的声码器参数被用来驱动声码器的输出阶段来产生语音waveform。</p>
<p><strong>从模型中生成参数</strong>：最大似然概率被用作从模型中生成观测值序列。首先，我们考虑使用直白的方法来做这个，然后看到这会产生不自然的参数轨迹。然后，再使用实际所使用的方法。注意到<code>参数</code>项被指为模型的输出，而不是模型的参数（高斯分布的均值和方差）。</p>
<p><strong>持续时间</strong>：在直白的方法和下文描述的 MLPG算法，其持续时间(比如，由模型的每个状态生成的参数的帧的数目)都是提前决定的，它们是简化的确定状态持续时间分布的均值。</p>
<p><strong>直白方法的参数生成</strong>：此方法生成每个状态的最可能的观测值，它只考虑统计参数。最可能的观测值当然是那个状态的高斯均值。因此这个方法生成分段的常量参数轨迹，它会突兀的改变每个状态转换处的值。显然，当用做驱动声码器时，这听起来会不自然。这个问题将由MLPG算法解决。</p>
<p><strong>MLP最大似然参数生成算法</strong>：上述方法忽略了自然语音中参数轨迹的非常重要的一个方面。它只考虑了静态(static)参数的统计属性。但是在自然语音中，它不仅是声码器参数以固定方式呈现的绝对值，它还包含了改变值的速度。我们需要将delta系数的统计属性也考虑在内。实际上，我们还可以考虑delta-delta系数的统计属性。下图演示了MLPG算法:</p>
<p><img src="/images/blog/max_likehood_speech.png" alt="最大似然状态生成"></p>
 <p align="Center">最大似然参数生成：从离散的分布序列，将delta系数和delta-delta系数统计属性考虑在内，来生成平滑的轨迹</p>

<p>HMM已经被构建：是对全上下文标签序列的所对应的模型的拼接，其本身已经被前端工具从文本中预测。在生成参数之前，使用持续时间模型选取了状态序列。这会决定模型中每个状态将会生成多少帧。上图展示了每个状态的一帧一帧的输出分布的序列。MLPG根据静态参数、delta、delta-delta分布找到最大可能的生成的参数的序列。此图只给第0个倒谱系数($c(0)$ )，但是对所有由模型生成的参数使用相同的规律，比如<code>F0</code>。</p>
<p>理解MLPG算法生成的东西的最简单的方法是，考虑一个例子：在图中找到一个$\delta c(0)$为正的区域：静态参数 $c(0)$在该点处于上升，它有正的斜率。因此，静态系数的统计属性是分段常量，最可能的参数轨迹是以一种合适的方式平滑变动的。</p>
<h2 id="四-生成新语音：未预见的上下文"><a href="#四-生成新语音：未预见的上下文" class="headerlink" title="四 生成新语音：未预见的上下文"></a>四 生成新语音：未预见的上下文</h2><p>生成语音的关键问题在于生成我们没有在自然状态下录制的语音。这就需要从更小单元（从模型拼接或生成）来构建语音。由于我们未曾预见一模一样的上下文环境的此类单元，此问题可以被描述为，从由训练集数据观察到的有限上下文集合泛化为几乎无限的未出现的上下文。</p>
<p>是否语料库够大就可以覆盖所有经常出现的上下文，不幸的是并不是这样。</p>
<p>显而易见的原因是，从表1可以知道有极其丰富的上下文，这会导致两个问题。首先，由于上下文横跨整个语音，语音语料库中每个上下文依赖的单元的出现几乎是唯一的：它只会出现一次（假设不存在重复的句子）。其二，海量的大多数可能的上下文依赖的单元将永不会在语音语料库中出现：语料库对语言只有很稀疏的覆盖。</p>
<p>即便暂时不考虑这种海量的上下文依赖，语料库中任意语言单元（比如，音素，音节，单词）分布远不能正态化。它有低频或0频率的长尾。换句话说，有很多类型的单元将会仅仅出现一次或者完全不在语料库中。此现象即大量稀有事件。尽管每种类型稀少，但是有太多的类型，这会导致还是很可能碰上。对于任意将要合成的语音，有很高的概率需要一些很稀有的单元类型（比如，上下文依赖的音素）。没有有限的语料库可以覆盖我们所需的全部的稀有类型，因此简单的增加语料库于事无补。</p>
<p>我们可以将此问题看做 从有限训练数据中泛化的问题，这就形成了一种模型复杂度控制形式的方法。</p>
<h3 id="4-1-泛化"><a href="#4-1-泛化" class="headerlink" title="4.1 泛化"></a>4.1 泛化</h3><p>常用方法，尤其是自然语言，是一个上文提到的长尾分布（类似Zipf分布）。即，数据中少量类型有较多实例，而大量类型仅有很少或者没有实例。这使得直接给稀有或者未观测到的类型建模不可能，因为实例太少无法学到任何东西。这在语音合成中必然会遇到，其中的类型是上下文中的音素。</p>
<p>给数据打标签可以减少类型的数目进而转移这个问题，在语音合成中即减少考虑的情景因素的数目。但是，我们并没有先验知识知道哪些情景因素可以被移除，哪些应该被保留，因为它们对问题中的音素的实现有很大影响。更进一步说，哪些情景因素比较重要是随着复杂的交互结合变化的。</p>
<p>一种较好的解决办法是继续使用大量的类型来给数据打标签并控制模型的复杂度，而不是控制标签的复杂度。 在常见的基于HMM的合成方法的控制模型复杂度的方法是借鉴自自动语音识别，并有关在相似模型中共享（或者tying）参数，以达到：</p>
<ol>
<li><p>合适的模型复杂度（例如，数据里合适数量的自由参数）</p>
</li>
<li><p>对那些仅有较少实例的更好的参数评估</p>
</li>
</ol>
<p>3.对于完全没有的实例的参数评估方法。</p>
<p>为了决定哪些模型足够相似（可用共享参数），再次考虑这些情景因素。由于（我们也这么期望）在任意时刻都只需要考虑少量因素，我们可以专注于一个情景依赖模型的集合，其中每个模型，只需要考虑相关情景。情景依赖的数量可能不同的模型也不一样。结果便是，只有被训练数据所支撑的上下文差异可以被建模。没有影响的上下文因素被丢弃，根据模型的偏差。一个简单示例，想象前音素的identity对于实现[S ]没有显著影响，但是接下来的音素的identity对其有影响。这种情况，模型组可以按照下述方式共享相同参数：对于所有上下文[…aft..],[…Ift…],[…eft..]来说是一个模型，对其他所有上下文如[..afe…],[…ife…],[…efe…]…来说是另外一个模型。</p>
<p>决定不同上下文之间哪些模型可以共享参数的机制是由数据驱动的。模型的复杂度（或者说，有多么多或多么少的参数绑定）是自动选择以适应可用的训练数据的数量的：越多的数据模型越复杂。</p>
<h3 id="4-2-使用参数绑定来控制模型复杂度"><a href="#4-2-使用参数绑定来控制模型复杂度" class="headerlink" title="4.2 使用参数绑定来控制模型复杂度"></a>4.2 使用参数绑定来控制模型复杂度</h3><p>模型复杂度控制即给模型选取合适数量的自由参数。在基于HMM的语音合成中，这意味着选取哪种情景分布值得去选取而哪些不重要。换句话说，对于两个不同的 情景，我们何时应该使用独立的模型，合适使用相同的模型。</p>
<p>一种在自动语音识别中广泛应用的模型复杂度控制的技术牵连到相似模型的聚类。情景因素中指定了哪种模型可以聚为一类，并且实际被选取的聚类是那种可以最好的将训练数据和模型拟合的。此方法被基于HMM的语音合成方法采用，这其实在语音识别中更重要，仅仅因为有更多的大量的不同情景需要应对。有一种聚类用的决策树方法(Martin 2009)。</p>
<p>模型被聚类之后，不同模型的数量远远小于不同情景的数量。对于指定数据，聚类过程会自动发现最优的情景差异。训练数据集越大，我们可以使用更多的模型并做出更多精细的差异。</p>
<p>注意：实际上状态绑定和参数绑定是独立的，但是规律是一样的。</p>
<h3 id="4-3-单元选取的关系"><a href="#4-3-单元选取的关系" class="headerlink" title="4.3 单元选取的关系"></a>4.3 单元选取的关系</h3><p>在单元选取合成中，情景因素在单元选取上的影响是由目标代价来衡量的。目标损失函数的最常见形式是简单的对每个不匹配的情景因素惩罚项加权求和。目标损失函数旨在在数据库中识别最不差的单元候选。一种可选形式的目标损失称为<code>clunits</code>，使用类似于上文描述的模型聚类方法的情景聚类树，但是树中的叶子节点代表的不是模型参数而是从数据库中获得的语音单元聚类。</p>
<p>目标是一致的：从数据中已知的来泛化出未知的。这是通过自动发现哪些情景在效力上是可互换的达到的。在单元选取中即找到一组足够相似的候选单元来用在不在语音语料库中的目标情景中；在语音合成中意味着将一组情景均值化来训练单一模型。</p>
<h2 id="五常见问题"><a href="#五常见问题" class="headerlink" title="五常见问题"></a>五常见问题</h2><p><strong>ASK1</strong>: 如何预测韵律</p>
<p><strong>ANS2</strong>:这里分两部分。首先，韵律的符号表征是由前端预测的，与拼接合成中类似 。其二，此符号表征用作在生成语音的全情景模型的情景因素的一部分。假设<strong>(a)</strong>每个韵律有足够的训练样本<strong>(b)</strong>训练数据的真实韵律和韵律标签有一些一致性，然后每个不同的韵律情景有不同的模型并且在语音合成时模型会 生成合适的韵律。如果(a)或(b)有一个不满足，那么参数聚类将无法形成指定韵律情景模型。</p>
<p><strong>ASK2</strong>什么导致了语音合成中的“嗡嗡”的问题</p>
<p><strong>ANS2</strong>：因为语音时声码。“嗡嗡”主要源于声源的过度简化的模型。使用混合激发（韵律和非周期性源的混合）的声码而不是在二者之间切换，可以减少“嗡嗡”。</p>
<p><strong>ASK3</strong> 什么导致了语音合成中的闷声</p>
<p><strong>ANS3</strong>：均值化，这是统计模型的训练过程中不可避免的步骤，可能导致语音听起来闷闷的。多帧语音的均值，每个帧都有轻微不同的频谱属性，这会有拓宽共振峰带宽并减少频谱包装的动态范围的影响。类似的，均值化可能导致过度平滑的频谱包装(envelopes)和过度平滑的轨迹。一种常用的以抵消这种影响的方法是，调整生成的参数使得它们有在自然语音中相同的偏差。此方法称为Global Variance（GV，全局偏差）。</p>
<p><strong>ASK4</strong>为什么持续时间要分开建模</p>
<p><strong>ANS4</strong>：标准HMM中的持续时间模型源自每个状态的自转移。此模型下，大部分可能的持续时间总是一个状态一帧，在自然状态下显然不对。因而，明确的持续时间模型就很有必要。持续时间模型并不是真的从频谱包络(envelop)和源分离的。它们在模型结构上交互。然而，影响持续时间的情景因素在不同的频谱和源特征下不同，所以这些各种各样的模型参数组是分开聚类的。</p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>经典卷积神经网络总结</title>
    <url>/2017/04/18/classic_cnn/</url>
    <content><![CDATA[<h2 id="一-AlexNet"><a href="#一-AlexNet" class="headerlink" title="一  AlexNet"></a>一  AlexNet</h2><p>2012年由Hinton的学生Alex Krizhevsky提出。以Top-5的错误率为16.4%赢得ILSVRC 2012年的比赛。它做出了如下创新：</p>
<ul>
<li><p>首次使用ReLU作为CNN激活函数，解决了Sigmod激活函数的梯度弥散问题。</p>
</li>
<li><p>使用Dropout随机丢弃部分神经元，可以避免模型的过拟合。AlexNet的最后几个全连接层使用了Dropout</p>
</li>
<li><p>使用重叠的最大池化，之前使用的都是平均池化。最大池化可以避免平均池化的模糊效果。同时，步长比卷积核的尺寸小，这样池化层的输出之间会有重叠，提升了特征的丰富性。</p>
</li>
<li><p>提出了LRN层（局部相应一体化），对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对较大，并抑制其他反馈较小的神经元，增强模型泛化能力。</p>
</li>
<li><p>使用CUDA加速深度卷积网络的训练，使用了GPU的并行计算能力。</p>
</li>
<li><p>数据增强，随机从256x256的原始图中截取224x224大小的区域，再做水平翻转，相当于增加了 $(256-224)^2\times 2=2048$ 倍的数据量。仅靠原始的数据量，参数众多的CNN会陷入过拟合。预测时，取图片四个角和中间共5个位置，再加上翻转，共10个位置，对它们的预测结果求均值。</p>
</li>
</ul>
<p><strong>网络结构</strong></p>
<p><img src="/images/blog/AlexNet_struct.jpg" alt="alexnet结构"></p>
<p><strong>参数</strong></p>
<p><img src="/images/blog/alexnet-params.jpg" alt="alexnet参数"></p>
<h2 id="二-VGGNet"><a href="#二-VGGNet" class="headerlink" title="二  VGGNet"></a>二  VGGNet</h2><p>VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind一起研发的深度卷积神经网络。通过反复堆叠3x3的小型卷积核和2x2的最大池化层，VGG成功的构筑了16-19层卷积神经网络。获得 ILSVRC 2014分类项目第二名和定位项目第一名。整个网络使用了同样大小的卷积核尺寸(3x3)和最大池化尺寸(2x2)。</p>
<p><strong>网络结构和参数</strong></p>
<p><img src="/images/blog/VGGNet.png" alt="VGGNet参数"></p>
<p>虽然从A到E每一级网络逐渐变深，但是网络参数数量没有太多增长，因为参数数量主要消耗在最后的三个全连接。前面卷积虽然很深，但是消耗的参数量不大，不过训练时比较耗时的还是卷积部分。上图中的D和E就是VGGNet-16和VGGNet-19.。</p>
<p><strong>技巧</strong><br> 网络中经常出现的多个完全一样的3x3的卷积串联相当于1个5x5的卷积层，即一个像素跟周围5x5像素产生关联，可以输感受野大小为5x5.而3个3x3卷积串联效果等同于1个7x7的卷积层。同时3个串联的3x3的卷积拥有比1个7x7的卷积更少的参数量，只有后者的3x3x3/7x7=55%。同时3个3x3的卷积层拥有比一个7x7的卷积层更多的非线性变换（前者使用了3次ReLU激活，而后者只使用了一次），使得CNN对特征的学习能力更强。</p>
<p><img src="/images/blog/vgg_3x3.jpg" alt="3x3的卷积比"></p>
<p><strong>训练技巧</strong>，先训练级别A的简单网络再复用A网络的权重来初始化后面的几个复杂网络，这样收敛速度更快。<br><strong>预测时</strong>：VGG采用Multi-scale方法，将图像scale到一个尺寸Q，并将图片输入卷积网络计算，再将不同尺寸的Q的结果平均得到最后结果，这样可以提高图片数的利用率并提升预测准确率。</p>
<p><strong>结论</strong></p>
<ul>
<li><p>LRN层作用不大</p>
</li>
<li><p>越深的网络效果越好</p>
</li>
<li><p>1x1的卷积也是有效的，但是没有3x3的卷积好，大的卷积核可以学习更大的空间特征。</p>
</li>
</ul>
<h2 id="三-Google-Inception-Net"><a href="#三-Google-Inception-Net" class="headerlink" title="三 Google Inception Net"></a>三 Google Inception Net</h2><p>2014年ILSVRC 冠军，最大的特点是控制住计算量和参数量的同时，获得了很好的分类性能，top-5错误率 6.7%。Inception v1有22层，比AlexNet的8层或VGGNet的19层要深，但是 计算量只有15亿次，500万参数量，仅为AlexNet的1/12(6000万)。</p>
<p><strong>特点</strong></p>
<ul>
<li><p>去除了最后的全连接层，用全局平均池化层（将图片尺寸变为1x1）来取代它。全连接层占据了AlexNet或VGGNet的90%的参数量，而且会引起过拟合。用全局平均池化层取代全连接层的做法借鉴了Network in Network。</p>
</li>
<li><p>其精心设计的Inception Module 提高了参数的利用率。一般来说，卷积层要提升表达能力，主要依靠增加输出通道数，但是副作用是计算量增大和过拟合。每个输出通道对应一个滤波器，同一个滤波器共享参数，只能提取一类特征，因此一个输出通道只能做一种特征处理。而NIN中的MLPConv则拥有更强大的能力，允许在输出通道之间组合信息。MLPConv基本等效于普通卷积层后再连接1x1的卷积和ReLU激活函数。</p>
</li>
</ul>
<p>Inception Module的基本结构如下：</p>
<p><img src="/images/blog/inception_module.jpg" alt="inception module结构"></p>
<p> 其中的1x1卷积可以以很小的计算量就增加一层特征变换和非线性化，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维。</p>
<p>人脑神经元是稀疏激活的，模拟的神经网络也是类似。应该把相关性高的一簇神经元节点连接在一起。图片数据中，临近区域的数据相关性高，因此相邻像素点被卷积操作连接在一起。因此，一个1x1的卷积就可以自然地把这些相关性很高的、在同一空间位置但是不同通道的特征连接在一起，这就是为什么1x1的卷积反复被应用在Inception Net中的原因。</p>
<p>Inception Net是一个大家族，包括了以下系列。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络</th>
<th>年代</th>
<th>错误率</th>
<th>创新</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inception V1</td>
<td>2014年9月</td>
<td>Top-5 6.67%</td>
<td>使用了Network In Network的思想</td>
</tr>
<tr>
<td>Inception V2</td>
<td>2015年2月</td>
<td>Top-5 4.8%</td>
<td><strong>(1)</strong>使用了两个3x3来替代5x5的大卷积。<strong>(2)</strong>提出了Batch Normalization，正则化方法，加速大型卷积神经网络的训练速度，同时提升收敛后的分类准确率。</td>
</tr>
<tr>
<td>Inception V3</td>
<td>2015年12月</td>
<td>Top-5 3.5%</td>
<td><strong>(1)</strong>引入Factorization into small convolutions，将较大卷积拆分为两个小卷积，比如7x7拆成1x7和7x1卷积，节省了大量参数，加速运算同时减轻过拟合，同时增加一层非线性拓展模型表达能力<strong>(2)</strong>在Inception V3中使用了分之，还在分支之中使用了分支</td>
</tr>
<tr>
<td>Inception V4</td>
<td>2016年2月</td>
<td>Top-5 3.08%</td>
<td>结合了微软的ResNet</td>
</tr>
</tbody>
</table>
</div>
<h2 id="四-ResNet"><a href="#四-ResNet" class="headerlink" title="四  ResNet"></a>四  ResNet</h2><p>ResNet(Residual Neural Network)由微软研究院Kaiming He等四位华人提出，通过使用Residual Unit成功训练152层深的神经网络，在ILSVRC 2015比赛中获得冠军，获得3.57%的Top-5准确率，同时参数量比VGGNet低。</p>
<p>ResNet源于<strong>Highway Network</strong>，通常认为神经网络的深度对齐性能非常重要，但是网络越深其训练难度越大，Highway Network的目标就是解决极深网络的难以训练的问题。</p>
<p>Highway Network相当于修改了每一层的激活函数，此前的激活函数只是对输入做一个非线性变换 $y=H(x,W_H)$，Highway Network则允许保留一定比例的原始输入 $x$ ,即 $y=H(x,W_H)\dot T(x,W_T)+x\dot C(x,W_C)$,其中T为变换系数，C为保留系数。论文中令$C=1-T$。这样前面一层的信息，有一定比率可以不经过矩阵乘法和非线性变换，直接传输到下一层，仿佛一条高速公路。</p>
<p>假定某段神经网络的输入是x,期望输出是$H(x)$，如果直接把输入x传都输出作为初始结果，那么此时我们需要学习的目标就是$F(x)=H(x)-x$。如下图所示，这就是一个ResNet的残差学习单元(Residual Unit)，ResNet相当于将学习目标改变了，不再是学习一个完整的输出$H(x)$，只是输出和输入的差别$H(x)-x$即残差。</p>
<p><img src="/images/blog/Residual_Unit.png" alt="残差单元"></p>
<p>传统卷积层或全连接层在信息传递时，或多或少会存在信息丢失、损耗等问题。ResNet在某种程度上解决了这个问题，通过直接将输入信息绕道到输出，保护信息的完整性，整个网络则只需要学习输入、输出差别的那一部分，简化学习目标和难度。</p>
<p>下图是两层或三层的ResNet残差学习模块。</p>
<p><img src="/images/blog/resnet_block.jpg" alt="残差模块"></p>
<p>下图是ResNet不同层数时的网络配置</p>
<p><img src="/images/blog/resnet_architecture.png" alt="网络架构"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>使用merlin从头构建你的声音</title>
    <url>/2017/04/16/merlin-tts/</url>
    <content><![CDATA[<p>本文参考自: <a href="http://www.speech.zone/exercises/build-a-unit-selection-voice/label-the-speech/" target="_blank" rel="noopener">http://www.speech.zone/exercises/build-a-unit-selection-voice/label-the-speech/</a><br>而该文章又参考自爱丁堡大学的一篇论文的思路，<a href="http://www.cstr.ed.ac.uk/downloads/publications/2004/clarkrichmondking_ssw504.pdf" target="_blank" rel="noopener">论文</a>,<a href="http://www.cstr.ed.ac.uk/downloads/festival/multisyn_build/" target="_blank" rel="noopener">论文实现工程文件</a> 。下面的教程中直接使用了很多此工程中的文件，如果在教程中没找到对应的文件夹，可能需要下载此工程。</p>
<h2 id="一-需要的工具"><a href="#一-需要的工具" class="headerlink" title="一  需要的工具"></a>一  需要的工具</h2><ul>
<li><p>python:2.7</p>
</li>
<li><p>Festival:一个语音合成工具 <a href="http://www.cstr.ed.ac.uk/projects/festival/" target="_blank" rel="noopener">Festival</a> </p>
</li>
<li><p>Edinburgh Speech Tools: 一些地方也称为Festival，下载之后为speech_tools/ 文件夹</p>
</li>
<li><p>其他依赖：详见Festival或Edinburgh Speech tools的说明文档。以及<code>lib32ncurses5-dev</code>和<code>libX11-dev</code>(linux安装)</p>
</li>
<li><p>HTK :语音识别工具，如果安装是3.4，需要修复这个 <a href="https://github.com/JoFrhwld/FAVE/wiki/HTK-3.4.1" target="_blank" rel="noopener">bug</a></p>
</li>
</ul>
<h2 id="二-介绍"><a href="#二-介绍" class="headerlink" title="二  介绍"></a>二  介绍</h2><p>本文主要关注于合成流程中的波形生成器阶段，尽管可以在前端工具(festival和HTK)做对应的修改，即在发音词典中加入新的词。<br><strong>不要在windows上玩这个</strong></p>
<h3 id="2-1-本文主要流程"><a href="#2-1-本文主要流程" class="headerlink" title="2.1 本文主要流程:"></a>2.1 本文主要流程:</h3><ol>
<li>选取recording脚本</li>
<li>在studio中做recording</li>
<li>准备好workspace</li>
<li>将录音转换为要求的格式，并仔细检查</li>
<li>label 语音文件</li>
<li>Pitchmark 语音<br>7.创建声音<br>8.评估声音文件</li>
</ol>
<h3 id="2-2-相关文章"><a href="#2-2-相关文章" class="headerlink" title="2.2 相关文章"></a>2.2 相关文章</h3><p>文字转语音的流程架构,<a href="http://www.speech.zone/pipeline-architecture-for-text-to-speech/" target="_blank" rel="noopener">TTS流程</a></p>
<p><a href="http://www.speech.zone/forums/forum/speech-synthesis/" target="_blank" rel="noopener">speech论坛</a></p>
<h2 id="三-准备好workspace"><a href="#三-准备好workspace" class="headerlink" title="三 准备好workspace"></a>三 准备好workspace</h2><p>假设所有的文件将放在<code>/workspace/merlin</code>文件夹下，下载并解压此文件<a href="http://www.speech.zone/wp-content/uploads/2015/12/ss.zip" target="_blank" rel="noopener">SS</a>,文件组织结构如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xiatao@sjkxbgpu:~&#x2F;workspace&#x2F;merlin$ cd ss</span><br><span class="line">xiatao@sjkxbgpu:~&#x2F;workspace&#x2F;merlin&#x2F;ss$ tree .&#x2F;</span><br><span class="line">.&#x2F;</span><br><span class="line">├── pm</span><br><span class="line">├── recordings</span><br><span class="line">├── setup.sh</span><br><span class="line">├── utts.data</span><br><span class="line">├── utts.pauses</span><br><span class="line">└── wav</span><br><span class="line"></span><br><span class="line">3 directories, 3 files</span><br></pre></td></tr></table></figure>
<p>如果所需工具都已经正确安装，现在需要编辑<code>setup.sh</code>文件中的两个变量<code>SSROOTDIR</code>和<code>FROOTDIR</code>，分别指向安装工具目录和festival安装目录。原文件的两个变量值为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SSROOTDIR&#x3D;&#x2F;Volumes&#x2F;Network&#x2F;courses&#x2F;ss&#x2F;</span><br><span class="line">FROOTDIR&#x3D;&#x2F;Volumes&#x2F;Network&#x2F;courses&#x2F;ss&#x2F;festival&#x2F;festival_mac</span><br></pre></td></tr></table></figure>
<p>修改为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SSROOTDIR&#x3D;&#x2F;home&#x2F;xiatao&#x2F;my_soft_install&#x2F;merlin&#x2F;tools</span><br><span class="line">FROOTDIR&#x3D;&#x2F;home&#x2F;xiatao&#x2F;&#x2F;my_soft_install&#x2F;merlin&#x2F;tools</span><br></pre></td></tr></table></figure>
<p>我对应的目录组织结构如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xiatao@sjkxbgpu:~&#x2F;my_soft_install&#x2F;merlin&#x2F;tools&#x2F;festival$ ls</span><br><span class="line">ACKNOWLEDGMENTS  bin  config  config.cache  config.guess  config.log  config.status  config.sub  configure  configure.in  COPYING  doc  examples  INSTALL  install-sh  lib  Makefile  make.include  missing  mkinstalldirs  NEWS  README  src  testsuite</span><br><span class="line">xiatao@sjkxbgpu:~&#x2F;my_soft_install&#x2F;merlin&#x2F;tools&#x2F;festival$ cd ..</span><br><span class="line">xiatao@sjkxbgpu:~&#x2F;my_soft_install&#x2F;merlin&#x2F;tools$ ls</span><br><span class="line">bin  compile_tools.sh  festival  festvox  INSTALL  install_package_and_shell  readme  speech_tools  SPTK-3.9  WORLD  WORLD_v2</span><br></pre></td></tr></table></figure>
<p>然后执行:<code>source setup.sh</code>。此命令不会有任何输出，但是相关变量会被配置。本文流程走完之后，会有如下目录以及其对应的内容:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Folder</th>
<th>Contains</th>
</tr>
</thead>
<tbody>
<tr>
<td>recordings</td>
<td>speech recordings, copied from the studio</td>
</tr>
<tr>
<td>wav</td>
<td>individual wav files for each utterance</td>
</tr>
<tr>
<td>pm</td>
<td>pitch marks</td>
</tr>
<tr>
<td>mfcc</td>
<td>MFCCs for use in automatic alignment</td>
</tr>
<tr>
<td>lab</td>
<td>label files from automatic alignment</td>
</tr>
<tr>
<td>utt</td>
<td>Festival utterance structures</td>
</tr>
<tr>
<td>f0</td>
<td>Pitch contours</td>
</tr>
<tr>
<td>coef</td>
<td>MFCCs + f0, for the join cost</td>
</tr>
<tr>
<td>coef2</td>
<td>coef2, but stripped of unnecessary frames to save space, for the join cost</td>
</tr>
<tr>
<td>lpc</td>
<td>LPCs and residuals, for waveform generation</td>
</tr>
</tbody>
</table>
</div>
<h2 id="四-recording脚本"><a href="#四-recording脚本" class="headerlink" title="四 recording脚本"></a>四 recording脚本</h2><p>鉴于单元(音素)选取极端依赖于数据库内容，我们需要仔细考虑应该做哪些recording。</p>
<p>我们需要选取一个recording脚本。标准方法是一句句的贪心选取句子，从一个大的文本语料库（比如，小说或报纸）来尽可能覆盖最多的语音(以及可能的韵律)。本文不会直接走这一步骤，而是直接使用已经存在的<a href="http://festvox.org/cmu_arctic/" target="_blank" rel="noopener">CMU ARCTIC</a></p>
<p>如果记录全部的内容（CMU语音所读的文本），大概会得到一个小时的语音。但是建议先从记录(读)A集合的593个prompts 开始，并创建对应的语音</p>
<h3 id="4-1-关于CMU-ARCTIC数据库"><a href="#4-1-关于CMU-ARCTIC数据库" class="headerlink" title="4.1 关于CMU ARCTIC数据库"></a>4.1 关于CMU ARCTIC数据库</h3><p>数据库包含了从静心挑选的文本里录的1150个语音，包含了US English男性(bdl)和女性(slt)播音员。</p>
<p><code>cmuarctic.data</code>包含了1132个句子-语音准备清单，其内容示例如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">( arctic_a0001 &quot;Author of the danger trail, Philip Steels, etc.&quot; )</span><br><span class="line">( arctic_a0002 &quot;Not at this particular case, Tom, apologized Whittemore.&quot; )</span><br><span class="line">( arctic_a0003 &quot;For the twentieth time that evening the two men shook hands.&quot; )</span><br><span class="line">( arctic_a0004 &quot;Lord, but I&#39;m glad to see you again, Phil.&quot; )</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>它既包含了16khz的波形(waveform)语音，同时也包含了EGG信号。全部的labeling由基于<code>Festvox</code>(festival依赖的一个工具)的labeling脚本完成。数据库中已经包含了完整的可运行的Festival Voices.</p>
<p><strong>CMU ARCTIC Database</strong></p>
<ul>
<li>US Enlish bdl(male)(0.95)</li>
<li>US English slt(female)(0.95)</li>
<li>US English clb(female)(0.95)</li>
<li>US English rms(male)(0.95)</li>
</ul>
<h3 id="4-2-使用说明"><a href="#4-2-使用说明" class="headerlink" title="4.2 使用说明"></a>4.2 使用说明</h3><p>建议下载上面的<code>ARCTIC</code>数据库中的<code>slt</code>，它们是完整的Festival Voices，所以需要删除其他的而只保留波形文件（wav）和<code>utts.data</code>。将这些文件(wav和utts.data)复制到workspace目录下。</p>
<h4 id="4-2-1-utts-data文件"><a href="#4-2-1-utts-data文件" class="headerlink" title="4.2.1 utts.data文件"></a>4.2.1 utts.data文件</h4><p>utts.data文件是Festival用来定义unit(音素)选取的数据库。它的内容示例如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">( arctic_a0001 &quot;Author of the danger trail, Philip Steels, etc.&quot; )</span><br><span class="line">( arctic_a0002 &quot;Not at this particular case, Tom, apologized Whittemore.&quot; )</span><br><span class="line">( arctic_a0003 &quot;For the twentieth time that evening the two men shook hands.&quot; )</span><br></pre></td></tr></table></figure>
<p>其中的每一行代表 了数据库中的一个utterance(语音)。格式如下:</p>
<p>1.一个开的插入语,<code>(</code><br>2.一个唯一的识别码，被用作任何与此utterance（音素）相关的文件的文件名。ARTCTIC的文件名形式为 <code>arctic_lnnnn</code>，我们自己准备的材料可以以<code>yourname_nnnn</code>这种形式，其中<code>nnnn</code>以0001开始。</p>
<ol>
<li>文本，双引号内部<br>4.关闭的插入语<code>)</code></li>
</ol>
<h4 id="4-2-2-添加自己的素材"><a href="#4-2-2-添加自己的素材" class="headerlink" title="4.2.2 添加自己的素材"></a>4.2.2 添加自己的素材</h4><p>由于ARCTIC脚本给的是通用的双音，合成全部类型的句子并不完美。你可以通过添加特定领域的自己的声音来提高它的自然性，添加更多的素材到数据库。这一步并不是一定要做的。</p>
<p>建议添加一些prompts 以包含如下：</p>
<ol>
<li>在不同上下文环境里包含了自己名字的5个句子（比如，句子的开始和结尾处）<br>2.10个短的高频使用的短语集，比如”Hello””Hi””How are you”等等。</li>
<li>大概50个可以覆盖某个很小领域的句子</li>
</ol>
<p>选择一个可以获取词汇表中所有单词的有限领域，其中每一项（单词）多次发音（不同的上下文和位置），并且在50句话内覆盖。根据这些，你可以合成更广泛的新句子。示例:</p>
<ul>
<li><p>时间和日期</p>
</li>
<li><p>街道地址，街道名很少。</p>
</li>
</ul>
<p>设计完自己的领域素材之后，准备一份新的与utts.data相同的文件，包含了自己的句子。此文件将用于SpeechRecorder tool来记录这些句子。不必包含ARCTIC 脚本：它已经被包含入SpeechRecorder。除此之外，需要在utts.data的末尾添加自己的新句子。</p>
<p><strong>注意：使用一个文本编辑器来创建这些文件，避免出现非ASCII字符</strong></p>
<h2 id="4-2-3-自动文本选取"><a href="#4-2-3-自动文本选取" class="headerlink" title="4.2.3 自动文本选取"></a>4.2.3 自动文本选取</h2><p>这是一个可选的步骤，用以实现自己的贪心文本选取算法</p>
<p>与从限定领域手工添加小集合的句子相比，我们可以实现一个简单的文本选取算法来选取其他素材。</p>
<p>此步骤可以让我们选取与ARCTIC的A集合同等大小（记录的语音）的额外数据集来形成自己的素材。</p>
<p>切记：仍然需要在studio中记录这些素材。</p>
<p>你可以：</p>
<ul>
<li><p>抽取与ARCTIC文本选取算法完全一样的副本，或者自己实现</p>
</li>
<li><p>可以使用更新的文本数据源，而不是ARCTIC里的古老小说</p>
</li>
<li><p>如果你选取特定领域的源文本库，就可以创建一个较好的特定领域声音。其中的难点在于找到足够大的文本（比如，爬虫）</p>
</li>
</ul>
<h2 id="5-make-the-recording"><a href="#5-make-the-recording" class="headerlink" title="5  make the recording"></a>5  make the recording</h2><p>即在录音棚中读文本，并录音。</p>
<h2 id="6-准备录音"><a href="#6-准备录音" class="headerlink" title="6 准备录音"></a>6 准备录音</h2><p>16khz的波形文件即可。</p>
<p>SpeechRecorder tool在 文件名前加 了前缀。需要移除，使得文件名完全匹配utts.data中的utterance 识别码（前缀无非是”_1”,”_2”等）。不要重复记录发音utterance </p>
<p><strong>下采样</strong><br>下面的是下采样单个文件的示例，保存为所需的RIFF格式(Linux为wav格式)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ ch_wave -otype riff -F 16000 -o wav&#x2F;arctic_a0001.wav recordings&#x2F;arctic_a0001.wav</span><br></pre></td></tr></table></figure>
<p>你需要些个shell脚本来处理所有的在<code>recording</code>目录的文件。如果你的record数据是24bit而非16bit，则需要使用<code>sox</code>来改变bit深度，并同时下采样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ sox recordings&#x2F;arctic_a0001.wav -b16 -r 16k wav&#x2F;arctic_a0001.wav</span><br></pre></td></tr></table></figure>
<h2 id="7-标记语音"><a href="#7-标记语音" class="headerlink" title="7 标记语音"></a>7 标记语音</h2><p>使用text-to-speech（TTS） 系统的前端(front-end)工具从文本中来获取标签，接下来需要将它们 与已经记录的语音对齐，此处使用了自动语音识别处搬来一个技术。</p>
<p>在继续之前，需要确保已经完成如下步骤：</p>
<ol>
<li>至少完成了recording ARCTIC中的”A”集合</li>
<li>utts.data文件。</li>
<li>包含了wav文件的目录，wav文件都在utts.data中有对应。</li>
<li>确保文件名和数字是正确的。</li>
</ol>
<p>下一步是为语音创建时间对齐语音标签，使用强制对齐和HTK语音识别工具。首先得为HTK设置一个目录结构(安装HTK时会有个HTKDemo的目录，按照其目录创建对应的目录)，运行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ setup_alignment</span><br></pre></td></tr></table></figure>
<p>这一步骤会创建一个<code>alignment</code>目录包含了HTK相关的目录。脚本会告诉你需要创建其他文件，这在下一步。</p>
<h3 id="7-1-选择词典和语音集"><a href="#7-1-选择词典和语音集" class="headerlink" title="7.1 选择词典和语音集"></a>7.1 选择词典和语音集</h3><p>选一种口音的英语。此处的选择将会决定对齐的余下部分所使用的词典和语音集。此处我们假设使用的是British English词典<strong>unilex-rpx</strong>，如果使用的是不同的词典，你需要在所有命令中替换”unilex-rpx”为下面的其他选项:</p>
<ul>
<li><p>unilex-gam – General American English</p>
</li>
<li><p>unilex-rpx – British English (RP)</p>
</li>
<li><p>unilex-edi – Scottish English (Edinburgh)</p>
</li>
</ul>
<p><strong>定义语音集</strong></p>
<p>将定义了语音集的文件复制到对齐(alignment)目录。注意变量<code>$MBDIR</code>来自<code>setup.sh</code>文件内定义，执行完<code>source setup.sh</code>即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ cp $MBDIR&#x2F;resources&#x2F;phone_list.unilex-rpx alignment&#x2F;phone_list</span><br><span class="line">bash$ cp $MBDIR&#x2F;resources&#x2F;phone_substitutions.unilex-rpx alignment&#x2F;phone_substitutions</span><br></pre></td></tr></table></figure>
<p>phone_list包含的是语音集的列表。包含了一些特殊的语音，这在自动语音识别中很常见。如果X是一个stop或者破擦音，那么X_cl被加进来来标记局部闭合。标签<code>sp</code>（short pause）加进来作为词中间的静音，<code>sil</code>代表更长时间的静音（每个utterance(语音)的开始和结束）。</p>
<p>phone_substitutions文件包含了aligner允许的可能的替换列表。这些被限定为元音reduction（减少），比如规则<code>aa@</code>代表”aa”可以被标记为<code>@</code>(schwa),如果基于声学模型它是一个更可能的标签。</p>
<p><strong>处理不在词典中的单词</strong></p>
<p>鉴于强制对齐会从语音中产生语音标签以及它们的单词抄录(transcriptions),它需要知道每个单词的抄录。在语音合成中可以在所有未知单词中使用letter-to-sound规则，但是对于标记语音数据不精确。切记在语音记录数据库的任何错误会对语音合成产生直接的影响。</p>
<p><strong>对着词典检查脚本</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ festival $MBDIR&#x2F;scm&#x2F;build_unitsel.scm</span><br><span class="line">festival&gt;(check_script &quot;utts.data&quot; &#39;unilex-rpx)</span><br></pre></td></tr></table></figure>
<p>festival会告诉你哪些不在词典里的单词，以及按照letter-to-sound的规则它该如何发音。找到不在词典中的单词之后，创建一个文件my_lexicon.scm，格式如下(注意第一行中的词典名称，不同的词典名称不一样):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(setup_phoneset_and_lexicon &#39;unilex-rpx)</span><br><span class="line"> </span><br><span class="line">(lex.add.entry &#39;(&quot;pimpleknuckle&quot; nn (((p i m) 1) ((p l!) 0) ((n uh) 1) ((k l!) 0))))</span><br><span class="line">(lex.add.entry &#39;(&quot;womble&quot; nn (((w aa m) 1) ((b l!) 0))))</span><br></pre></td></tr></table></figure>
<p>为获得正确的单词发音，启动festival,并执行check_scipt脚本命令（参考上面），以确保正确的词典被载入。然后使用命令 <code>lex.lookup</code>找到近似发音单词来构建你的语音。如果有很强的非本地口音，别尝试匹配你所使用的真实声音，而尝试写与其他类似单词的发音一致的发音。如果此阶段没有添加任何发音，创建一个空白文档即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ touch my_lexicon.scm</span><br></pre></td></tr></table></figure>
<h3 id="7-2-时间对齐标签"><a href="#7-2-时间对齐标签" class="headerlink" title="7.2 时间对齐标签"></a>7.2 时间对齐标签</h3><p>数据库需要时间对齐标签。保持标签与前端在运行时做出的预测的一致性很重要， 因而我们需要使用相同的前端来创建初始标签序列，然后使用强制对齐在这些标签中加入时间戳。</p>
<p>初始的用于强制对齐的语音序列来自Festival,通过运行前端脚本。如果使用不同的词典，注意修改<code>unilex-rpx</code>。</p>
<p><strong>创建初始标签</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ festival $MBDIR&#x2F;scm&#x2F;build_unitsel.scm .&#x2F;my_lexicon.scm</span><br><span class="line">festival&gt;(make_initial_phone_labs &quot;utts.data&quot; &quot;utts.mlf&quot; &#39;unilex-rpx)</span><br></pre></td></tr></table></figure>
<p>输出文件utts.mlf，是一个包含了的utterances(语音)的语音转录(transcription)的HTK master label file(MLF)；其标签暂未与波形(waveform)时间对齐。</p>
<p>如果想要设计自己的 脚本，以上命令是最简单的方式来将文本转换为语音序列，这样就可以衡量覆盖率。</p>
<p>强制对齐涉及到训练 HMMs，正如自动语音识别。因此，语音需要参数化。我们所使用的特征是MFCCs。</p>
<p><strong>抽取MFCC</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ make_mfccs alignment wav&#x2F;*.wav</span><br></pre></td></tr></table></figure>
<p><strong>对齐</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ cd alignment</span><br><span class="line">bash$ make_mfcc_list ..&#x2F;mfcc ..&#x2F;utts.data train.scp</span><br><span class="line">bash$ do_alignment .</span><br></pre></td></tr></table></figure>
<p>(注意最后一个命令的空格和点号)</p>
<p>do_aligner命令将会持续超过20分钟，取决于机器配置和记录的语音。对齐结束后，需要产生的MLF文件切分为Festival能使用的独立的标签文件，此时它已经为标签包含了正确的时间对齐。</p>
<p><strong>切分MLF文件</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ cd ..</span><br><span class="line">bash$ mkdir lab</span><br><span class="line">bash$ break_mlf alignment&#x2F;aligned.3.mlf lab</span><br></pre></td></tr></table></figure>
<h3 id="7-2-1-修改对齐脚本"><a href="#7-2-1-修改对齐脚本" class="headerlink" title="7.2.1 修改对齐脚本"></a>7.2.1 修改对齐脚本</h3><p>此步骤不是必须，最好是觉得合成结果比较差时回头参考此步骤来调整。</p>
<p>修改do_alignment脚本会影响强制对齐的质量。修改脚本之前，首先得找到脚本位置，并复制一份。<code>which do_alignment</code> 。编辑此文件之后再运行。</p>
<p><strong>在数据子集上训练，但在整个数据集上对齐</strong></p>
<p>你需要创建一个train.scp文件，此文件中只包含了需要在模型上训练的MFCC文件列表。假设该文件名为train_subset.scp，执行如下命令:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HERest -C config -T 1023 -t 250.0 150.0 1000.0 -H hmm$&#123;i&#125;&#x2F;MMF -H hmm0&#x2F;vFloors -I aligned.0.mlf -M hmm$[$i +1] -S train_subset.scp phone_list</span><br></pre></td></tr></table></figure>
<p><strong>改变混合组件的数量</strong></p>
<p>默认模型的输出概率密度分布是8个组件的混合。HTK使用一个称为<code>mixing up</code>的方法逐步增加组件数量，此处我们从1到2，然后是3,5，最后到8个组件。可以修改脚本中的此行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Increase mixtures.</span><br><span class="line"> </span><br><span class="line">for m in 2 3 5 8 ; do</span><br></pre></td></tr></table></figure>
<p><strong>改变元音reductions</strong></p>
<p>不必修改do_alignment就可以达到，只需要修改phone_substitutions文件。尝试移除所有的”substitutions”（比如，创建phone_substitutions和空白文件）</p>
<h2 id="8-Pitchmark语音"><a href="#8-Pitchmark语音" class="headerlink" title="8 Pitchmark语音"></a>8 Pitchmark语音</h2><p>用于波形连接的信号处理是音调(pitch)同步的，因此语音数据库必须包含独立的语调周期标记。<code>make_pm_wave</code>可以用来从波形语音生成音调标记，其中的<code>-[mf]</code>代表选择其一<code>-m</code>(男性)或<code>-f</code>(女性)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ make_pm_wave -[mf] pm wav&#x2F;*.wav</span><br><span class="line">bash$ make_pm_fix pm&#x2F;*.pm</span><br></pre></td></tr></table></figure>
<p><code>make_pm_fix</code>用于调整语调标记，使得他们在波形中对齐于一个peak极点，并且插值在语音的无声区域。</p>
<p><strong>查看音调标记</strong></p>
<p>为查看音调标记（精确检查），需要将他们转化为标签文件。这些可以在与波形对应的波浪中看到。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir pm_lab</span><br><span class="line">bash$ make_pmlab_pm pm&#x2F;*.pm</span><br></pre></td></tr></table></figure>
<p><strong>调整音调标签设置</strong></p>
<p>在自己的声音上需要调整一些参数。此时需要复制脚本 <code>$MBDIR/bin/make_pm_wave</code>到当前<code>/ss</code>目录下，并对该文件做修改。记得对应的运行命令是，运行当前的<code>make_pm_wave</code>。</p>
<p>找到脚本中的<code>DEFAULT_PM_ARGS</code>，其中的<code>min</code>和<code>max</code>是音调标签之间的最小和最大值(例如，音调周期(period)是1/F0)。<code>1x_1f</code>和<code>1x_hf</code>的是值单位为Hertz的频率以及一个控制过滤器，该过滤器移除在音调标签之前的高频和低频。</p>
<p>如果决定修改默认值，需要找到你的声音的<code>F0</code>值域。可以使用<code>Praat</code>的编辑命令来检验一些音调轮廓的波形，并记录最大和最小值（剔除由Praat产生的明显纰漏）。如一段中设置高频和低频过滤器掉不在此音调范围内的值，同时为你的声音设置合适的max和min值。再次运行音调标记器并检查输出。需要将pitchmark转换为标签文件来查阅。</p>
<p>在调整pitchmarker设置时， 删除<code>-fill</code>选项。</p>
<h2 id="8-合成声音"><a href="#8-合成声音" class="headerlink" title="8  合成声音"></a>8  合成声音</h2><h3 id="8-1-Utterance-发声-结构"><a href="#8-1-Utterance-发声-结构" class="headerlink" title="8.1 Utterance(发声)结构"></a>8.1 Utterance(发声)结构</h3><p>Festival中的目标代价函数使用语言学信息来计算，所以我们需要提供语音数据库中所有候选单元的信息，这些信息存储在发声结构中。发声结构包括语音字符串，将这些语音和它们的parent（双亲）音节和单词连接的树形结构，等等。我们将由强制对齐获得的语音时间戳加入到这些结构中。</p>
<p>首先，创建发声结构:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir utt</span><br><span class="line">bash$ festival $MBDIR&#x2F;scm&#x2F;build_unitsel.scm my_lexicon.scm</span><br><span class="line">festival&gt;(build_utts &quot;utts.data&quot; &#39;unilex-rpx)</span><br></pre></td></tr></table></figure>
<p>然后，运行和分析语音持续时间分布，并标记任何异常值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir dur</span><br><span class="line">bash$ phone_lengths dur lab&#x2F;*.lab</span><br><span class="line">bash$ festival $MBDIR&#x2F;scm&#x2F;build_unitsel.scm</span><br><span class="line">festival&gt;(add_duration_info_utts &quot;utts.data&quot; &quot;dur&#x2F;durations&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="8-2-音调-pitch-追踪"><a href="#8-2-音调-pitch-追踪" class="headerlink" title="8.2 音调(pitch)追踪"></a>8.2 音调(pitch)追踪</h3><p>join cost(目标损失函数)的一个重要组件是基准频率，<code>F0</code>。这个从音调标记中独立抽取的，尽管二者很显然是紧密相关的。</p>
<p>而音调标记是波形生成所使用的信号处理必备的，音调轮廓(更准确的说是，<code>F0</code>轮廓)是join cost必须的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir f0</span><br><span class="line">bash$ make_f0 -[mf] wav&#x2F;*.wav</span><br></pre></td></tr></table></figure>
<p>其中<code>-[mf]</code>,<code>-f</code>指女性，<code>-m</code>指男性。</p>
<h3 id="8-3-join-cost系数"><a href="#8-3-join-cost系数" class="headerlink" title="8.3 join cost系数"></a>8.3 join cost系数</h3><p>join cost衡量的是数据库中的候选单元join处的潜在声音不匹配。为了在合成声音运行时更快，可以预处理用于计算join cost的声学特征。</p>
<p>Festival的join cost衡量的是spectrum(范围)(即MFCCs)的不匹配和F0的不匹配。接下来是对每个发声utterance规范化并结合MFCCs和F0为单个文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir coef</span><br><span class="line">bash$ make_norm_join_cost_coefs coef f0 mfcc &#39;.*.mfcc&#39;</span><br></pre></td></tr></table></figure>
<p>并且由于join cost仅使用每个候选单元的第一和最后一帧来评估，这些文件可以被剥离掉所有不在双音边界附近的值了，这使得文件变得更小、更快地载入到Festival。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir coef2</span><br><span class="line">bash$ strip_join_cost_coefs coef coef2 utt&#x2F;*.utt</span><br></pre></td></tr></table></figure>
<h3 id="8-4-波形表征"><a href="#8-4-波形表征" class="headerlink" title="8.4 波形表征"></a>8.4 波形表征</h3><p>尽管单元选取对于预先记录的波形片段的拼接至关重要，我们仍然可以为源过滤模型参数存储这些波形文件。</p>
<p>Festival在语音合成时所使用的语音表征是残差激励线性预测系数(RELP)。这样就可以操作spectrum(范围)和F0（比如，在拼接处）以及持续时间。然而，实际上Festival的Multisyn引擎没有做任何操作。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ mkdir lpc</span><br><span class="line">bash$ make_lpc wav&#x2F;*.wav</span><br></pre></td></tr></table></figure>
<h2 id="9-运行声音"><a href="#9-运行声音" class="headerlink" title="9 运行声音"></a>9 运行声音</h2><p>为了运行声音，启动Festival并载入声音(将的rpx修改为合适的名称)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash$ festival</span><br><span class="line">festival&gt;(voice_localdir_multisyn-rpx)</span><br><span class="line">festival&gt;(SayText &quot;Hello world.&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="10-提高"><a href="#10-提高" class="headerlink" title="10 提高"></a>10 提高</h2><p>前面的步骤已经完成声音的构建，接下来是如何提升声音质量了，我们所使用的办法是合成多版本的声音，然后对比和测试。</p>
<p>构建多版本声音的方法就是完全复制一份<code>ss</code>文件夹，同时对<code>wav</code>,<code>mfcc</code>,<code>lpc</code>文件夹建立软链接。</p>
<p>第一件事是重温构建声音的每个步骤，并查看是否有可以提高的地方。比如，可以调整 pitchmarking参数以适应你的声音。然后，尝试以下部分或全部变更。</p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>语音处理0：基础</title>
    <url>/2017/04/01/voice-basic/</url>
    <content><![CDATA[<h2 id="一-语音的基础概念"><a href="#一-语音的基础概念" class="headerlink" title="一 语音的基础概念"></a>一 语音的基础概念</h2><h3 id="1-1-什么是语音"><a href="#1-1-什么是语音" class="headerlink" title="1.1 什么是语音"></a>1.1 什么是语音</h3><p>语音是一个连续的音频流，它是由大部分的稳定态和部分动态改变的状态混合构成</p>
<h3 id="1-2-声音如何产生"><a href="#1-2-声音如何产生" class="headerlink" title="1.2 声音如何产生"></a>1.2 声音如何产生</h3><p><img src="/images/blog/voice_produce.png" alt="声音产生"></p>
<p>声音由肺部气流并经过声带，此为声源。当我们发<strong>元音</strong>时，声带被气流震动并生成<strong>脉冲序列</strong>。此脉冲决定了语音的<strong>基准频率</strong>。</p>
<p>当我们发出<strong>辅音</strong>时，声带没有被震动，并产生<strong>噪音</strong>。然后脉冲或噪声被声带转换或者个性化。</p>
<h3 id="1-3-语音的组成"><a href="#1-3-语音的组成" class="headerlink" title="1.3 语音的组成"></a>1.3 语音的组成</h3><p>最小单元是<strong>音素</strong>，音素组合为<strong>音节</strong>。</p>
<h4 id="1-3-1-音素"><a href="#1-3-1-音素" class="headerlink" title="1.3.1 音素"></a>1.3.1 音素</h4><p>音素是最小的语音单位，它是从音色的角度划分出来的。例如，汉语里的 ɑ、i、u都是音素。一种语言的语音系统大都是由几十个不同的音素组成的</p>
<p>音素分为元音和辅音</p>
<ul>
<li><p><strong>元音</strong>： 如ɑ、o、e、i、u。</p>
</li>
<li><p><strong>辅音</strong>：如b、p、d、t、ɡ、k、s、r。</p>
</li>
</ul>
<p><strong>元音和辅音的区别</strong></p>
<ol>
<li>元音发音时，气流不受阻碍；辅音发音时，气流通过口腔、鼻腔时要受到阻碍</li>
</ol>
<p>2．元音发音时，发音器官各部位保持均衡的紧张状态；辅音发音时，构成阻碍的部位比较紧张，其他部位比较松弛</p>
<p>3．元音发音时，气流较弱；辅音发音时，气流较强。</p>
<p>4．元音发音时，声带要颤动，发出的声音比较响亮；辅音发音时，有的声带颤动，声音响亮，如m、n、l、r，有的不颤动，声音不响亮，如b、t、z、c。</p>
<h4 id="1-3-2-音节（只适用于汉语）"><a href="#1-3-2-音节（只适用于汉语）" class="headerlink" title="1.3.2 音节（只适用于汉语）"></a>1.3.2 音节（只适用于汉语）</h4><p>音节是由音素构成的。如啊”（ā）（1个音素），“地”（dì）（2个音素），“民”（mín）（3个音素）。</p>
<p>音节示例：如“建设”是两个音节，“图书馆”是三个音节，“社会主义”是四个音节。汉语音节和汉字基本上是一对一，一个汉字也就是一个音节。</p>
<p>音节包含了<strong>声母</strong>、<strong>韵母</strong>、<strong>音调</strong>三个部分。</p>
<ul>
<li><p><strong>声母</strong>： 声母指音节开头的辅音，共有23个。如dā（搭）的声母是d</p>
</li>
<li><p><strong>韵母</strong>： 韵母指音节里声母后面的部分，共38。jiǎ（甲）的韵母是iǎ</p>
</li>
<li><p><strong>音节</strong>： 声调指整个音节的高低升降的变化。普通话里dū（督）、dú（毒）、dǔ（赌）、dù（度）</p>
</li>
</ul>
<p>根据《现代汉语词典》，汉语标准音节共 418 个</p>
<h2 id="2-音频的表示"><a href="#2-音频的表示" class="headerlink" title="2 音频的表示"></a>2 音频的表示</h2><h3 id="2-1-波形表示"><a href="#2-1-波形表示" class="headerlink" title="2.1 波形表示"></a>2.1 波形表示</h3><p>波形表示是大家很熟悉的波形表示，就是直接表示出在观测点上所测量到的振幅和时间的关系。当然为了能够将连续的波形记录为数字形式，我们需要对这个波形进行采样（每隔一个固定的时间采取一次测量）和数字化（将连续的数字转化为可用二进制表达的格式）。</p>
<p><img src="/images/blog/voice_wave_represtation.jpg" alt="音频的波形表示"></p>
<p>在上面的图中，展示了一秒钟的人声音频片段，以及截取其中一毫秒的数据的放大图</p>
<p><img src="/images/blog/voice_wave_represtation1ms.jpg" alt="音频的波形表示"></p>
<p>我们最熟悉的CD音频，每秒钟采样44100次（这是因为，根据Nyquist采样定理，如果要完美重现20kHz的音频，那么我们最少需要每秒采样40k次，而20kHz是人类的听觉上限），而每个样本都用16位的二进制来表示。</p>
<p>这样每一个样本可以表示最多65,536种不同的振幅。如果假定每一个时间点采用8位的二进制数字来表示，那么总共可能会有256种可能的值，我们就用一个256位的one－hot向量来表示它，最后在计算机中保存的声音片段，就可能是这样的。</p>
<p><img src="/images/blog/voice_wave_represtation_onehot.jpg" alt="音频的波形表示">  </p>
<h3 id="2-2-频域表示"><a href="#2-2-频域表示" class="headerlink" title="2.2 频域表示"></a>2.2 频域表示</h3><p>如果对输入的波形做一次傅立叶变换，会发现，一个复杂的sine波形，实际上在转换后的“频谱”上，可以被很简单的表示出来。</p>
<p>转换后的音频，是一种频率的表示——我们只关心这个波形到底是以什么样的频率在震动，而恰好，我们人类对于声音的认知，也是基于频率而不是振幅的——实际上这样的表示，更加符合人类对于声音的认知，也更容易对其进行数字处理（DSP）。</p>
<p><img src="/images/blog/voice_wave_represtation_fluir.jpg" alt="音频的傅里叶变换"> </p>
<h2 id="3-语音的初步处理"><a href="#3-语音的初步处理" class="headerlink" title="3 语音的初步处理"></a>3 语音的初步处理</h2><p>我们知道声音实际上是一种波。常见的mp3等格式都是压缩格式，必须转成非压缩的纯波形文件来处理，比如Windows PCM文件，也就是俗称的wav文件。wav文件里存储的除了一个文件头以外，就是声音波形的一个个点了。下图是一个波形的示例。</p>
<p><img src="/images/blog/voice_wave_example.jpg" alt="音频的傅里叶变换">  </p>
<p>在开始语音识别之前，有时需要把首尾端的静音切除，降低对后续步骤造成的干扰。这个静音切除的操作一般称为VAD，需要用到信号处理的一些技术。要对声音进行分析，需要对声音分帧，也就是把声音切开成一小段一小段，每小段称为一帧。分帧操作一般不是简单的切开，而是使用移动窗函数来实现。帧与帧之间一般是有交叠的，就像下图这样：</p>
<p><img src="/images/blog/voice_wave_segmentation.jpg" alt="音频的傅里叶变换">  </p>
<p>图中，每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠。我们称为以帧长25ms、帧移10ms分帧。</p>
<p>分帧后，语音就变成了很多小段。但波形在时域上几乎没有描述能力，因此必须将波形作变换。常见的一种变换方法是提取MFCC特征，根据人耳的生理特性，把每一帧波形变成一个多维向量，可以简单地理解为这个向量包含了这帧语音的内容信息。这个过程叫做声学特征提取。实际应用中，这一步有很多细节，声学特征也不止有MFCC这一种，具体这里不讲。</p>
<p>至此，声音就成了一个12行（假设声学特征是12维）、N列的一个矩阵，称之为观察序列，这里N为总帧数。观察序列如下图所示，图中，每一帧都用一个12维的向量表示，色块的颜色深浅表示向量值的大小。</p>
<p><img src="/images/blog/voice_mfcc.jpg" alt="音频的处理MFCC">  </p>
<p>接下来就要介绍怎样把这个矩阵变成文本了。首先要介绍两个概念：</p>
<ul>
<li><p>音素：单词的发音由音素构成。对英语，一种常用的音素集是卡内基梅隆大学的一套由39个音素构成的音素集，参见The CMU Pronouncing Dictionary‎。汉语一般直接用全部声母和韵母作为音素集，另外汉语识别还分有调无调，不详述。</p>
</li>
<li><p>状态：这里理解成比音素更细致的语音单位就行啦。通常把一个音素划分成3个状态。</p>
</li>
</ul>
<p><img src="/images/blog/voice_frames.jpg" alt="音频的处理MFCC">  </p>
<p>图中，每个小竖条代表一帧，若干帧语音对应一个状态，每三个状态组合成一个音素，若干个音素组合成一个单词。也就是说，只要知道每帧语音对应哪个状态了，语音识别的结果也就出来了。</p>
<p><strong>参考</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/25784028" target="_blank" rel="noopener">Emotibot Tech | WaveNet语音合成与深度生成模型解析 - 知乎专栏</a></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
  </entry>
  <entry>
    <title>医疗软件Mevislab使用</title>
    <url>/2017/04/01/mevislab/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在阅读肺结节CT图像处理的相关论文时，注意到很多作者都使用的是一款软件Mevislab。Mevislab是一款用于图像处理，尤其是医疗CT图像领域的，快速原型开发平台，做学术研究和个人学习是免费自由使用的（可自由开发的模块有限），<br>商业环境下使用时收费的。我曾发邮件咨询过，大概每年2万2(2017年3月份)。下图是工作台样例:</p>
<p><img src="/images/blog/mevislab_looks.jpg" alt="mevislab工作台"></p>
<p>官网地址：<a href="http://www.mevislab.de/mevislab/" target="_blank" rel="noopener">Mevislab</a></p>
<p>论坛地址:<a href="https://forum.mevis.fraunhofer.de/" target="_blank" rel="noopener">Mevislab论坛</a> ，很多问题自这里可以咨询，论坛活跃用户不多，一般可能要隔两天才有人回，可能是时差。</p>
<h2 id="一-特点"><a href="#一-特点" class="headerlink" title="一 特点"></a>一 特点</h2><ul>
<li><p>多平台支持：Mac,Linux,Windows</p>
</li>
<li><p>模块多：当前有920多标准模块，总共有3000多（包括360多ITK模块、1470个VTK模块、），基本涵盖了所有的图像处理模块，比如腐蚀、膨胀、阈值分割、区域增长等。在实际做图像处理时，直接拉模块连起来就可以。</p>
</li>
<li><p>支持脚本:目前支持python和C++，可以在工作台里直接调用python(使用RunPythonScripts模块)。C++ 可以直接重新定义mevislab内置的模块，或者自己重写一个模块。</p>
</li>
<li><p>自由高效：摸熟之后，你会发现它的高效、简洁，模块之间自由组合（流程上下衔接）</p>
</li>
<li><p>强大：强大的体现是，2D，3D(官方说法可以支持6D)都可以处理。尤其是3D渲染，简直神器，即刻呈现处理效果。</p>
</li>
</ul>
<h2 id="二-使用"><a href="#二-使用" class="headerlink" title="二 使用"></a>二 使用</h2><h3 id="2-1-基本使用"><a href="#2-1-基本使用" class="headerlink" title="2.1 基本使用"></a>2.1 基本使用</h3><p>首先，构建一个最基本的图像处理流程</p>
<p><img src="/images/blog/mevislab_basicpipeline.png" alt="基本流程"></p>
<p>其中<code>ImageLoad</code>,<code>Threshold</code>,<code>SoView2D</code>这三个模块可以在任务栏的<code>module</code>中搜索找到，或者直接搜索框里输入即可得到。我们可以在图中看到不同的module有不同的颜色，接口处的形状也不同，有尖三角，有半圆形，它们代表不同的意义。具体的请参考官方文档，本文重点不讨论这个。</p>
<p>常规的图像处理流程包括，载入图像，处理算法，展现模块。其中载入图像模块如上图<code>ImageLoad</code>，可以载入普通JPEG</p>
<h3 id="2-2-基本模块"><a href="#2-2-基本模块" class="headerlink" title="2.2 基本模块"></a>2.2 基本模块</h3><p><strong>模块类型</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>外形</th>
<th>特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>ML模块(蓝色)</td>
<td><img src="/images/blog/ml_module.png" alt="ML module"></td>
<td>基于分页的，命令驱动的体素处理</td>
</tr>
<tr>
<td>开放的素材模块(可以用来组合和协助构建处理过程)(绿色)</td>
<td><img src="/images/blog/inventor_module.png" alt="inventor module"></td>
<td>视觉场景图(3D),名称转换，所有模块以So(scene object)开头</td>
</tr>
<tr>
<td>宏模块(棕色)</td>
<td><img src="/images/blog/macro_module.png" alt="macro"></td>
<td>组合其他模块类型，允许层次继承和脚本交互</td>
</tr>
</tbody>
</table>
</div>
<p><strong>连接器</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>外观</th>
<th>形状</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/images/blog/triangle_look.png" alt="三角形"></td>
<td>三角形</td>
<td>ML 图像</td>
</tr>
<tr>
<td><img src="/images/blog/circle_look.png" alt="半圆形"></td>
<td>半圆形</td>
<td>场景素材</td>
</tr>
<tr>
<td><img src="/images/blog/square_look.png" alt="方形"></td>
<td>方形</td>
<td>基本对象：指向数据结构的指针</td>
</tr>
</tbody>
</table>
</div>
<p><strong>链接</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>外观</th>
<th>特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据链接(连接器链接)</td>
<td><img src="/images/blog/data_connect.png" alt="数据链接"></td>
<td>连接器之间的直接链接。不同的连接器会有不同的颜色,蓝色的为ML，绿色的为开放素材，棕色的为基本类型</td>
</tr>
<tr>
<td>参数链接(域链接)</td>
<td><img src="/images/blog/parameter_connect.png" alt="参数链接"></td>
<td>模块之间或模块内部的参数的链接形成的链接</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三-实现一个轮廓过滤"><a href="#三-实现一个轮廓过滤" class="headerlink" title="三 实现一个轮廓过滤"></a>三 实现一个轮廓过滤</h2><p>假设我们的轮廓过滤步骤为: 载入图像a—&gt;均值化[b]—&gt;形态学膨胀操作[c]—&gt;求差值[b,c]—&gt;查看图像</p>
<p>在Mevislab的工作台新建网络，并载入模块<code>LoadImage</code>,<code>Convolution</code>,<code>Morphology</code>,<code>Arithmetic2</code>。这些模块可以直接在搜索框搜索到。将这些模块按照如下图连接起来，连接操作鼠标左键点击模块的连接点，然后拖到下一个模块，松开即可。</p>
<p><img src="/images/blog/sample_filter_network.png" alt="网络流程"></p>
<p><strong>参数调整</strong></p>
<p>调整参数时，双击模块面板，有些隐含的参数需要右击面板—&gt;<code>show autopanel</code>。如果想让模块A中的参数param_a赋值给模块B中的参数param_b，在A面板中点击param_a拖住(可以看到此时面板中的param_a处出现了一个蓝色的箭头)，拉到面板B中的参数param_b处(也看到蓝色箭头激活)。如何保持参数同步(模块之间参数赋值)，示例如下:</p>
<p><img src="/images/blog/sync_params.png" alt="模块间参数同步"></p>
<p>其他参数设置参考下图:</p>
<p><img src="/images/blog/network_params_setting.png" alt="模块间参数同步"></p>
<h2 id="四-图像操作和处理"><a href="#四-图像操作和处理" class="headerlink" title="四 图像操作和处理"></a>四 图像操作和处理</h2><h3 id="4-1-图像操作"><a href="#4-1-图像操作" class="headerlink" title="4.1 图像操作"></a>4.1 图像操作</h3><ul>
<li><p><code>ImageLoad</code>:打开图像，格式可以为<code>DICOM</code>,<code>TIFF</code>,<code>DICOM/TIFF</code>,<code>RAW</code>,<code>LUMISYS</code>,<code>PNM</code>,<code>Analyze</code>,<code>PNG</code>,<code>JPEG</code></p>
</li>
<li><p><code>LocalImage</code>:与<code>ImageLoad</code>类似，载入的是相对于Mevislab安装位置或当前网络位置的图像</p>
</li>
<li><p><code>ImageSave</code>:存储图像，以<code>DICOM</code>,<code>TIFF</code>,<code>DICOM/TIFF</code>,<code>RAW</code>,<code>LUMISYS</code>,<code>PNM</code>,<code>Analyze</code>,<code>PNG</code>,<code>JPEG</code>这些格式</p>
</li>
</ul>
<h3 id="4-2-图像属性"><a href="#4-2-图像属性" class="headerlink" title="4.2 图像属性"></a>4.2 图像属性</h3><ul>
<li><p><code>Info</code>:展示当前连接的输入图像的信息，比如图像尺寸，page size，体素size，总容积，世界矩阵等。</p>
</li>
<li><p><code>MinMaxScan</code>:扫描输入并更新输出图像的最大最小值，可以改变数据类型。</p>
</li>
<li><p><code>ImagePropertyConvert</code>:允许自由改变图像的page size，最大、最小值、数据类型、世界矩阵</p>
</li>
<li><p><code>ImageStatistics</code>:计算输入图像体素的一些统计特性。</p>
</li>
</ul>
<h3 id="4-3-基本图像处理"><a href="#4-3-基本图像处理" class="headerlink" title="4.3 基本图像处理"></a>4.3 基本图像处理</h3><ul>
<li><p><code>SubImage</code>:从输入图像中基于体素、世界坐标的起止，尺寸抽取子图。也可以用于抽取比输入图像更大的区域</p>
</li>
<li><p><code>Resample3D</code>:在3D图像的任意平面抽样，有17个filter可以使用。</p>
</li>
<li><p><code>Reformat</code>:将图像重新格式化为一副引用图像，或者通过<code>SoView2D/SoOrthoView2D</code>创建重新格式化的叠加。</p>
</li>
<li><p><code>Scale</code>:将图像缩放到一个指定间隔，来源和目标缩放间隔可以自定义。</p>
</li>
<li><p><code>Arithmetic1</code>:对一副图像做算术运算。比如<code>Add</code>操作，则是对图像中体素的每个值加上一个常量。</p>
</li>
<li><p><code>Arithmetic2</code>:对两幅图像做算数运算。比如<code>Add</code>操作，则是将图像1中每个体素的值加到图像2上。</p>
</li>
<li><p><code>Mask</code>: 用图像2中的mask对图像1进行mask操作。(有Mask模块中有不同选项)</p>
</li>
<li><p><code>TestPattern</code> :基于指定的尺寸，page size，数据类型和模式生成一副测试图像</p>
</li>
<li><p><code>AddNoise</code>: 基于标量输入图像产生噪音数据，比如高斯噪音、盐粒噪音等。</p>
</li>
</ul>
<h3 id="4-4-过滤器"><a href="#4-4-过滤器" class="headerlink" title="4.4 过滤器"></a>4.4 过滤器</h3><ul>
<li><p><code>Convolution</code>：标准卷积，比如均值卷积，高斯卷积，拉普拉斯卷积和Sobel。</p>
</li>
<li><p><code>ExtendedConvolution</code>:提供与标准卷积类似的过滤器，但是更加灵活的Kernel size和kernel geometry</p>
</li>
<li><p><code>Rank</code>: 基于秩的卷积，比如最小、最大、中值、Rank、index</p>
</li>
<li><p><code>Morphology</code>: 形态学操作，比如腐蚀和膨胀</p>
</li>
<li><p><code>CalculateGradient</code>:计算输入图像每个体素周围的值得坡度(梯度)</p>
</li>
</ul>
<h3 id="4-5-分割"><a href="#4-5-分割" class="headerlink" title="4.5 分割"></a>4.5 分割</h3><ul>
<li><p><code>Threshold</code>: 将图像转换为二值图像，根据阈值</p>
</li>
<li><p><code>IntervalThreshold</code>: 通过过滤掉在指定值域范围的像素值来处理一副图像，在值域范围外的可以指定其他值或者为0。</p>
</li>
<li><p><code>RegionGrowing</code>:区域增长算法，提供简单的基于阈值或间隔的 1D/2D/3D/4D的区域增长算法。需要设置阈值或间隔和最少一个种子节点。</p>
</li>
<li><p><code>RegionGrowingMacro</code>:是<code>RegionGrowing</code>的宏拓展，添加了自定义的marker 编辑。</p>
</li>
<li><p><code>ComputeConnectedComponents</code>:在2D/3D灰度图上进行连通组件分析。</p>
</li>
</ul>
<h3 id="4-6-可视化"><a href="#4-6-可视化" class="headerlink" title="4.6 可视化"></a>4.6 可视化</h3><p><strong>2D可视化</strong></p>
<ul>
<li><p><code>View2D</code>: 以2D切片的形式查看3D图像。</p>
</li>
<li><p><code>View2DExtensions</code>: 封装了一系列的viewer，这些viewer都是常用于连接2D viewer的拓展，包括以切片形式查看、放大缩小、窗口调整。</p>
</li>
<li><p><code>SoView2D</code>: 在2D viewer中呈现一个容积(volume)图像的一个切片。</p>
</li>
<li><p><code>SoRenderArea</code>:提供一个开放的Inventor(素材)渲染器和Mevislab窗口内部的事件处理机制。</p>
</li>
<li><p><code>SoView2DOverlay</code>: 将一副2D图像与另外一副混合。</p>
</li>
<li><p><code>SoView2DPosition</code>: 显示2Dviewer 中最近点击的位置，显示形式可以自定义为圆形、空间矩形或叉叉。</p>
</li>
<li><p><code>SoView2DRectangle</code> : 在2D viewer中交互式的绘制或调整一个2D矩形。虽然此模块名称带2D，其实也可以操作3D。</p>
</li>
<li><p><code>SoMouseGrabber</code> :抓取Inventor sence中的鼠标事件并将其转化为float类型的x，y域。</p>
</li>
<li><p><code>SoKeyGrabber</code> :监听Inventor sence种的键盘事件，并触发依赖于不同键盘key按压操作的field。</p>
</li>
<li><p><code>OrthoView2D</code>: 提供一个2D view来展现三个正交视图方向的输入图像。</p>
</li>
<li><p><code>SoOrthoView2D</code>: 在2D viewer中渲染一副体素(volume)图像的正交切片</p>
</li>
<li><p><code>SynchroView2D</code>: 提供两个2D viewer，它们通过其世界坐标轴同步。</p>
</li>
</ul>
<p><strong>3D 视图</strong></p>
<ul>
<li><p><code>SoGVRVolumeRenderer</code>:一个基于八卦的渲染器，允许3D/4D图像的高质量体素渲染。此模块继承自允许设置渲染参数的拓展模块集合。</p>
</li>
<li><p><code>SoExaminerViewer</code> : 提供开放的Inventor渲染和Mevislab窗口内部的时间操作。开放Inventor渲染比如背景色、透明度类型、绘画风格等。</p>
</li>
<li><p><code>View3D</code> :直接3D查看图像。</p>
</li>
<li><p><code>SoBackground</code>: 渲染开放Inventor sence中背景色的颜色坡度。</p>
</li>
</ul>
<h3 id="4-7-LookUp-Table"><a href="#4-7-LookUp-Table" class="headerlink" title="4.7 LookUp Table"></a>4.7 LookUp Table</h3><p>此模块用于编辑网络中其他模块的参数（此模块后续再补充，没怎么用）</p>
<ul>
<li><p><code>ApplyLUT</code> :在输入图像上应用lookup table(LUT)。输入图像中的体素值用作LUT索引值，LUT实体值被缩放到最大实体参数，并存储到输出图像。</p>
</li>
<li><p><code>SoLUTEditor</code>:允许编辑RGBA LUT 并输出MLLut对象。</p>
</li>
</ul>
<h3 id="4-8-Markers"><a href="#4-8-Markers" class="headerlink" title="4. 8 Markers"></a>4. 8 Markers</h3><ul>
<li><p><code>XMarkerListContainer</code>:存储了XMarker对象列表为XMarkerList对象。其内容可以呈现，编辑和保存。一个XMarker对象由一个6D Position，一个3D Vector，一个Type，一个Name属性组成。</p>
</li>
<li><p><code>SoView2DMarkerEditor</code>: 允许在2D 视图上交互式放置、编辑和展现markers。</p>
</li>
<li><p><code>So3DMarkerEditor</code>: 在3D中呈现markers并提供可能的交互式编辑markers。</p>
</li>
</ul>
<h3 id="4-9-Curves"><a href="#4-9-Curves" class="headerlink" title="4.9 Curves"></a>4.9 Curves</h3><ul>
<li><p><code>ProfileCurve</code>:从一副图像的任意数据维度抽取概要轮廓，通过沿着指定的线路读取输入图像的体素值。</p>
</li>
<li><p><code>SoDiagram2D</code>:呈现2D曲线，比如时间序列，灰度缩放概要，直方图等。</p>
</li>
</ul>
<h3 id="4-10-Contours"><a href="#4-10-Contours" class="headerlink" title="4.10 Contours"></a>4.10 Contours</h3><ul>
<li><p><code>CSOManager</code>:允许编辑CSOs和CSOGroup设置参数和默认参数，以及维持CSO和CSOGroup的整齐度。</p>
</li>
<li><p><code>SoCSO3DVis</code>:在3D中某个CSOList中的CSOs以Open Inventor sence开启可视化。需要对input可用的CSOList(比如通过CSOManager)</p>
</li>
<li><p><code>CSOIsoGenerator</code>:允许以固定的ISO值对整幅图生成iso轮廓。需要一个可用填充值的CSOList(比如CSOManager)</p>
</li>
<li><p><code>SoView2DCSOExtensibleEditor</code>:允许编辑和拖拽CSOs。与CSOManager，一个CSO子编辑器和输出用的2D Viewer结合使用，</p>
</li>
<li><p><code>SoCSOSplineEditor</code>:允许徒手或一个点一个点的生成CSOs。</p>
</li>
<li><p><code>SoCSOEllipseEditor</code>:允许生成椭圆或圆形CSO。</p>
</li>
</ul>
<h3 id="4-10-Surface-objects"><a href="#4-10-Surface-objects" class="headerlink" title="4.10 Surface objects"></a>4.10 Surface objects</h3><ul>
<li><p><code>SoWEMRenderer</code>:将一个WEM渲染为一个Open Inventor sence</p>
</li>
<li><p><code>WEMIsoSurface</code>:以固定阈值生成标量体积图像的ISO表面</p>
</li>
<li><p><code>WEMSmooth</code>: 使用一个表面平滑(拉普拉斯)来平滑WEM，或者表面的平滑。</p>
</li>
<li><p><code>SoView2DWEMEditor</code>: 在特定球形范围交互式WEM表面变形。</p>
</li>
</ul>
<h2 id="五-创建-Open-Inventor-Scene"><a href="#五-创建-Open-Inventor-Scene" class="headerlink" title="五 创建 Open Inventor Scene"></a>五 创建 Open Inventor Scene</h2><p>此模块主要利用Mevislab提供的各种自木块来构建各类视图模型，Open Inventor是一个面向对象的3D开发工具。Inventor scenes以场景图的形式组织。一个场景图由代表即将绘制的3D对象的节点，3D对象的属性，与其他节点结合的节点组成层次树，其他如摄像机、灯光等组成。</p>
<p>注意Open Inventor中的遍历路径如下，这对于如何构建场景图很关键。</p>
<p><img src="/images/blog/open_inventor_travel_path.jpg" alt="遍历路径"></p>
<p>Open Inventor模块的函数有:</p>
<ul>
<li><p><code>Draggers and manipulators</code></p>
</li>
<li><p><code>Group nodes</code></p>
</li>
<li><p><code>Light sources</code></p>
</li>
<li><p><code>Transformations</code></p>
</li>
<li><p><code>Cameras</code></p>
</li>
<li><p><code>3D viewers</code></p>
</li>
<li><p><code>Geometric objects (Spheres, Cones, 3D Text, Nurbs, Triangle Meshes, etc.)</code></p>
</li>
<li><p><code>Object properties (Textures, Colors, Materials, etc.)</code></p>
</li>
</ul>
<p>注意：在ML模块中模块的域值更新是同步的，但是在Open Inventor中是异步的，更改值之后会先存储在延迟队列中。</p>
<p>关于如何构建这些场景，示例图如下:</p>
<p> <img src="/images/blog/open_inventor_sample1.jpg" alt="遍历路径"></p>
<p> <img src="/images/blog/open_inventor_sample2.jpg" alt="遍历路径"></p>
<p>你可以将Mevislab当做一个3D建模工具玩。</p>
<h2 id="六-构建宏模块"><a href="#六-构建宏模块" class="headerlink" title="六  构建宏模块"></a>六  构建宏模块</h2><p>宏模块可以通过MDL(Mevislab Definition Lanague)和python或JavaScript脚本实现。宏的功能与其他模块类似，可以理解为一系列完成某种功能的模块的集合被封装成了一个模块。</p>
<p>构建一个宏模块，你需要走如下三步。</p>
<h3 id="6-1-构建宏"><a href="#6-1-构建宏" class="headerlink" title="6.1 构建宏"></a>6.1 构建宏</h3><p>首先，你得把一系列用于完成特定任务的模块串起来定义好，放入工作台。如下图:</p>
<p> <img src="/images/blog/mevislab_macro1.jpg" alt="宏定义模块"></p>
<p>定义好之后将网络存储在某个位置，比如命名为test_macro.mlab。</p>
<p>然后选择<code>File</code> → <code>Project Wizard</code> 并选择 <code>Macro</code>。然后设置宏的一些参数，其中打星号的是必须的。</p>
<p> <img src="/images/blog/mevislab_macro2.jpg" alt="宏定义模块"></p>
<p> 然后下一步，选择<code>Network File name</code>时选择刚保存test_macro.mlab。点击创建之后会自动创建如下文件</p>
<p><img src="/images/blog/mevislab_macro3.jpg" alt="宏定义模块"></p>
<p>此时，即可在搜索栏搜到刚刚定义的宏模块。</p>
<h3 id="6-2-给宏添加宏参数和面板"><a href="#6-2-给宏添加宏参数和面板" class="headerlink" title="6.2 给宏添加宏参数和面板"></a>6.2 给宏添加宏参数和面板</h3><p>右键点击刚刚创建的宏的面板选择related files,选择mevislab_macro.script编辑此脚本。此脚本包含了区域:</p>
<ul>
<li><p><strong><em>interface</em></strong>:定义宏的输入输出。</p>
</li>
<li><p><strong><em>Commands</em></strong>:定义在此宏的某些field活动时要执行的脚本文件</p>
</li>
<li><p><strong><em>window</em></strong>: 定义了宏的面板，在面板上设置参数。</p>
</li>
</ul>
<p>参考示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Interface &#123;</span><br><span class="line">Inputs &#x3D; &quot;&quot;</span><br><span class="line">Outputs &#123;</span><br><span class="line">Field Scene &#123; internalName &#x3D; &quot;Applicator.self&quot; &#125;</span><br><span class="line">&#125;</span><br><span class="line">Parameters &#123;</span><br><span class="line">Field length &#123;</span><br><span class="line">type &#x3D; float</span><br><span class="line">value &#x3D; 20</span><br><span class="line">min &#x3D; 1</span><br><span class="line">max &#x3D; 50</span><br><span class="line">&#125;</span><br><span class="line">Field diameter &#123;</span><br><span class="line">type &#x3D; float</span><br><span class="line">value &#x3D; 3</span><br><span class="line">min &#x3D; 0.1</span><br><span class="line">max &#x3D; 10</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">Commands &#123;</span><br><span class="line">source &#x3D; $(LOCAL)&#x2F;test_macro.py</span><br><span class="line">FieldListener length &#123; command &#x3D; AdjustLength &#125;</span><br><span class="line">FieldListener diameter &#123; command &#x3D; AdjustDiameter &#125;</span><br><span class="line">&#125;</span><br><span class="line">Window &#123;</span><br><span class="line">Category &#123;</span><br><span class="line">Field length &#123; step &#x3D; 1 &#125;</span><br><span class="line">Field diameter &#123; step &#x3D; 0.1 &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">下图为定义之后的面板效果</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> ![宏定义模块](&#x2F;images&#x2F;blog&#x2F;mevislab_macro4.jpg)</span><br></pre></td></tr></table></figure>
<h3 id="6-3-python脚本"><a href="#6-3-python脚本" class="headerlink" title="6.3 python脚本"></a>6.3 python脚本</h3><p>上一步的scipt脚本中，Command所使用的test_macro.py需要编写。示例如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -----------------------------------------------------------------------------</span><br><span class="line">## This file implements scripting functions for the ApplicatorMacro module</span><br><span class="line">#</span><br><span class="line"># \file ApplicatorMacro.py</span><br><span class="line"># \author JDoe</span><br><span class="line"># \date 01&#x2F;2009</span><br><span class="line">#</span><br><span class="line"># -----------------------------------------------------------------------------</span><br><span class="line"># MeVis module import</span><br><span class="line">from mevis import *</span><br><span class="line">def AdjustLength():</span><br><span class="line">overallLength &#x3D; ctx.field(&quot;length&quot;).value</span><br><span class="line">tipLength &#x3D; ctx.field(&quot;SoCone.height&quot;).value</span><br><span class="line">shaftLength &#x3D; overallLength - tipLength</span><br><span class="line">ctx.field(&quot;SoCylinder.height&quot;).value &#x3D; shaftLength</span><br><span class="line">def AdjustDiameter():</span><br><span class="line">        diameter &#x3D; ctx.field (&quot;diameter&quot;).value</span><br></pre></td></tr></table></figure>
<p>注意观察，python脚本中如何调用和控制参数。其中的<code>ctx</code>是默认的上下文，它可以访问当前网络中任何其他模块的任何field。比如此处的<code>SoCone.height</code>，其中的<code>SoCone</code>是一个模块，height是该模块的一个field，如果它有其他实例名example_name，则使用example_name.height也可以直接访问。 此处通过<code>ctx.field(&quot;SoCone.height&quot;).value</code>访问值，而<code>ctx.field(&quot;SoCylinder.height&quot;).value = shaftLength</code>来改变值。</p>
<h2 id="七-渲染"><a href="#七-渲染" class="headerlink" title="七 渲染"></a>七 渲染</h2><p>其实Mevislab用起来只是快速实现模型，但是无法用于生产环境，处理速度太慢。平常用它来做渲染还是不错的，做3D渲染几乎不逊色于一般的3D建模软件。</p>
]]></content>
      <categories>
        <category>医疗</category>
      </categories>
  </entry>
  <entry>
    <title>Morphological Segmentation and Partial Volume of Solid Pulmonary Lesions in Thoracic CT Scans</title>
    <url>/2017/03/11/autoseg-bylungvolumne/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文基于形态学处理方法提出了一个快速的、自动肺分割方法，并适用于病变的大结节和小结节。除此之外，提出的方法称述了容积评估的一些挑战，比如图像协议的多样性，或者通过引入一种基于分割方法的部分容积分析(segmentation-based partial volume analysis (SPVA))的灵感状态(inspiration state)。</p>
<h2 id="一-分割方法"><a href="#一-分割方法" class="headerlink" title="一 分割方法"></a>一 分割方法</h2><h3 id="1-1-描述"><a href="#1-1-描述" class="headerlink" title="1.1 描述"></a>1.1 描述</h3><p>算法基于立方块输入容积VOI(Volume of interest)。下面的集合V代表了所有位于输入容积内的体素。强度值区间范围为 -1024HU到3071HU。在各向异性的情形中，在分割之前需要将体素重新取样为各向同性以允许3D体素矩阵中获得一致的图像处理过程。VOI的假设先决条件是</p>
<ul>
<li><p>P1):种子节点S位于结节内</p>
</li>
<li><p>P2):结节完全位于VOI内部，并且没有触碰到VOI边缘</p>
</li>
</ul>
<p>按照标准图像处理的流程来做，阈值、区域增长、相连组件分析、凸壳和欧几里得距离转换。</p>
<p>下一节中，新的分割方法是逐步描述的。首先，初始阈值分割创建了目标结节mask的超集。接着，在完成边界细化步骤之前，到胸壁的连接和血管组织的连接被移除。</p>
<h3 id="1-2-初始化"><a href="#1-2-初始化" class="headerlink" title="1.2 初始化"></a>1.2 初始化</h3><p>初始分割使用的是区域增长算法，从种子点S开始的，固定阈值。阈值-400HU选为结节放射强度的经验值，它是50个HU值的算术均值，以及-850作为薄壁组织的经验值。此步骤的结果是一个初始的体素 $N_0$ 集合，第一个结节区域评估如下图2b。</p>
<p> 接着，在 $N_0$ 上执行相连组件分析。最大的相连的，非分割区域P被抽取。它对应了环绕目标结节的VOI里面的肺薄壁组织的最大的连接区域（上图2c）。对于分割，P以两种方式使用：其一，对于没有胸壁关联的结节，P基本上就是 $N_0$ 的补充，除了结节内部的一些暗的区域（比如坏死或噪声）。我们定义了一个 $N_0$ 的超集 $N_1 = V-P$ ，最终 $N_0$ 中的孔洞闭合。其二，对于包含胸壁黏连的结节，P对接下来的分离步骤至关重要。</p>
<p><img src="/images/blog/lungvolumne1.png" alt=""></p>
<h3 id="1-3-从胸壁分离"><a href="#1-3-从胸壁分离" class="headerlink" title="1.3 从胸壁分离"></a>1.3 从胸壁分离</h3><p>如上图所示，不仅邻接脉管组织，同时胸壁的部分也可能会被包含在初始的分割集 $N_0$中。通过衡量V中的由 $N_0$ 覆盖的边界体素的占比，来决定是否需要进一步的胸壁分离算法。边界占比10%经验上可作为合适的阈值，并被用在小节IV中。由于结果在没有胸壁连接的情形中不会受到影响，此检测步骤可以被抹去并且仅仅使用于避免不必要的时间消耗中。</p>
<p>CT图像中通常在结节与胸壁黏连之间并不表现出可见的强度对比。因此，此步骤仅重现使用形态学操作从胸壁分离的结节。</p>
<h3 id="1-4-从黏连的脉管组织分离"><a href="#1-4-从黏连的脉管组织分离" class="headerlink" title="1.4 从黏连的脉管组织分离"></a>1.4 从黏连的脉管组织分离</h3><p>由于肺分离仅能够mask out不属于肺的部分结构，邻接组织依然被包含在$N_2$ 中。算法特地关注大结节转移的容积，因此与脉管组织相连的拓展连接也是在期望之内(上图的c和d)。CT图像上的强度信息不足以允许基于强度的分割步骤，但是结节和血管组织在形态学上区别明显。结节与外部脉管组织的连接通常比结节自身的连接要纤细。因此，可以使用形态学开运算(先腐蚀后膨胀)完成结构分离。但是形态学开运算不适于移除完全被病变结节包围的脉管组织的部分。因而，我们仅分离那些部分与脉管组织相连的并且容易被形态学特征清晰区分的。</p>
<p>有多种方法实现二分形态学操作。相比于经典方法，kernel-based的形态学操作，使用球形结构元素的腐蚀和膨胀操作可以以计算mask的内部或者外部边界的欧式距离以及紧接的在结果距离map上的过滤体素操作来模拟。距离转换被用来实现形态学操作。距离转换操作方法被用于一些相关问题，如从血管组织中分离动脉瘤。</p>
<p>对于当前任务，从紧凑病变邻接的血管移除血管，对形态学开运算使用距离map的主要原因是，一旦它们被计算，它们包含了仅仅使用阈值获得的任意腐蚀和膨胀的结果的形态学信息。除此之外的一些参数，比如血管半径可以直接从距离map中获得。接下来描述的是如何有效的计算这些特征</p>
<ol>
<li><p><strong>基本距离转换</strong> ：为实现腐蚀和接下来的膨胀操作，使用两个相反的距离变换。首先是一个基本的3D欧几里得距离转换，从 $E:V\rightarrow R$ ,计算 $N_2$ 中每个体素到背景的最小距离。世界坐标用来计       算体素的各向异性：</p>
<script type="math/tex; mode=display">
 E(v):=min\lbrace\|\|world(v)-world(v')\|\|_2:v'\notin N_2\rbrace</script><p>接着在距离map中再使用低阈值 $threshold$ 来实现形态学腐蚀操作，称为腐蚀阈值。此操作的关键部分是找到合适的 $threshold$ ，在切除不想要的邻接结构时无须切除显著的结节边界特征。</p>
</li>
<li><p><strong>种子优化</strong>：为了优化初始种子节点<code>S</code>，目标病变区域的中心需要被近似。基本距离map E对 $N_2$ 中的每个体素到最邻近的体素的距离做了编码。从初始的定义的结节上的种子节点<code>S</code>出发，E上的局部最大搜索可以近似得到病变区域中心，如下图。</p>
</li>
</ol>
<p><img src="/images/blog/lungvolumne2.png" alt=""></p>
<p>  这并不仅仅提供了一个新的种子节点<code>S&#39;</code>，同时也为得到了其边界距离 $E(S’)$（实际的结节半径） ，一个很好的近似 </p>
<script type="math/tex; mode=display">
    \tilde{r}:=E(S')</script><p>  通过构造，新的种子节点 <code>S&#39;</code>位于局部边界距离最大点，同时由于结节大部分是凸的，局部最大值很可能就是全局最大。使用一个大于 $\tilde{r}$ 的腐蚀阈值会导致所有的结节体素在腐蚀时被根除。因而，腐蚀   阈值  $\tilde{r}$  对应了100%的腐蚀强度，使得我们可以更加公式化的定义这一项，关于边界距离的腐蚀比例。由于腐蚀强度的定义使得我们可以对决定方法定义个更加启发式的描述，并且腐蚀阈值的绝对值    对于方法描述是不显著的，我们对任意 $e\in R$ 定义了归一化距离 $\phi$  </p>
<script type="math/tex; mode=display">
   \phi (e):=\frac{e}{\tilde{r}}</script><p> 并在接下来检验归一化距离map $E_{\phi}$ ，其定义如下</p>
<script type="math/tex; mode=display">
  E_{\phi} :=\phi(E(v))</script><p> 为最小化体素误差，每个将从病变组织分离的黏连结构尽可能地在靠近病变组织切除，这对于分割处理是至关重要的。在归一化的距离map $E_{\phi}$上 使用阈值来实现腐蚀操作。 如果腐蚀强度值存在如下:</p>
<ul>
<li><p><strong>C1</strong>:大到足以切除所有靠近连接点的邻接结构</p>
<ul>
<li><strong>C2</strong>: 与边界上限 $E_{\phi}(S’)$ 一致</li>
</ul>
<p>其中最小的接着会被称为<strong>最优腐蚀强度 $\theta_{\star}$ </strong> ，在满足条件 <strong>C1</strong> 和 <strong>C2</strong>的同时尽可能保留病变组织的原始形状。它是接下来提出的模型的最优。我们需要研究的是，在何种情况下存在此类最  优强度，以及如何高效地计算。</p>
</li>
</ul>
<ol>
<li><strong>血管连接性模型</strong>：这些问题的答案取决于邻接结构和目标病变区域本身的形态学特征。我们为此提出了一个讨论框架，我们定义为<strong>血管连接性模型</strong>，对血管黏连做了如下两个假设：</li>
</ol>
<ul>
<li><p><strong>A1</strong>：每个肺组织最终源于肺门区域</p>
</li>
<li><p><strong>A2</strong>：每个组织的半径随着其到肺门距离的增加而单调递减。</p>
</li>
</ul>
<p>鉴于模型以及其他约束，总是存在一个最优强度的，并且也是可以高效计算的。</p>
<ol>
<li><strong>最优强度存在</strong>：由于我们期望算法能够区分非常规结节边界特征和邻接组织，我们需要利用先决条件P2:结节本身完全位于VOI内部并且没有触及其边界。由于假设A2，组织的半径在从肺门进入VOI之后不会增加。最终，它要么衰退为一个无法再追踪的度，要么再次离开了VOI。两种情况下，以特定腐蚀强度腐蚀的后果是，如果组织在某些点被腐蚀操作根除，此点之外的组织部分将会消失。</li>
</ol>
<p>如果一个或多个组织连接到一个结节，会出现两种情况（如上图）：组织在结节内部完结，或者穿过结节继续拓展。两种情况下，实体点的组织的半径需待确定，由于任意大于此半径的腐蚀强度不仅会在实体点将结节从组织打断，而且由于假设A2，完全抹去留在结节中的组织部分。这暗含了一个约束：如果VOI与模型一致，但是在实体点结节半径小于组织半径，条件C2无法满足，并且最优腐蚀强度不存在。换句话说，基于脉管组织模型 假设A1和A2，当且仅当结节内的实体点处任意黏连组织半径都小于 $\tilde{r}$ 时从黏连的脉管组织分离结节才有可能。由于这些情形非常少，（700例中不到5例）,所以有理由认为可以通过全局形态学腐蚀操作完成从脉管组织分离。</p>
<ol>
<li><strong>最优腐蚀强度计算</strong>：下一步是高效的计算最优强度。利用 $E<em>{\phi}$ ，条件可以以最优种子节点S’到VOI边界的路径来表述。以 $P</em>{N<em>2}^{\gamma}$表示 $N_2$ 中任意长度 $n$ 的所有的路径集合 $(v_0,v_1,v_2,…v_n)$ ，从 $v_0=S’$ 开始在VOI边界上（基于3D邻居关系 $\gamma$）的体素 $v_n$ 终止。那么最优腐蚀强度 $\theta</em>{\star}$ 可以被定义为 :</li>
</ol>
<script type="math/tex; mode=display">
 \theta _{\star} = max\lbrace min\lbrace E_{\phi}(v):v\in p \rbrace :p\in P^{\gamma}_{N_2}\rbrace</script><p>换句话说，$\theta <em>{\star}$ 沿着 $P^{\gamma}</em>{N<em>2}$ 路径中所有最小半径的最大半径。为了高效的计算 $\theta </em>{\star}$ ，在 $E_{\phi}$ 使用 一个动态的阈值区域增长方法 。从S’开始，阈值无上限，下限为1，达到VOI边界后一次递减。最优的腐蚀强度即为终止之前的最后一个阈值下限。</p>
<p>考虑到现实生活中的一些场景，离散效应，运动伪影，噪声和解剖异常。这可能会导致与假设A1（出现非常小的组织）和假设A2（关于半径单调性）的不符合。因而，在现实中原理上最优的腐蚀强度并不一定有最佳效果。为解决极小组织触及病变组织但无法全路径追踪到VOI边界的，结果值不仅仅aganist上限1剪切（确保腐蚀操作不会完全剔除病变组织），而且aganist人工下限 $\epsilon$ 。作为一种可能出现的单调性不一致的妥协，腐蚀强度值一般会比计算得到的最优值稍微高一点，如果检测到连接则加上一个偏移 $\mu$ ，从实际方法出发，有如下定义:</p>
<script type="math/tex; mode=display">
  \theta \_ := \lbrace\quad 1,\quad \theta_{\star}\gt 1\\ \epsilon ,\quad \theta _{\star} \lg \epsilon \\ \theta_{\star} +\mu,\quad otherwise</script><p>参数 $\mu$ 和 $\epsilon$ 是一种百分比形式独立定义于实际病变组织半径，由于它们会与腐蚀强度比如归一化的腐蚀阈值关联。$\mu$ 的一般值是 10%到30%之间，对于 $\epsilon$ 经验设置的最优值为25%。</p>
<ol>
<li><strong>腐蚀：</strong>既然已经决定了合适的强度，在移除距离低于绝对腐蚀阈值 $\circleddash _:=\phi ^{-1}(\theta_)$的非归一化、基本距离mapE上可以做腐蚀操作。第一张图的图e显示了在距离map上使用阈值 $\circleddash _$之后的结果，结果得到的腐蚀过后的结节mask $N_$定义如下</li>
</ol>
<script type="math/tex; mode=display">
  N\_:= \lbrace v\in N_2\|E(v)\ge \circleddash \_</script><p>注意到，此形态学操作通常无法成功将结节从胸壁上分离，由于肺连接通常更广，以及任意以必要强度完成的腐蚀操作要么会抹去任意非常规结节形态，要么移除整个结节。</p>
<p>鉴于腐蚀操作的目标是在连接点切除每个组织，组织的更远部分可能在膨胀操作之后然与结节相连。然而，通过重建这些阈值，这些组织部分以邻近关系 $\gamma$ 从结节部件中断开。考虑到目标结节部件是由修改后的种子点 <code>S&#39;</code> 唯一定义的，可以通过 $\gamma$ 连接性分析轻易得到。</p>
<ol>
<li><strong>二次距离变换</strong>：从 $N_$ 开始，开运算的第二步，膨胀操作使用了一个二次距离变换map D，将每个体素映射到其到腐蚀结节mask $N _$ 的距离：</li>
</ol>
<script type="math/tex; mode=display">
 D(v):= min \lbrace \|\|world(v)-world(v')\|\|_2:v' \in N\_</script><p>为了获得较好的结节近似，腐蚀阈值 $\circleddash _$ 本身初看起来是个在D上实现膨胀操作的上限阈值。然而，基于腐蚀强度，此结果可能会得到不精确的分割边界和在剪切得更精细的边界特征（下图左图）</p>
<p><img src="/images/blog/lungvolumne3.png" alt=""></p>
<p> 为避免这种欠分割，需要额外的边界重修，可以重现上图右边的精确的边界。</p>
<h3 id="1-5-边界重修"><a href="#1-5-边界重修" class="headerlink" title="1.5 边界重修"></a>1.5 边界重修</h3><p>为了纳入被前面步骤中腐蚀操作抹去的结节边界更小的不规则性，膨胀阈值 $\circleddash <em>{+}$ 的选择会比 $\circleddash \</em>$ 稍大。即 $\circleddash <em>{+}:=\circleddash \</em>+\delta$ 。膨胀操作的mask $N_+$ 定义为：</p>
<script type="math/tex; mode=display">
 N_+:= \lbrace v\in V:D(v)\lg \circleddash \rbrace</script><p>此步骤不仅添加了一些肺周围的薄壁组织和胸壁，这很容易通过与 $N<em>2$ 的交集运算来移除，同时也会无意将之前步骤已经移除过的组织纳入。我们因此接着计算初始分割和膨胀mask的交集，$I:=N_0\cap \partial  N</em>+$ 。用 结果 $I<em>{\partial}$ 中的 $\partial$ 来膨胀I，用来移除不需要的结构。最终的分割结果集 $N</em>{\star}$  定义为 </p>
<script type="math/tex; mode=display">
 N_{\star} := (N_+\cap N_2) /I_{\partial}</script><h3 id="1-6-交互式校验"><a href="#1-6-交互式校验" class="headerlink" title="1.6 交互式校验"></a>1.6 交互式校验</h3><p>分割步骤基于一些假设。一些特殊情况下，比如解剖异常，成像或运动伪影，或者与其他疾病相关的条件可能会导致不好的结果。这些情况下，需要医生的专业知识来交互式地纠正。为方便纠正，需要预定义一个带有启发式值域范围的形状参数。更新后的结果应该立即呈现，以完成交互式优化。</p>
<p>在提出的分割框架中，腐蚀强度参数满足这些条件。自动分割过程之后，初始值 $\theta_$ 可以在指定步骤迭代的增加或减少，可能的值域为0到100%之间。由于预处理，胸壁分割以及最优腐蚀阈值的计算，可以在交互式地腐蚀强度修改之后留出，腐蚀步骤之后仅需要对所有处理步骤的进行一次更新。因而，与初始分割步骤相比，更新所需的时间后续被减少。</p>
<h2 id="二-基于分割的部分体积分析方法（SPVA）"><a href="#二-基于分割的部分体积分析方法（SPVA）" class="headerlink" title="二 基于分割的部分体积分析方法（SPVA）"></a>二 基于分割的部分体积分析方法（SPVA）</h2><p>不使用分割和仅仅引用手工画在中心slice的结节中心的区域，意味着纯粹结节组织和周围薄壁组织的强度（密度）是可以从数据评估。对每个slice，通过累加每个slice内的体素强度和用纯粹的组织均值为权重，来计算容积。现存的一些研究证明，在CT图像上的海量的潜在的密度分析，并不适用于体内结节评估，由于黏连的高密度结构可能被算入结节容积。除此之外，结节和薄壁组织减去平均衰减需要完全自动化，同时保持鲁棒性和可靠性，以及在VOI内不被其他肺结构干扰。</p>
<p>为克服这些问题，我们的方法结合了先前的使用强度直方图分析的分割得到的形态学信息。某篇论文的模型中，假设暗示着所有体素的平均强度对于扫描和重建的变化是不变的参数。在一个已知强度的两种组织的双峰设置中，这种假设使得出现在任意容积内的组织容积均值变得可重现计算，使用平均频率直方图分析。为避免部分容积分析上的VOI中的脉管组织和胸壁区域的影响，使用分割结果。首先，由于容积均值只出现在结节边界，足以将部分容积分析限制在其直接的邻近。其次，所有连接到结节的高密度结构都在分割步骤被确定，并且可以在分析步骤明确的剔除。</p>
<p>SPVA分析定义如下：基于最终分析结果 $N<em>{\star}$ ，三个不同的区域，结节中心(nodule core NC)，薄壁组织区域(parenchyma area PC)，以及部分容积区域(partial volume PV)可以通过它们到最终mask $N</em>{\star}$ 边界的距离来自动确定。如下图</p>
<p><img src="/images/blog/lungvolumne4.png" alt=""></p>
<p>参数 $\triangle <em>{PC}$ 描述了PV内任意体素到分割边界的最大距离，然后定义区域PV，必须足够大来纳入所有结节组织非零比例的体素，必须足够小而不包含邻接的高密度结构的部分容积。同时，这些直接黏在结节上的结构在分割步骤已经确定，没有与结节相连的结构没有被删除。因此，如果位于PV区域内部，它们无法从容积上剔除。但是可以通过对每个scan和重构协议调整 $\triangle </em>{PC}$获得更加精确的结果，固定经验值2mm被证明是一个调和两方面的较好的折中。从NC和PC中减去平均衰减 $\mu <em>{SC}$ 和 $\mu</em> {PC}$，为PV内的体素对整个结节容积有个权重化的贡献，其中体素v的权重w被定义为</p>
<script type="math/tex; mode=display">
  w(v):= \lbrace 1\quad v\in NC \\ \frac{i(v)-\mu _{PC}}{\mu _{NC}-\mu _{pc}},\quad v \in PV \\ 0 ,\quad otherwise</script><p>其中 $i(v)$ 代表了在输入VOI中体素v的密度值。接着，最终结节容积通过累加所有VOI内的体素权重乘以体素容积得到。 </p>
<p>对重要的场合，区域NC太小二无法减去均值 $\mu <em>{NC}$ ，没办法决定纯粹的结节组织衰减。由于 增加NC会导致一些不纯的体素被纳入，在欠评估的结节密度和过度评估的结节容积中，对solid结节使用预定义的密度值来替代 $\mu </em>{SC}$。</p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>肺部CT大结节检测</title>
    <url>/2017/03/09/lungseg-bignodule-detect/</url>
    <content><![CDATA[<h2 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0 摘要"></a>0 摘要</h2><p><strong>目的</strong>：当前使用CAD系统检测对肺结节检测时，只在较小结节上有很好性能，通常不能检测更少的更大的结节的，这些可能是癌性。我们专门为大于10mm的solid结节检测设计了算法。</p>
<p><strong>方法</strong>：我们提出的检测流程，初始时由三维肺分割算法，通过形态学操作包含入黏着在肺壁上的大结节。额外的处理是mask out肺空间的外部结构，以确保肺和实质结节有着相似的轮廓。接着，通过多阶段的阈值和形态学操作获得结节候选，以检测大结节和小结节。对每个候选分割之后，计算出一个基于强度、形状、blobness、和空间结构的24个特征的集合。使用一个径向偏置SVM分类器对结节候选分类，在完全公开的肺部图像数据集上使用10折交叉验证评估性能。</p>
<p><strong>结果</strong>：本文提出的CAD系统弄获得了98.3%(234/238)的灵敏度，94.1%(224/238)大结节的准确率，平均4.0和1.0 FPs/scan。</p>
<h2 id="一-数据集"><a href="#一-数据集" class="headerlink" title="一  数据集"></a>一  数据集</h2><p>本论文中，使用的是LIDC-IDRI数据集。数据集包含了来自7个机构的1018个病例异构数据。在第一轮盲读阶段，每个可疑病变被标注同时被分类为 $non-nodule,nodeul&lt;3mm,nodule\ge 3mm$ 。对于 $nodules\ge 3mm$ 的被充分表征为3D分割，会提供其半径和形态学特征描述。</p>
<p>我们使用区域厚度小于等于2.5mm的，比这更厚的区域数据被丢弃，因为我们将这些数据定义为质量不足。尽管slice厚度为3mm的大结节依然可以被检测到，最近的临床指南推荐使用thin-slice,因而我们也使用thin-slice CT scan。除此之外，包含不连通的slice空间的也被丢弃，最后获得了888个适于分析的CT scan。</p>
<p>下表是数据概况</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Aggrement levels</th>
<th>Nodules $\ge 3mm$</th>
<th>Nodules $\ge 10mm$</th>
<th>solid nodules $\ge 10mm$</th>
</tr>
</thead>
<tbody>
<tr>
<td>At least 1</td>
<td>2290</td>
<td>393</td>
<td>322</td>
</tr>
<tr>
<td>At least 2</td>
<td>1602</td>
<td>325</td>
<td>277</td>
</tr>
<tr>
<td>At least 3</td>
<td>1186</td>
<td>269</td>
<td>238</td>
</tr>
<tr>
<td>At least 4</td>
<td>777</td>
<td>199</td>
<td>172</td>
</tr>
</tbody>
</table>
</div>
<p>从888个scan中制作了36378个标注。由于结节可能被多个读者标注，不同的读者的标注在近似位置（小于标注的半径之和）的会被合并。只对 $nodule\gt 3mm$的结节标注做合并，因为这个作为引用标准。其半径、容积、和坐标都平均化。我们仅仅选择被分类为潜在恶性结节的作为大结节的引用标准。根据Dutch-Belgian NELSON肺癌实验，潜在恶性结节指的是容积大于$500mm^3$的，其对应的半径为近似 10mm。然后，选择被大多数读者(4个中的3个)接受的结节，获得了269个结节。并且设计CAD系统只关注 solid 结节的。由读者打分的各种各样的形态学特征用来定义结节类型。本文中，只有当大多数读者给出的上下文特征分数高于3(1=ground-glass/nonsolid,3=part-solid,5=solid)结节被认为是solid。最终获得了238个solid结节，这形成了分析所用的最终结节集合。</p>
<p>评估过程中，CAD标记的位置在标注半径范围内的可被认为是命中了。如果CAD标记命中了引用集中的结节，则被分类为正样本(True  Positive)。不在病变引用集上的标记(即$nodule\ge 10mm$只被一两个放射医师接受，$nodules\gt 10mm$,subsolid nodules,non-nodules)被认为是不相关的，且没有被记入为假阳性(False Positive).</p>
<h2 id="三-方法"><a href="#三-方法" class="headerlink" title="三 方法"></a>三 方法</h2><p>算法的主要流程如下，算法的开始时肺分割。肺分割被细化以将那些与肺壁相连的结节包含进来。</p>
<p><img src="/images/blog/lung_bignodule1.png" alt=""></p>
<p> 之后的后续处理步骤是移除肺外部的背景并重新将图像抽样为各向同性分辨率。候选检测阶段确认大结节候选的位置并为每个候选构建segmentation。使用特征抽取来获得候选的判别特征，这在分类阶段将被用来将候选分为结节和非结节。</p>
<h3 id="3-1-肺分割"><a href="#3-1-肺分割" class="headerlink" title="3.1 肺分割"></a>3.1 肺分割</h3><p>CAD系统的第一步的肺分割用来决定肺的ROI(region of interest)。主流的算法依赖于一种基于阈值的方法。由于相似的强度特征，粘附在肺壁的结节通常无法包含入肺分割内。</p>
<p>我们使用Rikxoort提出的算法作为初始的肺分割segmentation。方法由这些组成：大气道抽取、肺分割、左右肺分割和segmentation 平滑。当肺分割算法失败时，我们通过手动衡量输入参数，比如气管的seed point来更正肺分割。</p>
<p>为了在肺分割segmentation包含入大的肺结节，使用了额外的细化的肺分割segmentation。肺结节被剔除的区域通常看起来像在肺segmentation的表面上有个洞。我们实验了两种方法<strong>(1)</strong>使用形态学逻辑滚球rolling-ball操作<strong>(2)</strong>在rolling-ball操作之后做扩张操作。所有的形态学操作使用的是球形结构元素。结构元素的半径$d<em>{struct}$设置为一个x维度在输入scan中的比率。我们评估了$d</em>{struct}=\lbrace 2\%,3\%,4\%,6\%,8\%\rbrace $ 的rolling-ball操作和$d_{struct}=\lbrace 1\%,2\%\rbrace $ 的扩张操作。</p>
<h3 id="3-2-预处理"><a href="#3-2-预处理" class="headerlink" title="3.2 预处理"></a>3.2 预处理</h3><p>肺segmentation用作mask，segmentation之外的区域被设置为肺组织的平均强度(-900HU)。这可以避免肺病变看起来与其内部的结节非常不同，这就要求有不同的或者额外的特征来准确检测这些结节。肺segmentation使用高斯过滤器重新取样为1.0mm的    各向同性分辨率(isotropic resolution)。</p>
<h3 id="3-3-候选检测"><a href="#3-3-候选检测" class="headerlink" title="3.3 候选检测"></a>3.3 候选检测</h3><p>候选检测步骤旨在局部化所有肺内部的结节。这个任务之所以艰难，是因为结节在形态上、尺寸上和强度上变化范围太广。本文中，候选检测由三部分组成:初始候选检测，连通组件分析，结节检测细化。</p>
<h4 id="3-3-1-初始候选检测"><a href="#3-3-1-初始候选检测" class="headerlink" title="3.3.1 初始候选检测"></a>3.3.1 初始候选检测</h4><p>与其他相似度密度较高的结构(主要是血管)相比，大结节通常有着十分不同的形态学特征，一个简单的阈值和形态学开运算就足够充分检测大部分大结节。然后，由于它们的尺寸，与其他非密度结构（大部分是胸膜壁和脉管系统）相比大结节倾向于内部相连。这使得检测器的参数选取变得复杂，尤其是形态学开运算的结构元素的尺寸。相连结构的尺寸可能变化范围广。举一个例子，一个大的结构元素需要移除结节上的大结构的黏着(attachment of large structure)，但是会导致小结节无法检测。</p>
<p>为检测不同尺寸的候选，使用多阶段阈值和形态开运算。强度阈值为-300HU来区分solid nodule。每个阶段，顺序使用半径为9,7,5或者3mm的球形结构的扩张操作，并产生了中间候选mask。候选检测始于大结构半径并渐渐使用更小的半径。为防止先前候选与新候选合并，在处理下一阶段之前先前候选使用了一个3mm的保障边界。在阶段n计算候选检测，新的中间mask与阶段n-1的输出mask合并，使用逻辑与操作。下图示意了候选检测的输出。每一阶段使用不同的阈值和形态扩张操作，mask的结果与上一阶段的mask合并。第一行是检测大结节，第二行是小一点的结节检测过程。</p>
<p><img src="/images/blog/lung_bignodule2.png" alt=""></p>
<h4 id="3-3-2-连通组件分析"><a href="#3-3-2-连通组件分析" class="headerlink" title="3.3.2 连通组件分析"></a>3.3.2 连通组件分析</h4><p>使用初始候选检测之后，所有相连体素使用连通组件分析聚合为候选。为了移除cluster size超出目标范围的，我们丢弃了cluster容积小于 $268mm^3$的和大于 $33524mm^3$的，其分别对应的完美球形半径为8mm和40mm。</p>
<h4 id="3-3-3-结节segmentation"><a href="#3-3-3-结节segmentation" class="headerlink" title="3.3.3 结节segmentation"></a>3.3.3 结节segmentation</h4><p>初始候选指示了候选cluster的坐标。作为候选的一个准确的容积和形态学评估，量化结节特性尤为重要，我们使用了由Kuhnigk提出的robust结节分割方法。给定cluster上的立方块输入容积和seed point，算法进行区域增长并使用有效的形态学开运算来从血脉组织和胸腔壁分离结节。对每个候选cluster，从cluster的主轴(major axis)上获得seed point。VOI(volume of interest)被定义为初始边长为60mm的在cluster附近的立方块。立方块的尺寸是自动适应的，必要的话，推荐更大的结节。</p>
<p>为进一步移除大于或小于预定目标的候选，我们使用3.3.2节中相同的阈值。为避免结节中出现重复结节，与任何已经被接受的segmentation位置小于10mm的都被丢弃。结节segmentation的结果集合将进一步用作特征抽取和分类。</p>
<h3 id="3-4-特征抽取"><a href="#3-4-特征抽取" class="headerlink" title="3.4 特征抽取"></a>3.4 特征抽取</h3><p>特征抽取用来获得可以区分结节和非结节的特征。定义了四个不同集合的特征：强度、cluster、blobness、和上下文特征。下表展示了所有特征的概览</p>
<p><strong>强度特征</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>强度特征</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-3</td>
<td>Density inside candidate segmentation (mean,stdev, entropy)</td>
<td></td>
</tr>
<tr>
<td>4-6</td>
<td>Density inside bounding box (mean, stdev, entropy)</td>
<td></td>
</tr>
<tr>
<td>7-9</td>
<td>Density around candidate segmentation (mean,stdev, entropy)</td>
<td>within 8mm outside segmentation</td>
</tr>
</tbody>
</table>
</div>
<p><strong>cluster feature</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>cluster feature</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>Volume of candidate segmentation $V_{cand}$</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Diameter of candidate segmentation $D_{cand}$</td>
<td>The longest diameter on axial plane</td>
</tr>
<tr>
<td>12</td>
<td>Sphericity: ratio of candidate’s volume inside sphere S to the volume of sphere S</td>
<td>Sphere S is centered at the candidate location with diameter $D_{cand}$</td>
</tr>
<tr>
<td>13</td>
<td>Compactness1: $V_{cand}/(dim_x ·dim_y ·dim_z)$</td>
<td>$dim_i$ is the width of bounding box indimension i</td>
</tr>
<tr>
<td>14</td>
<td>Compactness2: $V_{cand}/((max(dim_x, dim_y, dim_z))^3$)</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Blobness feature</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>Blobness feature</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>15</td>
<td>Maximum flter response</td>
<td>Features are computed using the flter</td>
</tr>
<tr>
<td>16-17</td>
<td>Filter response inside candidate segmentation(mean, stdev)</td>
<td>response of Li blobness flter(scale: 2, 5, and 8 mm)</td>
</tr>
<tr>
<td>18-19</td>
<td>Filter response inside bounding box (mean, stdev)</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Context feature</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>Context feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>20</td>
<td>Distance to pleural wall</td>
</tr>
<tr>
<td>21-23</td>
<td>Distance to carina in X, Y, and Z</td>
</tr>
<tr>
<td>24</td>
<td>Distance to top of the lung in Z</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3-4-1-强度特征"><a href="#3-4-1-强度特征" class="headerlink" title="3.4.1 强度特征"></a>3.4.1 强度特征</h4><p>强度特征用来量化候选segmentation内外区域的强度特征。特征集直接计算自各向同性（isotropically）重新取样的CT scan。使用了三个不同的区域<strong>(1)</strong>候选segmentation内的区域<strong>(2)</strong>候选segmentation的bounding box内的区域<strong>(3)</strong>在候选segmentation周围距离小于8mm的区域。每个区域，计算均值、标准差和体素强度熵。</p>
<h4 id="3-4-2-cluster特征"><a href="#3-4-2-cluster特征" class="headerlink" title="3.4.2 cluster特征"></a>3.4.2 cluster特征</h4><p>cluster 特征从候选segmentation中计算得来。这些特征集由半径、容积、球形性、紧凑性1(compactness1)和紧凑性2(compactness2)。</p>
<p>半径是通过segmentation的轴状位的部分最长的轴得到。容积通过计算cluster size的$mm^3$。为计算球形性，与候选有着相同容积的球S在候选质心处构建。球形性定义为候选容积在球S内与球形S的容积的比例。紧凑性为候选容积与候选segmentation周围的bounding box容积的比例。使用了两个不同的boundiing box。紧凑性1使用的bounding box为在全部x,y,z维度包容候选segmentation的最小box。紧凑性2使用的bounding box定义为立方块，其尺寸为候选segmentation的最大维度的大小。</p>
<h4 id="3-4-3-blobness特征"><a href="#3-4-3-blobness特征" class="headerlink" title="3.4.3 blobness特征"></a>3.4.3 blobness特征</h4><p>blobness特征广泛用于增强结节结构和提高结节检测的灵敏度。我们使用的是由<strong>Li,Sone 和 Doi</strong>开发出来的结节增强过滤器，并偶从过滤图中抽取blobness特征。</p>
<p>算法通过对输入图像进行高斯kernel的二阶导数做卷积来计算Hessian矩阵。给定Hessian矩阵，三个特征值，定义为 $\lambda<em>1=f</em>{xx},\lambda<em>2=f</em>{y},\lambda<em>3 = f</em>{xy}$，其中 $\lambda _1&gt;\lambda _2&gt;\lambda _3$。增强过滤器的最终输出是通过计算 $Z(\lambda _1,\lambda _2,\lambda_3)=|\lambda _3|^3/|\lambda _1|\quad if\quad \lambda _1&lt;0,\lambda _2&lt;0,\lambda _3&lt;0$；否则为0。为了获得范围分布广泛的结节尺寸，使用来了多尺寸增强过滤器。我们对高斯kernel定义了三种(2mm,5mm,8mm)，这是基于要增强的目标结节的经验值。结果会有三种不同的输出图像，是一种从所有图像中选择最大值的结合。</p>
<p>blobness特征的抽取通过衡量<strong>(1)</strong>过滤器的最大输出<strong>(2)</strong>结节segmentation内部的输出的均值和标准差<strong>(3)</strong>bounding box内部输出的均值和标准差。</p>
<h4 id="3-4-4-上下文特征"><a href="#3-4-4-上下文特征" class="headerlink" title="3.4.4 上下文特征"></a>3.4.4 上下文特征</h4><p>上下文特征描述了候选与其他肺结构如胸膜壁，隆突和肺上部的相对位置。根据它们的位置，肺结节和假阳性候选可能有着不同的形态学特征和恶性概率。结节粘附于更加刚性的结构上时，结节更可能是细长的。<strong>Horeweg</strong>的研究表明，大部分病变结节位于costal-hilar半径的外部的三分之一的上叶（at outer one-third of the costal-hilar diameter and at upper lobes ）。意味着这些区域应该引起注意。到carina的距离和肺的顶部区域需要给予更多的权重。</p>
<p>为计算相对于胸膜壁的距离，我们做了在肺分割内做了距离变换并抽取候选质心中心的值。carina点的检测方法是，找到轴向(axial)区域中气管被一分为二的地方，在分叉处周围选取气管区域，并抽取其质心。从carina到坐标轴的相对X，Y，Z距离作为特征。从候选到肺顶部的相对距离的计算方法为，计算候选的Z轴世界坐标到肺分割的top slice的Z轴世界坐标的距离。</p>
<h4 id="3-4-5-分类器"><a href="#3-4-5-分类器" class="headerlink" title="3.4.5 分类器"></a>3.4.5 分类器</h4><p>对特征向量的分类器，使用了径向偏置核函数SVM-RBF。其中的C和伽马参数在训练集上的网格搜索的10折交叉验证得到的。C定义了正则化参数，伽马定义了RBF核函数的宽度。搜索区间为 $C={2^1,2^2,2^3,…2^12}$，并且$\Gamma ={2^{-12},2^{-11},…2^{-5}}$。本文使用的是LIBSVM实现。分类之前先对所有特征进行正则化，均值为0，单位标准差。</p>
<h2 id="四-结果"><a href="#四-结果" class="headerlink" title="四 结果"></a>四 结果</h2><p>评估CAD系统检测大结节的性能。在病人级别的10折交叉验证，分类器在嵌套的交叉验证中优化得到。</p>
<h3 id="4-1-肺分割"><a href="#4-1-肺分割" class="headerlink" title="4.1 肺分割"></a>4.1 肺分割</h3><p>888个scan中有12个分割失败，因为气管处的seed point没有被正确检测到。这可以通过手动提供seed point来修正。下表显示了在CAD系统上使用细化的肺分割之后的影响。肺结节的位置坐标如果在segmentation内部，则其分类为包含在肺segmentation内。算法的评估方法为，被检测到的结节的数量和候选的数量。</p>
<p><strong>Without additional lung segmentation refinement</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rolling ball(kernel size (% of image))</th>
<th>Dilation(kernel size (% of image))</th>
<th>Inside segmentation (%)</th>
<th>After candidate detection(%)</th>
<th>Candidates/Scan</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>-</td>
<td>84.9</td>
<td>87.8</td>
<td>39.5</td>
</tr>
</tbody>
</table>
</div>
<p><strong>With additonal lung segmentation refinement</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rolling ball(kernel size (% of image))</th>
<th>Dilation(kernel size (% of image))</th>
<th>Inside segmentation (%)</th>
<th>After candidate detection(%)</th>
<th>Candidates/Scan</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>-</td>
<td>84.9</td>
<td>87.8</td>
<td>39.5</td>
</tr>
<tr>
<td>4</td>
<td>-</td>
<td>97.1</td>
<td>98.3</td>
<td>47.4</td>
</tr>
<tr>
<td>6</td>
<td>-</td>
<td>98.3</td>
<td>99.2</td>
<td>56.6</td>
</tr>
<tr>
<td>8</td>
<td>-</td>
<td>99.6</td>
<td>98.7</td>
<td>63.2</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>88.7</td>
<td>95.4</td>
<td>120.7</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>99.6</td>
<td>97.9</td>
<td>133.1</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>100.0</td>
<td>96.6</td>
<td>145.5</td>
</tr>
</tbody>
</table>
</div>
<p>上表显示没有使用额外的细化算法时，肺分割只有84.9%的大结节被包含入。使用额外的细化算法时可以减少被排除的结节数直至所有的结节都被包含入内。但是也可以看到，细化算法会增大肺segmentation，它可能会引入不相关区域(比如肺壁,hilar)，这会恶化候选检测的性能。因而，我们同时评估了候选检测的灵敏度。数据集中至少3个放射医师标注的，包含全部 $nodules\ge 10mm$的287个scan的数据集。实验表明，初始肺分割之后，使用 $d<em>{struct}=6%$ rolling-ball操作和 $d</em>{struct}=0%$的扩张操作，可以使得候选检测获得最高的灵敏度和差不多数目的 candidates/scan。此配置应用于后续的实验中。</p>
<h3 id="4-2-候选检测"><a href="#4-2-候选检测" class="headerlink" title="4.2 候选检测"></a>4.2 候选检测</h3><p>888个 CTscan中，候选检测平均生成48.3 candidates/scan，包括了99.2%（236/238）的全部大结节。此集合用作进一步的分类 任务。不同准入水平的候选检测性能如下表。结节的候选检测的灵敏度随着放射医师的准入水平的上升而增加。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Aggrement levels</th>
<th>Solid nodules &gt;10mm</th>
<th>Detected nodules (%)</th>
<th>Candidates/scan</th>
</tr>
</thead>
<tbody>
<tr>
<td>At least 1</td>
<td>322</td>
<td>97.2</td>
<td>48.3</td>
</tr>
<tr>
<td>At least 2</td>
<td>277</td>
<td>98.9</td>
<td>48.3</td>
</tr>
<tr>
<td>At least 3</td>
<td>238</td>
<td>99.2</td>
<td>48.3</td>
</tr>
<tr>
<td>At least 4</td>
<td>182</td>
<td>100.0</td>
<td>48.3</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-3-特征抽取和分类"><a href="#4-3-特征抽取和分类" class="headerlink" title="4.3 特征抽取和分类"></a>4.3 特征抽取和分类</h3><p>CAD系统在包含大结节数据集上的不同准入水平的FROC曲线如下图左图所示，右图表示的 是CAD系统包含或不包含不相关的发现(被当做false positive假阳性的)时的性能。假阳性的数目现实的是求对数之后的结果。</p>
<p><img src="/images/blog/lung_bignodule3.png" alt=""></p>
<p>为了量化比较，不同假阳性率的平均灵敏度如下表。CAD系统识别分别在 1FPs/scan和 4FPs/scan时识别了94.1%(224/238)和98.3%(234/238)的大结节。注意到分类阶段的最大灵敏度收敛到候选检测阶段的灵敏度，为99.2%(236/238)。这意味着与检测到的候选相关的，分类阶段的准确分类率在 1FP/scan和4FPs/scan分别为 94.9%和99.1%。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Agreement levels</th>
<th>1/8</th>
<th>1/4</th>
<th>1/2</th>
<th>1</th>
<th>2</th>
<th>4</th>
<th>8</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>At least 1</td>
<td>0.773</td>
<td>0.804</td>
<td>0.842</td>
<td>0.879</td>
<td>0.913</td>
<td>0.950</td>
<td>0.960</td>
<td>0.874</td>
</tr>
<tr>
<td>At least 2</td>
<td>0.841</td>
<td>0.866</td>
<td>0.895</td>
<td>0.924</td>
<td>0.949</td>
<td>0.978</td>
<td>0.982</td>
<td>0.920</td>
</tr>
<tr>
<td>At least 3</td>
<td>0.874</td>
<td>0.895</td>
<td>0.916</td>
<td>0.941</td>
<td>0.962</td>
<td>0.983</td>
<td>0.992</td>
<td>0.938</td>
</tr>
<tr>
<td>At least 4</td>
<td>0.929</td>
<td>0.940</td>
<td>0.956</td>
<td>0.978</td>
<td>0.995</td>
<td>1.000</td>
<td>1.000</td>
<td>0.971</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-4-与已有的CAD系统对比"><a href="#4-4-与已有的CAD系统对比" class="headerlink" title="4.4 与已有的CAD系统对比"></a>4.4 与已有的CAD系统对比</h3><p>ISICAD系统获得了ANODE09的最高排名，用作对比。两个CAD系统的FROC曲线如下图，候选检测阶段检测到99.2%(236/238)(本文算法)和84.9%(202/238)(ISICAD)。在 1FP/scan时，63.0%(150/238)更多的大结节被本文提出的算法正确分类。不过要注意的是两个CAD系统为不同类型的结节设计的。</p>
<p><img src="/images/blog/lung_bignodule4.png" alt=""></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>肺结节自动分割:使用图像局部特征【论文笔记】</title>
    <url>/2017/03/08/lungseg-localfeature/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>算法使用图像的局部特征，如shape index和弯曲度，用以检测肺体积内的候选结构，然后再使用两个后续的KNN分类器来剔除假阳性样本。</p>
<p>肺结节检测系统在三个数据集上训练和测试，这三个数据集来源于一个大规模筛选实验。数据集的构建，为了评估算法既考虑挑选的随机性，又考虑到较高概率出现结节的需求。此系统在随机挑选的813scans中灵敏度为80%，平均一个scans4.2个假阳性。</p>
<h2 id="二-处理方法"><a href="#二-处理方法" class="headerlink" title="二 处理方法"></a>二 处理方法</h2><p>下图展示了结节检测的大体流程</p>
<p><img src="/images/blog/lungseg_local1.png" alt="大体流程"></p>
<p> 需要注意的是，结节检测步骤需要很多经验阈值，这些经验阈值基于一个完全独立的小测试集。</p>
<h2 id="2-预训练"><a href="#2-预训练" class="headerlink" title="2 预训练"></a>2 预训练</h2><h3 id="2-1-图像数据子抽样"><a href="#2-1-图像数据子抽样" class="headerlink" title="2.1 图像数据子抽样"></a>2.1 图像数据子抽样</h3><p>第一步是下采样以提高算法速度。使用整副图像对最终结果帮助极小，反而非常消耗计算。下采样利用了block-average，比如图像的矩阵尺寸是512x512，减小到256x256，slices的数目减少以形成各向同性采样数据。线性插值用来体素位置(一个scans其实是个立方体数据)的灰度值。下采样scans的slices数目从149到428不等，平均每个scans有223个slices。</p>
<h3 id="2-2-肺容积切割"><a href="#2-2-肺容积切割" class="headerlink" title="2.2 肺容积切割"></a>2.2 肺容积切割</h3><p>第二个步骤是将肺部从周围组织中分割出，此步骤是紧接上一步骤，从子抽样中进行的。此切割获得的mask可用来确保结节检测仅限于肺容积内部。这个过程有两个好处，其一是减小计算时间，其二是避免在肺容积外去检测。</p>
<h2 id="3-初始候选检测"><a href="#3-初始候选检测" class="headerlink" title="3 初始候选检测"></a>3 初始候选检测</h2><p>初始候选检测的流程图如下</p>
<p><img src="/images/blog/lungseg_local2.png" alt="初始候选检测"></p>
<h3 id="3-1-shape-index-and-curvedness"><a href="#3-1-shape-index-and-curvedness" class="headerlink" title="3.1 shape index and curvedness"></a>3.1 shape index and curvedness</h3><p>结节检测使用了<code>shape index(SI)</code>和<code>curvedness</code>特征来检测初始结节候选(Koenderink, 1990)  。这些是逐个体素计算的3D局部图像特征，基于局部衰减值和图像容积内每个点的表面拓扑性。SI和CV从主曲率 $k_1$ 和 $k_2$ 中得出，但是有去耦拓扑形状和曲率大小的优势。结节检测中我们感兴趣的是那些<strong>明显球形的体素</strong>和<strong>球体半径在合适范围</strong>的。每个体素上的shape index和curvedness，使用主曲率 $k_1$ 和 $k_2$ 的计算方式如下</p>
<script type="math/tex; mode=display">
 SI =\frac{2}{\pi} arctan(\frac{k_1+k_2}{k_1-k_2}) \\
CV = \sqrt{k_1^2+k_2^2}</script><script type="math/tex; mode=display">
 K = \frac{F_{xx}\cdot F_{yy}-{F_{xy}}^2}{(1+{F_x}^2+{F_y}^2)^2}</script><p>肺容积内部所有的体素都要计算主曲率 $k_1$ 和 $k_2$ 都要计算，使用图像高斯过滤器 $\sigma =1$模糊的一阶和二阶导数。这个 $\sigma$ 的值是经验设置的以减少噪声同时不移除重要的结构细节。</p>
<h3 id="3-2-seed-point检测"><a href="#3-2-seed-point检测" class="headerlink" title="3.2 seed point检测"></a>3.2 seed point检测</h3><p>一旦知道了图像的SI和CV值，就可以获得其seed point点集合，这可以根据下表经验阈值来得到。在SI和CV阈值范围内的体素将会被选为seeds。这些seed points代表体素可能位于结节表面，并且其局部性值得进一步分析。胸膜表面的5个体素位置的SI阈值将会小一些，是为了增加肺部边缘的seed检测数目。这么做是因为胸膜结节的表面区域不明显，难以检查，并且其区域内的SI和CV值可能会受到邻接胸膜表面的拓扑结构影响。<br><strong>初始seed阈值</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Value</th>
<th>Upper threshold</th>
<th>Lower threshold</th>
</tr>
</thead>
<tbody>
<tr>
<td>SI</td>
<td>1</td>
<td>0.8(near pleural surface)</td>
</tr>
<tr>
<td></td>
<td></td>
<td>0.9(elsewhere)</td>
</tr>
<tr>
<td>CV</td>
<td>1</td>
<td>0.3</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-3-cluster-信息"><a href="#3-3-cluster-信息" class="headerlink" title="3.3 cluster 信息"></a>3.3 cluster 信息</h3><p>如今seed point扩大到形成VOI (voxels of interest)cluster。此扩张基于滞后阈值，使用下表中的边缘阈值。因此最终cluster包含的voxels(体素)，其SI和CV值都落在阈值范围区间内，并且是可使用链式的此类体素连接（使用six-connectively）到一个seedpoint。</p>
<p><strong>Hysteresis阈值</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Value</th>
<th>Upper threshold</th>
<th>Lower threshold</th>
</tr>
</thead>
<tbody>
<tr>
<td>SI</td>
<td>1</td>
<td>0.7(near pleural surface)</td>
</tr>
<tr>
<td>CV</td>
<td>1.3</td>
<td>0.2</td>
</tr>
</tbody>
</table>
</div>
<p>应当注意到一个完美的球形结构，最终cluster中的体素位于球形模糊表面。大量的cluster中心被当做这一阶段的兴趣点。</p>
<p>若某个cluster的原始seedpoint位于胸膜表面的5个体素之内，可以作为一个胸候选(?不是肺)。此阶段的cluster，若其容积(volumn)低于一个预先设置的阈值 $t<em>{vol}$ 则会被丢弃，因为如果不丢弃，后续的处理将会及其耗时，同时可能会引入假阳性样本。胸膜区域的候选 $t</em>{vol}$ 设置为4体素，剩下的候选中设置为15体素。</p>
<h3 id="3-4-cluster-merge"><a href="#3-4-cluster-merge" class="headerlink" title="3.4 cluster merge"></a>3.4 cluster merge</h3><p>此阶段，已经检测到大量的cluster。每一个代表图像中的一个区域表面，并且有理由假设一个真实的结构，比如结节可能有不止一个cluster 代表它。除非是那种特别大的结节，或者其形状怪异，这些结节彼此十分靠近。位置在三个体素内的cluster会被递归地合并，直至无法再合并为止。这个合并过程在那些相距7个体素的cluster之间会不断重复。下图展示了候选结构的合并过程。可以看到这个合并过程在几个后续point上重复。尽管后续阶段，只有少部分结构需要合并，这个过程主要有两个目的：<strong>(1)</strong>可以确保单个结节只会被单个检测，而不是被相邻的两个检测。<strong>(2)</strong>合并候选是假阳性的话，会产生奇怪的形状结构，这可以很容易被后续的分类步骤剔除。</p>
<p><img src="/images/blog/lungseg_local3.png" alt="大体流程"> </p>
<h3 id="3-5-候选位置调整"><a href="#3-5-候选位置调整" class="headerlink" title="3.5 候选位置调整"></a>3.5 候选位置调整</h3><p>此处候选位置会被检查并调整以确保它们处于局部最明亮的位置。这十分重要，因为结节的位置由大量的voxels(体素) cluster中心定义的，并且也不经常是结节中心。尤其是胸膜结节或者体素cluster在结节表面的中心位置的。位置调整过程使用原始候选位置的三个体素的最大距离检查所有的局部点。每个局部点，计算它与周围6个相连邻居(six-connectively)点的平均灰度值。有最高平均灰度值的位置将会被选为新的候选位置。局部平均有利于避免选择了高亮的噪音体素。</p>
<h2 id="4-假阳性剔除"><a href="#4-假阳性剔除" class="headerlink" title="4 假阳性剔除"></a>4 假阳性剔除</h2><p>假阳性剔除由两个连续使用KNN分类器分类步骤组成。数据的属性表明其并没有很好的线性分类性能，使用SVM分类器得到的结果比KNN要差。所有情况中，$k$ 设置为训练集样本数目的(奇数)平方根。对 $k$ 值的实验并不能再开发阶段获得性能提升(K值应该就确定为样本平方根)。分类之前，尽可能生成较好的训练集。生成过程如下图</p>
<p><img src="/images/blog/lungseg_local4.png" alt="假阳性剔除"></p>
<p> 对于初始分类器，使用小量的足够计算的特征是为了进一步减小候选数目，而不至于过头。初始分类之后剩下的候选已经足够小，可以使得最终分类器计算大量复杂特征。两个分类步骤的特征选择是Sequential Forward Floating Selection’ (SFFS)(Pudil et al.1994)。SFFS过程仅在训练集上使用留一法（从训练集里留出训练集和测试集），ROC曲线下的区域作为优化标准。第一个分类器选择的最大特征数定为15，第二个分类器为50。训练集上的留一法也用来决定第一个分类器的软分类器的后验概率阈值。这个阈值用来选择第一个分类器的哪些项会作为候选送到第二个分类步骤。这个阈值的选择要使得训练集中90%的真结节都可以正确的被分类器区分。</p>
<p>最终分类器也有个类似的后验概率，也是使得训练集上90%的真结节可以被 区分。结节检测完之后，这些结果会被保存，减小此阈值可以使得系统低灵敏度性能。论文的余下工作是此系统的<code>操作点</code>的变化。</p>
<h3 id="4-1-训练集和测试集的生成"><a href="#4-1-训练集和测试集的生成" class="headerlink" title="4.1 训练集和测试集的生成"></a>4.1 训练集和测试集的生成</h3><p>生成训练集的关键是应该收集哪种条件下的数据集，使得它们在测试阶段尽可能靠近。例如第二个分类阶段的训练集，应该是由被训练好的初始KNN分类器标记为正的候选组成。因此，我们使用了一个三步训练集生成过程，   最终训练集中<code>Steps123_c1</code>作为第一个分类器，<code>Steps123_c2</code>作为第二个分类器的训练集。过程如上图。</p>
<p>用于训练的scans随机分为三个差不多大小的组，<code>Scans_Step1</code>,<code>Scans_Step2</code>和<code>Scans_Step3</code>。第一步我们在<code>Scans_Steps</code>的图片上做的初始候选检测，并使用生成的候选(有ground-truth信息的)来创建第一步分类所需的特征。由于没有先验训练数据集，所以没法在<code>Scans_Step1</code>的数据集上训练KNN分类器。</p>
<p>第二步，我们使用<code>Scans_Step2</code>中的图像，如第一步一样，我们检测初始候选并构建第一个分类器的合适的训练集。接着，使用第一步构建的训练集来训练KNN分类器并做一个初始分类来减少假阳性样本数。第一个分类器之后剩余的候选合并之后用于构建一个训练集，该训练集包含了第二步分类所需的全部特征。</p>
<p>第三步使用<code>Scans_Step3</code>的图像，并重复第二步的过程。不同之处在于，第一个分类器所使用的训练集来自第一步和第二步处理之后的数据的融合。将第三步独立出来而不是将第二、三步合并的原因是，在第三步融合的数据上训练时可以获得比第二步骤中的第一个分类器更精确的输出结果。</p>
<p>这种方式产生的训练集将包含更多的假样本，真样本较少。为此，我们在每个训练步骤的最后减小了阴性(假)数据分类，比如每个数据集中的阴性：阳性比例大致为3:1。这个比例被证明可以在测试阶段获得较优的结果。移除阴性样本使用的是缩合方法(Mitra etal 2002)，为了不改变样本分布。此方法从一个较大的数据中从多种尺寸选择点来产生了一个小的表征子集。子集表征的准确率以原始数据集和缩减之后的数据集的密度评估误差来衡量。参数$K$值决定了浏览哪种尺寸的数据。本论文中 $K$ 的初始值设置为15，阴性样本集不断缩减直至目标数目(3倍的阳性样本数)。然后参数 $K$ 再次衰减。此循环至 $k\le 2$ 或者数据集已经达到了目标数量。</p>
<h3 id="4-2-初始KNN分类器"><a href="#4-2-初始KNN分类器" class="headerlink" title="4.2 初始KNN分类器"></a>4.2 初始KNN分类器</h3><p>第一个KNN分类器，总共18个特征需要在特征选取之前计算。整个需要计算的特征列表如表下表</p>
<p>下表是 Features of the voxel cluster</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>a1</td>
<td>Cluster size (number of voxels)</td>
<td>-</td>
</tr>
<tr>
<td>a2</td>
<td>Compactness1,$\frac{ClusterSize}{(dim_x)(dim_y)(dim_z)}$</td>
<td>$dim_i=width\quad in \quad dim.i$</td>
</tr>
<tr>
<td>a3</td>
<td>Compactness2,$\frac{ClusterSize}{max_dim^3}$</td>
<td>$max_dim=max_i(dim_i)$</td>
</tr>
<tr>
<td>a4</td>
<td>Ratio max_dim:min_dim</td>
<td>min_dim = mini(dimi)</td>
</tr>
<tr>
<td>a5</td>
<td>Ratio max_dim:med_dim</td>
<td>med_dim = mediani(dimi)</td>
</tr>
<tr>
<td>a6</td>
<td>Ratio Amed:Amax where Amax, Amed and Amin are the eigenvalues for the eigenvectors of the cluster data by principal component analysis</td>
<td>-</td>
</tr>
<tr>
<td>a7</td>
<td>Ratio Amin:Amax</td>
<td>as for a6 above</td>
</tr>
<tr>
<td>a8</td>
<td>Sphericity, $\frac{num_cluster_voxels_in_sphere_S}{vol_sphere_s}$ where sphere_S is a sphere at the candidate location with radius r</td>
<td>-</td>
</tr>
<tr>
<td>a9</td>
<td>Ratio Sphericity:r</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>下表是 Features of voxels in spherical kernels at the candidate location</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>a10-a18</td>
<td>On grey-values over spherical kernels K: Average, Median, Standard-Deviation</td>
<td>Halfsizes of K: 1 (a10-a12), 3 (a13-a15), r (a16-a18)</td>
</tr>
</tbody>
</table>
</div>
<p>上面两张表其实是一张表，由于markdown不支持表合并。</p>
<p>，其ID将会在此文后续引用。详细的关于那个特征会被SFFS选取在不同的实验里面不相同，如下表</p>
<p>分类器一</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>A</th>
<th>B</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>a1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>a2</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>a3</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a4</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a7</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a8</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>a9</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>a10</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>a11</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>a12</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a13</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>a14</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a15</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a16</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>a17</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>a18</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Total</td>
<td>10</td>
<td>8</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p> 分类器二</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>A</th>
<th>B</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>b1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b5</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b7</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b8</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b12</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b13</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b21</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b22</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b24</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b25</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b26</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b27</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b28</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b29</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b36</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b39</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b40</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b41</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b44</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b45</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b46</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b49</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b52</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b54</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b55</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b56</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b57</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b58</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b62</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b64</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b65</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b66</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b67</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b68</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b70</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>b72</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b75</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b79</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b83</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b90</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b92</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b93</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b94</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b103</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b107</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b113</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b115</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b116</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b120</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b122</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b123</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b124</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b125</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b126</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b129</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b130</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b131</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>b134</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>b135</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Total</td>
<td>20</td>
<td>19</td>
<td>44</td>
</tr>
</tbody>
</table>
</div>
<p>此阶段计算的特征与初始候选检测(a1-a9)步骤的体素cluster几何属性相关，也与候选位置周围区域的灰度值(a10-a18)。体素cluster的形状提供了一个线索，即其结构是细长(比如，像血管)还是类似球状，同时其尺寸在不同的阴性和阳性候选也是极有价值的。灰度值特征都是使用候选位置周围的球形体素kernel校验过的，以移除不在不在不够明亮区域的结构。注意，表中体素cluster的半径以<code>r</code>表述。此半径的计算是用X，Y，Z轴三个方向的cluster的最大半径的除以3得到的均值为平均直径，除以2得到平均半径。其中的<code>kernel halfsize</code>指的是问题中的球形kernel体素半径。</p>
<p>第一个分类器结束之后，候选需要合并(4.4节)。尽管原始的合并步骤已经很详细了，后续的位置调整步骤(4.5节)意味着进一步的合并成为可能。尤其当位置调整过程中将两个候选置于相同位置时，需要检查合并的可能性。只有当第一分类器结束之后合并才可以做，因为此阶段的假阳性数目已经缩减了，同时阴性和阳性样本之间的非法合并的概率也大幅度缩减。4.5节讲述的是结节位置合并之后需要再次调整最明亮的局部点。</p>
<h3 id="4-3-最终KNN分类器"><a href="#4-3-最终KNN分类器" class="headerlink" title="4.3 最终KNN分类器"></a>4.3 最终KNN分类器</h3><p>最终分类器需要计算的135个特征全集在下表，其指明了ID。</p>
<p><img src="/images/blog/lungseg_local5.png" alt="KNN分类器"></p>
<p> 其中 $r_{reg}$指的是使用相同方式计算的包含结节切割体素的半径。</p>
<p>所有在第一个分类器中计算的特征都被重弄使用(<code>b1-b18</code>)，并且被第一个分类器计算的结构为真结节的后验概率，成为第二阶段的计算特征。</p>
<p>除了结构(<code>b47-b106</code>)中心假象球面中心的梯度方向和大小外，候选位置的球形kernel的SI和SV值特征此次会被相加(<code>b27-b46</code>)。大部分真结节，其形状是大致球形并且梯度范围也是大致对称的。对于真阳性，我们期望其梯度大小在生成的球形表面所有点都是差不多的，同时梯度方向类似于一个正常的球面。这些特征都是在不同尺寸的球面上随机挑选的点来计算的。特定尺寸的球面随机抽样的点的数目是预先经验值设置的。梯度方向是径向的一部分，并且在点p处被定义为 $r\dot g/|r||g|$，其中g是点p处的梯度向量，r是从球状中心到点p的半径向量。</p>
<p>我们使用文章(Kostis et al 2003)提出的一种算法切割候选结构，为了计算切割对象的特征。为了提高切割准确率，此切割过程只在候选位置的感兴趣区域ROI，但是注意是直接从全分辨率图像中抽取的而不是subsample版本的数据集中。之所以这么做是因为在subsample中切割更容易失败，为后续处理的方便起见切割体素的坐标被转换为subsample图像的等同的值。被计算的切割的体素特征与初始检测的体素cluster的大体相似。这些特征用来决定被切割的对象的尺寸和形状与真实结节是否相称，同时我们期望好的切割应该比从初始体素cluster获得更准确的特征信息。</p>
<p>最后，候选合并过程将不止一次地被执行（如前面描述的，第一步的KNN分类之后进行位置调整后可能需要进行新的合并操作），最终位置将会被调整到局部最亮点。此步骤主要是确保每个标注只找到一个检测（这样就不用将剩余准确检测算为假阳性了）</p>
<h2 id="5-结果"><a href="#5-结果" class="headerlink" title="5 结果"></a>5 结果</h2><h3 id="5-1-数据集A"><a href="#5-1-数据集A" class="headerlink" title="5.1 数据集A"></a>5.1 数据集A</h3><p>数据集A的训练集已经在上文有提及。初始的750个scans随机分为三组，每组250个，被用在训练集生成的三个步骤。移除scans中肺部切割失败的，三组分别包含了242，243，237个scans。</p>
<p>初始分类器的训练集包含5776个样本，1351个是真结节，第一个分类器的训练集包含3436个样本，819个真结节。</p>
<p>测试阶段开始于SFFS特征选取过程，如上表（ 分类器二）中为每个分类器选取的特征。第一个分类器处理了10个选取的特征，最终分类器使用了全部的20个特征。测试集中移除切割失败的scans之后还有813个。</p>
<p>数据集A中的结节检测结果如下表，其中的FROC曲线通过改变如下图的最终分类器的操作点得到的。</p>
<p>检测结果表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Number of Scans</th>
<th>813</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of annotations</td>
<td>1525</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sensitivity</td>
<td>FP per scan</td>
</tr>
<tr>
<td>After initial candidate detection</td>
<td>97.2%</td>
<td>649.0</td>
</tr>
<tr>
<td>After first classification</td>
<td>92.3%</td>
<td>77.3</td>
</tr>
<tr>
<td>After final classification</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-At around 4 FP per scan</td>
<td>80.0%</td>
<td>4.2</td>
</tr>
</tbody>
</table>
</div>
<p>最终分类器的操作点变化图</p>
<p><img src="/images/blog/lungseg_local5.png" alt="分类器"></p>
<h3 id="5-2-数据集B"><a href="#5-2-数据集B" class="headerlink" title="5.2 数据集B"></a>5.2 数据集B</h3><p>此数据集包含了的scans至少包含了一个结节(根据其尺寸和物理属性)。这些scans中的大部分包含了其他更小的或者不明显的结节，即此数据集的结节尺寸变化很广。</p>
<p>训练集的构造上文已经提及。600个训练scans被分为3组，每组200个scans，用于三个生成步骤。这个划分是完全随机的，除了有确定的14个病人被划分到第三个数据集<code>Scans_Step3</code>。去除肺切割失败的样本之后，三个数据集分别包含192,196和193个样本。</p>
<p>初始分类器的训练集包含了7868个样本，1715个是真结节，最终分类器包含4490个样本，其中1090个真正样本。样本分布如下:</p>
<p><img src="/images/blog/lungseg_local6.png" alt="样本分布"></p>
<p> 测试阶段的开始时进行SFFS特征选取，SFFS选取的特征遵循上面的表(分类器二)，用于给每个分类器。给初始分类器8个特征，给最终分类器19个特征。测试数据集在移除了非切割失败的样本之后有541个scans。</p>
<p>表8演示了数据集B的结节检测结果，并在上图Fig8中展示了FROC曲线。从数据集B中选取的结节如下图。这些结节都是由系统以不同程度的概率(后验概率)检测到的。</p>
<p><img src="/images/blog/lungseg_local7.png" alt="FROC"></p>
<p> 基于上图这样的小数据集很难概括，尤其是结节的3D结构信息很难从这些图像中获取。但是可以看到首行的结节是使用0.9的先验概率检测到的，通常有差不多的球形外形并且在局部有很明显的特征。第二行的结节先验概率为[0.45,0.9],它们有着相似的特征，并且平均每个scans有4个假阳性的灵敏度可以检测到。第三行的结节先验概率范围是[0.35,0.45]，边界线上的点周围检测准确率是每个scans有4个假阳性。球状外形不明显或者/同时结构或表面相靠近的结节较难检测到。最底层的一行结节是每个scans4个假阳性时无法检测到的，先验概率为小于0.35。可以看到这些结节有着怪异的形状或者位于混杂的结构或外形中，或者处于病理区域内。</p>
<h3 id="5-3-数据集C"><a href="#5-3-数据集C" class="headerlink" title="5.3 数据集C"></a>5.3 数据集C</h3><p>此数据集有着与数据集B十分相似的图像，但是只有容积大于$50mm^3$ 的结节被放入训练集和测试集。数据分组如5.2节一样，只不过只使用容积大于 $50mm^3$ 。初始分类器的训练集包含了3127个样本，748个真结节。最终分类器的训练集包含了1900个样本，465个真结节。</p>
<p>测试阶段，SFFS特征选取过程也是按照上表(分类器二)。初始分类器使用10个特征，最终分类器使用全部的44个特征。测试阶段的scans数目，在移除肺切割失败样本之后包含了541个scans。</p>
<p>其检测结果如下表，FROC如上图</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Number of Scans</th>
<th>541</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of annotations</td>
<td>768</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sensitivity</td>
<td>FP per scan</td>
</tr>
<tr>
<td>After initial candidate detection</td>
<td>98.2%</td>
<td>752.1</td>
</tr>
<tr>
<td>After first classification</td>
<td>92.2%</td>
<td>51.2</td>
</tr>
<tr>
<td>After final classification</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-At around 4 FP per scan</td>
<td>77.7%</td>
<td>4.2</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>从胸椎中自动subsolid 病变结节检测方法【论文笔记】</title>
    <url>/2017/02/27/lungseg-subsolid/</url>
    <content><![CDATA[<h2 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0 摘要"></a>0 摘要</h2><p>病变结节被描述为不透明圆形，半径大约为3cm。病变结节可以分为subsolid)或solid结节。solid结节在CT Scan上有均匀的软组织衰减。subsolid结节可以进一步分为non-solid结节(即ground glass 结节)和part-solid结节(即semi-solid结节)。non-solid结节表现出没有消除支气管或者血管的焦点区域模糊衰减幅度的增加。模糊区域衰减幅度的增加称为ground 玻璃不透明度，因此这些结节称为ground玻璃结节。part-solid或者semi-solid结节既包含了ground玻璃部分也包含了solid部分。在Early Lung  Cancer Action Project(<strong>ELCAP</strong>)中，基准线上81%的正样本发现为solid结节和19%的subsolid结节。</p>
<h2 id="一-数据"><a href="#一-数据" class="headerlink" title="一  数据"></a>一  数据</h2><p>本文使用的数据来源于NELSON实验，一个大的多中心肺癌筛选实验。NELSON实验是一个正在进行的随机对照试验建立，以测试 在高风险吸烟者之间进行的低剂量CT检测是否会导致肺癌死亡率25%的降低。</p>
<p>实验中7557个志愿者接受了多轮低剂量CT筛选。所有在CT scan看图检测期间发现的肺结节都会被记入数据库。放射剂师会记录所有检测到的结节的位置、半径和结节类型。数据集总共从4500个对象里挑选了大概20000个scans。我们收集的thin-slices，低剂量的CT检测数据来源于两个 NELSON实验的网站，至少一个subsolid(part-solid或non-solid)被标注了的。所有subsolid 结节标注半径小于5mm的都会被丢弃，因为彼时的临床指南说明这些结节不需要后续的CT。需要注意的是solid结节标注并没有被包含入分析中。</p>
<p>两个网站的所有的CT检测需要在螺旋模式下 16x0.75mm的准值。轴(Axial)状位1.0mm厚度的图像将会以0.7mm的增量重构为512x512的矩阵，平面体素大小变化范围为0.53mm到0.89mm。</p>
<p>移除小于5mm的subsolid 结节的标注之后，第一个网站中收集了103个病人 的209个scans。这些病人中，发现了122个subsolid结节(63个part-solid，59个non-solid)。注意到，肺结节可能会在多个scan中被标注，因为每个病人可能还有后续的检测。后面的检测中，209个scans中有了225个标注。这个数据仅用于CAD系统欧冠的训练和优化。</p>
<p>第二个网站中，移除小结节之后，得到56个病人的109个scans。这些病人中，发现了60个subsolid(32个part-solid，28个non-solid)，最后获得了109个病人的114个标注。此数据集由于是来源于不同的扫描仪，所以用于CAD系统的独立评估。</p>
<p>两份数据集的有效半径设置为5-34mm，中值为10.7mm。</p>
<h2 id="二-方法"><a href="#二-方法" class="headerlink" title="二 方法"></a>二 方法</h2><p>在检测流程之前，使用之前已经发布的肺、气道和血管的分割算法(van Rikxoort et al., 2009; van Ginneken et al., 2008; Dongen and Ginneken,2010)需要先走一遍。初始的检测阶段包括结节分割生成候选。为subsolid 结节候选定义了丰富的特征集合。先前发布的论文中(es (Kim et al., 2005; Zhou et al., 2006; Ye et al., 2007; Tao et al., 2009)的关于subsolid 结节CAD系统使用了密度，形状和上下文特征。本文中我们通过加入上下文特征添加了另外一种类别的特征。我们进行一种添加或剔除上下文特征来展示这些特征的额外的价值。我们试验了不同的分类器方案和不同的分类器以探索他们在分类性能上的影响。本文提出了一个泛化的结构化的评估方法来选取最佳分类方案。基于这些实验结果，CAD系统的最优配置会被选出，同时这个最终系统会被在独立的测试集上测试。</p>
<h3 id="2-1-候选检测"><a href="#2-1-候选检测" class="headerlink" title="2.1 候选检测"></a>2.1 候选检测</h3><h4 id="2-1-1-粗粒度候选检测"><a href="#2-1-1-粗粒度候选检测" class="headerlink" title="2.1.1 粗粒度候选检测"></a>2.1.1 粗粒度候选检测</h4><p>候选检测过程是在肺腔内使用一个双阈值密度mask来获得一个体素mask，其衰减值是从通常从磨砂玻璃不透明度中观察到的。使用的是-750到-300的HU。肺部边缘、血管和器官的部分容积的影响可以提升在定义的区间范围内衰减值。为移除这些体素，使用了一个球形半径为3体素的形态学腐蚀操作。在此之后，聚合所有体素为候选的连通组件分析。由于<strong>半径</strong>小于5mm的subsolid不需要进一步的CT检测，所有<strong>体积</strong>小于 $35mm^3$的候选(对应的是一个被移除的半径为4mm的完美球形)。然后再做一个使用相同结构元素的形态学扩张操作来撤销这种由腐蚀操作导致的收缩。最终，所有候选的容积和质心都被计算，并且质心相距小于5mm的都被合并了。此合并过程用来确保结节只会存在于一个候选中。</p>
<h4 id="2-1-2-结节分割"><a href="#2-1-2-结节分割" class="headerlink" title="2.1.2 结节分割"></a>2.1.2 结节分割</h4><p>上述的候选检测过程生成的是聚合的区域，并不是subsolid结节的精确分割。因此使用算法来进一步分割，算法作用于立方块的VOI(Volumn of Interest)并使用一些形态学操作来获取鲁棒性的结节分割。算法分为自动胸壁移除和从附着脉管系统分离。此方法用于solid结节性能表现优异(Kuhnigk et al 2006)，我们做了稍微调整来适应subsolid结节。Kuhnigk的算法使用的是一个全局下限阈值为 -450HU,为了在subsolid结节分割上获得较好的结果，我们将阈值下限改变为-750HU。分割算法的VOI在候选质心周围创建，并且VOI半径设为初始候选的等同的1.5倍。使用了这种方法，所有候选的精确分割被创建，这也形成了进入分类处理的最终候选集。</p>
<h3 id="2-2-特征"><a href="#2-2-特征" class="headerlink" title="2.2 特征"></a>2.2 特征</h3><p>描述候选的丰富的特征的计算可以分为四类：密度，上下文，形状特征和上下文特征和质地。</p>
<h3 id="2-2-1-密度特征"><a href="#2-2-1-密度特征" class="headerlink" title="2.2.1 密度特征"></a>2.2.1 密度特征</h3><p>密度特征在四个不同的体素集合上计算</p>
<ul>
<li><p>分割(segmentation)：候选分割内的体素</p>
</li>
<li><p>边框(boundingbox)：边框内的体素定义在候选分割的周围</p>
</li>
<li><p>surrounding3:候选分割的周围内的体素，通过扩张候选分割为3x3x3体素的长方形结构元素。</p>
</li>
<li><p>surrounding5:候选分割的周围内的体素，通过扩张候选分割为5x5x5体素的长方形结构元素。</p>
</li>
</ul>
<p>下图展示了这四种特征的样例。下图从左到右分别是<code>segmentation</code>,<code>boundingbox</code>,<code>surrounding3</code>,<code>surrounding5</code></p>
<p><img src="/images/blog/subsolid1.png" alt="5种特征"></p>
<p>这四种区域是定义的用来从候选的内部和周围的强度分布抽取特征的。对每个体素集合，计算了一个归一化直方图，每个bin size为50HU。每个直方图统计如下信息：熵，均值， 平均bin的高度，mode(极值点的位置)，mode bin的高度，5%，25%，50%，75%和95%分位的bin值。进一步的，计算体素分割集的方差，最大值，最小值和先前的7个Hu moment(来自一篇论文 Hu,1962)。Hu moment是指转换、尺寸、旋转不变性，通常用来描述潜在的强度分布。最后，计算多尺寸(1.0,1.77,3.16,5.62和10体素)上的最大血管，segmentation(此处的应该指前面提到的四个特征之一)体素集合的最大血管性的最小值、最大值、均值和方差都作为了特征。部分容积效应可以创建靠近血管或在血管壁上的的磨砂玻璃不透明度的区域，通过这些特征可以知道候选是在血管壁附近或者在血管壁上。</p>
<p>总共有54个强度特征被收集到。</p>
<h4 id="2-2-2-质地特征"><a href="#2-2-2-质地特征" class="headerlink" title="2.2.2 质地特征"></a>2.2.2 质地特征</h4><p>质地分析中，使用了local binary patterns(LBP)和2D Haar小波。它们都是很常见的质地分析来表述局部空间信息。样本的这些特征可以从来去除运动伪影造成的磨砂玻璃不透明度区域的假阳性。从候选segmetation周围的boundingbox     创建的VOI，同时这个VOI的容积被重新取样(Lanczos 重新抽样,$\alpha =3$)为两个尺寸为 16x16x16体素和 32x32x32体素的立方块。然后对每个重新抽样之后的每个slices(两个立方块)使用一个邻居为<strong>3x3(P=8,R=1)</strong>的2D的LBP计算。然后，对每个容积的LBP输出计算 bin size为1的归一化直方图，同时计算2.2.1节提到的一些其他统计量，除了分位值，这些都用作质地描述。</p>
<p>进一步的，对重新取样之后的尺寸为32x32x32的容积2D做 Haar小波计算。每个重新取样的容积slices(两个立方块)分为4个频带。每个slice的四个频带创建四个容积。计算高频带的三个容积的三个归一化直方图。从低频带创建的容积没有使用。同时计算与质地描述相同的直方图统计量。</p>
<p>总共有40个质地特征。</p>
<h4 id="2-2-3-形状特征"><a href="#2-2-3-形状特征" class="headerlink" title="2.2.3 形状特征"></a>2.2.3 形状特征</h4><p>第三图特征由形状特征组成，这些都是从候选segmentation中计算得来。其他结构而不是结节上的segmentation会得到怪异的形状，因此形状是个区分假阳性和真阳性的重要特征。首先计算如下特征：<code>球状性(sphericity
)</code>,<code>紧凑性1(compactness1)</code>,<code>紧凑性2(compactness2)</code>,<code>猜想半径(guessRadius)</code>。为了计算球状性，一个球形S被定义为候选区域的质心，它与候选segmentation有着相同的容积。然后<code>球状性sphericity</code>被定义为候选segmentation在球形S内的体素与全部球形S的容积的比例。然后为了计算紧凑性1，紧凑性2和猜想半径，候选segmentation的boundingbox被使用，并且维度称为<code>dimx</code>,<code>dimy</code>,<code>dimz</code>。为了计算紧凑性1，使用boundingbox内的体素的数目除以候选cluster的体素的数目。紧凑性2的计算为，使用bounding box的最大维度($max(dimx,dimy,dimz)$) 构建的立方块的体素除以候选cluster内的体素的数目。猜想半径的计算方法为，将bounding box的容积除以6即可得到。如果是一个完美球形，将会计算得到球形的真正半径。第二步，计算体素的数目和cluster size($mm^3$)可以用来描述候选的尺寸。这些两个特征几乎似乎相同的，但是cluster的size将CT scan的分辨率也考虑进来了。最后，从候选mask体素中计算相同集合的7个Hu moments来描述其形状。注意：与前面为了强度特征计算的Hu moments相反，此处在segmentation内部的体素设为1，之外的设为0。</p>
<p>总共计算13个形状特征。</p>
<h4 id="2-2-2-4-纹理特征"><a href="#2-2-2-4-纹理特征" class="headerlink" title="2.2.2.4  纹理特征"></a>2.2.2.4  纹理特征</h4><p>最后定义了一组新的纹理特征，用于描述相对于肺边界、气道树和其他subsolid 结节候选的的候选区域的位置。例如，稍大点的磨砂玻璃不透明度可以被看做由于微观选择的原因造成的肺的重力依赖部分。这会导致候选沿着肺边界有着细长的形状。结合上下文和形状信息可以获悉这一点。另外一种情况是，气道被填满了粘液，在密度强度上可能表现得像磨砂玻璃不透明。这些候选将表现出与气道segmentation重叠，这个可以用来将他们分为假阳性。更进一步说，候选之间的关联也是相关的上下文信息。例如，被其他候选包围的候选更可能是微观选择区域的源头，而不是subsolid结节。</p>
<p>首先，需要计算肺部区域内的两个距离转换；第一个使用肺部segmentation，第二个使用气道树。到肺边界的距离和到气道最近的距离来自对所有候选segmentation内的体素距离转换。这意味着，到肺边界的距离和到气道的距离的方差、最大值、最小值也被用作纹理特征。</p>
<p>其次，bounding box被定义在肺周围，它被用来计算相对位置特征；相对的X，Y，Z位置，以及到bounding box左下角的距离也被计算。甚至会计算两个肺的质心的距离。</p>
<p>再者，计算绝对和相对的气道和血管重叠。为了计算这个，我们会算入 bounding box内的体素，这些体素是气道segmentation或者血管segmentation的部分。相对重叠和绝对重叠的精确体素数目的计算为，bounding box内的体素除以segmentation内的体素。</p>
<p>最后，描述的是候选与其他候选的关系。首先，一个scan内的候选的数目作为一种特征。这提供了肺内部磨砂玻璃区域的数目。其次，计算了候选距离为30mm以内的候选和50mm以内的候选，以及与其他候选最近的候选。</p>
<p>总共有21个这样的纹理特征。</p>
<h3 id="2-3-分类"><a href="#2-3-分类" class="headerlink" title="2.3 分类"></a>2.3 分类</h3><p>这部分描述的是优化分类器的性能。然后描述的是CAD系统在独立测试集上的评估。系统评估期间，若候选的质心与结节中心的距离为R之内时结节标记为<code>检测到</code>。为确保CAD系统的标记在CT scan上的结节之中，我们将R设置为结节的半径。</p>
<h4 id="2-3-1-分类模式优化"><a href="#2-3-1-分类模式优化" class="headerlink" title="2.3.1 分类模式优化"></a>2.3.1 分类模式优化</h4><p>为选择最佳的分类器模式，做了如下实验。在训练集上做10折交叉验证。因为的病人的同一个结节可能会出现在多个scan中，所以每一折都是以病人级别划分以避免偏差。候选被分类为结节或假阳性(FP)，然后CAD系统的最终性能评估使用的是自由响应操作特性(FROC)分析。</p>
<p>候选分类器的测试分为两个阶段。一阶段分类器是计算所有候选的完整特征集，然后使用一个监督学习的分类器将所有候选分为两类。相反的，二阶段分类器的分类器模式是仅仅使用第一阶段的5个特征来做第一阶段的分类。第一阶段的分类器主要是移除尽可能多的假阳性候选。。然后，仅计算剩下余下候选的特征集。这种二阶段的方法有两个优点，第一是计算时间消耗要少，因为第二阶段不用计算所有候选，第二个是第一阶段的分类器会使得数据更加均衡，因为第二阶段做了分类。两种方法都用来测试以评估那种分类模式在分类性能中更优。</p>
<p>二阶段分类的第一阶段的分类器使用的是线性判别分类器(Linear Discriminant Classifier)(LDC Fukunaga 1990)，因为其简单性和速度。第一阶段分类器的5个特征的最优集由3种方法获得的。前两种方法是，一种sequential forward floating selectoin(SFFS)。SFFS步骤将训练集中随机取50%作为训练集，剩余的50%作为测试集。第一种方法中，准确率用作SFFS步骤的优化标准。</p>
<p>第二个方法中，FROC曲线下的0-3FP/scan的部分区域用作优化标准。最后第三种方法中，计算所有特征的Fisher’s linear discriminant 比例，其中比例最高的5个特征会被选出。注意这种方法不考虑特征组合。三种不同的LDC分类器使用三种不同的5个特征集合，并为每个分类的所有候选算出一个似然度。第一阶段的每个分类器的似然度通过对所有正样本的似然度排序并选出<strong>最低的似然度</strong>。紧接着，移除训练集中的非正样本。所有低于此似然度的都被移除。这种方法中，我们获得5个移除了大部分假阳性候选同时没有移除正样本的特征集合。</p>
<p>对于一阶段分类器和二分阶段分类器的第二个阶段，测试了KNN分类器、随机森林分类器、GentleBoost 分类器，最近均值分类器(nearest mean classifier NM)，使用径向偏置核函数的支持向量机SVM-RBF和LDC。不同分类器的参数在训练集交叉验证上优化过。<br>KNN分类器中，K设置为正样本数的均方根。随机森林分类器使用100颗树来训练，最大深度为20. GentleBoost分类器，回归stumps用作若分类器，使用了250个若分类器。分类器之后，一个10GentleBoost的分类器，记为GB10。这100个 GentleBoost分类器都是独立的在训练集上随机去75%作为训练集来训练。GB10分类器的最终输出是10个分类器的输出概率的中位数。对于GB10，每个分类器都是用了250个回归stumps。SVM-RBF分类器的C和伽马参数是从一个10折交叉验证循环的数据集上的内部的5折交叉验证循环优化得到的。是用FROC曲线下方0-3FP/scans的部分区域作为优化标准。所有的特征都被归一化为均值为0和单位方差。</p>
<h4 id="2-3-2-评估上下文特征的收益"><a href="#2-3-2-评估上下文特征的收益" class="headerlink" title="2.3.2 评估上下文特征的收益"></a>2.3.2 评估上下文特征的收益</h4><p>我们假设上下文特征有助于更好的分类器性能。是用最终的CAD系统在训练集上对比包含了和不包含上下文特征来测试这个假设。使用Bootstrap方法来测试统计显著性。从交叉验证集中抽样5000次来获得scan。每个bootstrap样本有着原始数据集一样多的scans。FROC曲线下的0-8FPs/scan的区域作为分类器性能的衡量标准。</p>
<h4 id="2-3-3-独立测试集上的最优分类器模式的评估"><a href="#2-3-3-独立测试集上的最优分类器模式的评估" class="headerlink" title="2.3.3 独立测试集上的最优分类器模式的评估"></a>2.3.3 独立测试集上的最优分类器模式的评估</h4><p>最优分类器模式，一阶段分类器和二阶段分类器，最优分类器基于训练集上的交叉验证FROC分析。CAD系统的最优分类器模式使用的是完整训练集，并在独立的测试集上评估性能。注意，测试集在分类器模式优化期间并没有使用。</p>
<h3 id="2-4-结合solid-nodule-CAD"><a href="#2-4-结合solid-nodule-CAD" class="headerlink" title="2.4 结合solid nodule CAD"></a>2.4 结合solid nodule CAD</h3><p>临床实践中，subsolid 结节CAD系统通常与solid结节CAD系统结合使用。尽管solid 结节CAD算法并没有对subsolid结节检测做优化或训练，它们可能检测所有subsolid结节的部分。尤其是，它们可能对检测part-solid结节的solid core十分敏感。</p>
<h2 id="三-结果"><a href="#三-结果" class="headerlink" title="三 结果"></a>三 结果</h2><h3 id="3-1-候选检测"><a href="#3-1-候选检测" class="headerlink" title="3.1 候选检测"></a>3.1 候选检测</h3><p>训练集中每个scan在候选检测步骤生成 $237\pm267$ 个候选区域，测试集中每个scan生成 $109\pm127$ 个候选区域。训练集中，所有subsolid 结节的候选检测灵敏度为84%，同时part-solid结节和non-solid结节的灵敏度分别为81%和87%。测试集中所有subsolid结节的灵敏度为88%，part-solid和non-solid的灵敏度分别为85%和90%。</p>
<h4 id="3-2-分类器"><a href="#3-2-分类器" class="headerlink" title="3.2 分类器"></a>3.2 分类器</h4><h4 id="3-2-1-分类器模式的优化"><a href="#3-2-1-分类器模式的优化" class="headerlink" title="3.2.1 分类器模式的优化"></a>3.2.1 分类器模式的优化</h4><p>一阶段分类器模式的训练集上的10折交叉验证的不同分类器的FROC曲线如下图</p>
<p><img src="/images/blog/subsolid2.png" alt=""></p>
<p>可以看到GB10分类器性能最好，1FPs/scan时达到了69%的灵敏度，2FPs/scan时达到了74%。注意到候选检测灵敏度为84%，这意味着分类器灵敏度不可能高于这个值。</p>
<p>分类器的第一阶段旨在减少候选区域的FPs数量。使用的是2.3.1节的三种方法，构建第一阶段的分类器的三种不同集合的5特征，其性能在10折交叉验证中测试，结果如下表。下表显示的是阈值使用先验概率设置为T不移除训练集中正样本时的样本移除数量。此表显示，基于Fisher’s linear discriminant比例的特征集移除最多的样本，因此这个特征集被选为CAD系统第一阶段的特征集。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Feature set</th>
<th>Reduction ratio(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>SFFS-accuracy</td>
<td>68</td>
</tr>
<tr>
<td>SFFS-partial area under FROC curve</td>
<td>79</td>
</tr>
<tr>
<td>Fisher’s linear discriminant ratio</td>
<td>59</td>
</tr>
</tbody>
</table>
</div>
<p>第二阶段的不同分类器的性能如下图。与上图相比，GB10分类器表现最好。同时注意，候选检测灵敏度为84%，分类器的灵敏度不可能超过这个值。</p>
<p><img src="/images/blog/subsolid3.png" alt=""></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>CT图像中肺结节自动检测:使用CNN移除假阳性【论文笔记】</title>
    <url>/2017/02/26/mutliview-falseremove-usingcnn/</url>
    <content><![CDATA[<h2 id="0-综述"><a href="#0-综述" class="headerlink" title="0 综述"></a>0 综述</h2><p>本文做出以下共享:</p>
<ol>
<li><p>我们提出了一种用于肺结节检测的<strong>多角度</strong>卷积网络ConvNet用以剔除假阳性的步骤。候选样本是由三种现存的检测算法结合计算出来的，这也可用于增强候选检测步骤的灵敏性。</p>
</li>
<li><p>评估不同的多角度卷积网络的架构以及它们对检测性能的影响。同时也评估了增加更多角度和应用一种确定的融合技术对于每种架构的性能</p>
</li>
<li><p>提出一种性能基准和在完全独立的筛选实验的数据集上的外部验证。</p>
</li>
</ol>
<h2 id="1-数据材料"><a href="#1-数据材料" class="headerlink" title="1 数据材料"></a>1 数据材料</h2><h3 id="1-1-LIDC-IDRI"><a href="#1-1-LIDC-IDRI" class="headerlink" title="1.1 LIDC-IDRI"></a>1.1 LIDC-IDRI</h3><p>使用大的公开数据集<code>Lung Image Database Consortium(LIDC-IDRI)</code>训练和验证了CAD系统。该数据集包含了来自7个机构的<code>1018</code>个异质病例。CT图像的切片(slices)厚度从<code>0.6mm-5.0mm</code>之间,中值为2.0mm。数据的引用标准由人工标注，来自四位放射医师，对每个scan进行两轮评审。第一轮的盲审阶段，可疑病变是独立标注的并且每个都被分类 为<code>非结节</code>,<code>结节&lt;3mm</code>,<code>结节&gt;3mm</code>。人工的3D切割只针对<code>结节&gt;3mm</code>的。第二轮中，来自全部四位的放射医师的标注被评审（非盲审），同时每个放射医师决定接受或反对此标注。</p>
<p>我们抛弃了结节<strong>厚度大于2.5mm</strong>的样本(它们已经不被推荐使用)，以及包含了不相连切片(slices)空间的scans，最后获得<code>888</code>个scans。最后挑选的scans列表我们公开在 <a href="http://luna.grand-challenge.org/" target="_blank" rel="noopener">公开的scans列表</a> 。我们仅考虑标注中结节半径大于等于3mm的，小于3mm的被丢弃。最后选择了结节半径大于等于3mm的，并且被大部分放射医师 （4个中有3个）。最后选出了1186个结节。</p>
<h3 id="1-2-ANODE09"><a href="#1-2-ANODE09" class="headerlink" title="1.2 ANODE09"></a>1.2 ANODE09</h3><p>为了进一步验证系统的性能，使用完全独立于训练集的数据集 <strong>ANODE09竞赛</strong>, 该数据集由55个CT scans组成。每个CT由2个观察者盲审标注。5个scans作为训练集，剩余的50个作为测试集。这些引用没法公开获取。</p>
<p>所有的样本由 University Medical Center Utrecht 收集。图像被重构为 1.0mm厚度。</p>
<h3 id="1-3-DLCST"><a href="#1-3-DLCST" class="headerlink" title="1.3  DLCST"></a>1.3  DLCST</h3><p>为了进一步验证系统的性能，使用了Danish Lung Cancer Screening Trial的数据集。该评测基于612个baseline的scans，这些scans是最近公开在临床研究上的。两个观测者的半径被平均，然后正样本被定义为结节半径大于等于3mm。这个结果产生了898个结节。</p>
<h2 id="二-方法"><a href="#二-方法" class="headerlink" title="二 方法"></a>二 方法</h2><p>我们提出的CAD系统架构图如下，由两个主要部分组成：<strong>(1)</strong>候选检测<strong>(2)</strong>假阳性剔除。</p>
<p> <img src="/images/blog/multiview1.png" alt="总流程"></p>
<p>我们分别为<code>solid</code>,<code>subsolid</code>,<code>large solid</code>结节使用了三个定制候选检测器。这些检测器的结合用以增加结节灵敏度。注意到结节尺寸和形态特征变化很大，每个候选从固定平面抽取多个2-D视角。每个2-D视角被一个ConvNets流处理。ConvNets特征被用于计算最终分数。</p>
<h3 id="2-1-候选检测"><a href="#2-1-候选检测" class="headerlink" title="2.1 候选检测"></a>2.1 候选检测</h3><p>候选检测关系到后续阶段的最大检测灵敏度，候选检测算法应该完美的识别所有可疑的病变。</p>
<p>为了检测更广泛的结节，我们应用了多种候选检测算法的结合。结合已有的三种用以检测结节候选的CAD系统。每个算法关注于一种类别的结节，即<code>solid</code>,<code>subsolid</code>,<code>large solid</code> 结节。对于每一个候选，其位置 $p= (x,y,z)$ 和它为结节的概率被给出。三个集合的结节候选被计算，并合并以最大化检测器的灵敏度。候选位置密集度小于5mm的结节被合并，这些合并的结节的位置和为结节的概率以均值表示。</p>
<p>候选检测阶段的方法，获取每个VOI(volume of interest)的位置。</p>
<p><strong>对于solid结节</strong>，实现了论文3中的方法。计算肺部的每个体素，shape index和轮廓，并在两种方法使用了阈值用以定义seedpoint。一种自动切割方法会在seedpoint执行，这可获得 cluster of interest。然后cluster位置相邻的被合并。最后丢弃体积小于 $40mm^3$ 的cluster。</p>
<p><strong>对于subsolid结节</strong>，论文5(<em>Automatic detection of subsolid pulmonary nodules in thoracic computed tomography images </em>)中的一种方法是。一种双阈值密度mask (-750,-350 HU)首次用来获得 voxel of interest的mask。形态open运算（一种图形学的操作）用来移除相连的cluster，紧接着做3D相连组件分析。最后精确的候选切割是通过一种先前提出的结节算法（论文<em>Morphological segmentation and partial volume analysis for volumetry of solid pulmonary lesions in thoracic CT scans </em>）获得。</p>
<p><strong>对于Large solid结节(大于等于10mm</strong>，它有着与小结节病变不同的surface/shape index值，并且有着不同的密度分布，solid和subsolid结节检测算法无法检测到。除此之外，粘附于胸膜壁的large solid结节可能被肺部切割算法排除在外，因为它与胸部的对比度很低。基于这些原因，论文27提出第三种检测器算法，并由以下三步组成：</p>
<ol>
<li><p>肺部切割后处理：通过对切割mask应用一个滚球算法，这会在肺切割时包含粘附于胸腔的large solid结节。</p>
</li>
<li><p>密度阈值(-300HU)，为了获取 voxels of interest的mask</p>
</li>
<li><p>多阶段获取候选cluster的形态开运算(opening)(论文29)，我们以大的结构元素来抽取更大的结节，然后渐渐的继续使用小结构元素来抽取小的结节。</p>
</li>
</ol>
<p>有个重要问题是，训练算法时使用高度不均衡的数据会导致学习到的参数会向最常见的候选的特征(比如 vessels(血管))倾斜，并忽略更稀有的结节的重要特征。为防止对最普遍的假阳性结节的过拟合，我们丢弃了概率较低的结节候选。这些概率都是由后续的分类阶段的算法给出，并且阈值都是基于经验值设定用以减少大量的假阳性，同时保留高检测灵敏性。</p>
<h3 id="2-2-patches-抽取"><a href="#2-2-patches-抽取" class="headerlink" title="2.2 patches 抽取"></a>2.2 patches 抽取</h3><p>对于每个候选，我们抽取了多个2-D patches，尺寸为$50\times 50mm$，中心坐标位置为 $p$ 。选择此大小的尺寸是为了让所有的结节($\le 30mm$)在2D视角的完全可见，并包含充分的上下文信息，这有助于在候选分类阶段。我们将尺寸为 $50\times 50 mm$的patches转换为 $64\times 64 px$（注意单位），分辨率为0.78mm，这对应于典型的CT数据切片分辨率。像素密度范围从(-1000,400HU)转换为(0,1)，不在此范围的被切除。</p>
<p>为了抽取patches，我们首先考虑了立方块尺寸为 $50\times 50\times 50 mm$,完全包含候选。在立方块的9个对称轴面抽取9个patches。上图中的图a展示了9个对称轴面的pathes方法。</p>
<h3 id="2-3-假阳性剔除-2-D-ConvNets配置"><a href="#2-3-假阳性剔除-2-D-ConvNets配置" class="headerlink" title="2.3 假阳性剔除:2-D ConvNets配置"></a>2.3 假阳性剔除:2-D ConvNets配置</h3><p> 假阳性剔除阶段结合了不同流式的ConvNets，可被看做一种多视角架构。每种流处理一个候选的特定种角度的patches。</p>
<p>2-D卷积网络的架构由一个小数据集决定的。这个数据集上，几个超参数(比如，层数，kernel size，学习速率，视角数目，融合方法)在此优化。这些超参数中，我们规定了两个最重要的参数需要微调，即<strong>(1)视角数目(2)融合方法</strong>。其他参数被设置为最佳配置（从那个小数据集上获得的最优值）。</p>
<p>2-D ConvNets由3个连续卷积层和一个最大池化层组成。网络的输入是 $64\times 64 $的patches。具体如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络层数</th>
<th>类型</th>
<th>kernel 数目</th>
<th>kernel size</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>conv</td>
<td>24</td>
<td>5x5x1</td>
</tr>
<tr>
<td>2</td>
<td>conv</td>
<td>32</td>
<td>3x3x24</td>
</tr>
<tr>
<td>3</td>
<td>conv</td>
<td>48</td>
<td>3x3x32</td>
</tr>
<tr>
<td>4</td>
<td>maxpool</td>
<td>1</td>
<td>2x2(stride 2) non-overlap</td>
</tr>
</tbody>
</table>
</div>
<p>每个kernel产生一个2-D图像输出(比如第一层卷积之后，输出24个60x60的图像)。Kernel的值都是随机初始化的，并且在训练过程优化。最大池化层，使用非重叠窗口kernel size是2x2，步长为2，这会将patches suze减小为一半(例如，从24@60x60减小为24@30x30)。最后一层是全连接层，输出16个神经元。<strong>Relu激活函数</strong>用于<strong>卷积层和全连接层</strong></p>
<h3 id="2-4-假阳性剔除：ConvNets融合"><a href="#2-4-假阳性剔除：ConvNets融合" class="headerlink" title="2.4 假阳性剔除：ConvNets融合"></a>2.4 假阳性剔除：ConvNets融合</h3><p>有三种方式对多个2-D ConvNets网络的融合，做了如下调查：</p>
<p><strong>(1)committee-fusion</strong>:最常见的融合方法，它对多个卷积网络的输出预测做一个基于委员会的融合。我们连接每个流的全连接层的输出到一个分类层，该分类层由一个使用softmax激活函数的全连接层组成。每个ConvNets流使用不同视角的patches独立训练，并且输出预测使用一种product-rule来组合输出概率，上图图c所示。</p>
<p><strong>(2)late-fusion</strong>:该方法拼接第一个全连接层的输出，并直接将此拼接输出与分类层相连（上图图C）。此种方法，分类层可以通过对比多个ConvNets的输出学习3-D特征。这种配置中，不同流的卷积层参数是共享的。</p>
<p><strong>(3)mixed-fusion</strong>:上面两种方法的结合。多个<strong>late-fusion</strong> ConvNets用固定数量的正交平面实现。利用更多的视角，系统的预测准确率可以通过在委员会中结合多个late-fusion ConvNets来提高。我们将9个patches分为3个独立的集合，每个集合包含了三个不同的pacthes。</p>
<h3 id="2-5-训练"><a href="#2-5-训练" class="headerlink" title="2.5 训练"></a>2.5 训练</h3><p>评估方法是在888个<strong>LIDC-IDRI</strong>使用的是5-折交叉验证。我们将888个样本切分为5个子集，并每个子集的候选数目相近。每一折中，使用3个子集作为训练，一个子集为验证，一个子集为测试。使用ConvNets的一大挑战是在给定的训练集有效的优化ConvNets的权重。损失函数使用交叉熵误差，权重更新使用128样本的mini-batches。</p>
<ul>
<li><p>权重初始化：由Glorot and Bengio提出的一种正态随机初始化。</p>
</li>
<li><p>偏置初始化：偏置初始化为0</p>
</li>
<li><p>学习速率：RMSProp是一种学习算法，它会自动将学习速率除以最近梯度大小的平均值，常用语优化模型。</p>
</li>
<li><p>Dropout：用在第一个全连接层的输出，概率为0.5.</p>
</li>
<li><p>停止条件：验证数据集上的准确率3个epoches之后不再提高</p>
</li>
</ul>
<h3 id="2-6-数据增强"><a href="#2-6-数据增强" class="headerlink" title="2.6 数据增强"></a>2.6 数据增强</h3><p>ConvNets上使用不均衡的数据集会导致算法陷入局部最优，其预测结果会偏向出现频率最高的特征，并导致过拟合。</p>
<p>(1) 训练数据集增强：由于结节数远小于非结节数，因此数据增强仅用于结节。此步骤仅用于训练和验证。将候选坐标每个轴转换为1mm，并将patches的尺寸变换为40，45,50，和55mm。此类转换设置为1mm是为了保证结节(&gt;3mm)可以被patches捕获到。可以通过在结节上随机上采样候选 来进一步均衡数据集。</p>
<p>(2)测试数据集增强：测试集数据增强可能有助于提高系统的鲁棒性，因为候选早在非常多的不同条件下评估，比如分析不同尺寸的输入图像。测试数据增强在每个候选(结节和非结节上都有)，通过缩放patches尺寸为40，45,50,55mm，每个都是被ConvNets-CAD系统独立地处理。每个候选的预测是平均所有的增强的数据得来的。最终预测结果是预测的集合，它被用来作为补充信息，因此使得最终的预测结果更加准确和稳定。</p>
<h3 id="2-7-评估"><a href="#2-7-评估" class="headerlink" title="2.7 评估"></a>2.7 评估</h3><p>评测了两种性能指标。(1)ROC曲线下区域(2)竞赛性能指标CPM，即衡量FROC曲线7个操作点(1/8,1/4,1/2,1,2,4,8Fps/scan)的平均灵敏度。AUC显示ConvNets在是否为结节时分类候选的性能，CPM显示的CAD系统在操作点的性能。注意，系统有更好AUC分数并不一定会有更高的CPM。我们同时计算95%内部置信度和p值，使用1000个boostrapping。</p>
<h2 id="三-实验结果"><a href="#三-实验结果" class="headerlink" title="三 实验结果"></a>三 实验结果</h2><p>候选检测算法的检测灵敏度</p>
<ul>
<li><p><strong>total number of CT scans</strong>: 888</p>
</li>
<li><p><strong>Total number of nodules</strong>:1186</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Candidate detection</th>
<th>Detected nodules</th>
<th>Sensitivity(%)</th>
<th>False Positives(FPs)</th>
<th>FPs per scan</th>
</tr>
</thead>
<tbody>
<tr>
<td>solid</td>
<td>1016</td>
<td>85.7</td>
<td>292413</td>
<td>329.3</td>
</tr>
<tr>
<td>subsolid</td>
<td>428</td>
<td>36.1</td>
<td>255027</td>
<td>287.2</td>
</tr>
<tr>
<td>Large solid</td>
<td>377</td>
<td>31.8</td>
<td>41816</td>
<td>47.1</td>
</tr>
<tr>
<td>combines set</td>
<td>1120</td>
<td>94.4</td>
<td>543160</td>
<td>611.7</td>
</tr>
<tr>
<td>Reduced set</td>
<td>1106</td>
<td>93.3</td>
<td>239041</td>
<td>269.2</td>
</tr>
</tbody>
</table>
</div>
<p>训练集中结节和非结节数据统计。为了平衡数据集，数据增强(aug)和上采样(up)在结节上的表现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Training dataset</th>
<th>Fold 0</th>
<th>Fold 1</th>
<th>Fold 2</th>
<th>Fold 3</th>
<th>Fold 4</th>
</tr>
</thead>
<tbody>
<tr>
<td>scans</td>
<td>428</td>
<td>522</td>
<td>574</td>
<td>629</td>
<td>511</td>
</tr>
<tr>
<td>nodule</td>
<td>528</td>
<td>669</td>
<td>713</td>
<td>773</td>
<td>635</td>
</tr>
<tr>
<td>-aug</td>
<td>57552</td>
<td>72921</td>
<td>77717</td>
<td>84257</td>
<td>69215</td>
</tr>
<tr>
<td>-agu+up</td>
<td>143838</td>
<td>143796</td>
<td>143796</td>
<td>142823</td>
<td>142927</td>
</tr>
<tr>
<td>non-nodule</td>
<td>143838</td>
<td>143796</td>
<td>143739</td>
<td>142823</td>
<td>142927</td>
</tr>
</tbody>
</table>
</div>
<p> Performance benchmark of ConvNets configurations on LIDC-IDRI dataset.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Number of views</th>
<th>AUC</th>
<th>CPM</th>
</tr>
</thead>
<tbody>
<tr>
<td>combines algorithms</td>
<td>-</td>
<td>0.969</td>
<td>0.573</td>
</tr>
<tr>
<td>single-view</td>
<td>1</td>
<td>0.969</td>
<td>0.481</td>
</tr>
<tr>
<td>committee-fusion</td>
<td>3</td>
<td>0.981</td>
<td>0.696</td>
</tr>
<tr>
<td></td>
<td>9</td>
<td>0.987</td>
<td>0.780</td>
</tr>
<tr>
<td>late-fusion</td>
<td>3</td>
<td>0.987</td>
<td>0.742</td>
</tr>
<tr>
<td></td>
<td>9</td>
<td>0.993</td>
<td>0.827</td>
</tr>
<tr>
<td>mixed-fusion</td>
<td>3*3</td>
<td>0.996</td>
<td>0.824</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>医疗图像处理：LUNA2016 3DCNN 网络论文详解</title>
    <url>/2017/02/10/LUNA2016-3DCNN/</url>
    <content><![CDATA[<h2 id="0-背景知识"><a href="#0-背景知识" class="headerlink" title="0 背景知识"></a>0 背景知识</h2><p> 自动肺结节检测系统由以下两步组成(代码部分，参考 <a href="https://github.com/shartoo/luna16_multi_size_3dcnn" target="_blank" rel="noopener">luna16_3DCNN</a>):</p>
<ol>
<li>候选screen</li>
<li>假阳性剔除</li>
</ol>
<p>在候选screen中，大量粗粒度的候选经由多种标准，如放射密度阈值、数学形态操作、外形，筛选之后被喂入系统。为获得对候选screen敏感度高的分类器，这一步阈值标准都较宽泛、直接，这就导致大量的候选被送入第二步。这样一来，第二步的剔除假阳性成了整个系统的关键。</p>
<p>肺部恶性肿瘤结节的挑战主要来源于以下两方面。一：肺结节尺寸、形状和位置千变万化。而且，不同的周遭环境使得不同种类的肺结节更加多元化。其二：一些假阳性候选携带了与真的恶性肿瘤结节相似的形态。</p>
<p>目前已有方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>论文标题</th>
<th>方法</th>
<th>准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td>A new computationally efficient cad system for pulmonary nodule detection in ct imagery</td>
<td>形状、位置、密度和梯度特征集合</td>
<td>82.66%灵敏性，平均每个scan 3个假阳性</td>
</tr>
<tr>
<td>Automatic detection of subsolid pulmonary nodules in thoracic computed tomography images</td>
<td>密度、形状、纹理特征和并入的周遭信息</td>
<td>80% 灵敏性，平均每个scan 1个假阳性</td>
</tr>
<tr>
<td>Pulmonary nodule detection in ct images:false positive reduction using multi-view convolutional networks</td>
<td>多角度2D CNN方法</td>
<td>85.4%l灵敏度，平均每个scan 1个假阳性</td>
</tr>
</tbody>
</table>
</div>
<h2 id="一-本文方法"><a href="#一-本文方法" class="headerlink" title="一  本文方法"></a>一  本文方法</h2><p> 为应对肿瘤结节千变万化，以及提高分辨器的鲁棒性，我们提出了一个<strong>多层次</strong>肿瘤语境信息预测模型</p>
<h3 id="1-1-方法详细"><a href="#1-1-方法详细" class="headerlink" title="1.1 方法详细"></a>1.1 方法详细</h3><p>架构图</p>
<p> <img src="/images/blog/3DCNN_ALL.png" alt="架构图"></p>
<p>模型中每个网络的详细结构如下表</p>
<p><img src="/images/blog/3DCNN_detail.png" alt="模型每个网络结构"></p>
<h3 id="1-2-3D-卷积层"><a href="#1-2-3D-卷积层" class="headerlink" title="1.2  3D 卷积层"></a>1.2  3D 卷积层</h3><p>为构建3D卷积层，首先需要定义一系列小的3D特征抽取器(kernel)，抽取堆叠的高层次表征。为了生成新的特征空间，使用了不同的3D kernel抽取输入空间上不同的特征。然后添加偏置项，使用非线性激活函数。公式如下</p>
<script type="math/tex; mode=display">
 h^l_i (x,y,z )= \sigma (b^l_i+\sum _k \sum _{u,v,w}h^{l-1} _k(x-u,y-v,z-w)W^l_{ki}(u,v,w) )</script><p>其中</p>
<ul>
<li><p>$h^l_i$ 代表第<code>l</code>层的第<code>i</code>个3D特征空间</p>
</li>
<li><p>$h^l _{k-1}$ 代表前一层的第<code>k</code>个3D特征空间</p>
</li>
<li><p>$W^l<em>{ki}$ 连接 $h^l_i$ 和 $h^{l-1}</em>{k}$ 的卷积核</p>
</li>
<li><p>$h^l<em>i(x,y,z)$,$h^{l-1} _k(x-u,y-v,z-w)$,$,W^l </em>{ki}(u,v,w)$分别代表值(x,y,z)在坐标轴空间 $h^l<em>i$中的值，$(u,v,w)$代表3D kernel空间 $W^l</em>{ki}$ 中的坐标。</p>
</li>
<li><p>$\sigma$ 为非线性激活函数</p>
</li>
</ul>
<p>注意到，不同的3D kernel的激活函数值应该在偏置之前累加。</p>
<h3 id="1-3-3D-最大池化层"><a href="#1-3-3D-最大池化层" class="headerlink" title="1.3 3D 最大池化层"></a>1.3 3D 最大池化层</h3><p> 假设 $l$ 层为卷积层，$l+1$ 为紧随其后的3D池化层。最大池化层接受一个4D输入tensor $T=[h^l_1,h^l_2,h^l_3,…h^l_k]\epsilon R^{X\times Y\times Z\times K}$ 。</p>
<p>对于最大池化操作，它选取立方体内最大值，并生成抽象输出 $T’\epsilon R^{X’\times Y’\times Z’\times K}$ ，其中$(X,Y,Z)$ 和$(X’,Y’,Z’)$ 分别是最大池化特征抽取前后的尺寸。$K$为特征空间数目。</p>
<p>给定池化 kernel 尺寸M 和步长S，池化之后，特征空间减小到 $X’=(X-M/S+1)$</p>
<h3 id="1-4-全连接层"><a href="#1-4-全连接层" class="headerlink" title="1.4 全连接层"></a>1.4 全连接层</h3><p> 全连接层中，每个神经元与邻接层所有神经元相连。全连接层之前，<strong>首先需要将特征空间压平(flatten)到一个神经元向量</strong>，接下来再执行向量-矩阵乘法，再加上偏置项以及应用非线性激励函数。</p>
<script type="math/tex; mode=display">
   h^f = \sigma (b^f+W^fh^{f-1})</script><ul>
<li><p>$h^{f-1}$ 是输入特征向量，从第 $f-1$ 层的3D特征空间压平(flatten)而来。</p>
</li>
<li><p>$h^f$ 是第 $f$ 层的输出特征向量（是一个全连接层）。</p>
</li>
<li><p>$W^f$ 是权重矩阵</p>
</li>
<li><p>$b^f$代表偏置项</p>
</li>
<li><p>$\sigma$ 是激活函数 ReLU</p>
</li>
</ul>
<h3 id="1-5-softmax层"><a href="#1-5-softmax层" class="headerlink" title="1.5 softmax层"></a>1.5 softmax层</h3><p> 3D CNN的输出层就是softmax层。 $h^l$ 代表最后一层的神经元向量，C是目标分类数。通过softmax回归 $p<em>c(h^L)=exp(h^L_c)/\sum^{C-1}</em>{c=0}exp(h^L_c)$计算每个分类 $c$ 的概率，其中 $h_c^L$ 是神经元向量的第 $c$ 个元素。softmax层的激励函数输出都是(0,1)之间的正值，并且和为1。</p>
<h3 id="1-6-损失函数"><a href="#1-6-损失函数" class="headerlink" title="1.6 损失函数"></a>1.6 损失函数</h3><p> 对于给定N对3D训练样本集合 ${(I^{(1)},y^{(1)}),….(I^{(N)},y^{(N)})}$ ，其中$I^{(j)}$ 是输入立方块，$y^j$ 是对应的真实标签，$\hat y^{(j)}$ 为预测标签，$\theta$ 代表所有参数。损失函数如下:</p>
<script type="math/tex; mode=display">
  loss = -\frac{1}{N}\sum^N_{j=1}\sum^{C-1}_{c=0}indicator{y^{(j)}=c}P(\hat y^{(j)}=c\| I^{(j)};\theta)</script><p>其中 $indicator$ 代表指示函数，$P(\hat y^{(i)}=c|I^{(j)};\theta)$ 为样本 $I^{(j)}$ 属于类别$c$的预估概率(即softmax回归层的输出值 $p_c(h^L)$)。通过调整参数使得 $loss(\theta)$ 最小。</p>
<h2 id="二-接收域"><a href="#二-接收域" class="headerlink" title="二 接收域"></a>二 接收域</h2><p>肺部结节的多样性十分广泛，半径从3mm到30mm，形状和其他特性比如毛玻璃、固体、内部结构、刺孔等。除此之外，肺结节还与周遭环境十分相关。人工设计的判别规则辨别能力十分有限，无法从这一类别迁移到另外一类。</p>
<p>3D CNN肺结节检测，立方体样本以候选位置坐标为中心切割，并被输入到网络。<strong>立方体的样本的尺寸，即目标位置的环绕范围成为网络的接受域</strong>。接收域的大小对网络辨识准确率至关重要，接收域太小，只有有限的环境信息被包含入网络，会导致预测能力下降以及难以处理大量变化的目标；接收域太大，会包含太多噪音数据。</p>
<p>论文设计了一种多层次3D CNN来应对这一问题，并综合不同层次的判别结果。</p>
<h2 id="三-多层次语境网络和模型混合"><a href="#三-多层次语境网络和模型混合" class="headerlink" title="三 多层次语境网络和模型混合"></a>三 多层次语境网络和模型混合</h2><p><img src="/images/blog/radium.png" alt="结节半径尺寸分布"></p>
<p>上图为肺结节尺寸大小分布统计，小结节的半径大小分布，X和Y维度集中在9体素，Z维集中在4mm体素。</p>
<p>鉴于此，设计得第一个网络即<code>Archi-1</code> 接收域为 $20\times 20\times 6$ ，主要是处理小结节，覆盖了数据集中58%的结节。</p>
<p>第二个网络即<code>Archi-2</code> 的接收域为$30\times 30\times 10$ ，处理中等尺寸的结节，覆盖了数据集中85%的结节。</p>
<p>第三个网络即<code>Archi-3</code> 的接收域为$40\times 40\times 26$ ，处理中等尺寸的结节，覆盖了数据集中99%的结节。</p>
<p>使用softmax回归来综合三个模型的最后预测结果。模型 <code>Archi-1</code>预测 $I_j$属于分类 $c$ 的概率为 $P_1(\hat h_j=c|I_j;\theta _1)$ (不同的 $\theta$ )，<code>Archi-2</code>和<code>Archi-3</code> 类似。</p>
<p>混合概率由权重的线性组合评估 </p>
<script type="math/tex; mode=display">
P_{fusion}(\hat y_j)=c\|I_j=\sum _{\phi \epsilon{1,2,3}}\gamma _{\phi} P_{\phi}(\hat y_j=c\|I_j;\theta _\phi)</script><p>其中 $P_{fusion}(\hat y_j=c|I_j)$ 是接收域 $I_j$ 属于分类 $c$ 的整个框架的混合预测概率。常量权重$\gamma _phi$ 是在实验数据集中经过网格搜索得到的 $\gamma _1=0.3,\gamma _2=0.4,\gamma _3=0.3$</p>
<h2 id="四-训练过程"><a href="#四-训练过程" class="headerlink" title="四 训练过程"></a>四 训练过程</h2><p> 权重 $\theta$ 基于随机梯度下降来学习，即每次迭代中参数更新都是基于mini-batch的训练样本。比赛提供了正负样本，我们根据结节候选位置抽取了 $20\times 20\times 6,30\times 30\times 10,40\times 40\times 26$ 尺寸的立方块。</p>
<ul>
<li><p><strong>数据增强：</strong>由于数据集中正负样本比例为 <code>490:1</code>,我们对正样本(真的肺结节)进行转换、扭曲增强。转换过程中，对质心坐标每个体素，每个轴在横向平面做90,180，270度扭曲。最后获得了650个训练样本。</p>
</li>
<li><p><strong>归一化处理:</strong>我们将放射密度强度剪切（类似截断的正态分布之类）到(-1000，400)HU单位之间，并将它们归一化到(0,1),然后减去平均灰度值，来适应网络。</p>
<ul>
<li><strong>参数设置:</strong> 权重使用高斯分布 $N(0,0.001^2)$  的随机初始化，并使用标准后向传播更新。学习率初始为0.3，每5000次迭代衰减5%（较大的学习率，因为网络是从头开始训练）。mini-batch为200，冲量系数设置为0.9，dropout应用到卷积层和全连接层，比例为0.2。</li>
</ul>
</li>
<li><p><strong>网络:</strong> 三个网络独立的训练和验证，每个网络训练消耗6小时(GPU Nvidia TITAN Z)</p>
</li>
</ul>
<h2 id="五-实验-LUNA2016"><a href="#五-实验-LUNA2016" class="headerlink" title="五 实验(LUNA2016)"></a>五 实验(LUNA2016)</h2><h3 id="5-1-数据集和候选生成"><a href="#5-1-数据集和候选生成" class="headerlink" title="5.1 数据集和候选生成"></a>5.1 数据集和候选生成</h3><p>  本文是用于处理假阳性剔除任务，即给定一个候选位置集合，预测每个候选是肺结节的概率。</p>
<p> 比赛数据从 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041807/" target="_blank" rel="noopener">publicly available Lung Image Database Consortium (LIDC)</a>过滤掉888张CT。体积在水平面以 $515\times 512$ 分辨率，元素空间为 $0.74 \times 0.74 mm^2$ ,同时slices厚度小于2.5mm。 肺结节数据由经验丰富的外科医生标注位置，只保留了 1186个半径大于3mm的候选（4个外科医生中至少有3个标注了的）</p>
<h3 id="5-2-评估指标"><a href="#5-2-评估指标" class="headerlink" title="5.2 评估指标"></a>5.2 评估指标</h3><p>比赛评估检测结果，通过灵敏度和每个scan中出现假阳性的数目。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>常见医疗扫描图像处理步骤</title>
    <url>/2017/01/20/medical_image_process/</url>
    <content><![CDATA[<h2 id="一-数据格式"><a href="#一-数据格式" class="headerlink" title="一 数据格式"></a>一 数据格式</h2><h3 id="1-1-dicom"><a href="#1-1-dicom" class="headerlink" title="1.1  dicom"></a>1.1  dicom</h3><p>DICOM是医学图像中标准文件，这些文件包含了诸多的元数据信息（比如像素尺寸，每个维度的一像素代表真实世界里的长度）。此处以<strong>kaggle Data Science Bowl</strong> 数据集为例。</p>
<p> <a href="https://www.kaggle.com/c/data-science-bowl-2017/data" target="_blank" rel="noopener">data-science-bowl-2017</a>。数据列表如下:</p>
<p><img src="/images/blog/dicom_data_format.png" alt="dicom格式的图像"></p>
<p>后缀为 <code>.dcm</code>。</p>
<p>每个病人的一次扫描CT(scan)可能有<strong>几十到一百多</strong>个dcm数据文件(slices)。可以使用 python的<code>dicom</code>包读取，读取示例代码如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dicom.read_file(&#39;&#x2F;data&#x2F;lung_competition&#x2F;stage1&#x2F;7050f8141e92fa42fd9c471a8b2f50ce&#x2F;498d16aa2222d76cae1da144ddc59a13.dcm&#39;)</span><br></pre></td></tr></table></figure>
<p>其pixl_array包含了真实数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">slices &#x3D; [dicom.read_file(os.path.join(folder_name,filename)) for filename in os.listdir(folder_name)]</span><br><span class="line">slices &#x3D; np.stack([s.pixel_array for s in slices])</span><br></pre></td></tr></table></figure>
<h3 id="1-2-mhd格式"><a href="#1-2-mhd格式" class="headerlink" title="1.2 mhd格式"></a>1.2 mhd格式</h3><p>mhd格式是另外一种数据格式，来源于(LUNA2016)[<a href="https://luna16.grand-challenge.org/data/]。每个病人**一个mhd文件和一个同名的raw**文件。如下" target="_blank" rel="noopener">https://luna16.grand-challenge.org/data/]。每个病人**一个mhd文件和一个同名的raw**文件。如下</a>:</p>
<p><img src="/images/blog/mhd_2.png" alt="dicom格式的图像"></p>
<p>一个<code>mhd</code>通常有几百兆，对应的<code>raw</code>文件只有1kb。<code>mhd</code>文件需要借助python的<code>SimpleITK</code>包来处理。<a href="http://www.simpleitk.org/SimpleITK/help/documentation.html" target="_blank" rel="noopener">SimpleITK</a><br>示例代码如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import SimpleITK as sitk</span><br><span class="line">itk_img &#x3D; sitk.ReadImage(img_file)</span><br><span class="line">img_array &#x3D; sitk.GetArrayFromImage(itk_img) # indexes are z,y,x (notice the ordering)</span><br><span class="line">num_z, height, width &#x3D; img_array.shape        #heightXwidth constitute the transverse plane</span><br><span class="line">origin &#x3D; np.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)</span><br><span class="line">spacing &#x3D; np.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，SimpleITK的<code>img_array</code>的数组不是直接的像素值，而是相对于CT扫描中原点位置的差值，需要做进一步转换。转换步骤参考 <a href="https://chsasank.github.io/sitk-docs/user_guide/image.html" target="_blank" rel="noopener">SimpleITK图像转换</a></p>
<h3 id="1-3-查看CT扫描文件软件"><a href="#1-3-查看CT扫描文件软件" class="headerlink" title="1.3 查看CT扫描文件软件"></a>1.3 查看CT扫描文件软件</h3><p>一个开源免费的查看软件 <a href="http://ric.uthscsa.edu/mango/" target="_blank" rel="noopener">mango</a></p>
<p><img src="/images/blog/image_soft.png" alt="dicom格式的图像"></p>
<h2 id="二-dicom格式数据处理过程"><a href="#二-dicom格式数据处理过程" class="headerlink" title="二   dicom格式数据处理过程"></a>二   dicom格式数据处理过程</h2><h3 id="2-1-处理思路"><a href="#2-1-处理思路" class="headerlink" title="2.1  处理思路"></a>2.1  处理思路</h3><p> 首先，需要明白的是医学扫描图像(scan)其实是三维图像，使用代码读取之后开源查看不同的切面的切片(slices)，可以从不同轴切割</p>
<p><img src="/images/blog/3dscan.png" alt="dicom格式的图像"></p>
<p>如下图展示了一个病人CT扫描图中，其中部分切片slices</p>
<p><img src="/images/blog/lung_slices.png" alt="dicom格式的图像"></p>
<p>其次，CT扫描图是包含了所有组织的，如果直接去看，看不到任何有用信息。需要做一些预处理,预处理中一个重要的概念是放射剂量，衡量单位为<code>HU</code>(<strong>Hounsfield Unit</strong>)，下表是不同放射剂量对应的组织器官</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>substance</th>
<th>HU</th>
</tr>
</thead>
<tbody>
<tr>
<td>空气</td>
<td>-1000</td>
</tr>
<tr>
<td>肺</td>
<td>-500</td>
</tr>
<tr>
<td>脂肪</td>
<td>-100到-50</td>
</tr>
<tr>
<td>水</td>
<td>0</td>
</tr>
<tr>
<td>CSF</td>
<td>15</td>
</tr>
<tr>
<td>肾</td>
<td>30</td>
</tr>
<tr>
<td>血液</td>
<td>+30到+45</td>
</tr>
<tr>
<td>肌肉</td>
<td>+10到+40</td>
</tr>
<tr>
<td>灰质</td>
<td>+37到+45</td>
</tr>
<tr>
<td>白质</td>
<td>+20到+30</td>
</tr>
<tr>
<td>Liver</td>
<td>+40到+60</td>
</tr>
<tr>
<td>软组织,constrast</td>
<td>+100到+300</td>
</tr>
<tr>
<td>骨头</td>
<td>+700(软质骨)到+3000(皮质骨)</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Hounsfield Unit &#x3D; pixel_value * rescale_slope + rescale_intercept</span><br></pre></td></tr></table></figure>
<p>一般情况rescale slope = 1, intercept = -1024。</p>
<p>上表中肺部组织的HU数值为-500,但通常是大于这个值，比如-320、-400。挑选出这些区域，然后做其他变换抽取出肺部像素点。</p>
<h3 id="2-2-先载入必要的包"><a href="#2-2-先载入必要的包" class="headerlink" title="2.2  先载入必要的包"></a>2.2  先载入必要的包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">this script is used for basic process of lung 2017 in Data Science Bowl</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">import glob</span><br><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line">import SimpleITK as sitk</span><br><span class="line"></span><br><span class="line">import numpy as np # linear algebra</span><br><span class="line">import pandas as pd # data processing, CSV file I&#x2F;O (e.g. pd.read_csv)</span><br><span class="line">import skimage, os</span><br><span class="line">from skimage.morphology import ball, disk, dilation, binary_erosion, remove_small_objects, erosion, closing, reconstruction, binary_closing</span><br><span class="line">from skimage.measure import label,regionprops, perimeter</span><br><span class="line">from skimage.morphology import binary_dilation, binary_opening</span><br><span class="line">from skimage.filters import roberts, sobel</span><br><span class="line">from skimage import measure, feature</span><br><span class="line">from skimage.segmentation import clear_border</span><br><span class="line">from skimage import data</span><br><span class="line">from scipy import ndimage as ndi</span><br><span class="line">import matplotlib</span><br><span class="line">#matplotlib.use(&#39;Agg&#39;)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d.art3d import Poly3DCollection</span><br><span class="line">import dicom</span><br><span class="line">import scipy.misc</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>
<p>如下代码是载入一个扫描面，包含了多个切片(slices)，我们仅简化的将其存储为python列表。<strong>数据集中每个目录都是一个扫描面集（一个病人）</strong>。有个元数据域丢失，即Z轴方向上的像素尺寸，也即切片的厚度 。所幸，我们可以用其他值推测出来，并加入到元数据中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Load the scans in given folder path</span><br><span class="line">def load_scan(path):</span><br><span class="line">    slices &#x3D; [dicom.read_file(path + &#39;&#x2F;&#39; + s) for s in os.listdir(path)]</span><br><span class="line">    slices.sort(key &#x3D; lambda x: int(x.ImagePositionPatient[2]))</span><br><span class="line">    try:</span><br><span class="line">        slice_thickness &#x3D; np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])</span><br><span class="line">    except:</span><br><span class="line">        slice_thickness &#x3D; np.abs(slices[0].SliceLocation - slices[1].SliceLocation)</span><br><span class="line"></span><br><span class="line">    for s in slices:</span><br><span class="line">        s.SliceThickness &#x3D; slice_thickness</span><br><span class="line"></span><br><span class="line">    return slices</span><br></pre></td></tr></table></figure>
<h3 id="2-3-灰度值转换为HU单元"><a href="#2-3-灰度值转换为HU单元" class="headerlink" title="2.3 灰度值转换为HU单元"></a>2.3 灰度值转换为HU单元</h3><p>首先去除灰度值为 -2000的pixl_array，CT扫描边界之外的灰度值固定为-2000(<strong>dicom和mhd都是这个值</strong>)。第一步是设定这些值为0，当前对应为空气（值为0）</p>
<p>回到HU单元，乘以rescale比率并加上intercept(存储在扫描面的元数据中)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_pixels_hu(slices):</span><br><span class="line">    image &#x3D; np.stack([s.pixel_array for s in slices])</span><br><span class="line">    # Convert to int16 (from sometimes int16),</span><br><span class="line">    # should be possible as values should always be low enough (&lt;32k)</span><br><span class="line">    image &#x3D; image.astype(np.int16)</span><br><span class="line">    # Set outside-of-scan pixels to 0</span><br><span class="line">    # The intercept is usually -1024, so air is approximately 0</span><br><span class="line">    image[image &#x3D;&#x3D; -2000] &#x3D; 0</span><br><span class="line"></span><br><span class="line">    # Convert to Hounsfield units (HU)</span><br><span class="line">    for slice_number in range(len(slices)):</span><br><span class="line"></span><br><span class="line">        intercept &#x3D; slices[slice_number].RescaleIntercept</span><br><span class="line">        slope &#x3D; slices[slice_number].RescaleSlope</span><br><span class="line"></span><br><span class="line">        if slope !&#x3D; 1:</span><br><span class="line">            image[slice_number] &#x3D; slope * image[slice_number].astype(np.float64)</span><br><span class="line">            image[slice_number] &#x3D; image[slice_number].astype(np.int16)</span><br><span class="line"></span><br><span class="line">        image[slice_number] +&#x3D; np.int16(intercept)</span><br><span class="line"></span><br><span class="line">    return np.array(image, dtype&#x3D;np.int16)</span><br></pre></td></tr></table></figure>
<p>可以查看病人的扫描HU值分布情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">first_patient &#x3D; load_scan(INPUT_FOLDER + patients[0])</span><br><span class="line">first_patient_pixels &#x3D; get_pixels_hu(first_patient)</span><br><span class="line">plt.hist(first_patient_pixels.flatten(), bins&#x3D;80, color&#x3D;&#39;c&#39;)</span><br><span class="line">plt.xlabel(&quot;Hounsfield Units (HU)&quot;)</span><br><span class="line">plt.ylabel(&quot;Frequency&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/HU_histograph.png" alt="dicom格式的图像"></p>
<h3 id="2-4-重采样"><a href="#2-4-重采样" class="headerlink" title="2.4  重采样"></a>2.4  重采样</h3><p>不同扫描面的像素尺寸、粗细粒度是不同的。这不利于我们进行CNN任务，我们可以使用同构采样。</p>
<p>一个扫描面的像素区间可能是[2.5,0.5,0.5],即切片之间的距离为2.5mm。可能另外一个扫描面的范围是[1.5,0.725,0.725]。这可能不利于自动分析。</p>
<p>常见的处理方法是从全数据集中以固定的同构分辨率重新采样，将所有的东西采样为1mmx1mmx1mm像素。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def resample(image, scan, new_spacing&#x3D;[1,1,1]):</span><br><span class="line">    # Determine current pixel spacing</span><br><span class="line">    spacing &#x3D; map(float, ([scan[0].SliceThickness] + scan[0].PixelSpacing))</span><br><span class="line">    spacing &#x3D; np.array(list(spacing))</span><br><span class="line">    resize_factor &#x3D; spacing &#x2F; new_spacing</span><br><span class="line">    new_real_shape &#x3D; image.shape * resize_factor</span><br><span class="line">    new_shape &#x3D; np.round(new_real_shape)</span><br><span class="line">    real_resize_factor &#x3D; new_shape &#x2F; image.shape</span><br><span class="line">    new_spacing &#x3D; spacing &#x2F; real_resize_factor</span><br><span class="line"></span><br><span class="line">    image &#x3D; scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode&#x3D;&#39;nearest&#39;)</span><br><span class="line"></span><br><span class="line">    return image, new_spacing</span><br></pre></td></tr></table></figure>
<p>现在重新取样病人的像素，将其映射到一个同构分辨率 1mm x1mm x1mm。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pix_resampled, spacing &#x3D; resample(first_patient_pixels, first_patient, [1,1,1])</span><br></pre></td></tr></table></figure>
<p>使用matplotlib输出肺部扫描的3D图像方法。可能需要一两分钟。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_3d(image, threshold&#x3D;-300):</span><br><span class="line"></span><br><span class="line">    # Position the scan upright,</span><br><span class="line">    # so the head of the patient would be at the top facing the camera</span><br><span class="line">    p &#x3D; image.transpose(2,1,0)</span><br><span class="line"></span><br><span class="line">    verts, faces &#x3D; measure.marching_cubes(p, threshold)</span><br><span class="line">    fig &#x3D; plt.figure(figsize&#x3D;(10, 10))</span><br><span class="line">    ax &#x3D; fig.add_subplot(111, projection&#x3D;&#39;3d&#39;)</span><br><span class="line">    # Fancy indexing: &#96;verts[faces]&#96; to generate a collection of triangles</span><br><span class="line">    mesh &#x3D; Poly3DCollection(verts[faces], alpha&#x3D;0.1)</span><br><span class="line">    face_color &#x3D; [0.5, 0.5, 1]</span><br><span class="line">    mesh.set_facecolor(face_color)</span><br><span class="line">    ax.add_collection3d(mesh)</span><br><span class="line">    ax.set_xlim(0, p.shape[0])</span><br><span class="line">    ax.set_ylim(0, p.shape[1])</span><br><span class="line">    ax.set_zlim(0, p.shape[2])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>打印函数有个阈值参数，来打印特定的结构，比如tissue或者骨头。400是一个仅仅打印骨头的阈值(HU对照表)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plot_3d(pix_resampled, 400)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/lung3d_bone.jpg" alt="dicom格式的图像"></p>
<h3 id="2-5-输出一个病人scans中所有切面slices"><a href="#2-5-输出一个病人scans中所有切面slices" class="headerlink" title="2.5 输出一个病人scans中所有切面slices"></a>2.5 输出一个病人scans中所有切面slices</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_ct_scan(scan):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">            plot a few more images of the slices</span><br><span class="line">    :param scan:</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    f, plots &#x3D; plt.subplots(int(scan.shape[0] &#x2F; 20) + 1, 4, figsize&#x3D;(50, 50))</span><br><span class="line">    for i in range(0, scan.shape[0], 5):</span><br><span class="line">        plots[int(i &#x2F; 20), int((i % 20) &#x2F; 5)].axis(&#39;off&#39;)</span><br><span class="line">        plots[int(i &#x2F; 20), int((i % 20) &#x2F; 5)].imshow(scan[i], cmap&#x3D;plt.cm.bone)</span><br></pre></td></tr></table></figure>
<p>此方法的效果示例如下:</p>
<p><img src="/images/blog/lung_slices_2.png" alt="dicom格式的图像"></p>
<h3 id="2-6-定义分割出CT切面里面肺部组织的函数"><a href="#2-6-定义分割出CT切面里面肺部组织的函数" class="headerlink" title="2.6 定义分割出CT切面里面肺部组织的函数"></a>2.6 定义分割出CT切面里面肺部组织的函数</h3><p>下面的代码使用了pythonde 的图像形态学操作。具体可以参考<a href="http://www.cnblogs.com/denny402/p/5166258.html" target="_blank" rel="noopener">python高级形态学操作</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_segmented_lungs(im, plot&#x3D;False):</span><br><span class="line"></span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    This funtion segments the lungs from the given 2D slice.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        f, plots &#x3D; plt.subplots(8, 1, figsize&#x3D;(5, 40))</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 1: Convert into a binary image.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    binary &#x3D; im &lt; 604</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[0].axis(&#39;off&#39;)</span><br><span class="line">        plots[0].set_title(&#39;binary image&#39;)</span><br><span class="line">        plots[0].imshow(binary, cmap&#x3D;plt.cm.bone)</span><br><span class="line"></span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 2: Remove the blobs connected to the border of the image.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    cleared &#x3D; clear_border(binary)</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[1].axis(&#39;off&#39;)</span><br><span class="line">        plots[1].set_title(&#39;after clear border&#39;)</span><br><span class="line">        plots[1].imshow(cleared, cmap&#x3D;plt.cm.bone)</span><br><span class="line"></span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 3: Label the image.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    label_image &#x3D; label(cleared)</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[2].axis(&#39;off&#39;)</span><br><span class="line">        plots[2].set_title(&#39;found all connective graph&#39;)</span><br><span class="line">        plots[2].imshow(label_image, cmap&#x3D;plt.cm.bone)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 4: Keep the labels with 2 largest areas.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    areas &#x3D; [r.area for r in regionprops(label_image)]</span><br><span class="line">    areas.sort()</span><br><span class="line">    if len(areas) &gt; 2:</span><br><span class="line">        for region in regionprops(label_image):</span><br><span class="line">            if region.area &lt; areas[-2]:</span><br><span class="line">                for coordinates in region.coords:</span><br><span class="line">                       label_image[coordinates[0], coordinates[1]] &#x3D; 0</span><br><span class="line">    binary &#x3D; label_image &gt; 0</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[3].axis(&#39;off&#39;)</span><br><span class="line">        plots[3].set_title(&#39; Keep the labels with 2 largest areas&#39;)</span><br><span class="line">        plots[3].imshow(binary, cmap&#x3D;plt.cm.bone)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 5: Erosion operation with a disk of radius 2. This operation is</span><br><span class="line">    seperate the lung nodules attached to the blood vessels.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    selem &#x3D; disk(2)</span><br><span class="line">    binary &#x3D; binary_erosion(binary, selem)</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[4].axis(&#39;off&#39;)</span><br><span class="line">        plots[4].set_title(&#39;seperate the lung nodules attached to the blood vessels&#39;)</span><br><span class="line">        plots[4].imshow(binary, cmap&#x3D;plt.cm.bone)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 6: Closure operation with a disk of radius 10. This operation is</span><br><span class="line">    to keep nodules attached to the lung wall.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    selem &#x3D; disk(10)</span><br><span class="line">    binary &#x3D; binary_closing(binary, selem)</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[5].axis(&#39;off&#39;)</span><br><span class="line">        plots[5].set_title(&#39;keep nodules attached to the lung wall&#39;)</span><br><span class="line">        plots[5].imshow(binary, cmap&#x3D;plt.cm.bone)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 7: Fill in the small holes inside the binary mask of lungs.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    edges &#x3D; roberts(binary)</span><br><span class="line">    binary &#x3D; ndi.binary_fill_holes(edges)</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[6].axis(&#39;off&#39;)</span><br><span class="line">        plots[6].set_title(&#39;Fill in the small holes inside the binary mask of lungs&#39;)</span><br><span class="line">        plots[6].imshow(binary, cmap&#x3D;plt.cm.bone)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Step 8: Superimpose the binary mask on the input image.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    get_high_vals &#x3D; binary &#x3D;&#x3D; 0</span><br><span class="line">    im[get_high_vals] &#x3D; 0</span><br><span class="line">    if plot &#x3D;&#x3D; True:</span><br><span class="line">        plots[7].axis(&#39;off&#39;)</span><br><span class="line">        plots[7].set_title(&#39;Superimpose the binary mask on the input image&#39;)</span><br><span class="line">        plots[7].imshow(im, cmap&#x3D;plt.cm.bone)</span><br><span class="line"></span><br><span class="line">    return im</span><br></pre></td></tr></table></figure>
<p>此方法每个步骤对图像做不同的处理，依次为二值化、清除边界、连通区域标记、腐蚀操作、闭合运算、孔洞填充、效果如下:</p>
<p><img src="/images/blog/lung_seg_example.png" alt="dicom格式的图像"></p>
<h3 id="2-7-肺部图像分割"><a href="#2-7-肺部图像分割" class="headerlink" title="2.7 肺部图像分割"></a>2.7 肺部图像分割</h3><p>为了减少有问题的空间，我们可以分割肺部图像（有时候是附近的组织）。这包含一些步骤，包括区域增长和形态运算，此时，我们只分析相连组件。</p>
<p>步骤如下：</p>
<ul>
<li><p>阈值图像（-320HU是个极佳的阈值，但是此方法中不是必要）</p>
</li>
<li><p>处理相连的组件，以决定当前患者的空气的标签，以1填充这些二值图像</p>
</li>
<li><p>可选：当前扫描的每个轴上的切片，选定最大固态连接的组织（当前患者的肉体和空气），并且其他的为0。以掩码的方式填充肺部结构。</p>
</li>
<li><p>只保留最大的气袋（人类躯体内到处都有气袋）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def largest_label_volume(im, bg&#x3D;-1):</span><br><span class="line">    vals, counts &#x3D; np.unique(im, return_counts&#x3D;True)</span><br><span class="line">    counts &#x3D; counts[vals !&#x3D; bg]</span><br><span class="line">    vals &#x3D; vals[vals !&#x3D; bg]</span><br><span class="line">    if len(counts) &gt; 0:</span><br><span class="line">        return vals[np.argmax(counts)]</span><br><span class="line">    else:</span><br><span class="line">        return None</span><br><span class="line">def segment_lung_mask(image, fill_lung_structures&#x3D;True):</span><br><span class="line"></span><br><span class="line">    # not actually binary, but 1 and 2.</span><br><span class="line">    # 0 is treated as background, which we do not want</span><br><span class="line">    binary_image &#x3D; np.array(image &gt; -320, dtype&#x3D;np.int8)+1</span><br><span class="line">    labels &#x3D; measure.label(binary_image)</span><br><span class="line"></span><br><span class="line">    # Pick the pixel in the very corner to determine which label is air.</span><br><span class="line">    #   Improvement: Pick multiple background labels from around the patient</span><br><span class="line">    #   More resistant to &quot;trays&quot; on which the patient lays cutting the air</span><br><span class="line">    #   around the person in half</span><br><span class="line">    background_label &#x3D; labels[0,0,0]</span><br><span class="line"></span><br><span class="line">    #Fill the air around the person</span><br><span class="line">    binary_image[background_label &#x3D;&#x3D; labels] &#x3D; 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # Method of filling the lung structures (that is superior to something like</span><br><span class="line">    # morphological closing)</span><br><span class="line">    if fill_lung_structures:</span><br><span class="line">        # For every slice we determine the largest solid structure</span><br><span class="line">        for i, axial_slice in enumerate(binary_image):</span><br><span class="line">            axial_slice &#x3D; axial_slice - 1</span><br><span class="line">            labeling &#x3D; measure.label(axial_slice)</span><br><span class="line">            l_max &#x3D; largest_label_volume(labeling, bg&#x3D;0)</span><br><span class="line"></span><br><span class="line">            if l_max is not None: #This slice contains some lung</span><br><span class="line">                binary_image[i][labeling !&#x3D; l_max] &#x3D; 1</span><br><span class="line"></span><br><span class="line">    binary_image -&#x3D; 1 #Make the image actual binary</span><br><span class="line">    binary_image &#x3D; 1-binary_image # Invert it, lungs are now 1</span><br><span class="line"></span><br><span class="line">    # Remove other air pockets insided body</span><br><span class="line">    labels &#x3D; measure.label(binary_image, background&#x3D;0)</span><br><span class="line">    l_max &#x3D; largest_label_volume(labels, bg&#x3D;0)</span><br><span class="line">    if l_max is not None: # There are air pockets</span><br><span class="line">        binary_image[labels !&#x3D; l_max] &#x3D; 0</span><br><span class="line"></span><br><span class="line">    return binary_image</span><br></pre></td></tr></table></figure>
<p>查看切割效果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">segmented_lungs &#x3D; segment_lung_mask(pix_resampled, False)</span><br><span class="line">segmented_lungs_fill &#x3D; segment_lung_mask(pix_resampled, True)</span><br><span class="line">plot_3d(segmented_lungs, 0)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/lung3d_bone2.jpg" alt="dicom格式的图像"></p>
<p>我们可以将肺内的结构也包含进来（结节是固体），不仅仅只是肺部内的空气</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plot_3d(segmented_lungs_fill, 0)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/lung3d_bone3.jpg" alt="dicom格式的图像"></p>
<p>使用mask时，要注意首先进行形态扩充(python的<code>skimage</code>的skimage.morphology)操作（即使用圆形kernel，结节是球体），参考 <a href="http://www.cnblogs.com/denny402/p/5166258.html" target="_blank" rel="noopener">python形态操作</a>。这会在所有方向（维度）上扩充mask。仅仅肺部的空气+结构将不会包含所有结节，事实上有可能遗漏黏在肺部一侧的结节（这会经常出现，所以建议最好是扩充mask）。</p>
<h3 id="2-8-数据标准化处理"><a href="#2-8-数据标准化处理" class="headerlink" title="2.8 数据标准化处理"></a>2.8 数据标准化处理</h3><p>归一化处理</p>
<p>当前的值范围是[-1024,2000]。而任意大于400的值并不是处理肺结节需要考虑，因为它们都是不同反射密度下的骨头。LUNA16竞赛中常用来做归一化处理的阈值集是-1000和400.以下代码</p>
<p><strong>归一化</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MIN_BOUND &#x3D; -1000.0</span><br><span class="line">MAX_BOUND &#x3D; 400.0</span><br><span class="line"></span><br><span class="line">def normalize(image):</span><br><span class="line">    image &#x3D; (image - MIN_BOUND) &#x2F; (MAX_BOUND - MIN_BOUND)</span><br><span class="line">    image[image&gt;1] &#x3D; 1.</span><br><span class="line">    image[image&lt;0] &#x3D; 0.</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure>
<p><strong>0值中心化</strong></p>
<p>简单来说就是所有像素值减去均值。LUNA16竞赛中的均值大约是0.25.</p>
<p><strong>不要对每一张图像做零值中心化（此处像是在kernel中完成的）CT扫描器返回的是校准后的精确HU计量。不会出现普通图像中会出现某些图像低对比度和明亮度的情况</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PIXEL_MEAN &#x3D; 0.25</span><br><span class="line"></span><br><span class="line">def zero_center(image):</span><br><span class="line">    image &#x3D; image - PIXEL_MEAN</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure>
<p>归一化和零值中心化的操作主要是为了后续训练网络，零值中心化是网络收敛的关键。</p>
<h2 id="三-mhd格式数据处理过程"><a href="#三-mhd格式数据处理过程" class="headerlink" title="三 mhd格式数据处理过程"></a>三 mhd格式数据处理过程</h2><h3 id="3-1-处理思路"><a href="#3-1-处理思路" class="headerlink" title="3.1 处理思路"></a>3.1 处理思路</h3><p>mhd的数据只是格式与dicom不一样，其实质包含的都是病人扫描。处理MHD需要借助<code>SimpleIKT</code>这个包，处理思路详情可以参考Data Science Bowl2017的toturail <a href="https://www.kaggle.com/c/data-science-bowl-2017#tutorial" target="_blank" rel="noopener">Data Science Bowl 2017</a>。需要注意的是MHD格式的数据没有HU值，它的值域范围与dicom很不同。</p>
<p>我们以LUNA2016年的数据处理流程为例。参考代码为 <a href="https://github.com/booz-allen-hamilton/DSB3Tutorial/tree/master/tutorial_code" target="_blank" rel="noopener">LUNA2016数据切割</a></p>
<h3 id="3-2-载入必要的包"><a href="#3-2-载入必要的包" class="headerlink" title="3.2 载入必要的包"></a>3.2 载入必要的包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import SimpleITK as sitk</span><br><span class="line">import numpy as np</span><br><span class="line">import csv</span><br><span class="line">from glob import glob</span><br><span class="line">import pandas as pd</span><br><span class="line">file_list&#x3D;glob(luna_subset_path+&quot;*.mhd&quot;)</span><br><span class="line">#####################</span><br><span class="line">#</span><br><span class="line"># Helper function to get rows in data frame associated</span><br><span class="line"># with each file</span><br><span class="line">def get_filename(case):</span><br><span class="line">    global file_list</span><br><span class="line">    for f in file_list:</span><br><span class="line">        if case in f:</span><br><span class="line">            return(f)</span><br><span class="line">#</span><br><span class="line"># The locations of the nodes</span><br><span class="line">df_node &#x3D; pd.read_csv(luna_path+&quot;annotations.csv&quot;)</span><br><span class="line">df_node[&quot;file&quot;] &#x3D; df_node[&quot;seriesuid&quot;].apply(get_filename)</span><br><span class="line">df_node &#x3D; df_node.dropna()</span><br><span class="line">#####</span><br><span class="line">#</span><br><span class="line"># Looping over the image files</span><br><span class="line">#</span><br><span class="line">fcount &#x3D; 0</span><br><span class="line">for img_file in file_list:</span><br><span class="line">    print &quot;Getting mask for image file %s&quot; % img_file.replace(luna_subset_path,&quot;&quot;)</span><br><span class="line">    mini_df &#x3D; df_node[df_node[&quot;file&quot;]&#x3D;&#x3D;img_file] #get all nodules associate with file</span><br><span class="line">    if len(mini_df)&gt;0:       # some files may not have a nodule--skipping those</span><br><span class="line">        biggest_node &#x3D; np.argsort(mini_df[&quot;diameter_mm&quot;].values)[-1]   # just using the biggest node</span><br><span class="line">        node_x &#x3D; mini_df[&quot;coordX&quot;].values[biggest_node]</span><br><span class="line">        node_y &#x3D; mini_df[&quot;coordY&quot;].values[biggest_node]</span><br><span class="line">        node_z &#x3D; mini_df[&quot;coordZ&quot;].values[biggest_node]</span><br><span class="line">        diam &#x3D; mini_df[&quot;diameter_mm&quot;].values[biggest_node]</span><br></pre></td></tr></table></figure>
<h3 id="3-3-LUNA16的MHD格式数据的值"><a href="#3-3-LUNA16的MHD格式数据的值" class="headerlink" title="3.3 LUNA16的MHD格式数据的值"></a>3.3 LUNA16的MHD格式数据的值</h3><p>一直在寻找MHD格式数据的处理方法，对于dicom格式的CT有很多论文根据其HU值域可以轻易地分割肺、骨头、血液等，但是对于MHD没有这样的参考。从<a href="https://grand-challenge.org/site/luna16/forum/" target="_blank" rel="noopener">LUNA16论坛</a>得到的解释是，LUNA16的MHD数据已经转换为HU值了，不需要再使用slope和intercept来做rescale变换了。此论坛主题下，有人提出MHD格式没有提供pixel spacing(mm) 和 slice thickness(mm) ，而标准文件annotation.csv文件中结节的半径和坐标都是mm单位，最后确认的是MHD格式文件中只保留了体素尺寸以及坐标原点位置，没有保存slice thickness。即，dicom才是原始数据格式。</p>
<h3 id="3-4-坐标体系变换"><a href="#3-4-坐标体系变换" class="headerlink" title="3.4 坐标体系变换"></a>3.4 坐标体系变换</h3><p>MHD值的坐标体系是体素，以mm为单位（dicom的值是GV灰度值）。结节的位置是CT scanner坐标轴里面相对原点的mm值，需要将其转换到真实坐标轴位置，可以使用<code>SimpleITK</code>包中的 <code>GetOrigin()</code> <code>GetSpacing()</code>。图像数据是以512x512数组的形式给出的。</p>
<p>坐标变换如下：</p>
<p><img src="/images/blog/mhd_coordinate_transfer.png" alt="dicom格式的图像"></p>
<p>相应的代码处理如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">itk_img &#x3D; sitk.ReadImage(img_file)</span><br><span class="line">img_array &#x3D; sitk.GetArrayFromImage(itk_img) # indexes are z,y,x (notice the ordering)</span><br><span class="line">center &#x3D; np.array([node_x,node_y,node_z])   # nodule center</span><br><span class="line">origin &#x3D; np.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)</span><br><span class="line">spacing &#x3D; np.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)</span><br><span class="line">v_center &#x3D;np.rint((center-origin)&#x2F;spacing)  # nodule center in voxel space (still x,y,z ordering)</span><br></pre></td></tr></table></figure>
<p><strong>在LUNA16的标注CSV文件中标注了结节中心的X,Y,Z轴坐标，但是实际取值的时候取的是Z轴最后三层的数组(img_array)</strong>。</p>
<p>下述代码只提取了包含结节的最后三个slice的数据，代码参考自<code>LUNA_mask_extraction.py</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i &#x3D; 0</span><br><span class="line">for i_z in range(int(v_center[2])-1,int(v_center[2])+2):</span><br><span class="line">    mask &#x3D; make_mask(center,diam,i_z*spacing[2]+origin[2],width,height,spacing,origin)</span><br><span class="line">    masks[i] &#x3D; mask</span><br><span class="line">    imgs[i] &#x3D; matrix2int16(img_array[i_z])</span><br><span class="line">    i+&#x3D;1</span><br><span class="line">np.save(output_path+&quot;images_%d.npy&quot; % (fcount) ,imgs)</span><br><span class="line">np.save(output_path+&quot;masks_%d.npy&quot; % (fcount) ,masks)</span><br></pre></td></tr></table></figure>
<h3 id="3-5-查看结节"><a href="#3-5-查看结节" class="headerlink" title="3.5 查看结节"></a>3.5 查看结节</h3><p>以下代码用于查看原始CT和结节mask。其实就是用matplotlib打印上一步存储的npy文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">imgs &#x3D; np.load(output_path+&#39;images_0.npy&#39;)</span><br><span class="line">masks &#x3D; np.load(output_path+&#39;masks_0.npy&#39;)</span><br><span class="line">for i in range(len(imgs)):</span><br><span class="line">    print &quot;image %d&quot; % i</span><br><span class="line">    fig,ax &#x3D; plt.subplots(2,2,figsize&#x3D;[8,8])</span><br><span class="line">    ax[0,0].imshow(imgs[i],cmap&#x3D;&#39;gray&#39;)</span><br><span class="line">    ax[0,1].imshow(masks[i],cmap&#x3D;&#39;gray&#39;)</span><br><span class="line">    ax[1,0].imshow(imgs[i]*masks[i],cmap&#x3D;&#39;gray&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line">    raw_input(&quot;hit enter to cont : &quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/nodule_mask1.png" alt="示例结节和mask"></p>
<p>接下来的处理和DICOM格式数据差不多，腐蚀膨胀、连通区域标记等。</p>
<p>参考信息：</p>
<blockquote>
<p>灰度值是pixel value经过重重LUT转换得到的用来进行显示的值，而这个转换过程是不可逆的，也就是说，灰度值无法转换为ct值。只能根据窗宽窗位得到一个大概的范围。<br> pixel value经过modality lut得到Hu，但是怀疑pixelvalue的读取出了问题。dicom文件中存在（0028，0106）（0028，0107）两个tag，分别是最大最小pixel value，可以用来检验你读取的pixel value 矩阵是否正确。</p>
<p>LUT全称look up table，实际上就是一张像素灰度值的映射表，它将实际采样到的像素灰度值经过一定的变换如阈值、反转、二值化、对比度调整、线性变换等，变成了另外一 个与之对应的灰度值，这样可以起到突出图像的有用信息，增强图像的光对比度的作用。</p>
</blockquote>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>RCNN,Fast RCNN,Faster RCNN 总结</title>
    <url>/2017/01/13/RCNN-series/</url>
    <content><![CDATA[<h2 id="一-背景知识"><a href="#一-背景知识" class="headerlink" title="一 背景知识"></a>一 背景知识</h2><h3 id="1-1-IOU的定义"><a href="#1-1-IOU的定义" class="headerlink" title="1.1  IOU的定义"></a>1.1  IOU的定义</h3><p>物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。</p>
<p><img src="/images/blog/rcnn1.jpg" alt="iou1"></p>
<p>IOU定义了两个bounding box的重叠度，如下图所示:</p>
<p><img src="/images/blog/rcnn2.jpg" alt="iou1"></p>
<p>矩形框A、B的一个重合度IOU计算公式为：</p>
<p>IOU=(A∩B)/(A∪B)</p>
<p>就是矩形框A、B的重叠面积占A、B并集的面积比例:</p>
<p>IOU=SI/(SA+SB-SI)</p>
<h3 id="1-2-非极大值抑制"><a href="#1-2-非极大值抑制" class="headerlink" title="1.2 非极大值抑制"></a>1.2 非极大值抑制</h3><p> RCNN算法，会从一张图片中找出n多个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：</p>
<p><img src="/images/blog/rcnn3.jpg" alt="iou1"></p>
<p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。</p>
<ol>
<li><p>从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p>
</li>
<li><p>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p>
</li>
<li><p>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p>
</li>
</ol>
<p>就这样一直重复，找到所有被保留下来的矩形框。</p>
<h3 id="1-3-一张图概览RCNN"><a href="#1-3-一张图概览RCNN" class="headerlink" title="1.3 一张图概览RCNN"></a>1.3 一张图概览RCNN</h3><p><img src="/images/blog/rcnn11.png" alt="RCNN相关方法对比"></p>
<h2 id="二-RCNN"><a href="#二-RCNN" class="headerlink" title="二 RCNN"></a>二 RCNN</h2><p> 算法概要：首先输入一张图片，我们先定位出2000个物体候选框，然后采用CNN提取每个候选框中图片的特征向量，特征向量的维度为4096维，接着采用svm算法对各个候选框中的物体进行分类识别。也就是总个过程分为三个程序：a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。具体的流程如下图片所示：</p>
<p><img src="/images/blog/rcnn4.png" alt="iou1"></p>
<p>下面分别讲解各个步骤。</p>
<h3 id="2-1-候选框搜索"><a href="#2-1-候选框搜索" class="headerlink" title="2.1 候选框搜索"></a>2.1 候选框搜索</h3><p> 当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这个采用的方法是传统文献的算法selective search (github上有源码)，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：</p>
<p><strong>(1)各向异性缩放</strong></p>
<p>这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；</p>
<p><strong>(2)各向同性缩放</strong></p>
<p>因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。这个有两种办法</p>
<p><strong>A.</strong> 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如下图(B)所示;</p>
<p><strong>B.</strong> 先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如下图(C)所示;</p>
<p><img src="/images/blog/rcnn5.png" alt="iou1"></p>
<p>对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。</p>
<p>上面处理完后，可以得到指定大小的图片，因为我们后面还要继续用这2000个候选框图片，继续训练CNN、SVM。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此我们需要用IOU为2000个bounding box打标签，以便下一步CNN训练使用。在CNN阶段，如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别，否则我们就把它当做背景类别。</p>
<h3 id="2-2-网络设计"><a href="#2-2-网络设计" class="headerlink" title="2.2 网络设计"></a>2.2 网络设计</h3><p> 网络架构我们有两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。</p>
<h4 id="2-2-1-网络初始化"><a href="#2-2-1-网络初始化" class="headerlink" title="2.2.1 网络初始化"></a>2.2.1 网络初始化</h4><p> 直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。</p>
<p> 网络优化求解：采用随机梯度下降法，学习速率大小为0.001；</p>
<h4 id="2-2-2-fine-tuning阶段"><a href="#2-2-2-fine-tuning阶段" class="headerlink" title="2.2.2 fine-tuning阶段"></a>2.2.2 fine-tuning阶段</h4><p>  我们接着采用selective search 搜索出来的候选框，然后处理到指定大小图片，继续对上面预训练的cnn模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个正样本、96个负样本。</p>
<h2 id="三-Fast-RCNN"><a href="#三-Fast-RCNN" class="headerlink" title="三 Fast RCNN"></a>三 Fast RCNN</h2><h3 id="3-1-引入原因"><a href="#3-1-引入原因" class="headerlink" title="3.1 引入原因"></a>3.1 引入原因</h3><p> FRCNN针对RCNN在训练时是multi-stage pipeline和训练的过程中很耗费时间空间的问题进行改进。它主要是将深度网络和后面的SVM分类两个阶段整合到一起，使用一个新的网络直接做分类和回归。主要做以下改进:</p>
<ol>
<li><p>最后一个卷积层后加了一个ROI pooling layer。ROI pooling layer首先可以将image中的ROI定位到feature map，然后是用一个单层的SPP layer将这个feature map patch池化为固定大小的feature之后再传入全连接层。</p>
</li>
<li><p>损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。</p>
</li>
</ol>
<h3 id="3-2-模型"><a href="#3-2-模型" class="headerlink" title="3.2 模型"></a>3.2 模型</h3><p>fast rcnn 的结构如下</p>
<p><img src="/images/blog/rcnn7.png" alt="fast rcnn结构"></p>
<p>图中省略了通过ss获得proposal的过程，第一张图中红框里的内容即为通过ss提取到的proposal，中间的一块是经过深度卷积之后得到的conv feature map，图中灰色的部分就是我们红框中的proposal对应于conv feature map中的位置，之后对这个特征经过ROI pooling layer处理，之后进行全连接。在这里得到的ROI feature vector最终被分享，一个进行全连接之后用来做softmax回归，用来进行分类，另一个经过全连接之后用来做bbox回归。</p>
<p><strong>注意：</strong> 对中间的Conv feature map进行特征提取。每一个区域经过RoI pooling layer和FC layers得到一个 <strong>固定长度</strong> 的feature vector(这里需要注意的是，输入到后面RoI pooling layer的feature map是在Conv feature map上提取的，故整个特征提取过程，只计算了一次卷积。虽然在最开始也提取出了大量的RoI，但他们还是作为整体输入进卷积网络的，最开始提取出的RoI区域只是为了最后的Bounding box 回归时使用，用来输出原图中的位置)。</p>
<h3 id="3-3-SPP网络"><a href="#3-3-SPP网络" class="headerlink" title="3.3 SPP网络"></a>3.3 SPP网络</h3><p>何恺明研究员于14年撰写的论文，主要是把经典的Spatial Pyramid Pooling结构引入CNN中，从而使CNN可以处理任意size和scale的图片；这中方法不仅提升了分类的准确率，而且还非常适合Detection，比经典的RNN快速准确。</p>
<p>本文不打算详细解释SPP网络，只介绍其中的SPP-layer，由于fast rcnn会使用到SPP-layer。</p>
<p><strong>SPP layer</strong></p>
<p>根据pooling规则，每个pooling   bin（window）对应一个输出，所以最终pooling后特征输出由bin的个数来决定。本文就是分级固定bin的个数，调整bin的尺寸来实现多级pooling固定输出。</p>
<p>如图所示，layer-5的unpooled FM维数为16*24，按照图中所示分为3级，</p>
<p><img src="/images/blog/rcnn8.png" alt="fast rcnn结构"></p>
<p>第一级bin个数为1，最终对应的window大小为16*24；</p>
<p>第二级bin个数为4个，最终对应的window大小为4*8</p>
<p>第三级bin个数为16个，最终对应的window大小为1*1.5（小数需要舍入处理）</p>
<p>通过融合各级bin的输出，最终每一个unpooled FM经过SPP处理后，得到了1+4+16维的SPPed FM输出特征，经过融合后输入分类器。</p>
<p>这样就可以在任意输入size和scale下获得固定的输出；不同scale下网络可以提取不同尺度的特征，有利于分类。</p>
<h3 id="3-4-RoI-pooling-layer"><a href="#3-4-RoI-pooling-layer" class="headerlink" title="3.4  RoI pooling layer"></a>3.4  RoI pooling layer</h3><p>每一个RoI都有一个四元组（r,c,h,w）表示，其中（r，c）表示左上角，而（h，w）则代表高度和宽度。这一层使用最大池化（max pooling）来将RoI区域转化成固定大小的H<em>W的特征图。假设一个RoI的窗口大小为h</em>w,则转换成H<em>W之后，每一个网格都是一个h/H </em> w/W大小的子网，利用最大池化将这个子网中的值映射到H*W窗口即可。Pooling对每一个特征图通道都是独立的，这是SPP layer的特例，即只有一层的空间金字塔。</p>
<h3 id="3-5-从预训练的网络中初始化数据"><a href="#3-5-从预训练的网络中初始化数据" class="headerlink" title="3.5 从预训练的网络中初始化数据"></a>3.5 从预训练的网络中初始化数据</h3><p>有三种预训练的网络：CaffeNet，VGG_CNN_M_1024，VGG-16，他们都有5个最大池化层和5到13个不等的卷积层。用他们来初始化Fast R-CNN时，需要修改三处：</p>
<p>①最后一个池化层被RoI pooling layer取代</p>
<p>②最后一个全连接层和softmax被替换成之前介绍过的两个兄弟并列层</p>
<p>③网络输入两组数据：一组图片和那些图片的一组RoIs</p>
<h3 id="3-6-检测中的微调"><a href="#3-6-检测中的微调" class="headerlink" title="3.6 检测中的微调"></a>3.6 检测中的微调</h3><p>使用BP算法训练网络是Fast R-CNN的重要能力，前面已经说过，SPP-net不能微调spp层之前的层，主要是因为当每一个训练样本来自于不同的图片时，经过SPP层的BP算法是很低效的（感受野太大）. Fast R-CNN提出SGD mini_batch分层取样的方法：首先随机取样N张图片，然后每张图片取样R/N个RoIs  e.g.  N=2 and R=128<br>除了分层取样，还有一个就是FRCN在一次微调中联合优化softmax分类器和bbox回归，看似一步，实际包含了多任务损失（multi-task loss）、小批量取样（mini-batch sampling）、RoI pooling层的反向传播（backpropagation through RoI pooling layers）、SGD超参数（SGD hyperparameters）。</p>
<h2 id="4-Faster-RCNN"><a href="#4-Faster-RCNN" class="headerlink" title="4 Faster RCNN"></a>4 Faster RCNN</h2><p>Faster R-CNN统一的网络结构如下图所示，可以简单看作RPN网络+Fast R-CNN网络。</p>
<p><img src="/images/blog/rcnn9.png" alt="fast rcnn结构"></p>
<p>原理步骤如下:</p>
<ol>
<li><p>首先向CNN网络【ZF或VGG-16】输入任意大小图片；</p>
</li>
<li><p>经过CNN网络前向传播至最后共享的卷积层，一方面得到供RPN网络输入的特征图，另一方面继续前向传播至特有卷积层，产生更高维特征图；</p>
</li>
<li><p>供RPN网络输入的特征图经过RPN网络得到区域建议和区域得分，并对区域得分采用非极大值抑制【阈值为0.7】，输出其Top-N【文中为300】得分的区域建议给RoI池化层；</p>
</li>
<li><p>第2步得到的高维特征图和第3步输出的区域建议同时输入RoI池化层，提取对应区域建议的特征；</p>
</li>
<li><p>第4步得到的区域建议特征通过全连接层后，输出该区域的分类得分以及回归后的bounding-box。</p>
</li>
</ol>
<h3 id="4-1-单个RPN网络结构"><a href="#4-1-单个RPN网络结构" class="headerlink" title="4.1 单个RPN网络结构"></a>4.1 单个RPN网络结构</h3><p>单个RPN网络结构如下:</p>
<p><img src="/images/blog/rcnn10.png" alt="fast rcnn结构"></p>
<p><strong>注意：</strong> 上图中卷积层/全连接层表示卷积层或者全连接层，作者在论文中表示这两层实际上是全连接层，但是网络在所有滑窗位置共享全连接层，可以很自然地用n×n卷积核【论文中设计为3×3】跟随两个并行的1×1卷积核实现</p>
<p><strong>RPN的作用</strong>：RPN在CNN卷积层后增加滑动窗口操作以及两个卷积层完成区域建议功能，第一个卷积层将特征图每个滑窗位置编码成一个特征向量，第二个卷积层对应每个滑窗位置输出k个区域得分和k个回归后的区域建议，并对得分区域进行非极大值抑制后输出得分Top-N【文中为300】区域，告诉检测网络应该注意哪些区域，本质上实现了Selective Search、EdgeBoxes等方法的功能。</p>
<h3 id="4-2-RPN层的具体流程"><a href="#4-2-RPN层的具体流程" class="headerlink" title="4.2 RPN层的具体流程"></a>4.2 RPN层的具体流程</h3><ol>
<li><p>首先套用ImageNet上常用的图像分类网络，本文中试验了两种网络：ZF或VGG-16，利用这两种网络的部分卷积层产生原始图像的特征图；</p>
</li>
<li><p>对于1中特征图，用n×n【<strong>论文中设计为3×3，n=3看起来很小，但是要考虑到这是非常高层的feature map，其size本身也没有多大，因此9个矩形中，每个矩形窗框都是可以感知到很大范围的】的滑动窗口在特征图上滑动扫描【代替了从原始图滑窗获取特征</strong>】，每个滑窗位置通过卷积层1映射到一个低维的特征向量【<strong>ZF网络：256维；VGG-16网络：512维，低维是相对于特征图大小W×H，typically~60×40=2400</strong>】后采用ReLU，并为每个滑窗位置考虑k种【<strong>论文中k=9</strong>】可能的参考窗口【<strong>论文中称为anchors，见下解释</strong>】，这就意味着每个滑窗位置会同时预测最多9个区域建议【<strong>超出边界的不考虑</strong>】，对于一个W×H的特征图，就会产生W×H×k个区域建议；</p>
</li>
<li><p>步骤2中的低维特征向量输入两个并行连接的卷积层2：reg窗口回归层【<strong>位置精修</strong>】和cls窗口分类层，分别用于回归区域建议产生bounding-box【<strong>超出图像边界的裁剪到图像边缘位置</strong>】和对区域建议是否为前景或背景打分，这里由于每个滑窗位置产生k个区域建议，所以reg层有4k个输出来编码【平移缩放参数】k个区域建议的坐标，cls层有2k个得分估计k个区域建议为前景或者背景的概率。</p>
</li>
</ol>
<h3 id="4-3-Anchor"><a href="#4-3-Anchor" class="headerlink" title="4.3 Anchor"></a>4.3 Anchor</h3><p>Anchors是一组大小固定的参考窗口：三种尺度{ $128^2，256^2，512^2$ }×三种长宽比{1:1，1:2，2:1}，如下图所示，表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小，相当于模板，对任意图像任意滑窗位置都是这9中模板。继而根据图像大小计算滑窗中心点对应原图区域的中心点，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可。对于每个滑窗位置，产生k=9个anchor对于一个大小为W*H的卷积feature map，总共会产生WHk个anchor。</p>
<p><img src="/images/blog/rcnn12.png" alt="fast rcnn结构"></p>
<p><strong>平移不变性</strong></p>
<p>Anchors这种方法具有平移不变性，就是说在图像中平移了物体，窗口建议也会跟着平移。同时这种方式也减少了整个模型的size，输出层 $512×(4+2)×9=2.8×10^4$ 个参数【512是前一层特征维度，(4+2)×9是9个Anchors的前景背景得分和平移缩放参数】，而MultiBox有 $1536×（4+1）×800=6.1×10^6个$ 参数，而较小的参数可以在小数据集上减少过拟合风险。</p>
<p>当然，在RPN网络中我们只需要找到大致的地方，无论是位置还是尺寸，后面的工作都可以完成，这样的话采用小网络进行简单的学习【估计和猜差不多，反正有50%概率】，还不如用深度网络【还可以实现卷积共享】，固定尺度变化，固定长宽比变化，固定采样方式来大致判断是否是物体以及所对应的位置并降低任务复杂度。</p>
<h3 id="4-4-多尺度多长宽比率"><a href="#4-4-多尺度多长宽比率" class="headerlink" title="4.4 多尺度多长宽比率"></a>4.4 多尺度多长宽比率</h3><p>有两种方法解决多尺度多长宽比问题:</p>
<ol>
<li><p><strong>图像金字塔</strong>:对伸缩到不同size的输入图像进行特征提取，虽然有效但是费时.</p>
</li>
<li><p><strong>feature map上使用多尺度（和/或长宽比）的滑窗</strong>:例如，DPM分别使用不同大小的filter来训练不同长宽比的模型。若这种方法用来解决多尺度问题，可以认为是“filter金字塔(pyramid of filters)”</p>
</li>
</ol>
<h3 id="4-5-训练过程"><a href="#4-5-训练过程" class="headerlink" title="4.5 训练过程"></a>4.5 训练过程</h3><h4 id="4-5-1-RPN网络训练过程"><a href="#4-5-1-RPN网络训练过程" class="headerlink" title="4.5.1 RPN网络训练过程"></a>4.5.1 RPN网络训练过程</h4><p>RPN网络被ImageNet网络【ZF或VGG-16】进行了有监督预训练，利用其训练好的网络参数初始化；<br>用标准差0.01均值为0的高斯分布对新增的层随机初始化。</p>
<h4 id="4-5-2-Fast-R-CNN网络预训练"><a href="#4-5-2-Fast-R-CNN网络预训练" class="headerlink" title="4.5.2 Fast R-CNN网络预训练"></a>4.5.2 Fast R-CNN网络预训练</h4><p>同样使用mageNet网络【ZF或VGG-16】进行了有监督预训练，利用其训练好的网络参数初始化。</p>
<h4 id="4-5-3-RPN网络微调训练"><a href="#4-5-3-RPN网络微调训练" class="headerlink" title="4.5.3 RPN网络微调训练"></a>4.5.3 RPN网络微调训练</h4><p>PASCAL VOC 数据集中既有物体类别标签，也有物体位置标签；<br>正样本仅表示前景，负样本仅表示背景；<br>回归操作仅针对正样本进行；<br>训练时弃用所有超出图像边界的anchors，否则在训练过程中会产生较大难以处理的修正误差项，导致训练过程无法收敛；<br>对去掉超出边界后的anchors集采用非极大值抑制，最终一张图有2000个anchors用于训练【详细见下】；<br>对于ZF网络微调所有层，对VGG-16网络仅微调conv3_1及conv3_1以上的层，以便节省内存。</p>
<p><strong>SGD mini-batch采样方式：</strong> 同Fast R-CNN网络，采取 <code>image-centric</code> 方式采样，即采用层次采样，先对图像取样，再对anchors取样，同一图像的anchors共享计算和内存。每个mini-batch包含从一张图中随机提取的256个anchors，正负样本比例为1:1【当然可以对一张图所有anchors进行优化，但由于负样本过多最终模型会对正样本预测准确率很低】来计算一个mini-batch的损失函数，如果一张图中不够128个正样本，拿负样本补凑齐。</p>
<p><strong>训练超参数选择：</strong> 在PASCAL VOC数据集上前60k次迭代学习率为0.001，后20k次迭代学习率为0.0001；动量设置为0.9，权重衰减设置为0.0005。</p>
<p>多任务目标函数【<code>分类损失+回归损失</code>】具体如下：</p>
<script type="math/tex; mode=display">

 L(\{p_i\},\{t_i\})=\frac{1}{N_{cls}}\sum _i L_{cls}(p_i,p_i^*)+\lambda \frac{1}{N_{reg}}\sum _i p_i ^* L_{reg}(t_i,t_i^*)</script><ul>
<li><p><code>i</code> 为一个anchor在一个mini-batch中的下标</p>
</li>
<li><p>$p_i$ 是anchor i为一个object的预测可能性</p>
</li>
<li><p>$p_i^\star$ 为ground-truth标签。如果这个anchor是positive的，则ground-truth标签 $p_i^\star$ 为1，否则为0。</p>
</li>
<li><p>$t_i$ 表示表示正样本anchor到预测区域bounding box的4个参数化坐标，【<strong>以anchor为基准的变换</strong>】</p>
</li>
<li><p>$t_i^\star$ 是这个positive anchor对应的ground-truth  box。【<strong>以anchor为基准的变换</strong>】</p>
</li>
<li><p>$L<em>{cls}$  分类的损失（classification loss），是一个二值分类器（是object或者不是）的softmax loss。其公式为 $L</em>{cls}(p_i,p_i^\star)=-log[p_i*p_i^\star+(1-p_i^\star)(1-p_i)]$</p>
</li>
<li><p>$L<em>{reg}$ 回归损失（regression loss），$L</em>{reg}(t<em>i,t_i^\star)=R(t_i-t_i^\star)$ 【两种变换之差越小越好】，其中R是Fast R-CNN中定义的robust ross function (smooth L1)。$p_i^\star L</em>{reg}$ 表示回归损失只有在positive anchor（ $p<em>i^\star=1$ )的时候才会被激活。cls与reg层的输出分别包含{$p_i$}和{ $t_i$ }。R函数的定义为:<br>$smooth</em>{L1}(x)= 0.5x^2 \quad if \mid x\mid &lt;1 \quad otherwise \quad \mid x \mid-0.5$</p>
</li>
<li><p>λ参数用来权衡分类损失 $L_{cls}$ 和回归损失 $L_reg$ ，默认值λ=10【文中实验表明 λ从1变化到100对mAP影响不超过1%】；</p>
</li>
<li><p>$N<em>{cls}$ 和 $N</em>{reg}$ 分别用来标准化分类损失项 $L<em>{cls}$ 和回归损失项 $L</em>{reg}$，默认用mini-batch size=256设置 $N<em>{cls}$，用anchor位置数目~2400初始化 $N</em>{reg}$，文中也说明标准化操作并不是必须的，可以简化省略。</p>
</li>
</ul>
<h4 id="4-5-4-RPN网络、Fast-R-CNN网络联合训练"><a href="#4-5-4-RPN网络、Fast-R-CNN网络联合训练" class="headerlink" title="4.5.4 RPN网络、Fast R-CNN网络联合训练"></a>4.5.4 RPN网络、Fast R-CNN网络联合训练</h4><p>训练网络结构示意图如下所示：</p>
<p><img src="/images/blog/rcnn13.png" alt="fast rcnn结构"></p>
<p>如上图所示，<strong>RPN网络、Fast R-CNN网络联合训练是为了让两个网络共享卷积层，降低计算量</strong>。</p>
<p>文中通过4步训练算法，交替优化学习至共享特征：</p>
<ol>
<li><p>进行上面RPN网络预训练，和以区域建议为目的的RPN网络end-to-end微调训练。</p>
</li>
<li><p>进行上面Fast R-CNN网络预训练，用第①步中得到的区域建议进行以检测为目的的Fast R-CNN网络end-to-end微调训练【此时无共享卷积层】。</p>
</li>
<li><p>使用第2步中微调后的Fast R-CNN网络重新初始化RPN网络，固定共享卷积层【即设置学习率为0，不更新】，仅微调RPN网络独有的层【此时共享卷积层】。</p>
</li>
<li><p>固定第3步中共享卷积层，利用第③步中得到的区域建议，仅微调Fast R-CNN独有的层，至此形成统一网络如上图所示。</p>
</li>
</ol>
<h3 id="4-6-相关解释"><a href="#4-6-相关解释" class="headerlink" title="4.6 相关解释"></a>4.6 相关解释</h3><p><strong>RPN网络中bounding-box回归怎么理解？同Fast R-CNN中的bounding-box回归相比有什么区别？ </strong></p>
<p>对于bounding-box回归，采用以下公式：</p>
<ul>
<li>t</li>
</ul>
<script type="math/tex; mode=display">
t_x = \frac{(x-x_a)}{w_a}\\
t_y = \frac{(y-y_a)}{h_a}\\
t_w =log\frac{w}{w_a}\\
t_h = log\frac{h}{h_a}</script><ul>
<li>$t^*$</li>
</ul>
<script type="math/tex; mode=display">

t_x^* = \frac{(x^*-x_a)}{w_a}\\
t_y^* = \frac{(y^*-y_a)}{h_a}\\
t_w^* =log\frac{w^*}{w_a}\\
t_h^* = log\frac{h^*}{h_a}</script><p>其中，x，y，w，h表示窗口中心坐标和窗口的宽度和高度，变量x，$x_a$ 和 $x^∗$ 分别表示预测窗口、anchor窗口和Ground Truth的坐标【y，w，h同理】，因此这可以被认为是一个从anchor窗口到附近Ground Truth的bounding-box 回归；</p>
<p>RPN网络中bounding-box回归的实质其实就是计算出预测窗口。这里以anchor窗口为基准，计算Ground Truth对其的平移缩放变化参数，以及预测窗口【可能第一次迭代就是anchor】对其的平移缩放参数，因为是以anchor窗口为基准，所以只要使这两组参数越接近，以此构建目标函数求最小值，那预测窗口就越接近Ground Truth，达到回归的目的；</p>
<p>文中提到， Fast R-CNN中基于RoI的bounding-box回归所输入的特征是在特征图上对任意size的RoIs进行Pool操作提取的，所有size RoI共享回归参数，而在Faster R-CNN中，用来bounding-box回归所输入的特征是在特征图上相同的空间size【3×3】上提取的，为了解决不同尺度变化的问题，同时训练和学习了k个不同的回归器，依次对应为上述9种anchors，这k个回归量并不分享权重。因此尽管特征提取上空间是固定的【3×3】，但由于anchors的设计，仍能够预测不同size的窗口。</p>
<p><strong>文中提到了三种共享特征网络的训练方式？</strong></p>
<ol>
<li><p><strong>交替训练</strong>,训练RPN，得到的区域建议来训练Fast R-CNN网络进行微调；此时网络用来初始化RPN网络，迭代此过程【文中所有实验采用】；</p>
</li>
<li><p><strong>近似联合训练:</strong> 如上图所示，合并两个网络进行训练，前向计算产生的区域建议被固定以训练Fast R-CNN；反向计算到共享卷积层时RPN网络损失和Fast R-CNN网络损失叠加进行优化，但此时把区域建议【Fast R-CNN输入，需要计算梯度并更新】当成固定值看待，忽视了Fast R-CNN一个输入：区域建议的导数，则无法更新训练，所以称之为近似联合训练。实验发现，这种方法得到和交替训练相近的结果，还能减少20%~25%的训练时间，公开的python代码中使用这种方法；</p>
</li>
<li><p><strong>联合训练</strong> 需要RoI池化层对区域建议可微，需要RoI变形层实现，具体请参考这片paper：Instance-aware Semantic Segmentation via Multi-task Network Cascades。</p>
</li>
</ol>
<p><strong>图像Scale细节问题？</strong></p>
<p>文中提到训练和检测RPN、Fast R-CNN都使用单一尺度，统一缩放图像短边至600像素；<br>在缩放的图像上，对于ZF网络和VGG-16网络的最后卷积层总共的步长是16像素，因此在缩放前典型的PASCAL图像上大约是10像素【~500×375；600/16=375/10】。</p>
<p><strong>Faster R-CNN中三种尺度怎么解释：</strong></p>
<ul>
<li><p><strong>原始尺度</strong>：原始输入的大小，不受任何限制，不影响性能；</p>
</li>
<li><p><strong>归一化尺度</strong>：输入特征提取网络的大小，在测试时设置，源码中opts.test_scale=600。anchor在这个尺度上设定，这个参数和anchor的相对大小决定了想要检测的目标范围；</p>
</li>
<li><p><strong>网络输入尺度</strong>：输入特征检测网络的大小，在训练时设置，源码中为224×224。</p>
</li>
</ul>
<p><strong>理清文中anchors的数目</strong></p>
<p>文中提到对于1000×600的一张图像，大约有20000(~60×40×9)个anchors，忽略超出边界的anchors剩下6000个anchors，利用非极大值抑制去掉重叠区域，剩2000个区域建议用于训练；<br>测试时在2000个区域建议中选择Top-N【文中为300】个区域建议用于Fast R-CNN检测。</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="http://blog.csdn.net/hjimce/article/details/50187029" target="_blank" rel="noopener">RCNN 介绍</a></p>
<p><a href="http://blog.csdn.net/shenxiaolu1984/article/details/51036677" target="_blank" rel="noopener">Fast RCNN介绍</a></p>
<p><a href="http://blog.csdn.net/qq_17448289/article/details/52871461" target="_blank" rel="noopener">Faster RCNN论文笔记</a></p>
<p><a href="http://www.cnblogs.com/RayShea/p/5568841.html" target="_blank" rel="noopener">Fast RCNN简要笔记</a></p>
<p><a href="http://www.itdadao.com/articles/c15a296465p0.html" target="_blank" rel="noopener">SPP网络</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>GAN：论文笔记</title>
    <url>/2016/12/27/gan-paper/</url>
    <content><![CDATA[<h2 id="一-GAN网络"><a href="#一-GAN网络" class="headerlink" title="一 GAN网络"></a>一 GAN网络</h2><p>与一般的被用于Supervised Learning任务的深度神经网络不同，GAN同时要训练一个生成网络(Generator)和一个判别网络(Discriminator)，前者输入一个noise变量 z ，输出一个伪图片数据 $G(z;θ_g)$ ，后者输入一个图片(real image)／伪图片(fake image)数据 x ，输出一个表示该输入是自然图片或者伪造图片的二分类置信度 $D(x;θ_d)$ ，理想情况下，判别器 D 需要尽可能准确的判断输入数据到底是一个真实的图片还是某种伪造的图片，而生成器G又需要尽最大可能去欺骗D，让D把自己产生的伪造图片全部判断成真实的图片。</p>
<p>根据上述训练过程的描述，我们可以定义一个损失函数：</p>
<script type="math/tex; mode=display">
  Loss = \frac{1}{m}\sum[logD(x^i)+log(1-D(G(z^i)))]</script><h2 id="二-DCGAN网络"><a href="#二-DCGAN网络" class="headerlink" title="二 DCGAN网络"></a>二 DCGAN网络</h2><h3 id="2-0-介绍"><a href="#2-0-介绍" class="headerlink" title="2.0 介绍"></a>2.0 介绍</h3><p>本论文做了以下贡献。</p>
<ul>
<li><p>提出并评估了一系列的加在卷积GAN网络拓扑结构的约束，使得卷积GAN网络在大部分设置中稳定的用于训练。给这种架构取名为深度卷积GAN(DCGAN)</p>
</li>
<li><p>对图像分类任务使用训练好的分类器，结合其他无监督学习算法，性能较好。</p>
</li>
<li><p>可视化了GAN学习到的过滤器(filter)，并且实验表明特定的过滤器已经学习到画特定的物体。</p>
</li>
<li><p>表明，生成器有些有趣的矢量算数性质，它可以很容易地完成生成模型的语义质量的操作。</p>
</li>
</ul>
<h3 id="2-1-无标签数据中的表征学习"><a href="#2-1-无标签数据中的表征学习" class="headerlink" title="2.1 无标签数据中的表征学习"></a>2.1 无标签数据中的表征学习</h3><ul>
<li><p>经典的无标签表征学习是在数据上做聚类（K均值聚类）。图像领域，可以在图像patches上做层次聚类，以此来学习表征</p>
</li>
<li><p>另外一个方法是训练自动编码器（卷积，堆叠），它可以分离编码的组件是什么以及组件位置。梯形结构将图像编码成压缩编码，然后解码编码来重构图像。</p>
</li>
</ul>
<h3 id="2-2-生成自然图像"><a href="#2-2-生成自然图像" class="headerlink" title="2.2 生成自然图像"></a>2.2 生成自然图像</h3><p>  生成自然图像模型主要有两种：参数化和非参数化。</p>
<ul>
<li><p>非参数化模型一般是从已存在的数据库中做匹配，通常是匹配图像块(patches)。这种方法已经用在了<code>纹理合成</code>,<code>超分辨率</code>,<code>绘画</code>。</p>
</li>
<li><p>参数模型目前已经被研究得较充分（例如MNIST数据集或者纹理合成）。但是生成真实世界的自然图像还没有取得成功。一种<code>变分抽样方法</code>生成图像，生成的图像中容易有模糊干扰。另外一种方法是，使用迭代前向扩散方法生成图像。GAN生成网络生成的图像有噪音，并且可能产生无法理解的图像。一种拉普拉斯金字塔拓展方法的方法生成质量更高的图像，但是依然摆脱不了噪音。一种RNN和反卷积网络可以生成一些较好的自然图片。</p>
</li>
</ul>
<h3 id="2-3-CNN内部的可视化"><a href="#2-3-CNN内部的可视化" class="headerlink" title="2.3 CNN内部的可视化"></a>2.3 CNN内部的可视化</h3><p> 使用反卷过滤最大激励，可以知道CNN中每个卷积过滤器（卷积核）的效果。</p>
<h3 id="2-4-方法和模型架构"><a href="#2-4-方法和模型架构" class="headerlink" title="2.4 方法和模型架构"></a>2.4 方法和模型架构</h3><p> 经验表明：使用CNN来增大GAN的方法来给图像建模的方法是失败的。我们定义了一组架构，它可以在多个不同数据集上稳定训练，并且可以训练更高的分辨率和更深的生成模型。</p>
<p> 方法的核心在于，采用并修改了CNN架构中三个方面。</p>
<ol>
<li><p>所有的用stride卷积替换确定性空间池化函数（例如最大池化）卷积网络，这允许网络学习它自己的空间下采样。我们在生成器中使用了这种方法，这使得它能够学习自己的空间下采样和分类器。</p>
<p>2.去掉顶层卷积特征上的全连接层。最好的例子是全局均值池化用在艺术图像分类模型。我们发现，全局均值池化增加了模型的稳定性，但是拖累了其收敛速度。一个中间层的生成器和分类器，分别直接将顶层的卷积特征链接到输入和输出，其结果较好。GAN的第一层，以正态噪音分布Z作为输入，可以称为全连接，因为它只是矩阵操作，但是结果被reshaped成一个4维tensor，并用作卷积堆叠的起始。对于分类器，最后的卷积层被平铺(flatten)并喂入单个sigmoid输出。</p>
</li>
<li><p>第三个是Batch Normalization，通过对每个输入神经元进行归一化（均值为0，方差是单位方差（1））来稳定学习。它可以避免由于poor initialization和深层模型中的梯度扩散问题。直接将batchnorm应用到所有层，将会导致抽样震荡和模型不稳定。因此，不要将batchnorm用在输出层的生成器和输入层的分类器。</p>
<p>Relu激活函数用在生成器中，输出层使用Tanh激活函数的除外。我们发现使用收敛的激活函数可以让模型更快的饱和并覆盖训练分布的颜色空间。在分类器中，我们发现，使用leaky Retified 激活函数表现较好，尤其是高分辨率模型。这与原始的GAN模型相反，它用的是maxout激励函数。</p>
<p><strong>总结</strong></p>
</li>
</ol>
<ul>
<li><p>使用strided 卷积（分类器）和fractional-strided卷积（生成器）替换所有池化层。</p>
</li>
<li><p>分类器和生成器中都是用batchnorm</p>
</li>
<li><p>深层架构，移除全连接层</p>
</li>
<li><p>除了输出层使用Tanh激活函数，其他层都使用Relu</p>
</li>
<li><p>所有层的分类器都使用 LeakyRelu</p>
</li>
</ul>
<h3 id="2-5-模型细节"><a href="#2-5-模型细节" class="headerlink" title="2.5 模型细节"></a>2.5 模型细节</h3><ul>
<li><p>数据集：, Large-scale Scene Understanding (LSUN) (Yu et al., 2015),Imagenet-1k and a newly assembled Faces dataset</p>
</li>
<li><p>预处理：图像不做预处理，激活函数tanh范围拓展到[-1,1]</p>
</li>
<li><p>所有模型预训练都是使用mini-batch SGD，mini-batch 是128。</p>
</li>
<li><p>所有权重初始化是，均值为0，方差为0.02</p>
</li>
<li><p>LeakyRelu：the slope of the leak was set to 0.2 in all models</p>
</li>
<li><p>之前的GAN使用动量来加速训练，此文使用带超参数的Adam 优化器</p>
</li>
<li><p>学习率为 0.0002</p>
</li>
<li><p>动量项 $\beta _1 =0.5$ 更稳定（原始的是0.9）</p>
<p><img src="/images/blog/gan1.png" alt="网络细节"></p>
<p>一个100维的正态分布Z被投影到一个小的空间范围，许多特征maps的卷积表征。一系列的四fractionally-strided卷积（有些论文误称为反卷积），然后将这个高度表征表示为64x64像素的图像。注意：没有使用全连接层或池化层。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>theano 深度学习数据准备</title>
    <url>/2016/12/22/theano-deeplearning/</url>
    <content><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p> 使用的是MNIST手写字识别,<a href="http://deeplearning.net/data/mnist/mnist.pkl.gz" target="_blank" rel="noopener">下载</a>。数据由60000个样本，50000个训练样本和10000个测试样本，每个样本被统一为28x28像素。原始数据集中每个样本的像素值区间是[0,255]，其中0代表黑色，255为白色，中间的是灰色。</p>
<p> <img src="/images/blog/theano1.png" alt="样本"><img src="/images/blog/theano2.png" alt="样本"><img src="/images/blog/theano3.png" alt="样本"><img src="/images/blog/theano4.png" alt="样本"></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p> CNN每层都是由多个特征map组成，( $h^{()k},k=0…k$ )。隐藏层的权重可以用一个四维的Tensor表示，分别代表<strong>目标特征map</strong>，<strong>源特征map</strong>，<strong>原始图像中竖轴坐标</strong>，<strong>原始图像中横轴坐标</strong>。偏置b可以用一个向量表示，向量中每个元素是目标特征map中的一个元素。下图演示了这个过程:</p>
<p> <img src="/images/blog/theano5.png" alt="CNN"></p>
<p>上图展示了两层CNN网络，<code>m-1</code>层包含了4个特征map，隐藏层<code>m</code>包含了两个特征map ( $h^0和h^1$ )。 输出神经元中的像素，$h^0$ 和 $h^1$ (途中蓝色( $W^1$ 旁边)和红色矩形框( $W^2$ 旁边))，是从 <code>m-1</code> 层的2x2的接收域的像素中计算得来的。注意看，接受域是如何覆盖全部的四个输入特征map的。特征 $h^0$ 和 $h^1$ 的权重 $W^0$ 和 $W^1$ 都是3维Tensor,第一个维度代表的是第几个输入特征maps的，其他两个是像素坐标。</p>
<p>结合起来看 $W^{kl} _{ij}$ 代表第 <code>m</code>层的第 <code>k</code> 个特征的每个像素与第 <code>m-1</code>层的第 <code>l</code> 个特征的坐标点为 <code>i,j</code> 像素的权重。所以，在上图由4个通道卷积得到2个通道的过程中，参数的数目为4×2×2×2个，其中4表示4个通道，第一个2表示生成2个通道，最后的2×2表示卷积核大小。</p>
<p>下面演示 theano的代码和详细数据过程：</p>
<p><strong>导入基本包</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import theano</span><br><span class="line">from theano import tensor as T</span><br><span class="line">from theano.tensor.nnet import conv2d</span><br><span class="line"></span><br><span class="line">import numpy</span><br></pre></td></tr></table></figure>
<p><strong>初始化一个随机数生成器</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rng &#x3D; numpy.random.RandomState(23455)</span><br></pre></td></tr></table></figure>
<p><strong>初始化输入</strong></p>
<p>注意，深度学习框架中，一般会先构架计算图（即，把计算过程定义好），然后再输入数据。所以，此时的初始化时一个空的占位变量。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># instantiate 4D tensor for input</span><br><span class="line">input &#x3D; T.tensor4(name&#x3D;&#39;input&#39;)</span><br></pre></td></tr></table></figure>
<p>通过调试，查看变量的初始化<br><img src="/images/blog/theano6.png" alt="CNN"></p>
<p><strong>初始化权重矩阵</strong></p>
<p>注意：权重的数据类型是 <code>theano.shared</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># initialize shared variable for weights.</span><br><span class="line">w_shp &#x3D; (2, 3, 9, 9)</span><br><span class="line">w_bound &#x3D; numpy.sqrt(3 * 9 * 9)</span><br><span class="line">W &#x3D; theano.shared( numpy.asarray(</span><br><span class="line">            rng.uniform(</span><br><span class="line">                low&#x3D;-1.0 &#x2F; w_bound,</span><br><span class="line">                high&#x3D;1.0 &#x2F; w_bound,</span><br><span class="line">                size&#x3D;w_shp),</span><br><span class="line">            dtype&#x3D;input.dtype), name &#x3D;&#39;W&#39;)</span><br></pre></td></tr></table></figure>
<p>观察权重的数据结构和内部的值。</p>
<p><img src="/images/blog/theano7.png" alt="CNN"></p>
<p><strong>初始化偏置</strong></p>
<p>偏置只有2个，因为此处只使用了 2个特征抽取map。</p>
<p>此时的偏置的数据类型也是 <code>theano.shared</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b_shp &#x3D; (2,)</span><br><span class="line">b &#x3D; theano.shared(numpy.asarray(</span><br><span class="line">           rng.uniform(low&#x3D;-.5, high&#x3D;.5, size&#x3D;b_shp),</span><br><span class="line">           dtype&#x3D;input.dtype), name &#x3D;&#39;b&#39;)</span><br></pre></td></tr></table></figure>
<p><strong>计算卷积</strong></p>
<p>构建符号表达的计算图，此时是个空架子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># build symbolic expression that computes the convolution of input with filters in w</span><br><span class="line">conv_out &#x3D; conv2d(input, W)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/theano8.png" alt="CNN"></p>
<p><strong>卷积网络的激活输出</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">output &#x3D; T.nnet.sigmoid(conv_out + b.dimshuffle(&#39;x&#39;, 0, &#39;x&#39;, &#39;x&#39;))</span><br><span class="line"></span><br><span class="line"># create theano function to compute filtered images</span><br><span class="line">f &#x3D; theano.function([input], output)</span><br></pre></td></tr></table></figure>
<p><strong> 给卷积网络灌入图像数据</strong></p>
<p>下面的代码直接使用了CNN网络中的函数 <code>f</code>，注意下面代码中的 <code>filtered_img = f(img_)</code>，其他部分是普通的图像处理。此处往图像中灌入图像数据，并返回卷积之后的结果。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy</span><br><span class="line">import pylab</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"># open random image of dimensions 639x516</span><br><span class="line">img &#x3D; Image.open(open(&#39;doc&#x2F;images&#x2F;3wolfmoon.jpg&#39;))</span><br><span class="line"># dimensions are (height, width, channel)</span><br><span class="line">img &#x3D; numpy.asarray(img, dtype&#x3D;&#39;float64&#39;) &#x2F; 256.</span><br><span class="line"></span><br><span class="line"># put image in 4D tensor of shape (1, 3, height, width)</span><br><span class="line">img_ &#x3D; img.transpose(2, 0, 1).reshape(1, 3, 639, 516)</span><br><span class="line">filtered_img &#x3D; f(img_)</span><br><span class="line"></span><br><span class="line"># plot original image and first and second components of output</span><br><span class="line">pylab.subplot(1, 3, 1); pylab.axis(&#39;off&#39;); pylab.imshow(img)</span><br><span class="line">pylab.gray();</span><br><span class="line"># recall that the convOp output (filtered image) is actually a &quot;minibatch&quot;,</span><br><span class="line"># of size 1 here, so we take index 0 in the first dimension:</span><br><span class="line">pylab.subplot(1, 3, 2); pylab.axis(&#39;off&#39;); pylab.imshow(filtered_img[0, 0, :, :])</span><br><span class="line">pylab.subplot(1, 3, 3); pylab.axis(&#39;off&#39;); pylab.imshow(filtered_img[0, 1, :, :])</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure>
<p>下图演示了图片被输入到图像中的结构:</p>
<p>图片被处理为<code>ndarray</code>类型，并保留了RGB通道的结构。</p>
<p><img src="/images/blog/theano9.png" alt="CNN"></p>
<p><strong>详细的卷积过程</strong></p>
<p>查看函数 <code>conv_out=conv2d(input,w)</code> 在<code>theano</code>中如何定义：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def conv2d(input, filters, input_shape&#x3D;None, filter_shape&#x3D;None,</span><br><span class="line">           border_mode&#x3D;&#39;valid&#39;, subsample&#x3D;(1, 1), filter_flip&#x3D;True,</span><br><span class="line">           image_shape&#x3D;None, filter_dilation&#x3D;(1, 1), **kwargs):</span><br><span class="line"></span><br><span class="line">    if &#39;imshp_logical&#39; in kwargs or &#39;kshp_logical&#39; in kwargs:</span><br><span class="line">        raise ValueError(</span><br><span class="line">            &quot;Keyword arguments &#39;imshp_logical&#39; and &#39;kshp_logical&#39; for conv2d &quot;</span><br><span class="line">            &quot;are not supported anymore (and have not been a reliable way to &quot;</span><br><span class="line">            &quot;perform upsampling). That feature is still available by calling &quot;</span><br><span class="line">            &quot;theano.tensor.nnet.conv.conv2d() for the time being.&quot;)</span><br><span class="line">    if len(kwargs.keys()) &gt; 0:</span><br><span class="line">        warnings.warn(str(kwargs.keys()) +</span><br><span class="line">                      &quot; are now deprecated in &quot;</span><br><span class="line">                      &quot;&#96;tensor.nnet.abstract_conv.conv2d&#96; interface&quot;</span><br><span class="line">                      &quot; and will be ignored.&quot;,</span><br><span class="line">                      stacklevel&#x3D;2)</span><br><span class="line"></span><br><span class="line">    if image_shape is not None:</span><br><span class="line">        warnings.warn(&quot;The &#96;image_shape&#96; keyword argument to &quot;</span><br><span class="line">                      &quot;&#96;tensor.nnet.conv2d&#96; is deprecated, it has been &quot;</span><br><span class="line">                      &quot;renamed to &#96;input_shape&#96;.&quot;,</span><br><span class="line">                      stacklevel&#x3D;2)</span><br><span class="line">        if input_shape is None:</span><br><span class="line">            input_shape &#x3D; image_shape</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(&quot;input_shape and image_shape should not&quot;</span><br><span class="line">                             &quot; be provided at the same time.&quot;)</span><br><span class="line"></span><br><span class="line">    return abstract_conv2d(input, filters, input_shape, filter_shape,</span><br><span class="line">                           border_mode, subsample, filter_flip,</span><br><span class="line">                           filter_dilation)</span><br></pre></td></tr></table></figure>
<p>看代码可知，此函数只是个外壳，最后的<code>abstract_conv2d</code>才是实际的处理方法。继续进入到 <code>abstract_conv2d</code>函数内部：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def conv2d(input,</span><br><span class="line">          filters,</span><br><span class="line">          input_shape&#x3D;None,</span><br><span class="line">          filter_shape&#x3D;None,</span><br><span class="line">          border_mode&#x3D;&#39;valid&#39;,</span><br><span class="line">          subsample&#x3D;(1, 1),</span><br><span class="line">          filter_flip&#x3D;True,</span><br><span class="line">          filter_dilation&#x3D;(1, 1)):</span><br><span class="line">   &quot;&quot;&quot;This function will build the symbolic graph for convolving a mini-batch of a</span><br><span class="line">   stack of 2D inputs with a set of 2D filters. The implementation is modelled</span><br><span class="line">   after Convolutional Neural Networks (CNN).</span><br><span class="line"></span><br><span class="line">   Refer to :func:&#96;nnet.conv2d &lt;theano.tensor.nnet.conv2d&gt;&#96; for a more detailed documentation.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">   input &#x3D; as_tensor_variable(input)</span><br><span class="line">   filters &#x3D; as_tensor_variable(filters)</span><br><span class="line">   conv_op &#x3D; AbstractConv2d(imshp&#x3D;input_shape,</span><br><span class="line">                            kshp&#x3D;filter_shape,</span><br><span class="line">                            border_mode&#x3D;border_mode,</span><br><span class="line">                            subsample&#x3D;subsample,</span><br><span class="line">                            filter_flip&#x3D;filter_flip,</span><br><span class="line">                            filter_dilation&#x3D;filter_dilation)</span><br><span class="line">   return conv_op(input, filters)</span><br></pre></td></tr></table></figure>
<p>此函数，也只将python中的<code>input</code>变量和<code>filters</code>变量转换为<code>theano</code> 中的张量Tensor。之后将初始化了一个<code>AbstractConv2d</code>对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">input &#x3D; as_tensor_variable(input)</span><br><span class="line">filters &#x3D; as_tensor_variable(filters)</span><br><span class="line">conv_op &#x3D; AbstractConv2d(imshp&#x3D;input_shape,</span><br><span class="line">                          kshp&#x3D;filter_shape,</span><br><span class="line">                          border_mode&#x3D;border_mode,</span><br><span class="line">                          subsample&#x3D;subsample,</span><br><span class="line">                          filter_flip&#x3D;filter_flip,</span><br><span class="line">                          filter_dilation&#x3D;filter_dilation)</span><br><span class="line"> return conv_op(input, filters)</span><br></pre></td></tr></table></figure>
<p>继续深入 <code>AbstractConv2d</code>这个构造方法内部</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">class AbstractConv2d(AbstractConv):</span><br><span class="line">    &quot;&quot;&quot; Abstract Op for the forward convolution.</span><br><span class="line">    Refer to :func:&#96;BaseAbstractConv &lt;theano.tensor.nnet.abstract_conv.BaseAbstractConv&gt;&#96;</span><br><span class="line">    for a more detailed documentation.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self,</span><br><span class="line">                 imshp&#x3D;None,</span><br><span class="line">                 kshp&#x3D;None,</span><br><span class="line">                 border_mode&#x3D;&quot;valid&quot;,</span><br><span class="line">                 subsample&#x3D;(1, 1),</span><br><span class="line">                 filter_flip&#x3D;True,</span><br><span class="line">                 filter_dilation&#x3D;(1, 1)):</span><br><span class="line">        super(AbstractConv2d, self).__init__(convdim&#x3D;2,</span><br><span class="line">                                             imshp&#x3D;imshp, kshp&#x3D;kshp,</span><br><span class="line">                                             border_mode&#x3D;border_mode,</span><br><span class="line">                                             subsample&#x3D;subsample,</span><br><span class="line">                                             filter_flip&#x3D;filter_flip,</span><br><span class="line">                                             filter_dilation&#x3D;filter_dilation)</span><br></pre></td></tr></table></figure>
<p>从代码中可以看到，它实际是<code>super(AbstractConv2d, self)</code>继续调用父类<code>AbstractConv</code>的<code>super(AbstractConv2d, self)</code>的构造方法。但是<code>theano</code>中此处的父类是个抽象类，它在执行<code>super(AbstractConv2d, self)</code>时，实际是调用了自己的构造方法。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：表征学习</title>
    <url>/2016/11/24/represationlearning/</url>
    <content><![CDATA[<p><strong>概述</strong>：本章，我们首先讨论了表征学习的概念以及深度学习中表述学习的可用之处。我们讨论了学习算法在不同任务之间共享统计特性，包括使用无监督学习的信息来作监督学习任务。共享的表征可用于处理多领域问题，或者将已学习的知识转换到只有几个或者完全没有样本的但是包含表征的任务中。最后会回头讨论表述学习的成功的原因，起于分布式表述和深度表述的优势，终于数据处理的隐性假设，也即观测数据的隐藏促因。</p>
<p>机器学习中，一个好的表述方法可以使得后续的 学习任务更简单。因而，表述方法的选择通常依赖于后续学习任务的选择。</p>
<p>我们可以将使用监督学习方法的前馈网络看做一种学习表征的方法。一般网络最后一层是线性分类器，比如softmax线性回归分类，网络其他部分其实可以看做在给此分类器提供表征。比如，输入的特征可能是线性不可分，但是到最后隐藏层时变得线性可分了。</p>
<p>其他学习算法一般会细致地被设计为以某种特殊的方式具化其表征。比如，假若我们想学习使得密度评估方法更简单的表征，越多独立变量的表征越易于建模，所以我们的目标函数被设计成使表征向量V的元素尽可能独立。如同监督学习网络，非监督学习算法除了有个主要的训练目标，还会学习表述方法这个副产品。这个副产品用在多任务学习中，可以使其在各个任务之间共享。</p>
<p>表述学习的亮点在于，它可以提供一种执行半监督和无监督学习的方法。我们通常有大量无标签数据集和少量有标签的作为训练样本。<strong>在标签数据集中做的监督学习容易导致过拟合，半监督学习由于可以从无标签数据集中学习，可以有一定概率化解这种情况。</strong></p>
<h2 id="1-逐层贪婪的无监督预训练"><a href="#1-逐层贪婪的无监督预训练" class="headerlink" title="1 逐层贪婪的无监督预训练"></a>1 逐层贪婪的无监督预训练</h2><p> 无监督学习在深度神经网络历史中扮演重要角色，但是如今（2014年往后）广泛使用的CNN模型已<strong>不再</strong>使用逐层训练方式进行初始化。但是无监督学习仍然是今后深度学习领域的“圣杯”，是解决大量无标签数据集问题的利器。关于无监督学习的一段小变迁参考 <a href="http://www.caffecn.cn/?/question/53" target="_blank" rel="noopener">无监督逐层预训练目前是否还广泛应用于CNN的训练</a>。</p>
<p> 逐层贪婪的无监督预训练依赖于一个单层表述学习算法如RBM（关于RBM 可以参考<a href="http://www.cnblogs.com/zhangchaoyang/articles/5537643.html" target="_blank" rel="noopener">基于RBM的推荐算法</a>），一个单层自动编码器， 一个稀疏编码模型，或者其他学习潜在表征的模型。每一层使用无监督学习来预训练，以上一层的输出作为输入，并输出数据的另外一种表征，整个分布朝着更简单的方向。</p>
<p>更详细的无监督方法可以参考国内的一篇 <a href="http://www.c-s-a.org.cn/ch/reader/create_pdf.aspx?file_no=20160801&amp;flag=1&amp;journal_id=cas&amp;year_id=2016" target="_blank" rel="noopener">无监督学习综述论文</a></p>
<p>特点：</p>
<ul>
<li><p><strong>贪婪</strong>：它基于贪婪算法，它独立地优化问题的解的方方面面，每次一个方面，而不是同时优化全局</p>
</li>
<li><p><strong>逐层</strong>：这些独立的方面都是网络的每一层，每次只训练其中一层，并且训练第 $i$ 层时会固定前面的网络层。</p>
</li>
<li><p><strong>无监督</strong>：每一层的训练都是使用无监督表述学习算法。</p>
</li>
<li><p><strong>预训练</strong>：它只是训练其他所有网络层之前的一小步。</p>
</li>
</ul>
<h3 id="1-1-无监督预训练何时以及为何有用"><a href="#1-1-无监督预训练何时以及为何有用" class="headerlink" title="1.1 无监督预训练何时以及为何有用"></a>1.1 无监督预训练何时以及为何有用</h3><p>许多任务中，逐层贪婪的无监督预训练可以在分类任务中减小测试误差。</p>
<p>  <hr><br>  <strong>逐层贪婪无监督预训练算法</strong></p>
<p>  假若：无监督学习算法 <strong>L</strong> ,接收训练样本集并返回一个编码器或者特征函数 $f$ 。</p>
<p>  原始输入数据是 $X$ ，每个样本为一行， $f^{(1)}(X)$ 是第一阶段的在数据 $X$ 上的输出编码器，它同时作为第二阶段无监督特征学习的输入。</p>
<p>  在微调阶段，我们使用一个学习函数 $T$ ，以一个初始函数 $f$ 作为输入，输入样本 $X$ ，然后返回调整后的函数。<br>  <hr></p>
<p>  无监督预训练方法结合两个不同的论据。<strong>首先，初始化参数的选择会对模型产生非常显著的正则化影响（往近一点说，它可以提高优化效果），其次，学习输入的分布可有助于学习如何将输入映射到输出</strong>。</p>
<p>第一个观点，初始化参数的选择对网络的正则化效果是很难理解。在预训练流行时，它被理解为局部性地初始化了一个模型，而这会导致网络陷入局部最优。现在我们认为网络不会被局部最小问题困扰。但是仍然有这种可能的情况，某些预训练初始化的网络在某些位置变得不可接近（进一步优化）。例如，某些区域minibatch的不同样本点的轻微变动会导致损失函数极大变动，或者某些区域Hessian矩阵几近无力，此时的梯度下降算法必须使用非常小的步长。然而，我们通过监督学习阶段预训练参数获得的特征难以精确控制，现在的办法都是使用一种近似无监督学习和监督学习的方法，而不是分开使用两种方法。如果想知道监督学习如何从无监督学习阶段如何保存优化信息而不太麻烦的话，可以将特征抽取的参数固定并只在学习的参数的顶层的分类器上使用监督学习。</p>
<p>另外一个观点，学习算法可以将无监督阶段学习到的信息用于监督学习阶段来提高算法。这个好理解，即无监督任务中的某些有用特征同时可用于监督学习任务。</p>
<p>以无监督预训练学习特征表述的观点来看，我们认为无监督预训练在初始特征表述十分贫乏时尤为有效。一个突出实例是词袋模型。以one-hot向量词表述并没有很丰富的信息，因为任意两个不同的one-hot词向量都是相同的距离(欧氏距离 $L^2$ 都是2)。学习的词袋表征天然的可以编码词之间的相似度。但是这一点对于图像没有太大用处。</p>
<p>考虑下其他因素。例如，无监督预训练在要学习的函数极端复杂时十分有用。无监督学习与正则化处理的不同之处在于，它不会偏移（正则化方法会在目标函数上做变动）学习器去挖掘简单的表述函数，而是挖掘无监督学习所需的特征表述函数。</p>
<p> 撇开以上，我们分析下无监督预训练能带来提供的实例，并解释其中原因。无监督预训练通常用于提升分类器性能，并能减小测试误差。但是，无监督预训练不止是作用于分类，它能提升优化性能，而不是仅仅正则化。比如，它能同时提升训练和测试深度自编码网络的重构误差。神经网络训练过程，每次都会拟合成不同函数。<strong>经过无监督预训练的神经网络会在解空间的相同区域收敛，但是普通网络可能会停留在不同区域</strong>。下图演示了这种现象:</p>
<p> <img src="/images/blog/representation1.png" alt="无监督预训练与普通网络的对照"></p>
<p>图中预训练网络达到更小的区域，说明预训练可以减小评估过程的方差（更稳定）。</p>
<p>那么，无监督预训练是如何起正则化作用呢？我们的假设是它可以使得学习算法挖掘观测数据背后的相关特征方向。</p>
<p><strong>缺点一</strong>：无监督预训练的缺点是，它需要两个独立训练阶段，即需要更多的参数，其结果便是很难在事前预知训练效果。同时（与分两阶段进行的无监督预训练对比）使用有监督和无监督学习时，通常只需要一个超参数，即在无监督学习的代价函数上添加一个额外项。该额外项用以衡量无监督目标函数对有监督模型的正则化影响。可确信的是，额外项系数减小，正则化效果也会相应减弱。</p>
<p><strong>缺点二</strong>：第二个不足之处是，分成两个独立阶段时，每个阶段都会有其独立的超参数。通常无法从第一阶段来预测第二阶段的性能，因此从第二阶段的反馈来更新第一阶段的超参数时存在很长的滞后。<strong>其中的一个基本原则是，使用监督学习阶段的验证数据集的误差去选择预训练阶段的超参数。</strong><br>现实之中，一些超参数，比如预训练迭代次数，使用在无监督目标函数上的使用提前终止的方法，虽不完美，但是用于监督目标函数上易于计算。</p>
<p>如今，无监督预训练基本被遗忘，除了自然语言处理领域。其中的词的one-hot向量表征没有承载任何相似信息，并且会存在大量的无标签数据集。此时，预训练的优势是，可以在海量无标签数据集上预训练一次，学习到一个较好的表征（通常是词语，也可以是句子），然后使用这个表征或者微调之后的，用于训练集样本较少的监督学习任务。</p>
<h3 id="1-2-无监督学习的四种实现模型"><a href="#1-2-无监督学习的四种实现模型" class="headerlink" title="1.2 无监督学习的四种实现模型"></a>1.2 无监督学习的四种实现模型</h3><p> <strong>自动编码器（auto-encoders）</strong></p>
<p> 一个典型的自编码例子如下图所示, 从可见层到第一个隐含层的转换相当于是一个编码过程(encoder), 从第一个隐含层到输出层相当于一个解码过程(decoder)。 自编码过程使用的一般都是无标签数据, 输入据(input data)经过第一层变换(encoder), 就会被进行一定程度的抽象, 得到一个更深层的编码(code), 然后可以通过第二层变换(decoder)得到一个近似于输入数据(input data)的输出数据(output data)。</p>
<p> <img src="/images/blog/representation15.png" alt=""></p>
<p> 优点：</p>
<ul>
<li><p>简单技术：重建输入</p>
</li>
<li><p>可堆栈多层</p>
</li>
<li><p>直觉型，且基于神经科学研究</p>
<p>缺点：</p>
</li>
<li><p>贪婪训练每一层</p>
</li>
<li><p>没有全局优化</p>
</li>
<li><p>比不上监督学习的表现</p>
</li>
<li><p>层一多会失效</p>
</li>
<li><p>输入的重建可能不是学习通用表征的理想度量（metric）</p>
<p><strong>聚类学习（Clustering Learning）</strong></p>
<p>所周知，受限玻尔兹曼机（RBMs）、深度玻尔兹曼机（DBMs）、深度信念网络（DBNs）难以训练，因为解决其配分函数（partition function）的数值难题。因此它们还未被普遍用来解决问题。</p>
</li>
</ul>
<p>优点：</p>
<ul>
<li><p>简单技术：聚类相似输出</p>
</li>
<li><p>可被多层堆栈</p>
</li>
<li><p>直觉型，且基于神经科学研究</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>贪婪训练每一层</p>
</li>
<li><p>没有全局优化</p>
</li>
<li><p>在一些情况下，比不上监督学习的表现</p>
</li>
<li><p>层数增加时会失效，收益递减</p>
</li>
</ul>
<p><strong>生成模型（generative models）</strong></p>
<p>生成模型，尝试在同一时间创建一个分类（识别器或编码器）网络和一个生成图像（生成模型）模型。这种方法起源于 Ian Goodfellow 和 Yoshua Bengio（参见论文：Generative Adversarial Networks）的开创性工作。</p>
<p>下面是系统框架图：</p>
<p> <img src="/images/blog/representation16.png" alt=""></p>
<p> 优点：</p>
<ul>
<li><p>整个网络的全局训练（global training）</p>
</li>
<li><p>代码和应用简单明了</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>难以训练和转化（conversion）</p>
</li>
<li><p>在某些情况下，与有监督学习的表现相似</p>
</li>
<li><p>需论证展示方法（representation）的可用性（这是所有无监督算法面临的问题）</p>
</li>
</ul>
<h2 id="2-迁移学习和领域自适应"><a href="#2-迁移学习和领域自适应" class="headerlink" title="2 迁移学习和领域自适应"></a>2 迁移学习和领域自适应</h2><h3 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h3><p> <strong>迁移学习和领域自适应：</strong>指的是在某种情况下学习到的设置(比如概率分布 $P_1$ )迁移到另外一种情形下(概率分布 $P_2$ )。这其实是前面部分的思想推广，前面将无监督学习任务和监督学习任务之间的表征做了迁移。考虑到大部分数据或任务是存在相关性的，所以通过transfer learning我们可以将已经学到的parameter 分享给新模型从而加快并优化模型的学习不用像之前那样learn from zero.</p>
<p> 在迁移学习中，学习者必须同时进行两个或多个不同任务，但是我们假设其中用于解释变量 $P_1$ 的许多因子与另外一个学习参数 $P_2$ 的许多变量相关。这可以理解为，在监督学习上下文中，输入是相同的，但是实质目标可能不同。比如，我们可能在第一步的参数设置时学习了一些视觉领域集合，比如猫和狗，然后在第二步的参数设置中学习了另外一个视觉领域，比如蚂蚁和黄蜂。如果第一步的学习设置（从 $P_1$ 抽样）时有非常多的数据，那么这可能有助于从包含很少样本的 $P_2$ 中学习表征。许多视觉类别共享了低层次的比如边和形状这些概念，几何上的改变，其效果较小。 一般来说，如果存在可以被被不同设置或任务共同使用的特征，这对应了在多种设置中的潜在因子，那么通过表征学习可以完成迁移学习，多任务学习和领域自适应。</p>
<p> 但有的时候，不同任务之间共享的不是输入的语义而是输出的语义。比如，语音识别系统需要在输出层产生合法的句子，但是靠近输入层的浅层网络可能需要识别相同意音素的不同版本或者取决于讲话人的亚音素发声。此时，共享靠近输出层的高层网络并有一些特定任务预处理显得更有意义。如下图:</p>
<p> <img src="/images/blog/representation2.png" alt="迁移学习"></p>
<p> 【多任务学习或迁移学习架构，对所有任务输出变量y是相同语义，而输入变量x有不同意义】</p>
<p>在领域自适应的相关实例中，每种设置的任务相同（包括从输入映射到输出的优化过程），但是输入分布可能不同。比如，语音分析任务中，需要分析情感倾向（正面或负面）。如果情感预测器的训练集用的是媒体预料，比如书籍、视频和音乐，但是用于分析的却是用户对电子产品，如电视和手机，领域适应问题就会出现。可以想象，存在一个隐藏函数，它能区分评论的情感倾向，积极、消极、中立，但是不同领域之间表达情感倾向的词汇和样式却不相同，这使得跨领域的泛化有难度。不过一些应用在情感分析中的简单的无监督预训练（领域适应）被证明很有效。</p>
<p>一个相关问题是概念错位，我们可以看成一种随时间变迁，其数据分布逐步变动的迁移学习。概念错位和迁移学习都可以看做一种特殊形式的多任务学习。其中“多任务学习”特指监督学习任务，迁移学习更多的时候用于描述无监督学习和增强学习。</p>
<p>以上所有，目标都是利用第一步配置来抽取信息，这些信息可能有用在第二步的配置（参数）或者直接预测了第二步的配置。<strong>表征学习的核心观点是，相同的表征可同时用于两步的配置</strong>。</p>
<p>前面提到过，用于迁移学习的无监督深度学习在机器学习竞赛中很有效。竞赛中的第一步，每位选手会被给予根据第一步配置（分布 $P_1$ ）的数据集，某些分类的数据样例。选手必须根据这些来学习较好的特征空间（从原始数据输入映射到其他表征），这样，当将这些学习到的转换</p>
<p><strong>迁移学习的两个极端</strong> 是：<strong><em>one-shot</em></strong> 学习和 <strong><em>zero-shot</em></strong> 学习(也称为<strong><em>zero-data</em></strong> 学习)。对于 <em>one-shot</em> ,迁移学习中只有一个标签样本，而 <em>zero-shot</em> 则所有样本都是无标签的。</p>
<p>one-shot学习是可能的，因为表征学习可以在第一阶段干净地区分隐藏的分类。在迁移学习阶段，只需要有一个有标签的样本，用以推测许多测试样本的可能标签，围绕在表征空间的某点聚类簇中心的其他数据点可能是同一标签。从某种程度上说，对应于这些不变特性的变异因素已经在表征空间被彻底与其他因素分离，并且已经学习到哪些因素是决定样本隶属某个分类的重要特性。关于<code>one-shot</code>学习的，可以参考源码 <a href="https://github.com/tristandeleu/ntm-one-shot" target="_blank" rel="noopener">源码</a>,论文<a href="https://arxiv.org/pdf/1605.06065v1.pdf" target="_blank" rel="noopener">论文</a>,数据<a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">数据</a></p>
<p>在zero-shot 学习配置的实例中，考虑以下场景，某位学者已经阅读了大量的文本，然后去解决物体识别问题。倘若物体被足够充分的数据描述，识别一个不曾见过的该物体图像也是可能的（人类的学习）。比如，阅读了一只猫有四只腿和尖耳朵，学习者可能会猜出一幅猫的图像，而不需要见过猫。</p>
<p><strong>只有其他（额外）信息足够充分时，zero-shot和one-shot才有可能。</strong> 考虑一种zero-shot场景，包含了三个变量，正常的输入 $x$ ，正常的输出或者目标 $y$，以及一个描述任务的随机变量 $T$。模型被训练为评估条件分布 $p(y\mid x,T)$ ，其中T是一个描述模型该如何起作用的描述。在识别猫的例子中，是已经阅读了猫，输出是一个二分变量y,其中 $y=1$ 表示yes, $y=0$ 代表no。任务变量T 表示要回答的问题，例如‘‘这个图像中是否有猫？” 如果训练集包含和T 在相同空间的无监督对象样本，我们也许能够推断未知的T 实例的含义。在我们的例子中，没有提前看到猫的图像而去识别<br>猫，拥有一些未标记的文本数据包含句子诸如‘‘猫有四条腿’’ 或‘‘猫有尖耳朵’’ 是很重要的。</p>
<p>zero-shot学习要求T被表示为某种泛化的形式。例如，T不仅仅是指示对象类别的one-shot。通过使用关于每个类别的词，学习到的词嵌入表征，论文提出了对象类别的分布式表示。</p>
<p>一种类似的现象出现在机器翻译中。我们已知一种语言中的单词，和从非语言语料库中学习到的关系；另一方面，我们已经翻译了一种语言中的单词与另一种语言中的单词相关的句子。即使我们可能没有将语言X中的单词A翻译成语言Y中的单词B的标记样本，那么我们可以泛化并猜出单词A的翻译，这是由于我们已经学习了语言X和Y的分布式表示，并且通过两种语言相匹配句子组成的训练样本，产生了关联于两个空间的链接（可能是双向的）。如果所有的三种成分（两种表述形式和他们之间的关系）是联合学习的，那么这种迁移学习将是成功的。</p>
<p>zero-shot是一种特殊形式的迁移学习。同样的原理可以解释如何能执行多模态学习(multimodal learning)，学习两种模态的表示，和一种模态中的观察x 和另一种模态中的观察y 组成的对(x; y) 之间的关系（通常是一个联合分）(Srivastavaand Salakhutdinov, 2012)。通过学习所有的三组参数（从x 到它的表示，从y 到它的表示，以及两个表示之间的关系），一个表示中的概念被锚定在另一个表示中，反之亦然，从而可以有意义地推广到新的对组。这个过程如图15.3所示。</p>
<p><img src="/images/blog/representation3.png" alt="迁移学习"><br>关于迁移学习应用到机器翻译中，我们可以参考谷歌翻译的得 <a href="http://www.im2maker.com/news/20161123/f0995a12c3328fc8.html" target="_blank" rel="noopener">谷歌发布 Zero-Shot 神经机器翻译系统：AI 巴别塔有望成真</a>                                </p>
<h3 id="2-2-实例"><a href="#2-2-实例" class="headerlink" title="2.2 实例"></a>2.2 实例</h3><p> <strong>1.游戏之间的参数迁移</strong></p>
<p>   Deepmind的作品 progressive neural network. <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04671v3.pdf"> progressive neural network</a> 文章里将三个小游戏Pong, Labyrinth, Atari 通过将已学其一的游戏的parameter 通过一个 lateral connection feed 到一个新游戏。有一个视频展示了其结果 <a href="https://www.youtube.com/watch?v=aWAP_CWEtSI" target="_blank" rel="noopener">游戏之间迁移学习参数</a> 。迁移学习的一个简单框架如下图</p>
<p>  <img src="/images/blog/representation11.png" alt="迁移学习"></p>
<p> 迁移学习的一篇博客介绍参考 <a href="https://zhuanlan.zhihu.com/p/21470871" target="_blank" rel="noopener">从虚拟到现实，迁移深度增强学习让机器人革命成为可能！</a></p>
<p><strong>2. 图像之间迁移学习</strong></p>
<p> 最近比较火的风格图像生成，<code>tensorflow</code>,<code>caffe</code>,<code>torch</code>都出了对应的开源代码。比如 tensorflow的 <a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">eural-style</a>。<br>  该代码的原理论文来自 <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" target="_blank" rel="noopener">图像风格迁移学习</a></p>
<p>   <img src="/images/blog/repretation12.jpg" alt="迁移学习"></p>
<p>上图中，输入图像被表示为CNN不同阶段地的被过滤后的图像集合,处理层次越深，过滤器的数量越多，尺寸越小（CNN中使用了最大池化），这导致了每一层的神经元总数减少。</p>
<ul>
<li><p><strong>内容重构：</strong>上图中，仅仅使用已知的输入图像在每一层的网络得响应来可视化了CNN不同阶段处理，上图重构的是输入图像在原始<strong>VGG网络</strong>在<code>conv1 2</code> (a),<code>1conv2 2</code> (b), <code>conv3 2</code> (c), <code>conv4 2</code> (d) 和 <code>conv5 2</code> (e)上的重构信息。</p>
</li>
<li><p><strong>风格重构：</strong>在原始CNN网络的顶层激活函数，使用的是输入图像的质地信息作为特征。风格表征计算不同CNN网络层之间的不同特征的相关性。这可以构建图像，其风格与输入图像上不断增长的特征尺寸相仿，同时丢弃场景中全局布局信息。</p>
</li>
</ul>
<p>关于风格的计算参考该论文。</p>
<h2 id="3-半监督解释因果关系"><a href="#3-半监督解释因果关系" class="headerlink" title="3 半监督解释因果关系"></a>3 半监督解释因果关系</h2><p> 如何确定一种表征比另外一种更好？我们假设，最理想的表征的特征能够追溯到观测数据分布的本质。特征空间中不同的特征或方向源于不同的促因，从而使得表征有能力将这些促因区分。如果y是产生x的重要原因之一，那么 $p(x)$ 可能是计算 $p(y\mid x)$ 的一种良好表征形式。</p>
<p> 在表征学习的其他方法中，我们通常会考虑易于建模的表征，比如全部实体是稀疏的或者相互独立的。完全独立不相关的并不一定易于建模。但是，更进一步看，目前的AI任务都是用无监督表征来促进半监督学习。其实两者是相通的，一旦我们获知观测数据的潜在促因，通常很容易分离样本的独立属性。</p>
<p> 首先，我们看看如果半监督学习 $p(x)$ 无益于于学习 $p(y\mid x)$  。 如果样本 $p(x)$ 是正态分布的，我们想学习  $f(x)=E[y\mid x]$ 。很显然，仅仅从观测训练集x，无法提供给我们任何有用信息。</p>
<p> 接下来，对照看下半监督学习如何变得有用。假若x是由不同部件混合，不同的y部件会构成不同的x值，如下图所示。如果构成x的部件y是完全独立的，那么对 $p(x)$ 建模即精确调和每个部件，每种分类的单标签样本将足以学习 $p(y\mid x)$ 。如果更进一步思考，是什么将 $p(y\mid x)$ 和 $p(x)$ 搭上联系。</p>
<p><img src="/images/blog/representation4.png" alt="迁移学习"></p>
<p>【此图展示的是由三个构件混合构成的x的密度，部件标识了潜在的解释因子y。由于混合构件都是统计显著的，以不使用标签样本的无监督方式建模p(x)已经能够获取因子y了】</p>
<p>如果y与x的某个促因紧密相关， 那么 $p(y)$ 和 $p(y\mid x)$ 也会是强相关的，并且作为一种半监督学习策略，与变量潜在因子相关的无监督表征学习也会是十分有用的。</p>
<p>考虑到我们的假设是y是x的促因之一，现在让h代表所有的促因。真实的生成过程可以通过以下有向图模型获得，h作为x的祖先（在图中的关系是h—&gt;x）</p>
<script type="math/tex; mode=display">
   p(h,x) =p(x\mid h)p(h)</script><p>其结果是，数据具有边际概率</p>
<script type="math/tex; mode=display">
   p(x)=E_hp(x\mid h)</script><p>从以上直观的观察，我们可以得出，表述x的最好的模型是可以揭示真正结构的一个，其中h是解释x中观测变量的潜在变量。以上讨论的完美的表征学习应该重现这些潜在因子。如果y是其中之一（或者与其中之一紧密相关），那么将很容易从这种表述中学习预测y。我们也可以看到，给定x，y的条件分布是由贝叶斯法则约束的（上述等式）。</p>
<p>因此边缘概率 $p(x)$ 是与条件概率 $p(y\mid x)$和前者的结构信息紧密相关的。因而，基于不同情形对应的假设，半监督学习有助于提升性能。</p>
<p>一个重要的研究问题是，大部分观测数据是由极其多的潜在促因形成的。假设 $y=h_i$ ,但是无监督学习器并不知道是哪个  $h_i$ 。学习表征时学习器的暴力解决方案是尝试所有的生成因子 $h_i$ ，并将它们组合，因而为了易于从 <code>h</code> 预测 <code>y</code> 时，我们应该不考虑哪个 $h_i$ 与 <code>y</code> 有关联。</p>
<p>暴力解并不可取，因为不可能穷尽所有或者大部分的变量促因在观测数据上面的影响。例如，在图像处理中，表征应该对背景中所有物体编码吗？这是个有记录的心理学现象，人类无法感知与他们手头任务不直接相关的变化。半监督学习的一个比较前沿的研究是，决定在不同情形下对什么进行编码。当前，处理大量潜在促因的两种策略是同时使用<strong>监督学习</strong>信号和<strong>无监督学习</strong>信号，这样模型就会选择捕捉与变量最相关的促因，或者，如果只有纯粹的无监督学习就会使用更大的表征。</p>
<p>无监督学习中的一种应急策略是修改最显著潜在促因的定义。以经验来说，自动编码和生成模型被训练成优化一个固定的标准，通常是类似均方误差。这种固定的标准决定了那种促因会被当做最显著的。例如，均方误差应用到图像像素时，隐含地指定了只有当大量像素的亮度发生改变时潜在促因才是显著的。如果我们要解决的问题是与小物体交互，将会出问题。下图演示的是一个机器人任务，机器人的自动编码器无法学习对小乒乓球编码。但是，对于大一点的物体，比如棒球，却是可以的，这是因为根据军方误差，棒球更显著。</p>
<p><strong>其他关于显著的定义</strong>。例如，一组以高度可识别的模式组合起来的像素组，即便组合模式不是与亮度或暗度相关的，这种模式也被认为是极度显著的。其中一中重新定义显著的实现方法是使用最近提出的称为<strong>生成对抗网络</strong>。此方法中，生成模型被训练成欺骗前馈分类器。前馈分类器尝试将所有来自生成模型的所有样本识别为负样本，而所有来自训练集的为正样本 。这种组织架构中，任何前馈网络能够识别的结构模式都被认为是高度显著的。论文(Lotter et al. (2015)表明使用均方误差训练成生模型，生成人类头部图像的网络生成的人头经常会忽略掉耳朵，但是如果使用对抗网络架构的话则可以成功生成耳朵。因为耳朵与面部其他地方比起来并不是特别明亮或者暗，对于均方误差来说它们并不显著，但是它们的高度可识别形状和连续的坐标位置对于前馈网络来说易于学习和识别，这对于生成对抗网路来说是足够显著的。</p>
<p><img src="/images/blog/representation5.png" alt="对抗网络生成人的头部图像"></p>
<p>学习潜在促因的一个好处是，如果真实的生成过程中<strong>x</strong>是效果，而<strong>y</strong>是促因，那么模型 $p(x\mid y)$ 对于 $p(y)$ 上的改变是健壮的。如果因果关系逆转，可能就不是真实的，因为根据贝叶斯法则， $p(x\mid y)$ 对于 $p(y)$ 的变化是敏感的。对抗网络另外一个生成人类语音的例子是 <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">GoogleDeepMind模拟人语音读文本</a></p>
<h2 id="4-分布式表征"><a href="#4-分布式表征" class="headerlink" title="4 分布式表征"></a>4 分布式表征</h2><p> 分布式表征的概念是，表征由许多元素组成，每个元素都是独立的，这是表征学习中最重要的工具。分布式表征可以用 $n$ 个特征的 $k$ 个不同值来描述 $k^n$ 个不同概念。如我们所见，有多个隐藏神经元的神经网络以及有多个显著变量的概率模型都使用了分布式表征策略。再引入一个额外的观测。许多深度学习算法都是假设隐藏神经元可以学习表征潜在促因，那些潜在促因解释 了样本数据。分布式表征天然适用于这种方法，因为表征空间的每个方向可以对应到不同潜在配置变量的值。</p>
<p> 一个分布式表征的样本是一个有 $n$ 个二分特征的向量，它可以有 $2^n$ 个配置，每个配置潜在的对应了输入空间的不同区域。如下图：</p>
<p> <img src="/images/blog/representation13.png" alt="表征学习"></p>
<p> 可以与符号表示对比来开，输入是与单个符号或分类相关。如果词典中有n个符号，我们可以想象n个特征检测器，每个检测器只负责其 相关领域的分类。此时的表征空间只有n个不同配置，刻画了输入空间的n个不同区域，如下图。</p>
<p> <img src="/images/blog/representation7.png" alt="表征学习"></p>
<p>这种符号表述也称为 one-hot表征，它可以被一个n个字节的互斥的0(1)向量（n个字节中只有一个是1）描述。符号表征是非分布式表征的一种特殊例子，非分布式表征是指其表述可能会包含许多实体，但是对每个实体并没有有意义的实质独立控制。</p>
<p>基于非分布式表征学习的算法有以下：</p>
<ul>
<li><p><strong>聚类方法</strong>：包括K均值聚类，每个输入点被当做一个聚类</p>
</li>
<li><p><strong>KNN算法</strong>：对于某个输入会有相关的一个或几个模板或原型。</p>
</li>
<li><p><strong>决策树</strong>：给定输入，只有一个叶子节点是激活的。</p>
</li>
<li><p><strong>高斯混合和专家混合</strong>：模板（聚类中心）或者专家现在与一个激活度相关。如同KNN，每个输入表述了多个值，但是这些值不能很好的与其他值区分开。</p>
</li>
<li><p><strong>基于n-grams的语言或翻译模型</strong>：上下文集合（符号序列）按照一个树形结构的前缀区分。叶节点可能对应的是最后两个单词为 $w_1$ 和 $w_2$ 的，例如，参数会独立的为树种每个叶子评估（可能有些是共享的）。</p>
</li>
</ul>
<p>对于这些非分布式表征算法，其输出不是恒定的，而是会比临接区域影响的。参数数量（或者样本）和区域数量之间的关系是可以线性定义的。</p>
<p>一个区分分布式表征与符号表征的重要概念是，不同概念之间由于需要共享属性而需要泛化。对于纯粹的符号”cat” 和”dog” 是两个完全不同的符号。但是，如果将其中之一以某种形式的分布式表征表示，那么许多关于猫的事情可能被泛化为狗(都是动物？四条腿？)。例如，我们的分布式表征可能包含实体，如“有毛”或者“腿的数量”，这样猫和狗的这两个实体的值是一样的。基于词的分布式表征的神经语言模型，比直接在词的one-hot表征的模型要生成的更好。分布式表征引发了丰富的相似空间，这种语义相近概念与距离的概念很像，这是符号表征所不具备的属性。</p>
<p>何时使用分布式表征作为学习算法的一部分有概率优势？—-当看起来很复杂的结构可以用很少的参数表述的时候。一些传统的非分布式表征学习算法，只是因为平滑假设而泛化，该假设认为如果 $u \approx v$ ,那么要学习的目标函数 $f$ 也会有属性 $f(u)\approx f(v)$  。有多种方式来公式化这种假设，但结果是如果我们有样本 $(x,y)$  其中 $f(x)\approx y$ ,那么我们会选择一个评估器 $\hat f$ 可以近似满足在输入 $x+\epsilon $ 处移动时，函数 $\hat f$ 结果不变这一受限条件。这种假设是有用的，但是会引入维度问题：为了在许多不同区间增加或减小的目标函数，若有N个不同区间，我们可能至少需要N样本。我们可以将每个区域看成一个分类或者符号，我们可以学习任意从符号到值的解码。 然而，这样我们就没法泛化来学习新的符号或分类了。</p>
<p>幸运的话，目标函数中除了平滑之外，可能会有些规律。例如，最大池化的卷积神经网络可以无视物体坐标位置去识别，甚至即使物体的空间变换与输入空间的平滑转换对应不上。</p>
<p>我们来检验一个分布式表征学习的特殊例子，即通过对输入值的线性函数的阈值做二分特征抽取（达到某个阈值为1，否则该特征为0）。此表征中的每个二分特征将空间 $R^d$ 划分为一对半空间，如上上一张图（ $h_1,h_2,h_3$ 那张）。大到指数级的n个交叉点对应了的半空间决定了分布式表征学习器可以分辨的区域。空间 $R^d$ 中的n个超平面可以生成多少个区域？论文 (Pascanu et al., 2014b)证明此二分特征表征可以分辨得区域数量为</p>
<script type="math/tex; mode=display">
 \sum _{j=1}^d(_j ^d)=O(n^d)</script><p>我们可以看到区域数是输入数量的指数级增长，隐藏神经元数量以多项式级增长。</p>
<p>这为分布式表征的泛化能力提供了一种几何参数上的解释：$O(nd)$ 个参数（空间 $R^d$ 中n个线性阈值特征）可以代表输入空间的 $O(n^d)$ 的不同区域。如果我们不对任何数据做任何假设，并使用一种表征，每个区域只有一个唯一的符号标识，并且每个符号标识都有独立的参数来识别空间 $R^d$ 中对应的那部分，然后识别 $O(n^d)$ 个区域需要 $O(n^d)$ 个样本。更一般的说，倾向于使用分布式表征的论据在于可以拓展到使用非线性阈值单元（二分0-1函数）。此时的参数处于这种情形，如果k个参数的参数转换可以学习输入空间中r个区域，其中 $k&lt;&lt;r$ 。使用更少参数可以表述模型意味着，我们有更少的参数去拟合，这样就需要极少训练样本就可以泛化得很好。关于模型基于分布式表征可以泛化得很好的更深远的论据是，尽管它可以唯一的编码非常多的不同区域，它所能表征的容量是有限的。例如，神经网络的线性阈值神经元的VC维度仅有 $O(wlogw)$，其中 $w$ 是权重数。这种限制产生的原因是，尽管我们可以给表征空间分配非常多唯一的符号标识，我们也无法绝对的全部使用所有的标识空间，同时也不能用线性分类器任意学习从表征空间 $h$ 到输出 $y$ 的函数。（线性分类器的组成的神经元只能拟合线性函数）。结合线性分类器的分布式表征只能识别线性可分的分类。我们要学习的分类只能是类似，所有图像集中的全部绿色物体，图像中所有的汽车集合，而不是那种需要非线性XOR逻辑的分类。比如，我们不能将数据集分为，所有的红色汽车和绿色卡车作为一类，所有的绿色汽车和红色卡车作为一类。</p>
<p>我们可以考想象，学习某种特征时不必看见所有其他特征的配置。论文Radfordet al. (2015)展示了一个生成模型，该模型学习人的面部表征，表征空间中不同方向可以捕获变量不同的潜在促因。下图演示了，表征空间中一个方向对应了某个人是男人或女人，另一个方向对应的是此人是否有戴眼镜。</p>
<p><img src="/images/blog/representation14.png" alt="表征学习8"></p>
<p>这些特征都是自动发现的，没有任何先验知识。隐藏神经元分类器没必要有标签：如果任务需要类似的特征，所求的目标函数上的梯度下降算法会自然而然地从语义上去学习感兴趣特征。我们可以学习男性和女性的差异，也可以学习是否有戴眼镜，而不必对所有其他n-1个其他特征上所有的值的组合做抽样来进行符号化。（类似VSM模型，但是不需要事先知道整个词典）。这种形式的概率独立性允许我们生成训练期间没有出现的人的新的特征配置（新的特征）。</p>
<h2 id="5-源于深度的指数增益"><a href="#5-源于深度的指数增益" class="headerlink" title="5 源于深度的指数增益"></a>5 源于深度的指数增益</h2><p> 多层感知机都是广义近似，一些函数的表述使用深层网络可以比浅层网络要指数级减少。这种模型尺寸上的较少可以提高统计效率。此处，我们会陈述，将分布式表征应用到其他类型的模型会获得的类似的效果。</p>
<p> 上一节看到的一个生成模型的例子，可以学习人脸图像的描述性隐藏因子，包括人脸的性别和是否有戴眼镜。生成模型完成此类任务是基于深度神经网络。浅层网络是没法学习这种图像像素到抽象的描述性因子之间的复杂关系的。在这个以及其他AI任务中，为了生成与高度非线性的输入高度相关的数据，这类因子可以被几乎完全独立的选取。我们认为，这需要<strong>深度</strong>分布式表征，更高层次的特征（看做输入的函数）或者因子（看做生成促因）都是通过许多非线性组合获得的。</p>
<p> 在许多不同的设置中已经证明，通过许多非线性和一种层次的特征的二次使用组成的组织计算，可以在统计效率上得到指数级的增长。许多不同网络（比如，饱和的非线性，Boolean门，Sum/product，或者RBF神经元）使用一层隐藏层的可以被广义近似呈现。给予足够多的隐藏神经元，一个广义近似模型族可以拟合非常多类别的函数（包括所有的连续函数）。但是隐藏神经元的数量需要非常多。关于深度架构的表述能力的理论结果表明，存在可以被深度为 $k$ 的深层架构有效地表述的函数族，但是会需要输入数量的指数倍的隐藏神经元和完全的深度（2到k-1的深度）。</p>
<h2 id="6-提供线索以挖掘潜在促因"><a href="#6-提供线索以挖掘潜在促因" class="headerlink" title="6 提供线索以挖掘潜在促因"></a>6 提供线索以挖掘潜在促因</h2><p>再回头看看我们的原始问题，什么样的表征比另一个更好？第三小节提出了一种答案，完美的表征应该是不扭曲生成数据的变量的潜在促因（变量由促因组合，变量一般是数据的某一种特征），特别是与我们的应用相关的促因。表征学习的大部分策略是基于引入线索来帮助学习找到变量的潜在促因。<br>监督学习提供的是很强的线索：对于每个 $x$ 都有个对应标签 $y$ ，这通常直接指定了变量的至少一个促因的值。更一般的说，为了利用大量无标签数据，表征学习利用了其他的，非直接的，与潜在促因相关的提示。这种提示以隐含的学习算法设计者的先验知识的形式，指导学习器。一般认为正则化策略有助于获得很好的泛化。尽管无法获得通用的监督正则策略，深度学习的目标是找到一个相对通用的正则化策略，可以应用于相对广泛的AI任务。</p>
<p>下面提供了一些通用的正则化策略。以下列表虽不不是详尽无遗，但也给出了一些集中的例子。</p>
<ul>
<li><strong>平滑：</strong>这是对神经元 $d$ 和某个非常小的数 $\epsilon$,有 $f(x+\epsilon d)\approx f(x)$ 。这种假设允许学习器从训练样本泛化到输入空间附近的某些点。许多机器学习算法继承 了这种思路，但不足以克服维度灾难。</li>
</ul>
<p><strong>线性性：</strong>许多学习算法假设一些变量间的关系是线性的。这允许算法做远超观测数据的预测，但是有时候会导致极限预测。<strong>大部分简单的机器学习算法不做平滑假设，而作线性假设。</strong>这些其实是不同的假设：有很多权重的线性函数应用到高维空间可能并不平滑。</p>
<p><strong>多解释因子：</strong>许多表征学习算法基于一种假设：数据是被多个解释因子生成的，并且在给定每个这些因子的状态时大部分任务可以轻易地解决。</p>
<p><strong>促因：</strong>模型以这种方式构建：它将变量的经学习好的表征 $h$ 描述的因子看做观测数据 $x$ 的促成原因，但反之则不成立。如第三小节讨论的，这是半监督学习的优势，并且使得学习好的模型在潜在促因的分布产生变动或应用到新任务时更健壮。</p>
<p><strong>深度或解释因子的层次组织：</strong>高层次的抽样概念可以被一些简单的概念组合来定义，以形成一种层次。以另外一种观点来看，用深度架构表述我们的思想可以通过多步程序完成，其中每一步都是回头处理前一步输出结果。</p>
<p><strong>任务间共享（促因）因子：</strong>许多任务中的上下文，对应不同 $y_i$ 的变量共享了相同的输入 $x$ 或者每个任务都是与全局输入 $x$ 的一个子集或者函数 $f^{(i)}(x)$ 相关，假设是每个 $y_i$ 与某个源于相同相关因子 $h$ 的共同池的不同的子集相关。因为这些子集之间的重叠，借助共享的中间表征 $P(h\mid x)$ 学习所有的 $P(y_i\mid x)$ 可以在多个任务之间共享统计特性（比较抽象，应该是学习到的表征有共同的统计特性）。</p>
<p><strong>歧管：</strong>（要理解<strong>歧管</strong>可以参考排气歧管，它是与发动机气缸体相连的，将各缸的排气集中起来导人排气总管的，带有分歧的管路。对它的要求主要是，尽量减少排气阻力，并避免各缸之间相互干扰）概率大量浓缩（集中），集中的区域都是局部相连的并且只占用很小的容量。在连续变量场景，这些区域可以用比原始数据少得多的维度的低维复用</p>
<p><strong>天然聚类簇：</strong>许多机器学习算法假设输入空间中的每个相连的歧管可以被分配到单个类别。数据可能位于许多不相连的歧管，但是每个歧管里面的类别数依然是常量。这种假设促使各种各样的机器学习算法产生，包括切线传播，双反向传播，歧管切线分类器和对抗训练。</p>
<p><strong>时间和空间的相干性：</strong>慢特征分析和相关算法做的假设是，大部分重要的描述因子会随时间缓慢变动，或者，与预测原始观测数据如像素值相比，真正的潜在的描述因子更容易被预测。</p>
<p><strong>稀疏性：</strong>大部分特征想必与大部分输入描述不相关，没必要使用全量的输入特征，当表征一只猫时不必检测大象和卡车。因此，任意特征大多情况下可以被描述为“出现”或“没出现”。</p>
<p><strong>简化因子依赖：</strong>在好的高层次表征中，特征之间是通过简单的依赖来彼此相关的。最简单的可能是边缘独立，$P(h)=\prod _iP(h_i)$ ，但是被浅层自动编码器捕获的线性依赖也是可信的假设。这可以在许多物理定律中看到，在学习到的表征上插入线性预测器或因子分解的先验知识就是一种假设。</p>
<p><br><br> <a href="https://www.zhihu.com/question/41979241" target="_blank" rel="noopener">知乎 刘诗昆 关于迁移学习的回答</a><br> <a href="http://it.sohu.com/20161030/n471784836.shtml" target="_blank" rel="noopener">机器之心 一篇文章入门无监督学习</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>LAB特征和特征中心级联的快速、准确的人脸检测方法</title>
    <url>/2016/11/18/facedetecet_LB/</url>
    <content><![CDATA[<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p><a href="http://blog.csdn.net/iracer/article/details/49029239" target="_blank" rel="noopener">积分图像统计像素</a></p>
<p>进一步提取LAB特征时，如何提取其中的Haar特征。<a href="http://ju.outofmemory.cn/entry/260301" target="_blank" rel="noopener">中文博客</a></p>
<p>如果想知道具体的Haar特征，<a href="http://ju.outofmemory.cn/entry/260301" target="_blank" rel="noopener">参考下文（英文论文原文）</a></p>
<h2 id="二-LAB-特征"><a href="#二-LAB-特征" class="headerlink" title="二 LAB 特征"></a>二 LAB 特征</h2><h3 id="2-1-二分Haar特征"><a href="#2-1-二分Haar特征" class="headerlink" title="2.1 二分Haar特征"></a>2.1 二分Haar特征</h3><p> Haar特征是几个邻接矩形区域的强度差异。下图是Haar特征中矩形的一种经典出现方式，特征值即为填充矩形和非填充矩形之间的差异。更进一步说 ，矩形的布局方式是任意的。矩形区域的累积强度是可以用一种称为积分图的方式高效计算。Haar特征的计算包括增加和抽取相关矩形的累积强度。例如，对于一个2-矩形的Haar特征（图1.a和图1.b）可以如下计算:</p>
<p><img src="/images/blog/haar1.jpg" alt="计算"></p>
<script type="math/tex; mode=display">
 f_j(x)=(s_1)_j-(s_2)_j</script><p>其中 $(s_1)_j$ 和 $(s_2)_j$ 分别代表了输入图像 $x$ 的Haar特征 $j$ 填充矩形和非填充矩形的强度和。</p>
<h3 id="2-2-ABH特征"><a href="#2-2-ABH特征" class="headerlink" title="2.2 ABH特征"></a>2.2 ABH特征</h3><p> 为提高二分Haar特征的区分能力，我们提出 了一种多二分Haar特征，并使用它们的共现性作为新的特征，成为ABH(Asseming Binary Haar)特征。下图演示了ABH特征的一个例子：</p>
<p><img src="/images/blog/haar2.jpg" alt="计算"></p>
<p>上图中ABH特征集成了三个二分Haar特征，当三个二分Haar特征值分别为1,1,0时，ABH特征为 </p>
<script type="math/tex; mode=display">
 a(b_1,b_2,b_3)=(110)_2=6</script><p>其中 <strong>a</strong>为三个二分Haar特征 $b_1,b_2,b_3$ 的ABH特征计算函数， $(.)_2$ 是一个从二进制转十进制的操作。特征值说明了对 $2^F$个不同结合的index，其中<strong>F</strong>是结合的二分特征数。</p>
<h3 id="2-3-LAB-Locally-Assembing-Binary-特征"><a href="#2-3-LAB-Locally-Assembing-Binary-特征" class="headerlink" title="2.3 LAB(Locally Assembing Binary)特征"></a>2.3 LAB(Locally Assembing Binary)特征</h3><p>ABH特征的数目巨大。为了枚举所有的特征，需要几个自由参数，比如二分Haar特征的集合数目，每个二分Haar特征的大小，每个二分Haar特征的坐标位置。从如此巨大的特征池中学习是不可逆的。我们发现了一种对应的用于人脸检测的缩减集合，称为LAB Haar特征。</p>
<p>ABH特征之中，LAB特征是那些结合8个局部邻接2-矩形的二分Haar特征，它们大小相同并且共享同一个中心矩形。下图展示了一个8个二分Haar特征用以集合为一个LAB特征。</p>
<p><img src="/images/blog/haar3.jpg" alt="计算"></p>
<p>下图是一个2个LAB特征的示例</p>
<p><img src="/images/blog/haar4.jpg" alt="计算"></p>
<p> 图中展示了两个不同的LAB特征，中心的黑色矩形被8个相邻的二分Haar特征共享，所有9个矩形都是相同的大小。</p>
<p>从公式上看，一个LAB特征可以用一个四元组表示 $l(x,y,w,h)$ ，其中 $x,y$ 分别代表了左上角的x和y轴坐标，$(w,h)$ 代表了矩形的宽和高。</p>
<p>LAB特征保留了所有二分Haar特征的优势，同时又很强的区分能力，大小也很小。LAB特征抓取了图像的局部强度。计算LAB特征需要计算8个2-矩形Haar特征。LAB特征值区间为 {0,…255}，每个值对应了特别的局部结构。</p>
<h2 id="三-使用LAB特征做人脸检测"><a href="#三-使用LAB特征做人脸检测" class="headerlink" title="三 使用LAB特征做人脸检测"></a>三 使用LAB特征做人脸检测</h2><p><strong>什么是级联</strong></p>
<p>级联分类器就是如下图所示的一种退化了的决策树。为什么说是退化了的决策树呢？是因为一般决策树中，判断后的两个分支都会有新的分支出现，而级联分类器中，图像被拒绝后就直接被抛弃，不会再有判断了。</p>
<p><img src="/images/blog/cascade_classfier_tempt.jpg" alt="级联分类器示意图"></p>
<p>级联强分类器的策略是，将若干个强分类器由简单到复杂排列，希望经过训练使每个强分类器都有较高检测率，而误识率可以放低，比如几乎99%的人脸可以通过，但50%的非人脸也可以通过，这样如果有20个强分类器级联，那么他们的总识别率为 $0.99^20$ 约等于98%，错误接受率也仅为 $0.5^20$ 约等于0.0001%。</p>
<p> 级联结构用于检测方法中。下图展示了人脸检测器的级联结构‘，可以分为两个很直观的部分。第一部分是一些子分类器，总称为特征中心级联。第二部分是其他的子分类器，成为窗口中心级联。</p>
<p> <img src="/images/blog/haar5.jpg" alt="计算"></p>
<h3 id="3-1-特征中心检测方法"><a href="#3-1-特征中心检测方法" class="headerlink" title="3.1 特征中心检测方法"></a>3.1 特征中心检测方法</h3><p> 为了搜索一张人脸，我们需要在图像中做“穷尽搜索”。这就牵扯到构造一个分类器以区分目标和非目标，却只需要在目标位置和大小上容忍有限的偏差。查找目标的方法是，扫描所有的分类器，这些分类器会在图像上搜索所有可能的位置和大小。下图展示了这一过程，所有的分类器计算了图像中所有可能的窗口，并以矩形的方式展示。</p>
<p> <img src="/images/blog/haar6.jpg" alt="计算"></p>
<p>大部分级联，使用窗口中心方法。这些方法计算对每个窗口分开的计算亮度矫正和特征计算。分类器的每个可能窗口的扫描，会计算图像中每个坐标的每个特征。这说明包含在某些窗口的特征可能同时被其他分类器计算了，但是并没有被当前分类器用来分类。特征中心方法旨在使用每个窗口的更多的被计算过的特征。</p>
<p>如果上图不详细的话，请参考我个人绘制的示意图</p>
<p><img src="/images/blog/feature_center_my_sample.jpg" alt="窗口中心的方法示意图"></p>
<p>如何理解两种（窗口中心和特征中心）方法呢？下图是一个窗口中心方法的示例</p>
<p><img src="/images/blog/haar7.jpg" alt="计算"></p>
<p><strong>窗口中心方法：</strong>对于窗口中心方法，我们假设分类器只包含了一个LAB特征。，特征为上图a中的矩形。检测时图像中每个窗口被分类。因此属于该分类器的特征同时在图像中每个位置都会被计算，这会产生一个副产品：特征值图像（上图总图b）。此示例中，对于每个窗口，只使用了一个特征来做分类，包含在窗口中的其他特征被其他临接窗口分类器计算。这存在一种计算浪费。因此，特种中心的方法被提出，用以提高计算特征的使用率。</p>
<p><strong>特征中心方法：</strong>对于特征中心方法，特征值图像（下图中中间那幅，与上图中图b一样），可以通过扫描图像中所有坐标的upper特征来计算。然后特征中心的分类器会运行在特征值图像中，并不再需要特征计算操作。在学习过程中，特征中心分类器从所有属于当前窗口的所有特征中学习。实际上，所有的特征都是相同大小，因为他们都是通过在图像平移了一个特殊特征来搜集的。当然，任意大小窗口都可以用来构建特征中心分类器。但是最好是选择最有效率的一个。可以使用一种贪心所有的方法来找到最优大小。</p>
<p><img src="/images/blog/haar8.jpg" alt="计算"></p>
<p>理论上说，任意学习算法都可以用来构建窗口中心和特征中心的分类器，我们使用 <strong>RealBoost</strong>学习算法来学习线性分类函数，如下：</p>
<script type="math/tex; mode=display">
 c(x) =\sum_{i=1}^Th(l_i(x))</script><p>其中 $c$ 是分类函数，$x$ 是样本窗口，$h$ 是弱分类函数，$l_i$ 是第 $i$ 个特征的特征计算函数， $T$ 是总的特征数。其中分类操作 $h$ 包含了一个特征值查阅表，一个置信度和额外查阅表。上面的Figure8和Figure9分别表示的是窗口中心和特征中心的线性分类器。Figure9中，对于所有特征中心的检测方法，分类器包含所有的窗口中的所有特征。分类函数中的特征数为 $N$ 。</p>
<h2 id="3-2-特征中心级联"><a href="#3-2-特征中心级联" class="headerlink" title="3.2 特征中心级联"></a>3.2 特征中心级联</h2><p> 考虑到计算效率，我们将特征中心分类器修改为一个级联。在特征中心方法中，所有包含在窗口内的特征都被用来构建一整个的分类器。将输入图像的每个坐标点作为一个整体扫描并非明智之举。因而，我们考虑将其分解为一个级联。</p>
<p><strong>两种方法的计算差异：</strong>假若分类窗口尺寸为24<em>24,特征为3</em>3的LAB特征，因此一个窗口中会有256（（24-9+1）*（24-9+1））个特征。因为对于特征中心方法和特征中心级联方法，其他过程都是相同的，因此这两种方法在各自窗口的平均分类操作代表了计算差异。对于特征中心方法，所有的256个分类操作将导致这256个特征在每个候选窗口中被操作一次。所以，每个窗口的平均分类操作数是256。但是对于特征中心级联方法，由于随着过程的推进某些窗口会被抛弃，每个窗口的平均分类操作是小于256的。</p>
<p>下图演示了特征中心分类器的特征和特征中心级联。图中 $l_i$ 是由RealBoost挑选出来的第 $i$ 个LAB特征， $N$ 是特征中心分类器的总特征数。圆弧箭头数代表过程的阶段数。由弧形箭头覆盖的特征是属于对应阶段地特征。</p>
<p><img src="/images/blog/haar9.jpg" alt="计算"></p>
<h2 id="3-3-多角度人脸检测"><a href="#3-3-多角度人脸检测" class="headerlink" title="3.3  多角度人脸检测"></a>3.3  多角度人脸检测</h2><p>为了构建 一个多角度人脸检测器，我们首先根据从左到右的平面旋转将所有的脸分为5个类别，然后继续将每个类别分为三个角度，每个代表了从平面30度旋转。除此之外，每个角度覆盖了[-30°,+30°]从上至下的平面旋转。下图展示了这15个不同角度。</p>
<p><img src="/images/blog/haar10.jpg" alt="计算"></p>
<p> 对每个角度，我们构建了一个特征中心级联和窗口中心级联。为了检测，下图展示了其过程</p>
<p> <img src="/images/blog/haar11.jpg" alt="计算"></p>
<p> 对于给定输入图像，我们首先计算特征值图像。对每个角度，特征中心级联首先基于计算特征图运行，然后窗口中心级联在原始图像上运行。<strong>注意：</strong>对于多角度人脸检测，特征值图像被所有的15个角度的特征中心级联共享。这会极大加速检测器速度。</p>
<h2 id="4-实时人脸校准"><a href="#4-实时人脸校准" class="headerlink" title="4 实时人脸校准"></a>4 实时人脸校准</h2><h3 id="4-1-CFAN简介"><a href="#4-1-CFAN简介" class="headerlink" title="4.1 CFAN简介"></a>4.1 CFAN简介</h3><p><strong>CFAN</strong>由4个级连的SAN网络构成，每个都是四层网络，三个隐层，用sigmoid激活，最后一层为线性激活。每一个SAN的输出图像分辨率逐渐变大，定位逐渐逼近精准</p>
<p>第一个全局SAN用于粗定位68个形状特征点，输入为 50x50 的低分辨率图像，即2500个输入单元，最终输出为68个形状特征点的位置，即 68x2=136个输出元素。中间层分别为1600,900,400个单元。<br>三个局部SAN的输入为68个特征点在高分辨率图中从周围区域提取出来的形状索引特征(SIFT)。输出仍然为逐步校正后的136个特征位置。原始输入应该是从每个形状特征点周围提取了128个SIFT特征，即共 68x128=8704个特征，太大，采用PCA的方法，分别降维到了1695、2418、2440输入元素。中间层分别为1296,784,400个单元。</p>
<h3 id="4-2-训练过程"><a href="#4-2-训练过程" class="headerlink" title="4.2 训练过程"></a>4.2 训练过程</h3><p>SAN的训练采用先用无监督的预训练进行分层训练，粗调参数（可采用sparse autoencoder的方法来预训练），然后用有监督的训练方法进行全局训练，精调参数。<br>训练样本要进行一些随机的平移、旋转、放缩，可有效防止过拟合和增加不同场合的稳定性。<br>全局SAN训练目标函数：</p>
<script type="math/tex; mode=display">
 F^{*}=arg\quad min_F \mid \mid S_g(x)-f_k(f_{k-1}...(f_1(x)) \mid \mid _2 ^2+\alpha \sum _{i=1}^k \mid \mid W_i\mid \mid _F^2</script><p>局部SAN训练目标函数：</p>
<script type="math/tex; mode=display">
  H^*_1= arg\quad min_{H_1}\mid\mid \triangle S_1(x)-h_k^1(h_{k-1}^1(...h_1(\phi (S_0))))\mid \mid ^2_2+\alpha \sum_{i=1}^k\mid\mid W^1_i\mid\mid _F^2</script><p>测试结果：</p>
<h2 id="5-人脸对齐算法"><a href="#5-人脸对齐算法" class="headerlink" title="5 人脸对齐算法"></a>5 人脸对齐算法</h2><h3 id="5-1-问题"><a href="#5-1-问题" class="headerlink" title="5.1 问题"></a>5.1 问题</h3><p> 最小二乘问题中，用牛顿法求解是常用的办法，但用在求解计算机视觉的问题的时候，会遇到一些问题，比如1）、Hessian矩阵最优在局部最优的时候才是正定的，其他地方可能就不是正定的了，这就意味着求解出来的梯度方向未必是下降的方向；2）、牛顿法要求目标函数是二次可微的，但实际中未必就一定能达到要求的了；3）、Hessian矩阵会特别的大，比如人脸对其中有66个特征点，每个特征点有128维度，那么展成的向量就能达到66x128= 8448，从而Hessian矩阵就能达到8448x8448，如此大维度的逆矩阵求解，是计算量特别大的（O(p^3)次的操作和O(p^2)的存储空间）。因此避免掉Hessian矩阵的计算，Hessian不正定问题，大存储空间和计算量，寻找这样一种方法是这篇论文要解决的问题。</p>
<h3 id="5-2-原理"><a href="#5-2-原理" class="headerlink" title="5.2  原理"></a>5.2  原理</h3><p> 大家都知道，梯度下降法的关键是找到梯度方向和步长，对于计算机视觉问题，牛顿法求解未必能常常达到好的下降方向和步长，如下图所示</p>
<p> <img src="/images/blog/face_detection_2.png" alt=""></p>
<p>（a）为牛顿法的下降量，收敛不能达到最理想的步长和方向。而（b）本文的SDM算法，对于不同的正面侧面等情况都能得到很好的收敛方向和步长。既然Hessian矩阵的计算那么可恶，我们就直接计算梯度下降方向和步长嘛。开始讨论之前，为方便讨论，我们需要问题形式化，假设给定一张要测试的图片（这里把图像自左向右自上而下地展成了一维的向量，具有m个像素），表示图像中的p个标记点，这篇文章里面有66个标记点，如下图黑人肖像所示。表示一个非线性特征提取函数，例如 SIFT，那么。<br>    在训练阶段，已经知道了每张训练图片的真实的66个标记点，把这些点看做了是GroundTrue即参考点，如下图（a）所示。在测试的场景中，会用一个检测器把人脸检测出来，然后给一个初始化的平均标记点，如下图（b）所示：</p>
<p><img src="/images/blog/face_detection_4.png" alt=""></p>
<p>那么人脸对齐问题是需要寻找一个梯度方向步长，使得下面的目标函数误差最小：</p>
<script type="math/tex; mode=display">
 f(x_0+\triangle x)=\mid\mid h (d(x_0+\triangle x))-\phi _0\mid\mid ^2_2</script><p>其中 $\phi <em>*=h(d(x</em><em>))$ 是人工标定的66个标记点的SIFT特征向量，在训练阶段 $\phi _</em>$ 和 $\triangle x$ 都是知道的。</p>
<p>好了，用牛顿法求解上述问题，其迭代的公式为：</p>
<script type="math/tex; mode=display">
 x_k=x_{k-1}-2H^{-1}J_h^T(\phi _{k-1}-\phi _*)</script><p>其中，H和J分别表示Hessian矩阵和雅克比矩阵。它可以被进一步的拆分为下面的迭代公式：</p>
<script type="math/tex; mode=display">
 x_k=x_{k-1}+R_{k-1}\phi _{k-1}+2H^{-1}J_h^T\phi_*</script><p>注意到，既然H和J难求，那就直接求它们的乘积，即可，于是上述的迭代公式又可以变为：</p>
<script type="math/tex; mode=display">
 x_k=x_{k-1}+R_{k-1} \phi_{k-1}+b_{k-1}</script><p>其中 $R<em>{k-1}=-2H^{-1}J_h^T$ 和 $b</em>{k-1}=2H^{-1}J<em>h^T$ ，这样就转化为了之求解 $R</em>{k-1}$ 和 $b_{k-1}$ 的问题。接下来就是怎么求解这两个参数的问题了。</p>
<p>思路很简单，就是用训练数据告诉算法下一步该往哪里走，即用当前（及之前）的迭代误差之和最小化，该问题也是一个最优化问题。如下公式所示：</p>
<script type="math/tex; mode=display">
 arg _{R_k} \quad min_{b_k} \sum_{d^i}\sum_{x_k^i} \mid\mid \triangle x_*^{ki}-R_k\phi _k^i-b_k\mid \mid ^2</script><p>$d_i$ 表示第i张训练图片，$x_ki$ 表示第 $i$ 张图片在第 $k$ 次迭代后的标记点的位置。实际中这样的迭代4-5次即可得到最优解，用贪心法求解。</p>
<p>至此，根据以上描述的迭代步骤，即可不断地寻找到最优的人脸对齐拟合位置。SDM的流程图如下所示：</p>
<p><img src="/images/blog/face_detection_3.png" alt=""></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://blog.csdn.net/stdcoutzyx/article/details/34842233" target="_blank" rel="noopener">基于Haar特征的Adaboost级联人脸检测分类器</a></p>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title>《漏斗形级联结构的多角度人脸检测算法》论文阅读笔记</title>
    <url>/2016/11/16/facedetect-paper-note/</url>
    <content><![CDATA[<h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p>  目前主流的三类人脸识别方法。</p>
<ul>
<li>最经典的是增强级联框架（boosted cascade framework）。这些检测器(detector)计算高效，可快速抽取特征。</li>
<li>为了处理精确处理面部变化较大，DPM（deformable part models可变形部件模型）：用以同时抽取图像的全局和局部特征。它基于一种覆盖分类内部变化的启发式方法，因此对于图像中人物表情姿势的变化有较好鲁棒性。但是非常耗时。</li>
<li>最新的是使用CNN卷积神经网络的方法。缺点是计算代价高，因为网络得复杂性和许多复杂的非线性操作。</li>
</ul>
<p>以上工作都没有考虑特殊场景，比如<strong>多角度人脸识别</strong>。为了多角度识别人脸，一种直接的方法就是并行使用多个人脸检测器(detector)。并行架构需要所有候选窗口被所有模型分类，这导致计算成本和误报率的飙升。为缓解此类问题，所有模型需要精心地训练，使模型具有较好的区分能力去辨别人脸和非人脸。</p>
<p><img src="/images/blog/facedetect_model.jpg" alt="人脸识别模型"></p>
<p>多视角的多模型可以如上图这样组织成树形或金字塔形。这些结构中，根分类器都是区分是否为人脸，接下来的其他分类模型将人脸按照不同的精细粒度分为不同子分类，这里的每个模型都是独立的。金字塔模型实际是将共享了某些高层节点的模型压缩了，因此金字塔模型与并行模型有一样的问题。树形结构分类器不同之处在于，分支的动机是避免在同一层评估所有的分类器，但是这会导致检测错误分类分支。</p>
<p>为此我们提出了一种漏斗形级联的多视角人脸检测结构，获得较高准确率和较快速度。该结构上宽下窄，模型如下图。</p>
<p><img src="/images/blog/facedetect_model2.jpg" alt="人脸识别模型"></p>
<p>模型的顶层是一些并行运行的，快速而粗粒度的分类器，用来快速地移除非人脸窗口。每个模型都是针对性地使用一小段区间范围的视角的人脸，因而可以保证多角度人脸的较高召回率。越往下，模型的区分能力越强，但是也越耗时，它们被用来筛选符合条件的窗口候选。模型的底部收集最后通过的窗口，最后一阶段是一个统一的多层干感知机。</p>
<h2 id="二-漏斗结构级联的多视角人脸检测器"><a href="#二-漏斗结构级联的多视角人脸检测器" class="headerlink" title="二 漏斗结构级联的多视角人脸检测器"></a>二 漏斗结构级联的多视角人脸检测器</h2><p> 输入图像根据滑动窗口树状图扫描，然后每个窗口依次分阶段地穿过探测器。</p>
<p> <strong>Fast LAB接连分类器</strong>用来快速移除大部分非人脸窗口，（LAB（Locally Assembled Binary））同时保证人脸窗口的较高召回率。<strong>Coarse MLP Cascade</strong>分类器以较低代价来进一步调整候选窗口。最后，统一<strong>Fine MLP Cascade</strong>分类器使用形状索引特征精确地区分人脸。</p>
<h3 id="2-1-Fast-LAB-cascade"><a href="#2-1-Fast-LAB-cascade" class="headerlink" title="2.1 Fast LAB cascade"></a>2.1 Fast LAB cascade</h3><p> 实时人脸识别时，最大的障碍在于需要检验的滑动窗口树状图的候选窗口太多。在一个640x480的图像上，要检测脸特征尺寸超过20x20的人脸，需要检查超过一百万个窗口。使用增强级联分类器，由Yan et al提出了一种有效的LAB((Locally Assembled Binary)，只需要考虑Haar 特征的相对关系，并使用look-up（查阅表）加速。一个窗口中抽取一个LAB特征仅需要访问内存一次。我们可以使用LAB 特征，可以在程序开始时快速地移除占比非常大的非人脸特征。</p>
<p> 尽管LAB 特征方法有速度，但是对于多角度人脸窗口的复杂变换表现较差。因此我们采取了一种分而治之的思路，将较难的多视角人脸问题分解为容易的单视角人脸检测问题。多个LAB 级联分类器，每个角度一个分类器，并行处理，然后最终的候选人脸窗口是所有经分类器筛选过后的结果合集。</p>
<p> <strong>公式：</strong>定义整个包含了多角度人脸的训练集为 <strong><em>S</em></strong>，根据角度划分为 <strong><em>v</em></strong> 个子集，<br> 定义为 $S_i,i=1,2,…v$ 。对每个训练集 $S_i$ ,一个LAB级联分类器 $c_i$ 被训练，它用于检测第 $i$ 个角度的人脸。对于输入图像中的窗口 $x$ ，它是否为人脸取决于如下所有的LAB 级联分类器：</p>
<script type="math/tex; mode=display">
  y=c_i(x)\vee c_2(x)...\vee c_v(x)</script><p> 其中 $y \epsilon \lbrace0,1\rbrace$ ，$c_i(x)\epsilon \lbrace0,1\rbrace$ 表明 $x$ 是否为人脸。使用多模型消耗更多时间，但是所有模型共享相同的LAB特征映射（用来特征抽取）。</p>
<h3 id="2-2-Coarse-MLP-cascade-粗粒度多层感知机级联"><a href="#2-2-Coarse-MLP-cascade-粗粒度多层感知机级联" class="headerlink" title="2.2  Coarse MLP cascade 粗粒度多层感知机级联"></a>2.2  Coarse MLP cascade 粗粒度多层感知机级联</h3><p>  LAB级联阶段之后，大部分非人脸窗口被抛弃，剩下的部分对于单个LAB 特征难以处理。因此，接下来，候选窗口将交给更复杂的分类器来处理，比如带 <strong>SURF（Speeded-up Robust Feature）</strong> 的MLP。为避免增加太多计算，小型网络被开发为更好，但是依旧粗粒度的校验。</p>
<p>  此外，使用SURF特征的MLP用于窗口分类，可以更好的建模非线性多角度人脸和带有等同的非线性激活函数的非人脸模式。</p>
<p>  MLP由输入层，输出层和一个或多个隐藏层组成。公式化n层的MLP如下:</p>
<script type="math/tex; mode=display">
  F(x)=f_{n-1}(f_{n-2}(...f_1(x)))\quad tag 2\\
  f_i(z)=\sigma(W_iz+b_i)</script><p>其中 $x$   是输入，比如候选窗口的SURF特征； $W_i$ 和 $b_i$ 分别为链接第 $i$ 层和第 $i+1$ 层的权重和偏置。激活函数 $\sigma$ 形如： $\sigma (x)=\frac{1}{1+e^{-x}}$ ，从上式可以看出，隐藏层和输出都做了非线性变换。MLP的训练目标是最小化预测值和实际值之间的均方误差</p>
<script type="math/tex; mode=display">
 min_F\sum_{i=1}^n \mid \mid F(x_i)-y_i \mid \mid ^2</script><p>其中 $x_i$ 是第 $i$ 个训练样本， $y_i$ 是对应的标签(0或1)。</p>
<p>由于MLP级联分类器有足够能力建模人脸和非人脸变换，穿过多个LAB级联分类器之间的窗口可以由同一个模型处理，也即MLP级联可以连接多个LAB级联分类器。</p>
<h3 id="2-3-带形状索引特征的细粒度MLP级联"><a href="#2-3-带形状索引特征的细粒度MLP级联" class="headerlink" title="2.3 带形状索引特征的细粒度MLP级联"></a>2.3 带形状索引特征的细粒度MLP级联</h3><p> 多视角人脸外貌之间存在一些冲突，主要源于非对齐特征，比如基于坐标抽取的特征存在语义不一致问题。比如，一个面向前方的人脸的中央区域包含了鼻子，但是面部外形也是脖子的一部分。为解决这个问题，我们采取了一种基于形状索引的方法在语义相同的位置上抽取特征作为细粒度MLP级联分类器的输入。如下图所示，选择了四个语义位置，分别对应的面部坐标是左、右眼中心，鼻尖和嘴中心。对于侧脸，不可见的眼部被视为与另外一只眼睛处于相同坐标。</p>
<p><img src="/images/blog/facedetect_model3.jpg" alt="人脸识别模型"></p>
<p>对于表情更丰富的基于形状索引的特征，更大、性能更强的非线性变换用来实现面部和非面部微调。与之前的不同的是，更大的MLPs同时预测标签，推测一个候选窗口是否为一张脸，推测其形状。一个额外的形状预测误差项加入到目标函数，新的优化问题变为如下：</p>
<script type="math/tex; mode=display">
min_F \sum_{i=1}^n \mid \mid F_c(\phi (x_i,\hat S_i))-y_i \mid \mid ^2+\lambda \sum_{i=1}^n \mid\mid F_s(\phi (x_i-\hat S_i))-s_i \mid\mid ^2_2</script><p>其中 $F_c$ 是面部分类输出， $F_s$ 是预测形状输出。 $\phi (x_i,\hat s_i)$ 代表的是基于形状索引的特征（比如SIFT），它是按照平均形状或预测形状为 $\hat s_i$ 从第 $i$ 个训练样本抽取的，其中 $s_i$ 是实际形状。 $\lambda$ 是平衡两类误差的权重因子，一般设置为 $\frac{1}{d}$，其中d为形状的维度。从上面的等式可以看出，可以获得一个比输入 $\hat s_i$更精确地外形 $F_s(\phi(x_i,\hat s_i))$ （注意看下标）。因此，多个级联的MLPs，用于特征抽取的形状越来越精确，这会获得更加有区分力的基于形状索引的特征，并且最后让多角度人脸与非人脸区域差异更大。下图展示了这一过程：</p>
<p><img src="/images/blog/facedetect_model4.jpg" alt="人脸识别模型"></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
  </entry>
  <entry>
    <title>softmax函数</title>
    <url>/2016/11/10/softmax/</url>
    <content><![CDATA[<p>本文根据 <a href="http://blog.csdn.net/hejunqing14/article/details/48980321" target="_blank" rel="noopener">softmax函数定义</a><br>整理</p>
<h2 id="softmax函数的定义"><a href="#softmax函数的定义" class="headerlink" title="softmax函数的定义"></a>softmax函数的定义</h2><p>softmax是sigmoid函数的普遍形式，用于多分类。用在输出层，作为一个分类器，表述为多个分类上的概率分布。 softmax函数的常见形式如下：</p>
<script type="math/tex; mode=display">
  P(i) =\frac{exp(\theta_i^Tx)}{\sum _{i=1}^Kexp(\theta_i^Tx)}</script><p>通过softmax函数，可以使得 $P(i)$ 的范围在[0,1]之间。在回归和分类问题中，通常θ是待求参数，通过寻找使得 $P(i)$ 最大的 $\theta_i$作为最佳参数。回顾logistic函数形式</p>
<script type="math/tex; mode=display">
  P(i)=\frac{1}{1+exp(-\theta^T_ix)}</script><p>这个函数的作用就是使得 $P(i)$ 在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。同样，softmax函数加入了e的幂函数正是为了两极化：正样本的结果将趋近于1，而负样本的结果趋近于0。</p>
<h2 id="softmax的可以作为最终概率的证明"><a href="#softmax的可以作为最终概率的证明" class="headerlink" title="softmax的可以作为最终概率的证明"></a>softmax的可以作为最终概率的证明</h2><p>虽然Softmax函数得到的是一个[0,1]之间的值，且 $\sum _{i=1}^KP(i)=1$ ，但是这个softmax求出的概率是否就是真正的概率？换句话说，这个概率是否严格等于期望呢？为此在这里进行推导。</p>
<p>假设现在有K个类，样本属于类别 $i$ 的概率为 $ϕ(i),i=1,…,K$ ,由于 $\sum _{i=1}^KP(i)=1$ ,所以只需要 <strong>前K-1</strong>个参数即可：   </p>
<script type="math/tex; mode=display">
 \phi _i =P(y=i,\phi),i=1,...K-1.\quad \phi _k=1-\sum_{i=1}^{K-1} \phi _i</script><p>先引入T(y)，它是一个k-1维的向量，如下所示：</p>
<script type="math/tex; mode=display">
 T(1)=\begin {bmatrix}
  1\\ 0\\0\\0\\.\\.\\0\end{bmatrix}
  \quad
  T(2)=\begin {bmatrix}
   0\\ 1\\0\\0\\.\\.\\0\end{bmatrix}
   \quad
   ...
   T(k-1)=\begin {bmatrix}
    0\\ 0\\0\\0\\.\\.\\1\end{bmatrix}
    \quad
    T(k)=\begin {bmatrix}
     0\\ 0\\0\\0\\.\\.\\0\end{bmatrix}</script><p>样本属于第i类则第 $i$ 行元素为1，其余为0，即：$(T(i))i=1$ (注意第i个列的第i行)。因为y只能属于1类，故(y不等于k时)T(y)只有一个元素为1，其余元素都为0，则y的期望为：</p>
<script type="math/tex; mode=display">
  E(T(y))_i=P(y=i)=\phi_i,i\neq K</script><p>令 $\beta_i=log\frac{\phi_i}{\phi_K},i=1,…,K$(同时除以     $\phi_K$ ),则有:</p>
<script type="math/tex; mode=display">
  e^{\beta_i}=\frac{\phi_i}{\phi_K}\Longrightarrow\phi_K=\frac{\phi_i}{\beta_i}\Longrightarrow\phi_K\sum_i^Ke^{\beta_i}=\sum_i^K\phi_i=1</script><p><strong>注意观察：第三个式子是在第一个式子的左右乘以 $\phi_K$ ,然后累加求和。（第二个式子用在第二步）</strong></p>
<p>所以：</p>
<script type="math/tex; mode=display">
  \phi_k=\frac{1}{\sum_{i=1}Ke^{\beta _i}}</script><p>此时再将 $\phi_K=\frac{\phi_i}{\beta}$ 带入（上面推导的第二个式子），有：</p>
<script type="math/tex; mode=display">
  \phi_i=\frac{e^{\beta_i}}{\sum_{i=1}^Ke^{\beta_i}}</script><p>由于分母中是求和操作，可以将i换成K。得到</p>
<script type="math/tex; mode=display">
  \phi_i=\frac{e^{\beta_i}}{\sum_{k=1}^Ke^{\beta_k}}</script><p>所以实际的期望是具有softmax函数的形式的，当 $f_i(x)=\beta_i=log\frac{\phi_i}{\phi_k}$ 时实际期望与softmax函数严格相等，所求概率为真实值。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：深度前馈网络</title>
    <url>/2016/11/06/deepfeedforwardnn/</url>
    <content><![CDATA[<h2 id="一-如何理解"><a href="#一-如何理解" class="headerlink" title="一 如何理解"></a>一 如何理解</h2><p> 前馈网络之所以称为网络，是因为它们是由多种不同函数组成。网络模型可以看做一个有向无环图，该图描述了函数之间是如何关联的。<br> 例如，我们有三个函数 $f^{(1)},f^{(2)},f^{(3)}$ 链式相连，形成 $f(x) =f^{(3)}(f^{(2)}(f^{(1)}))$ ，这种链式结构是神经网络中最常用的结构。其中 $f^{(1)}$ 被称为网络的第一层，$f^{(1)}$ 称为第二层…链式的长度即为网络的深度，网络的最后一层为输出层，这也是深度学习的来源。</p>
<p> 网络训练过程中，我们让 $f(x)$ 逐渐匹配 $f^<em>(x)$ (为我们需要拟合的函数) 。训练样本提供了由  $f^</em>(x)$ 在不同点的带有噪音的近似的样本。每个样本 $x$ 都有一个相对应的标签 $y\sim f^*(x)$  ，训练样本直接指明了输出层在每个点 $x$ 的行为，它应该输出一个近似于 $y$ 的输出。</p>
<h2 id="二-示例：学习XOR"><a href="#二-示例：学习XOR" class="headerlink" title="二 示例：学习XOR"></a>二 示例：学习XOR</h2><p>  我们通过一个学习XOR函数的任务来让前馈网络的概念更具体。XOR函数是一个在两个二值变量 $x_1,x_2$ 上的操作。当 $x_1,x_2$ 中的某一个值为1时，XOR函数返回1，否则返回0。XOR函数提供了一个目标函数 $y=f^<em>(x)$ 。我们的模型提供了函数 $y=f(x;\theta)$ ，我们的学习算法将会逐步调整参数 $\theta$  来使得 $f$ 尽可能地趋近于 $f^</em>$。</p>
<p>  此实例中，我们的目标是使得网络能够正确的预测变量 $x_1,x_2$ 在全部的域值空间的四个点 $X={[0,0]^T,[0,1],[1,0],[1,1]}$。可以将此问题看做回归问题，选择均方误差作为损失函数，在全部训练数据集中，MSE(均方误差)损失函数如下:</p>
<script type="math/tex; mode=display">
    J(\theta) = \frac{1}{4}\sum_{x\epsilon X}(f^*(x)-f(x;\theta))^2 \tag {6.1}</script><h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>  此时我们需要选择模型的形式 $f(x;\theta)$。假若我选择了一个线性模型，其中 $\theta$ 由 $w$ 和 $b$ 组成。我们的模型定义如下</p>
<script type="math/tex; mode=display">
     f(x;w,b) = x^Tw+b</script><p>将四个训练样本带入可求解得 $w=0,b=1/2$ 此时的线性模型将一直输出0.5。 下图演示了线性模型无法表述 XOR函数。</p>
<p><img src="/images/blog/dnn1.png" alt=""></p>
<p>图中四个点是学习函数需要在四个坐标输出的值（左下点坐标是(0,0)，要输出0），从左边可以看出线性模型无法实现XOR功能。图右是被神经网络抽取特征的变换空间，此时线性模型可以解决XOR问题了。在样本解中，输出值为1的两个点在特征空间中合并为一个点了</p>
<p>下图演示了一个包含了一个隐藏层（有两个神经元）的网络(之前只有两层，输入和输出)，隐藏层神经元的作用函数 $h$ 为 $f^{(1)}(x;w,c)$ ,隐藏单元的输出将作为第二层的输入。输出层依然是一个线性回归模型，但是它作用对象是 $h$  而不是之前的直接的 $x$ 了。</p>
<p><img src="/images/blog/dnn2.png" alt=""></p>
<p>网络此时包含两个链式函数，$h=f^{(1)}(w;w,c)$ 和 $y=f^{(2)}(h;w,b)$ 。完整的模型可以表述为:</p>
<script type="math/tex; mode=display">
   f(x;W,c,w,b) =f^{(2)}(f^{(1)}(x))</script><p>此时，函数 $f^{(1)}$ 该如何计算呢。如果使用线性模型，将会导致整个网络都沦为输入的线性函数（ $f^{(2)}$(x) 是线性回归）。假若 $f^{(1)}(W^Tx)$ 而 $f^{(2)}(h)=h^Tw$ ，此时整个网络可以表述为 $f(x)=w^TW^Tx$  依然是线性的。</p>
<p>很显然，我们必须使用一个非线性函数来描述这些特征。大多数神经网络是通过一个由学习参数控制的映射转换，加上一个固定的称为激活函数的非线性函数。大多数网络选用 ReLU作为激活函数</p>
<h3 id="2-2-ReLU激活函数"><a href="#2-2-ReLU激活函数" class="headerlink" title="2.2 ReLU激活函数"></a>2.2 ReLU激活函数</h3><p> ReLU作为激活函数的依据是，生物的稀疏激活性。人脑神经元同时被激活的只有1%-4%，这可提高学习精度，更好更快地提取稀疏特征。ReLU的<strong>非线性</strong>来源于神经元的部分选择性激活。</p>
<p> 使用ReLU的另一个原因是，Sigmod（另外一种激活函数）网络会出现梯度消失情况。误差反向传播时，梯度</p>
<script type="math/tex; mode=display">
   Gradient = (y-y^{'})sigmod^{'}*x \\
   sigmod^{'}(x) \quad\quad\epsilon (0,1) \\
   x \quad \epsilon (0,1)</script><p> 会成倍衰减（ $sigmod^{‘}(x) \quad\epsilon (0,1)$ ,$x\epsilon (0,1)$）。而ReLU的梯度为 $max{0,W^Tx+b}$ 始终为1，只有一端饱和，梯度易流动，同时提高了训练速度。</p>
<p> 关于ReLU的详细可参考 <a href="http://www.cnblogs.com/neopenx/p/4453161.html" target="_blank" rel="noopener">ReLU激活函数</a></p>
<h2 id="三-基于梯度的学习"><a href="#三-基于梯度的学习" class="headerlink" title="三 基于梯度的学习"></a>三 基于梯度的学习</h2><p> 神经网络的非线性将导致损失函数非凸，没法全局收敛。应用于非凸的损失函数的SGD（随机梯度下降）方法也无法全局收敛，而且对初始值敏感。对于<strong>前馈网络而言，初始权重应随机初始化为很小的值，偏置 $b$ 初始化为0，或很小的值</strong>。 梯度下降方法应用在各种网络中，用来最小化损失函数。其实在训练SVM，线性回归模型中都可以使用梯度下降方法</p>
<h3 id="3-1-损失函数"><a href="#3-1-损失函数" class="headerlink" title="3.1 损失函数"></a>3.1 损失函数</h3><p>设计神经网咯时，一个重要的组成部分是损失函数的选择，尽管大部分神经网络的损失函数都与参数模型相似。</p>
<p>大多数情况下，参数模型的定义了一个概率分布 $p(y|x;\theta)$ ,我们使用的是最大似然定理（深度学习，机器学习的本质，将训练时最优的参数当做实际最优的参数），即，使用训练数据和模型预测值的交叉熵作为损失函数。</p>
<p>交叉熵定义：</p>
<script type="math/tex; mode=display">
  c = -\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]</script><p>交叉熵特性:</p>
<ul>
<li>非负性</li>
<li>期望与实际误差小时，损失值小，反之则大。</li>
</ul>
<p>有时候我们会使用一种简化解，不使用全部的 $y$的概率分布，仅预测满足一定条件的x对应的输出y的概率分布。</p>
<h4 id="3-1-1-1-学习条件最大似然的条件分布"><a href="#3-1-1-1-学习条件最大似然的条件分布" class="headerlink" title="3.1.1.1 学习条件最大似然的条件分布"></a>3.1.1.1 学习条件最大似然的条件分布</h4><p> 使用最大似然方法训练网络，其损失函数即负的对数似然，可定义为</p>
<script type="math/tex; mode=display">
  J(\theta) = -E_{x,y\sim p^{\sim}_{data}}logp_{model}(y\|x)</script><p>不同的参数模型，具体的损失函数不一样。</p>
<p>线性模型的输出的概率分布的最大似然估计等价于最小均方误差（损失函数的一种），实际上，这种等价是以忽略 $f(x;\theta)$ 对高斯分布的均值预测为前提的。</p>
<p>设计神经网络时的一个常见问题是：梯度需要足够大，且易于预测。饱和函数（上面一章中ReLU参考文章中有解释饱和函数）会破坏这一期望，它会将梯度变得特别小，这会导致网络很难调整。如果网络输出层是 $e^x$ 类的函数，若 $x$ 为较小负数，也会饱和。在这样的函数上应用对数函数可以有效化解饱和问题（对数和 $e^x$ 相互抵消）。</p>
<p>交叉熵损失函数可用来完成类似的最大似然估计的功能，因为它没有最小值，但可以无限逼近于0或1，这可同时用于离散化输出，logistic回归就是一个类似的模型。<strong>问题</strong>：若模型可控制输出分布的密度，就有可能给正确的训练集极高的密度，这会使得交叉熵趋于无穷大，使用正则化可用来避免这个问题。</p>
<h4 id="3-1-1-2-学习条件概率"><a href="#3-1-1-2-学习条件概率" class="headerlink" title="3.1.1.2 学习条件概率"></a>3.1.1.2 学习条件概率</h4><p>相比于 $p(y|x,\theta)$ 的完全概率分布，我们一般仅学习一个给定$x$ 的 $y$ 的条件概率分布。假若我们使用一个足够强大的网络，我们可以认为网络能够表述任意函数 $f$ (函数有足够多的分类),此时函数的分类仅受限于特征，比如特征的连续性和无界性，而不是某种固定的参数形式。从这个视角来看，我们可以将损失函数视为<strong>函数式</strong>而非简单的<strong>函数</strong>。<strong>函数式</strong>指的是从函数到实数的映射，而学习过程将会是选择一个函数，而不是一系列参数。此时，我们设计<strong>损失函数式</strong>为在某些期望函数上才取得最小值。比如，我们设计的损失函数式取得最小值的时候，对于给定的特征 $x$ 刚好输出了期望的 $y$ 。</p>
<h3 id="3-2-输出神经元"><a href="#3-2-输出神经元" class="headerlink" title="3.2 输出神经元"></a>3.2 输出神经元</h3><p> 损失函数的选择与输出神经元的选择直接相关。下文简述了一些不同输出分布时对应的神经元。</p>
<h4 id="3-2-1-高斯分布输出的线性神经元"><a href="#3-2-1-高斯分布输出的线性神经元" class="headerlink" title="3.2.1 高斯分布输出的线性神经元"></a>3.2.1 高斯分布输出的线性神经元</h4><p> 对于给定特征 $h$ ，线性输出神经元的输出为 $\bar y =W^Th+B$ 。线性输出层通常用来生产条件高斯分布的均值，此时求对数似然的最大值等价于最小化均方误差。(方差越小，分布越集中)。</p>
<p> 最大似然框架易于学习高斯分布的方差，使得高斯分布的协方差成为输入的一个函数。但是所有的输入其输出必须为正的有限的矩阵。</p>
<h4 id="3-2-2-Bernoulli输出分布的Sigmoid神经元"><a href="#3-2-2-Bernoulli输出分布的Sigmoid神经元" class="headerlink" title="3.2.2 Bernoulli输出分布的Sigmoid神经元"></a>3.2.2 Bernoulli输出分布的Sigmoid神经元</h4><p>许多二分类问题都可以使用Sigmoid神经元输出。最大似然估计的方法是定义一个在条件 $x$ 上的 $y$ 的Bernoulli分布。</p>
<p>定义Bernoulli 分布只需要一个参数（概率），神经网络只需预测概率 $P(y=1|x)$ 即可。假若使用线性激活神经元，网络输出的预测概率</p>
<script type="math/tex; mode=display">
  P(y=1\mid x) =max \lbrace 0,min\lbrace 1,w^Th+b\rbrace \rbrace</script><p>其中 $w^Th+b$ 很容易就不在区间 [0,1]上，为0.梯度为0，如何保证一直有较强的梯度呢？我们选用 Sigmoid 作为激活函数即可。Sigmoid激活神经元的输出为</p>
<script type="math/tex; mode=display">
  \bar y = \sigma (w^Th+b)</script><p>而 $\sigma (x) =\frac{1}{1+e^{-x}}$ 。我们可以把 $sigma$ 输出单元看做两个组成部分：1.第一步是一个线性层，计算输出 $z= w^Th+b$ 即可。2. 使用<strong>Sigmoid</strong>将输出 $z$ 转换为概率。</p>
<p>我们考虑如何使用上面提到的 $z$ 来定义 $y$ 上的概率分布。我们可以通过构建一个非正态概率分布 $\tilde p(y)$(概率和不等于1)来理解 <strong>Sigmoid</strong>。下面是公式推导过程:</p>
<p>先假设对数概率在y,z上是线性</p>
<script type="math/tex; mode=display">
  log\tilde p(y)=yz \quad \\</script><p>取指数得非正态概率</p>
<script type="math/tex; mode=display">
  \tilde p(y)  =exp(yz)</script><p>对每个概率除以总概率即可归一化（概率和为1）</p>
<script type="math/tex; mode=display">
  p(y) =\frac{exp(yz)}{\sum^1_{y'=0}exp(y'z)}</script><p>通过对 $z$ 的Sigmoid转换，获取一个Bernoulli分布</p>
<script type="math/tex; mode=display">
 p(y) =\sigma ((2y-1)z)</script><p>此方法来预测对数空间的概率可以用最大似然方法。因为最大似然的代价函数是 $-log p(y\mid x)$ ，这个对数log 移除了Sigmoid中的指数式exp。如果没有这一步，sigmoid的饱和性会阻止梯度学习。</p>
<p>使用sigmoid参数化的Bernoulli分布的最大似然学习过程的损失函数如下:</p>
<script type="math/tex; mode=display">
 J(\theta) = -log P(y\mid x) \\
 =-log \sigma ((2y-1)z) \\
 =\varsigma ((2y-1)z)</script><p>重写softplus损失函数中的部分项，我们可以看到当(1-2y)z，是负数且很大时，损失函数loss才会饱和。<strong>只有当模型基本准确时，才会出现饱和现象，其中 $y=1$ 时，z将会是很大的正数(very positive), $y=0$ 时，z是很小的负数(very negative)</strong>。如果使用其他损失函数，如均方差， $\sigma (z)$ 饱和时，损失函数就会饱和。</p>
<h4 id="3-2-3-多重Bernoulli分布Softmax输出神经元"><a href="#3-2-3-多重Bernoulli分布Softmax输出神经元" class="headerlink" title="3.2.3  多重Bernoulli分布Softmax输出神经元"></a>3.2.3  多重Bernoulli分布Softmax输出神经元</h4><p>softmax是sigmoid函数的普遍形式，用于多分类。用在输出层，作为一个分类器，表述为多个分类上的概率分布。 softmax函数的常见形式如下：</p>
<p>对于二分类变量，我们一般只产生单一输出（两个分类，输出某个分类的概率）。公式:</p>
<script type="math/tex; mode=display">
  \hat y=P(y=1\mid x) \quad \epsilon (0,1)</script><p>上式<strong>值区间为(0,1)</strong>，如果想让此值的<strong>log对数</strong>能在基于梯度的算法中应用，我们选择预测一个值 $z=log \tilde P(y=1 \mid x)$ (概率的log对数)。指数化和归一化，外加一个Sigmoid函数，即可得到一个Bernoulli分布。</p>
<p><strong>推广</strong>：假若我们将其推广到有<strong>n个离散值</strong>的场景中，输出向量 $\hat y$ ，其中 $\hat y$ 中每个分量 $\hat y_i=P(y=i \mid x)$ 取值区间为(0,1)，向量各分量之和为1。一个线性层预测的<strong>非正态对数概率</strong>为: $z=W^Th+b$ ，其中 $z_i=log \tilde P(y=i \mid x)$</p>
<script type="math/tex; mode=display">
  P(i) =\frac{exp(\theta_i^Tx)}{\sum _{i=1}^Kexp(\theta_i^Tx)}</script><p>通过softmax函数，可以使得 $P(i)$ 的范围在[0,1]之间。在回归和分类问题中，通常θ是待求参数，通过寻找使得 $P(i)$ 最大的 $\theta_i$作为最佳参数。回顾logistic函数形式.</p>
<p>对于logistic simoid，使用最大似然框架来训练softmax输出目标y时，exp函数效果很好。此时我们期望最大化 $log\quad P(y=i;z)=log\quad softmax(z)_i$ ，将softmax定义为指数形式是因为最大虽然估计中的log可以抵消。比如:</p>
<script type="math/tex; mode=display">
 log\quad softmax(z)_i=z_i-log\sum_iexp(z_j)</script><p>最大化此等式即最大化 $z_i$,后面一项 $log\sum _iexp(z_j)$ 不显著。最大化此等式时，会使得 $z_i$ 增大，而其他所有 $z$ 变小。</p>
<p><strong>通过近似，我们发现负的对数似然损失函数会惩罚大部分错误的激活函数。</strong></p>
<h3 id="3-2-4-softmax评价"><a href="#3-2-4-softmax评价" class="headerlink" title="3.2.4 softmax评价"></a>3.2.4 softmax评价</h3><p> 除了对数似然目标函数外，许多目标函数与softmax搭配效果不好，尤其是那些没有使用Log来抵消指数部分的。</p>
<p> 如果损失函数没有设计好，将导致网络很难学习，在某些区域网络的学习速度极慢。以神经网络的角度来看，softmax实际上是创建了一个竞争环境，总和为1，一方增加时会导致其他所有减少，极端情况下某一方为1，其他全为0.</p>
<h3 id="3-2-5-其他输出神经元"><a href="#3-2-5-其他输出神经元" class="headerlink" title="3.2.5  其他输出神经元"></a>3.2.5  其他输出神经元</h3><p> 最大似然框架指导如何根据输出层来设计一个好的损失函数。如果定义了一个条件分布 $P(y\mid x;\theta)$ 则建议使用 $logP(y\mid x;\theta)$ 作为损失函数。</p>
<p> 通常，我们可以将神经网络看做函数 $f(x;\theta)$ 的表述，函数输出不会直接预测 $y$ 的值，而是提供 $y$ 分布上的参数，此时的损失函数为 $-logp(y;w(x))$。</p>
<h2 id="3-3-隐藏神经元"><a href="#3-3-隐藏神经元" class="headerlink" title="3.3 隐藏神经元"></a>3.3 隐藏神经元</h2><p>隐藏神经元的设计并没有权威的指导原则。一般选择Relu作为隐藏神经元的激活函数。隐藏神经元的激活函数需要不断去尝试，无法预知使用哪种激活函数最好。</p>
<p>有些神经元的激活函数并不是严格可微的，比如Relu在 $z=0$ 时不可导（只是在部分点上）。而实际应用梯度方法依然表现很好，因为训练算法不会使损失函数达到局部最小，而只是大幅度减小。</p>
<p>神经元中的激活函数一般有左右导数，Relu中 $z=0$ 处左导数为0，右导数为1。但是程序实现时，一般只返回单侧导数，而且即便我们在求 $g(0)$ 的值，也只是逼近0的极小值，不是真的 $x=0$时的值。所以，实际上我们可以放心地无视激活函数中的不可微点。</p>
<h3 id="3-3-1-Relu神经元和其泛化"><a href="#3-3-1-Relu神经元和其泛化" class="headerlink" title="3.3.1 Relu神经元和其泛化"></a>3.3.1 Relu神经元和其泛化</h3><p>可以参考两篇博客</p>
<p><a href="http://www.cnblogs.com/neopenx/p/4453161.html" target="_blank" rel="noopener">Relu激活函数，全面</a></p>
<p><a href="http://blog.csdn.net/lg1259156776/article/details/48379321" target="_blank" rel="noopener">Relu，更学术，前沿</a></p>
<p> Relu线性神经元的激活函数为 $g(z)=max\lbrace 0,z \rbrace$ 。</p>
<p> Relu与线性神经元很像，唯一的不同是，Relu的输出取值范围有一半为0（非激活）。这使得其导数一直很大，梯度不仅大而且连续。激活的神经元的二阶导数几乎处处为0，一阶导数处处为1。</p>
<p> <strong>Relu一般用在外层映射，比如 $h=g(W^Tx+b)$ 。初始化Relu时，建议取较小的b</strong>，这会使得Relu对大部分输入都是激活的。</p>
<p> 缺点：激活函数输出为0时（ $z_i&lt;0$ ），Relu无法通过梯度学习，需要做一个泛化补充。<br> $z_i&lt;0$ 时使用一种基于非0的斜率 $\alpha _i$ 来作变换，Relu变为:</p>
<script type="math/tex; mode=display">
  h_i=g(z,\alpha)_i=max(0,z_i)+\alpha_imin(0,z_i)</script><p> 有如下三种方法:</p>
<ul>
<li><p>绝对值Relu:令 $\alpha_i=-1$，那么 $g(z)=\mid z\mid$ 。可以用在图像识别中寻找极性反转时特征不变的特征。</p>
</li>
<li><p>leaky Relu:令 $\alpha =0.001$ 取一个较小值。</p>
</li>
<li><p>参数化Relu:将 $\alpha$ 作为一个可以学习的参数。</p>
</li>
</ul>
<h2 id="3-4-其他隐藏神经元"><a href="#3-4-其他隐藏神经元" class="headerlink" title="3.4 其他隐藏神经元"></a>3.4 其他隐藏神经元</h2><p> 许多未公开发表的激活函数其实与现在流行的激活函数一样有效。比如 $h=cos(Wx+b)$ 在 <strong>MNIST</strong>数据集上错误率只有1%，这完全可以媲美于使用多种卷积激活函数。通常情况下只有当新的隐藏神经元能够取得非常大提升时才会被公开发表。</p>
<p> 神经网络中某些层可以使用纯线性激活函数，有助于减少网络参数。softmax一般用在输出层，但是也可以用于隐藏层。</p>
<p> 其他可使用的隐藏层激活函数：</p>
<ul>
<li><p>Radial basis function（RBF）：$h<em>i =exp(-\frac{1}{\sigma _i^2\mid \mid W</em>{:,i}-x\mid \mid ^2})$ 。只有当x接近于模板 $W_{:,i}$ 时函数才会被激活，由于其对大多数的x都是饱和状态，所以很难优化。</p>
</li>
<li><p>softplus ：$g(a)=\varsigma (a)=log(1+e^a)$ 是Relu的平滑版本，通常不鼓励使用softplus。</p>
</li>
<li><p>Hard tanh：形状与tanh和Relu很像，但是与Relu不同的是，它是有界的。 $g(a)=max(-1,min(1,a))$ 。</p>
</li>
</ul>
<h2 id="3-5-网络架构设计"><a href="#3-5-网络架构设计" class="headerlink" title="3.5 网络架构设计"></a>3.5 网络架构设计</h2><p>网络架构设计关乎，需要多少个神经元，神经元之间是如何连接等问题。网络由一层层的神经元组成，每层由一组神经元构成。假设我们有第一层网络</p>
<script type="math/tex; mode=display">
  h^{(1)}=g^{(1)}(W^{(1)T}x+b^{(1)})</script><p>其中 $x$ 为网络输入， $g^{(1)}$ 为第一层的激活函数。网络的第二层如下</p>
<script type="math/tex; mode=display">
  h^{(2)}=g^{(2)}(W^{(2)T}h^{(1)}+b^{(2)})</script><p>其中 $h^{1}$ 为第一层的输出。</p>
<p>在这种链式结构中，<strong>网络架构需要考虑的是网络的深度和宽度</strong>,一般，更深的网络需要更少神经元，更少参数，但是难以优化。</p>
<h3 id="3-5-1-通用近似属性和深度"><a href="#3-5-1-通用近似属性和深度" class="headerlink" title="3.5.1 通用近似属性和深度"></a>3.5.1 通用近似属性和深度</h3><p>通用近似定理表明，使用 至少一个隐藏层和线性输出层，以及激活函数的神经网络可以拟合任意Borel测度的函数。</p>
<p>多层前馈网络能表述任意函数，但是不能保证我们的训练算法可以学习此函数。有两个如下原因：</p>
<ul>
<li>用来训练的优化函数，可能无法找到拟合目标函数的正确参数。</li>
<li>训练算法有可能因为过拟合选择错误的函数。</li>
</ul>
<p>给定任意函数，必然存在可以拟合的神经网络，但是目前并没有一种通用的方法来校验特定训练样本和函数来泛化不在训练集中的数据点。</p>
<p>广义近似定律表明，存在一个大网络可以拟合任意准确率的预测函数，但是没有提及网络究竟会有多大。最新论文表明，最坏情况下，有隐藏层指数级数量的复杂度。比如一个向量 $V\epsilon \lbrace 0,1 \rbrace$ 上的可能的2分类函数有 $2^{2^n}$ 个，存储这些函数需要 $2^n$ 字节。</p>
<p>在选择机器学习算法时，其实我们是在表述算法所需要学习哪种先验知识。在选择深度学习模型时，其实是在将一个函数表述为几个简单的函数组合，此时学习问题变为挖掘变量间隐藏的关联参数问题，而这可进一步分解为更简单的变量之间的参数问题。</p>
<h3 id="3-5-2-其他架构考虑"><a href="#3-5-2-其他架构考虑" class="headerlink" title="3.5.2  其他架构考虑"></a>3.5.2  其他架构考虑</h3><p> 在设计网络时，需要明白不同的网络有不同用途，比如CNN用于图像处理，RNN用于做序列处理。网络也不必完全链式，从第 $i$ 层网络直接连接到第 $i+3$ 层这样的跳跃，使得梯度更容易从输出反馈到输入。</p>
<p> 另外一个需要考虑的问题是，网络之间是如何连接的。默认是一个以矩阵 $w$ 的全连接，许多网络只是部分连接。比如CNN中用于图像处理，此时参数更少，计算速度快，这与具体问题相关。</p>
<p> <img src="/images/blog/dfn1.jpg" alt=""></p>
<p> 上图显示的是一个参数数量与测试准确率之间的关系。图中显示，<strong>增加CNN的参数而不增加深度并不能提升性能</strong> 。浅层网络在参数达到2000万时过拟合而深层网络在参数达到6000万时性能更强。表明，<strong>网络应该由更多更简单的函数组成。</strong></p>
<h2 id="4-反向传播算法"><a href="#4-反向传播算法" class="headerlink" title="4 反向传播算法"></a>4 反向传播算法</h2><p>后向传播算法并不是指整个网络，<strong>它只是计算梯度的一个方法，有其他方法，如随机梯度下降，它还可以用于对任意函数求导</strong>。</p>
<p>以下解释来自知乎 <a href="https://www.zhihu.com/question/27239198?rf=24827633" target="_blank" rel="noopener">反向传播算法</a></p>
<p>假若我们有如下简单网络：</p>
<p><img src="/images/blog/dfn2.png" alt="简单神经网络"></p>
<p>对应的函数表达式如下:</p>
<script type="math/tex; mode=display">
 a_1^{(2)}=f(W_{11}^{(1)}x_1+W_{12}^{(1)}x_2+W_{13}^{(1)}x_3+b_1^{(1)}) \\

 a_2^{(2)}=f(W_{21}^{(1)}x_1+W_{22}^{(1)}x_2+W_{23}^{(1)}x_3+b_2^{(1)}) \\

 a_3^{(2)}=f(W_{31}^{(1)}x_1+W_{32}^{(1)}x_2+W_{33}^{(1)}x_3+b_3^{(1)}) \\

 h_{W,b}(x)=a_1^{(3)}=f(W^{(2)}_{11}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+W_{13}^{(2)}a^{(2)}_3+b_1^{(2)})</script><p>上面式中的 $W_{ij}$ 就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合 $y=k*x+b$ 中的待求参数k和b。</p>
<p>和直线拟合一样，深度学习的训练也有一个<strong>目标函数</strong>，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用 <strong>成本函数（cost function)</strong>，然后，训练目标就是通过调整每一个权值 $W<em>{ij}$ 来使得cost达到最小。cost函数也可以看成是由所有待求权值 $W</em>{ij}$ 为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。</p>
<p>梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？</p>
<p>我们把cost函数定义为 $H(W<em>{11},W</em>{12},W<em>{13},..W</em>{ij},..,W_{mn})$ ,那么它的梯度向量就等于</p>
<script type="math/tex; mode=display">
   \triangledown H=\frac{\partial H}{\partial W_{11}}e_{11}+...\frac{\partial H}{\partial W_{mn}}e_{mn}</script><p>此，我们需求出cost函数H对每一个权值 $W_{ij}$ 的偏导数。而BP算法正是用来求解这种多层复合函数的所有变量的偏导数的利器。我们以求 $e=(a+b)*(b+1)$ 的偏导为例。它的复合关系画出图可以表示如下：</p>
<p><img src="/images/blog/dnn3.png" alt="简单神经网络"></p>
<p>在图中，引入了中间变量c,d。</p>
<p>为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。</p>
<p><img src="/images/blog/dnn4.png" alt="简单神经网络"></p>
<p>利用链式法则我们知道:</p>
<script type="math/tex; mode=display">
  \frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\frac{\partial c}{\partial a} \\

  \frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\frac{\partial d}{\partial b}</script><p>链式法则在上图中的意义是什么呢？其实不难发现， $\frac{\partial e}{\partial a}$ 的值等于从 $a$ 到 $e$ 的路径上的偏导值的乘积，而 $\frac{\partial e}{\partial b}$ 的值等于从 $b$ 到 $e$ 的路径 $1(b-c-e)$ 上的偏导值的乘积加上路径 $2(b-d-e)$ 上的偏导值的乘积。也就是说，对于上层节点p和下层节点q，要求得 $\frac{\partial p}{\partial q}$ ，需要找到从 $q$ 节点到 $p$ 节点的所有路径，并且对每条路径，求得该路径上的所有偏导数之乘积，然后将所有路径的 “乘积” 累加起来才能得到 $\frac{\partial p}{\partial q}$ 的值。</p>
<p>大家也许已经注意到，这样做是十分冗余的， <strong>因为很多路径被重复访问</strong>了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。</p>
<p>同样是 <strong>利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的</strong>。</p>
<p>从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有 <strong>“堆放”</strong> 的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。</p>
<p>以上图为例，节点c接受e发送的1<em>2并堆放起来，节点d接受e发送的1</em>3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2<em>1并对堆放起来，节点c向b发送2</em>1并堆放起来，节点d向b发送3<em>1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2</em>1+3*1=5, 即顶点e对b的偏导数为5.</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习基础：SVD奇异值分解及其意义【转】</title>
    <url>/2016/10/25/SVD-decomponent/</url>
    <content><![CDATA[<p>英文原文:<a href="http://blog.csdn.net/redline2005/article/details/24099377" target="_blank" rel="noopener">英文原文</a><br>中文转自:<a href="http://blog.sciencenet.cn/blog-696950-699432.html" target="_blank" rel="noopener">中文原文</a></p>
<h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p>SVD实际上是数学专业内容，但它现在已经渗入到不同的领域中。SVD的过程不是很好理解，因为它不够直观，但它对矩阵分解的效果却非常好。比如，Netflix（一个提供在线电影租赁的公司）曾经就悬赏100万美金，如果谁能提高它的电影推荐系统评分预测准确率提高10%的话。令人惊讶的是，这个目标充满了挑战，来自世界各地的团队运用了各种不同的技术。最终的获胜队伍”BellKor’s Pragmatic Chaos”采用的核心算法就是基于SVD。<br>SVD提供了一种非常便捷的矩阵分解方式，能够发现数据中十分有意思的潜在模式。在这篇文章中，我们将会提供对SVD几何上的理解和一些简单的应用实例。</p>
<h3 id="1-1-线性变换的几何意义"><a href="#1-1-线性变换的几何意义" class="headerlink" title="1.1 线性变换的几何意义"></a>1.1 线性变换的几何意义</h3><p>  <strong>奇异值分解应该就是把一个线性变换分解成两个线性变换，一个线性变换代表旋转，另一个代表拉伸。</strong></p>
<p> 让我们来看一些简单的线性变换例子，以 2 X 2 的线性变换矩阵为例，首先来看一个较为特殊的，对角矩阵：</p>
<script type="math/tex; mode=display">
 M=
 \begin{bmatrix}
  3\quad 0\\
  0\quad 1 \\
 \end{bmatrix}</script><p> 从几何上讲，M 是将二维平面上的点(x,y)经过线性变换到另外一个点的变换矩阵，如下图所示</p>
<script type="math/tex; mode=display">
   \begin{bmatrix}
    3\quad 0 \\
    0\quad 1\\
   \end{bmatrix}
   \begin{bmatrix}
    x\\
    y\\
   \end{bmatrix} =
   \begin{bmatrix}
   3x\\
   y\\
   \end{bmatrix}</script><p>变换的效果如下图所示，变换后的平面仅仅是沿 X 水平方面进行了拉伸3倍，垂直方向是并没有发生变化。</p>
<p><img src="/images/blog/svd1.jpg" alt="水平变换效果"></p>
<p>现在看下矩阵：</p>
<script type="math/tex; mode=display">
M=
\begin{bmatrix}
 2\quad 1\\
 1\quad 2 \\
\end{bmatrix}</script><p>这个矩阵产生的变换效果如下图所示:</p>
<p><img src="/images/blog/svd2.jpg" alt="水平变换效果"></p>
<p>  这种变换效果看起来非常的奇怪，在实际环境下很难描述出来变换的规律 ( 这里应该是指无法清晰辨识出旋转的角度，拉伸的倍数之类的信息)。还是基于上面的对称矩阵，假设我们把左边的平面旋转45度角，然后再进行矩阵M 的线性变换，效果如下图所示：</p>
<p>  <img src="/images/blog/svd3.jpg" alt="水平变换效果"></p>
<p>看起来是不是有点熟悉？ 对的，经过 M 线性变换后，跟前面的对角矩阵的功能是相同的，都是将网格沿着一个方向拉伸了3倍。<br>这里的 M 是一个特例，因为它是对称的。非特殊的就是我们在实际应用中经常遇见一些 非对称的，非方阵的矩阵。如上图所示，如果我们有一个 2 X 2 的对称矩阵M 的话，我们先将网格平面旋转一定的角度，M 的变换效果就是在两个维度上进行拉伸变换了。</p>
<p>用更加数学的方式进行表示的话，给定一个对称矩阵 M ，我们可以找到一些相互正交 $V_i$ ，满足 $MV_i$ 就是沿着 $V_i$ 方向的拉伸变换，公式如下：</p>
<script type="math/tex; mode=display">
  Mv_i=\lambda _iv_i</script><p>这里的 $\lambda _i$是拉伸尺度(scalar)。从几何上看，M 对向量 $V_i$ 进行了拉伸，映射变换。$V_i$ 称作矩阵 M 的特征向量(eigenvector)，$\lambda _i$ 称作为矩阵M 特征值(eigenvalue)。这里有一个非常重要的定理，对称矩阵M 的特征向量是相互正交的。</p>
<p>如果我们用这些特征向量对网格平面进行线性变换的话，再通过 M 矩阵对网格平面进行线性换的效果跟对M 矩阵的特征向量进行线性变换的效果是一样的。<br>对于更为普通的矩阵而言，我们该怎么做才能让一个原来就是相互垂直的网格平面(orthogonal grid), 线性变换成另外一个网格平面同样垂直呢？PS：这里的垂直如图所示，就是两根交错的线条是垂直的。</p>
<script type="math/tex; mode=display">
M=
\begin{bmatrix}
 1\quad 1\\
 0\quad 1 \\
\end{bmatrix}</script><p>经过上述矩阵变换以后的效果如图:</p>
<p>  <img src="/images/blog/svd4.jpg" alt="水平变换效果"></p>
<p>  从图中可以看出，并没有达到我们想要的效果。我们把网格平面旋转 30 度角的话，然后再进行同样的线性变换以后的效果，如下图所示</p>
<p>  <img src="/images/blog/svd5.jpg" alt="水平变换效果"></p>
<p>  让我们来看下网格平面旋转60度角的时候的效果。</p>
<p>  <img src="/images/blog/svd6.jpg" alt="水平变换效果"></p>
<p>  嗯嗯，这个看起来挺不错的样子。如果在精确一点的话，应该把网格平面旋转 58.28 度才能达到理想的效果。</p>
<p>  <img src="/images/blog/svd7.jpg" alt="水平变换效果"></p>
<h2 id="二-几何意义"><a href="#二-几何意义" class="headerlink" title="二 几何意义"></a>二 几何意义</h2><p>该部分是从几何层面上去理解二维的SVD：对于任意的 2 x 2 矩阵，通过SVD可以将一个相互垂直的网格(orthogonal grid)变换到另外一个相互垂直的网格。<br>我们可以通过向量的方式来描述这个事实: 首先，选择两个相互正交的单位向量 $v_1$和 $v_2$, 向量 $Mv_1$ 和 $Mv_2$ 正交。</p>
<p>  <img src="/images/blog/svd8.jpg" alt="水平变换效果"></p>
<p>$u_1$ 和 $u_2$ 分别表示 $Mv_1$ 和 $Mv_2$ 的单位向量，$σ_1 <em> u_1 =  Mv_1$ 和 $σ_2 </em> u_2 =  Mv_2$ 。$σ_1$ 和 $σ_2$分别表示这不同方向向量上的模，也称作为矩阵M 的奇异值。</p>
<p>  <img src="/images/blog/svd9.jpg" alt="水平变换效果"></p>
<p>这样我们就有了如下关系式：</p>
<script type="math/tex; mode=display">
Mv_1 = σ_1u_1 \\
Mv_2 = σ_2u_2</script><p>我们现在可以简单描述下经过 M 线性变换后的向量 x 的表达形式。由于向量 $v_1$ 和 $v_2$ 是正交的单位向量，我们可以得到如下式子</p>
<script type="math/tex; mode=display">
  x = (v_1x)v_1 + (v_2x)v_2</script><p>这就意味着：</p>
<script type="math/tex; mode=display">
    Mx = (v_1x)Mv_1 + (v_2x)Mv_2    \\
     Mx = (v_1x) σ_1u_1 + (v_2x) σ_2u_2</script><p>向量内积可以用向量的转置来表示，如下所示:</p>
<script type="math/tex; mode=display">
   V. x= V^Tx</script><p>最终的式子为:</p>
<script type="math/tex; mode=display">
Mx = u_1σ_1 v_1^Tx + u_2σ_2 v_2^Tx   \\
M =u_1σ_1 v_1^T + u_2σ_2 v_2^T</script><p>上述的式子经常表示成</p>
<script type="math/tex; mode=display">
  M = U\sum V^T</script><p>u 矩阵的列向量分别是 $u_1,u_2，\sum $是一个对角矩阵，对角元素分别是对应的 $σ_1$ 和 $σ_2$ ，V矩阵的列向量分别是 $v_1,v_2$ 。上角标T 表示矩阵 V 的转置。</p>
<p>这就表明任意的矩阵 M 是可以分解成三个矩阵。V表示了原始域的标准正交基，u 表示经过M 变换后的co-domain的标准正交基，Σ表示了V 中的向量与u中相对应向量之间的关系。(V describes an orthonormal basis in the domain, and U describes an orthonormal basis in the co-domain, and Σ describes how much the vectors in V are stretched to give the vectors in U.)</p>
<h2 id="三-奇异值分解的物理意义"><a href="#三-奇异值分解的物理意义" class="headerlink" title="三 奇异值分解的物理意义"></a>三 奇异值分解的物理意义</h2><p>  此部分转载自知乎 <a href="https://www.zhihu.com/question/22237507/answer/28007137" target="_blank" rel="noopener">奇异值分解物理意义，郑宁的回答</a></p>
<p>  矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称SVD分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。下面先尽量避开严格的数学符号推导，直观的从一张图片出发，让我们来看看奇异值代表什么意义。</p>
<p>这是女神上野树里（Ueno Juri）的一张照片，像素为高度450*宽度333</p>
<p>  <img src="/images/blog/svd20.jpg" alt=""></p>
<p>  我们都知道，图片实际上对应着一个矩阵，矩阵的大小就是像素大小，比如这张图对应的矩阵阶数就是450*333，矩阵上每个元素的数值对应着像素值。我们记这个像素矩阵为 $A$ 。</p>
<p>  现在我们对矩阵 $A$ 进行奇异值分解。直观上，奇异值分解将矩阵分解成若干个秩一矩阵之和，用公式表示就是：</p>
<script type="math/tex; mode=display">
    A =\sigma_1\mu_1v_1^T+\sigma_2\mu_2v_2^T+...+\sigma_r\mu_rv_r^T</script><p>  其中等式右边每一项前的系数 $\sigma$就是奇异值，u和v分别表示列向量，秩一矩阵的意思是矩阵秩为1。注意到每一项 $\mu v$ 都是秩为1的矩阵。我们假定奇异值满足:</p>
<script type="math/tex; mode=display">
    \sigma_1\ge\sigma_2\ge....\sigma_r\ge0</script><p>（奇异值大于0是个重要的性质，但这里先别在意），如果不满足的话重新排列顺序即可，这无非是编号顺序的问题。</p>
<p>既然奇异值有从大到小排列的顺序，我们自然要问，如果只保留大的奇异值，舍去较小的奇异值，这样(1)式里的等式自然不再成立，那会得到怎样的矩阵——也就是图像？</p>
<p>令 $A_1=\sigma_1 u_1v_1^{\rm T}$ ，这只保留(1)中等式右边第一项，然后作图</p>
<p>  <img src="/images/blog/svd21.jpg" alt=""></p>
<p>结果就是完全看不清是啥……我们试着多增加几项进来:</p>
<script type="math/tex; mode=display">
  A_5 =\sigma_1\mu_1v_1^T+\sigma_2\mu_2v_2^T+...+\sigma_5\mu_5v_5^T</script><p>再作图</p>
<p><img src="/images/blog/svd22.jpg" alt=""></p>
<p>隐约可以辨别这是短发伽椰子的脸……但还是很模糊，毕竟我们只取了5个奇异值而已。下面我们取20个奇异值试试，也就是(1)式等式右边取前20项构成 $A_{20}$</p>
<p>虽然还有些马赛克般的模糊，但我们总算能辨别出这是Juri酱的脸。当我们取到(1)式等式右边前50项时：</p>
<p><img src="/images/blog/svd24.jpg" alt=""></p>
<p>我们得到和原图差别不大的图像。也就是说当k从1不断增大时，A_k不断的逼近A。让我们回到公式</p>
<script type="math/tex; mode=display">
     A =\sigma_1\mu_1v_1^T+\sigma_2\mu_2v_2^T+...+\sigma_r\mu_rv_r^T</script><p>矩阵表示一个450<em>333的矩阵，需要保存个元素的值。等式右边和分别是450</em>1和333*1的向量，每一项有个元素。如果我们要存储很多高清的图片，而又受限于存储空间的限制，在尽可能保证图像可被识别的精度的前提下，我们可以保留奇异值较大的若干项，舍去奇异值较小的项即可。例如在上面的例子中，如果我们只保留奇异值分解的前50项，则需要存储的元素为，和存储原始矩阵相比，存储量仅为后者的26%。</p>
<p><strong>奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵A都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重。</strong></p>
<p>在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为0时，就可以去除图片中的噪声。如下是一张25*15的图像（本例来源于[1]）</p>
<p><img src="/images/blog/svd25.jpg" alt=""></p>
<p>但往往我们只能得到如下带有噪声的图像（和无噪声图像相比，下图的部分白格子中带有灰色）：</p>
<p><img src="/images/blog/svd26.jpg" alt=""></p>
<p>通过奇异值分解，我们发现矩阵的奇异值从大到小分别为：14.15，4.67，3.00，0.21，……，0.05。除了前3个奇异值较大以外，其余奇异值相比之下都很小。强行令这些小奇异值为0，然后只用前3个奇异值构造新的矩阵，得到</p>
<p><img src="/images/blog/svd27.jpg" alt=""></p>
<p>可以明显看出噪声减少了（白格子上灰白相间的图案减少了）。</p>
<p>奇异值分解还广泛的用于主成分分析（Principle Component Analysis，简称PCA）和推荐系统（如Netflex的电影推荐系统）等。在这些应用领域，奇异值也有相应的意义</p>
<h2 id="四-如何获得奇异值分解"><a href="#四-如何获得奇异值分解" class="headerlink" title="四 如何获得奇异值分解"></a>四 如何获得奇异值分解</h2><p>事实上我们可以找到任何矩阵的奇异值分解，那么我们是如何做到的呢？假设在原始域中有一个单位圆，如下图所示。经过 M 矩阵变换以后在co-domain中单位圆会变成一个椭圆，它的长轴(Mv1)和短轴(Mv2)分别对应转换后的两个标准正交向量，也是在椭圆范围内最长和最短的两个向量。</p>
<p>  <img src="/images/blog/svd10.jpg" alt="水平变换效果"></p>
<p>换句话说，定义在单位圆上的函数|Mx|分别在 $v_1$ 和 $v_2$ 方向上取得最大和最小值。这样我们就把寻找矩阵的奇异值分解过程缩小到了优化函数|Mx|上了。结果发现（具体的推到过程这里就不详细介绍了）这个函数取得最优值的向量分别是矩阵 MT M 的特征向量。由于MTM是对称矩阵，因此不同特征值对应的特征向量都是互相正交的，我们用 $v_i$ 表示MTM的所有特征向量。奇异值 $σ_i = |Mv_i| $， 向量 $u_i$ 为 $Mv_i$ 方向上的单位向量。但为什么 $u_i$ 也是正交的呢？</p>
<p>推倒如下：</p>
<p>$σ_i$ 和 $σ_j$ 分别是不同两个奇异值</p>
<script type="math/tex; mode=display">
Mv_i = σ_iu_i  \\
Mv_j = σ_ju_j.</script><p>我们先看下MviMvj，并假设它们分别对应的奇异值都不为零。一方面这个表达的值为0，推到如下</p>
<script type="math/tex; mode=display">
Mv_i Mv_j = v_i^TM^T Mv_j = v_i M^TMv_j = λ_jv_i v_j = 0</script><p>另一方面，我们有</p>
<script type="math/tex; mode=display">
Mv_i Mv_j = σ_iσ_j u_i u_j = 0</script><p>因此，$u_i$ 和 $u_j$ 是正交的。但实际上，这并非是求解奇异值的方法，效率会非常低。这里也主要不是讨论如何求解奇异值，为了演示方便，采用的都是二阶矩阵。</p>
<h2 id="五-应用实例"><a href="#五-应用实例" class="headerlink" title="五 应用实例"></a>五 应用实例</h2><h3 id="5-1-应用实例一"><a href="#5-1-应用实例一" class="headerlink" title="5.1 应用实例一"></a>5.1 应用实例一</h3><script type="math/tex; mode=display">
M =
  \begin{bmatrix}
   1\quad 1 \\
   2\quad 2 \\
  \end{bmatrix}</script><p>经过这个矩阵变换后的效果如下图所示</p>
<p><img src="/images/blog/svd11.jpg" alt="水平变换效果"></p>
<p>在这个例子中，第二个奇异值为 0，因此经过变换后只有一个方向上有表达</p>
<script type="math/tex; mode=display">
  M =u_1σ_1 v_1^T</script><p>换句话说，如果某些奇异值非常小的话，其相对应的几项就可以不同出现在矩阵 M 的分解式中。因此，我们可以看到矩阵 M 的秩的大小等于非零奇异值的个数。</p>
<h3 id="5-2-应用实例二"><a href="#5-2-应用实例二" class="headerlink" title="5.2  应用实例二"></a>5.2  应用实例二</h3><p> 我们来看一个奇异值分解在数据表达上的应用。假设我们有如下的一张 15 x 25 的图像数据。</p>
<p><img src="/images/blog/svd12.jpg" alt="水平变换效果"></p>
<p>如图所示，该图像主要由下面三部分构成。</p>
<p><img src="/images/blog/svd13.jpg" alt="水平变换效果"></p>
<p>我们将图像表示成 15 x 25 的矩阵，矩阵的元素对应着图像的不同像素，如果像素是白色的话，就取 1，黑色的就取 0. 我们得到了一个具有375个元素的矩阵，如下图所示</p>
<p><img src="/images/blog/svd14.jpg" alt="水平变换效果"></p>
<p>如果我们对矩阵M进行奇异值分解以后，得到奇异值分别是</p>
<script type="math/tex; mode=display">
σ_1 = 14.72 \\
σ_2 = 5.22  \\
σ_3 = 3.31 \\</script><p>矩阵M就可以表示成</p>
<script type="math/tex; mode=display">
M=u_1σ_1 v_1^T + u_2σ_2 v_2^T + u_3σ_3 v_3^T</script><p>$v_i$ 具有15个元素，$u_i$ 具有25个元素，$σ_i$ 对应不同的奇异值。如上图所示，我们就可以用123个元素来表示具有375个元素的图像数据了。</p>
<h3 id="5-3-应用实例三：减噪-noise-reduction"><a href="#5-3-应用实例三：减噪-noise-reduction" class="headerlink" title="5.3 应用实例三：减噪(noise reduction)"></a>5.3 应用实例三：减噪(noise reduction)</h3><p> 前面的例子的奇异值都不为零，或者都还算比较大，下面我们来探索一下拥有零或者非常小的奇异值的情况。通常来讲，大的奇异值对应的部分会包含更多的信息。比如，我们有一张扫描的，带有噪声的图像，如下图所示</p>
<p> <img src="/images/blog/svd15.jpg" alt="水平变换效果"></p>
<p> 我们采用跟实例二相同的处理方式处理该扫描图像。得到图像矩阵的奇异值：</p>
<script type="math/tex; mode=display">
σ_1 = 14.15   \\
σ_2 = 4.67     \\
σ_3 = 3.00   \\
σ_4 = 0.21   \\
σ_5 = 0.19  \\
...
σ_15 = 0.05 \\</script><p>很明显，前面三个奇异值远远比后面的奇异值要大，这样矩阵 M 的分解方式就可以如下：</p>
<script type="math/tex; mode=display">
  M \approx  u_1σ_1 v_1^T + u_2σ_2 v_2^T + u_3σ_3 v_3^T</script><p>经过奇异值分解后，我们得到了一张降噪后的图像。</p>
<p><img src="/images/blog/svd16.jpg" alt="水平变换效果"></p>
<h3 id="5-4-应用实例四：数据分析-data-analysis"><a href="#5-4-应用实例四：数据分析-data-analysis" class="headerlink" title="5.4 应用实例四：数据分析(data analysis)"></a>5.4 应用实例四：数据分析(data analysis)</h3><p>我们搜集的数据中总是存在噪声：无论采用的设备多精密，方法有多好，总是会存在一些误差的。如果你们还记得上文提到的，大的奇异值对应了矩阵中的主要信息的话，运用SVD进行数据分析，提取其中的主要部分的话，还是相当合理的。<br>作为例子，假如我们搜集的数据如下所示：</p>
<p><img src="/images/blog/svd17.jpg" alt="水平变换效果"></p>
<p>我们将数据用矩阵的形式表示：</p>
<p><img src="/images/blog/svd18.jpg" alt="水平变换效果"></p>
<p>经过奇异值分解后，得到</p>
<script type="math/tex; mode=display">
σ_1 = 6.04  \\
σ_2 = 0.22 \\</script><p>由于第一个奇异值远比第二个要大，数据中有包含一些噪声，第二个奇异值在原始矩阵分解相对应的部分可以忽略。经过SVD分解后，保留了主要样本点如图所示</p>
<p><img src="/images/blog/svd19.jpg" alt="水平变换效果"></p>
<p>就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：训练模型的优化</title>
    <url>/2016/10/19/optimization-deep-models/</url>
    <content><![CDATA[<h2 id="一-概述"><a href="#一-概述" class="headerlink" title="一 概述"></a>一 概述</h2><p>本文主要集中探讨优化方法中的一种特例：找到使得神经网络损失大幅度减小的参数 $\theta$ ,这一般会包含了在整个训练集上的性能评估方法和一些额外的正则项。<br>本文所描述的机器学习算法的优化与纯粹的优化略有不同。接下来我们将阐述使得神经网络变得尤其复杂的挑战。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：正则化</title>
    <url>/2016/10/09/regularization-deeplearning/</url>
    <content><![CDATA[<h2 id="7-1-概念"><a href="#7-1-概念" class="headerlink" title="7.1 概念"></a>7.1 概念</h2><p>  深度学习中用以减小测试误差，但可能会增加训练误差的策略称为正则化。</p>
<p>  <strong>限制</strong>：有些正则化策略是<strong>机器学习模型</strong>上添加限制。有些在<strong>模型参数</strong>上，有些在<strong>目标函数</strong>上添加额外项。这些限制或惩罚部分被设计来对特定先验知识编码的，其余的则是为了提高模型泛化能力。</p>
<p>  深度学习中大部分正则化策略都是基于估计正则化，而估计正则化则是通过增加偏置来减少方差。 一个估计的正则化的目标是在大幅度减小方差的同时，尽可能小的带来偏置的增加。我们在讨论泛化和过拟合问题时会遇到以下三种情形:</p>
<p>  <img src="/images/blog/regular1.png" alt="拟合情况"></p>
  <p align="Center">图7.1</p>

<ul>
<li><strong>欠拟合</strong>:实际的正例没有完全被包含在模型预测域。</li>
<li><strong>绝佳</strong>：完全匹配了数据生成过程.</li>
<li><p><strong>过拟合</strong>：模型的预测域包含了全部的正例，同时也包含了负例。</p>
<p>正规化的目标就是将模型的<strong>过拟合</strong>情形改善至<strong>绝佳</strong>情形。</p>
<p>现实情况中，即便是极端复杂的模型也没法完全拟合目标函数，因为大部分情况，我们并不知道目标函数具体是如何映射的。<br>这表明设计一个模型的复杂度极高(模型大小，参数等)。而在近些年的实际实验中，我们发现比较好的模型都是正规化处理过后的大模型。</p>
</li>
</ul>
<h2 id="7-2-参数规范惩罚"><a href="#7-2-参数规范惩罚" class="headerlink" title="7.2 参数规范惩罚"></a>7.2 参数规范惩罚</h2><p> 目前许多正规化方法，如神经网络、线性回归、logistic回归通过在目标函数$J$上加一个参数规范惩罚项 $\Omega(\theta)$ 公式如下:</p>
<script type="math/tex; mode=display">
    \bar{J}(\theta;X,y) = J(\theta;X,y)+\alpha\Omega(\theta)\\\tag {7.1}

 其中 \alpha\epsilon [0,\infty)</script><p>其中，更大的 $\alpha$ 对应更强的正规化处理。</p>
<p> 在神经网络中，使用参数规范惩罚，只是对每一层映射转换的权重，不对偏置使用。这是因为权重决定了两个变量如何交互，而偏置只作用于单一变量。同时正规化偏置容易引入欠拟合。</p>
<p><strong>预定义</strong></p>
<ul>
<li>向量$w$代表所有需要被规范惩罚的权重</li>
<li><p>向量 $\theta$代表所有参数，包括$w$和其他非正规化的参数</p>
<p>尽管每一层使用独立的 $\alpha$参数的惩罚机制效果可能会更好，但是由于计算量太大。实际中所有层使用相同的权重衰减。</p>
</li>
</ul>
<p><strong>如何理解正则化</strong></p>
<p>从数学角度来看，成本函数增加了一个正则项 $\Omega(\theta)$ 后，成本函数不再唯一地与预测值与真实值的差距决定，还和参数 $\theta$ 的大小有关。有了这个限制之后，要实现成本函数最小的目的，$\theta$ 就不能随便取值了，比如某个比较大的 $\theta$ 值可能会让预测值与真实值的差距 $\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2$ 值很小，但会导致 $\theta_j^2$ 很大，最终的结果是成本函数太大。这样，通过调节参数 $\lambda$ 就可以控制正则项的权重。从而避免算法过拟合</p>
<h3 id="7-2-1-L-2-参数正规化"><a href="#7-2-1-L-2-参数正规化" class="headerlink" title="7.2.1 $L^2$ 参数正规化"></a>7.2.1 $L^2$ 参数正规化</h3><p>最简单最常见的正规惩罚莫过于$L^2$，有时候称为<em>权重衰减</em>，同时也称为<em>岭回归</em>或者<em>吉洪诺夫正规</em>。它是直接在目标函数后面添加一个正规项 $\Omega(\theta)=\frac{1}{2}|w|^2_2$</p>
<p>L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项 $|W|_2$最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。</p>
<p>回头看式子 $(7.1)$，假设没有偏置参数，因此 $\theta$参数就是$w$,模型目标函数如下:</p>
<script type="math/tex; mode=display">
    \bar J(w;X,y) =\frac{\alpha}{2}w^Tw+J(w;X,y) \tag{7.2}</script><p>对应的参数梯度如下:</p>
<script type="math/tex; mode=display">
     \bigtriangledown\bar J_w(w;X,y)=\alpha w+\bigtriangledown_wJ(w;X,y)</script><p>使用梯度更新权重时:</p>
<script type="math/tex; mode=display">
    w\leftarrow w-\epsilon(\alpha w+\bigtriangledown_wJ(w;X,y))</script><p>   重写为:</p>
<script type="math/tex; mode=display">
    w\leftarrow (1-\epsilon\alpha)w -\epsilon\bigtriangledown_wJ(w;X,y))</script><p>可以看到新增权重衰减项最终反映出，它通过对每一步的权重向量乘以一个常数因子修改了学习规则。由于 $\epsilon$ ，$\alpha$都是正值，所以它实际是减小了$w$。这就是它被称为<strong>权重衰减</strong>的原因。</p>
<h4 id="7-2-1-1-L-2-如何实现参数调整"><a href="#7-2-1-1-L-2-如何实现参数调整" class="headerlink" title="7.2.1.1 $L^2$如何实现参数调整"></a>7.2.1.1 $L^2$如何实现参数调整</h4><p>  假设没有进行正则化处理的模型的损失函数在权重 $w$ 更新为 $w^ <em>$ 时最小。我们通过在 $w^</em> $ 附近进行二次近似来简化分析（前提是损失函数是二次及以上）。此时对损失函数的近似为:</p>
<script type="math/tex; mode=display">
   \widetilde J(\theta)=J(w^*)+\frac{1}{2}(w-w^*)^TH(w-w^*) \\
   其中H为w在w^*处的Hessian矩阵</script><p>当梯度</p>
<script type="math/tex; mode=display">
   \bigtriangledown_w\widetilde J(w)=H(w-w^*) \tag 7</script><p>为0时， $\widetilde J$ 最小。将式子(7)，增加权重衰减项并修改为：</p>
<script type="math/tex; mode=display">
   \alpha \widetilde w+H(\widetilde w-w^*)=0 \tag 8\\
   (H+\alpha I)\widetilde w= Hw* \\
   \widetilde w = (H+\alpha I)^{-1}Hw*</script><p>如果 $\alpha$ 趋近于0，正则化的解 $\widetilde w$趋近于 $w*$。但是如果 $\alpha$增加，由于 $H$ 是实数并且是对称的，我们将上式解构为如下（其中 $\Lambda$ 为对角矩阵，$Q$ 为特征向量的正交基向量使得 $H=Q\Lambda Q^T$）：</p>
<script type="math/tex; mode=display">
  \widetilde w =(Q\Lambda Q^T +\alpha I)^{-1}Q\Lambda Q^Tw*\\
  =[Q(\Lambda +\alpha I)Q^T]^{-1}Q\Lambda Q^Tw*\\
  =Q(\Lambda +\alpha I)^{-1}\Lambda Q^Tw*</script><p>我们可以看到权重衰减的效果是由 $H$ 的特征向量定义的轴上重新调整 $w*$ , $H$ 的第 $i$ 个特征向量由一个因子 $\frac{\lambda _i}{\lambda _i+\alpha}$ 重新调整。可以得知</p>
<ul>
<li>当 $\lambda _i\gg \alpha$ 时，正则化效果十分有限。</li>
<li><p>当 $\lambda _i\ll \alpha$ 时，将极大地改变 $w*$，几乎可以得到$w\sim 0，但是不为0$。</p>
<p><strong>如何防止过拟合</strong>:更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好,而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。</p>
<p>过拟合的时候，拟合函数的系数往往非常大，为什么？如下图(图7.2)所示，过拟合，就是拟合函数需要顾及每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</p>
<p><img src="/images/blog/regular2.png" alt="L2正则化示例"></p>
<p align="Center">图7.2</p>

<p>而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
</li>
</ul>
<h3 id="7-2-2-L-1-正则化处理"><a href="#7-2-2-L-1-正则化处理" class="headerlink" title="7.2.2 $L^1$正则化处理"></a>7.2.2 $L^1$正则化处理</h3><p>对模型参数$w$上的$L^1$正归化是增加一个惩罚项</p>
<script type="math/tex; mode=display">
  \Omega(\theta) = \|w\|_1=\sum_i\|w_i\|</script><p>完整公式如下：</p>
<script type="math/tex; mode=display">
     \bar J(w;X,y) =\alpha\|w\|_1+J(w;X,y)</script><p> 对上式求梯度得到：</p>
<script type="math/tex; mode=display">
 \bigtriangledown _w \bar J(w;X,y) = \alpha sign(w)+\bigtriangledown _w J(X,y;w)</script><p> 可以看到，$L^1$正规化对梯度的影响不再与每个$w_i$线性相关，而是一个常量因子。符号只与$w$相关，当w为正时，更新后的w变小。当w为负时，更新后的w变大（因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。 当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉 $\alpha sign(w)$ 这一项，所以我们可以规定$sgn(0)=0$，这样就把w=0的情况也统一进来了）。这种形式的梯度使得没有必要继续求 $J(X,u;w)$ 的二次近似。</p>
<p> 简单的线性模型可以使用对应的<em>Taylor</em>级数来表述其二次损失函数。我们可以想象一个更复杂模型的损失函数的一个截断的泰勒级数。此时的梯度为:</p>
<script type="math/tex; mode=display">
\bigtriangledown _w\widetilde J(w)=H(w-w*)</script><p>此时 $H$ 依然为矩阵 $J$ 在 $w=w^<em>$ 处的</em>Hessian<em>矩阵。由于 $L^1$ 惩罚在完整的Hessian举证并没有清晰的代数表达式，我们可以作进一步的简化假设</em>Hessian*矩阵是对角矩阵（这个假设的前提是所有输入特征被移除了相关性）, $H=diag([H<em>{1,1},… H</em>{n,n}])$,其中每个 $H_{i,i}\gt 0$ 。</p>
<p>此时使用 $L^1$ 正则化的目标函数的二次近似可以分解为如下参数:</p>
<script type="math/tex; mode=display">
\widetilde J(w;X,y) = J(w*;X,y)+\sum_i[\frac{1}{2}H_{i,i}(w_i-w^*_i)^2+\alpha \|w_i\|]</script><p>最小化上面的损失函数(对每个维度 $i$ )有一个如下形式的解析解:</p>
<script type="math/tex; mode=display">
 w_i =sign(w_i^*)max\{\|w^*_i\|-\frac{\alpha}{H_{i,i}},0\}</script><p>考虑到条件是，对每个 $i$ 都有 $w_i\gt0$,此时可能的结果为(下列公式中 $w_i$应该都是 $w^*_i$):</p>
<ul>
<li><p>$w<em>i\le\frac{\alpha}{H</em>{i,i}}$ 时， $w_i=0$ ,因为 $\widetilde J(w;X,y)$ 中的 $J(w;X,y)$ 在方向 $i$ 上完全被 $L^1$ 正则项覆盖。</p>
</li>
<li><p>$w<em>i\gt\frac{\alpha}{H</em>{i,i}}$ 时，正则项没有将 $w<em>i$ 的最优置于0，而只是在该方向上移动了距离 $\frac{\alpha}{H</em>{i,i}}$</p>
</li>
</ul>
<p><strong>用途:</strong> 与 $L^2$ 正则化相比$L^1$ 会导致参数稀疏，参数稀疏指的是一些参数被优化到0。 $L^1$ 正则化产生的稀疏属性可被用作特征选取，参数被惩罚为0的特征可以被丢弃。</p>
<h2 id="7-3-规范惩罚用作约束优化"><a href="#7-3-规范惩罚用作约束优化" class="headerlink" title="7.3 规范惩罚用作约束优化"></a>7.3 规范惩罚用作约束优化</h2><p>考虑一个使用了参数规范惩罚正则化的损失函数</p>
<script type="math/tex; mode=display">
  \bar J(\theta;X,y) = J(\theta;X,y)+\alpha \Omega(\theta)</script><p>我们可以通过构建一个拉格朗日函数求在约束条件下目标函数的最小值，构建函数即在原始目标函数上添加一些惩罚项。假若我们想要约束条件 $\Omega(\theta)$ 小于一个常量k,可以构建如下拉格朗日函数:</p>
<script type="math/tex; mode=display">
  L(\theta,\alpha;X,y) =J(\theta;X,y)+\alpha(\Omega(\theta)-k)</script><p>此问题的解为</p>
<script type="math/tex; mode=display">
  \theta^* =arg \quad min_{\theta}\quad max_{\alpha,alpha\ge 0}L(\theta,\alpha)</script><p>可以通过修改 $\theta$ 和 $\alpha$ 的来解此问题，一些使用梯度，一些使用解析解(梯度为0)，但是需保证有</p>
<ul>
<li>$\Omega (\theta)\gt k$时 $\alpha$ 增加</li>
<li>$\Omega(\theta)&lt;k$时 $\alpha$ 减小</li>
</ul>
<p>所有的正 $\alpha$ 都会缩小 $\Omega(\theta)$ ，而最优的 $\alpha ^<em> $ 会缩小 $\Omega(\theta)$ 但是不会使得 $\Omega(\theta)$ 过于小于k。为了解受限条件的效果，我们可以固定 $\alpha ^</em> $，将问题转化为 $\theta$ 的函数。</p>
<script type="math/tex; mode=display">
  \theta ^* =arg\quad min_{\theta}L(\theta,\alpha ^*)=arg\quad min_{\theta}J(\theta;X,y)+\alpha ^*\Omega(\theta)</script><p>该式子与最小化 $\widetilde J$ 的正则化训练问题完全一致。可以将参数正则惩罚问题看做权重约束。通过系数 $\alpha ^<em> $ 的权重衰减，无法知晓约束区域，因为 $\alpha^</em> $ 并没有直接指明k的值。我们可以求解k,但k与 $\alpha^*$ 的关系依赖于 $J$ 的形式。</p>
<p>虽然无法精确制导约束区间大小，但是可通过调整 $\alpha$ 来控制，$\alpha$ 增加，则约束区间减小，反之则增大（参照惩罚项）。</p>
<h4 id="7-3-1-约束问题中精确约束"><a href="#7-3-1-约束问题中精确约束" class="headerlink" title="7.3.1 约束问题中精确约束"></a>7.3.1 约束问题中精确约束</h4><p>有时候我可能需要使用精确约束条件而不是惩罚项，此时需要通过修改算法如<strong><em>SGD</em></strong>在函数 $J(\theta)$ 上下降，并将 $\theta$ 投影回满足 $\Omega (\theta)&lt;k$的最邻近点。甚至如果我们知道何时的 $k$ ，而不想浪费时间继续搜索与 $k$ 相对应的 $\alpha$ ，此时将很有用。</p>
<p>另一个使用精确约束和重映射，而不是惩罚约束的原因是，惩罚机制会导致非凸优化而使得目标函数陷入局部最优，导致神经网络僵死。</p>
<p>最后，使用重新映射的精确约束可以在优化过程中增加稳定性。使用高学习率时会遇到正反馈循环问题，大权重将减小大梯度，进而大幅度更新权重。若不断更新权重，参数 $\theta$ 将从原始值更新到数值溢出。使用重新映射的精确约束可阻止反馈循环的权重无限增加。</p>
<h2 id="7-4-正则化和受限问题"><a href="#7-4-正则化和受限问题" class="headerlink" title="7.4 正则化和受限问题"></a>7.4 正则化和受限问题</h2><p> 机器学习中正则化是十分必要的，许多线性模型依赖转置矩阵 $X^TX$ 如线性回归模型，主成分分析(PCA)模型，但是若 $X^TX$ 是奇异的（行列式为0，有无穷个解），就没法实现。当数据在某一些方向上没有方差（所有的值相同）时，矩阵是奇异的。此时需要进行相对应的正则化来保证矩阵是可逆的。</p>
<p>当线性问题的相关矩阵是可逆时才有封闭解。欠定方程也可能没有封闭解。一个例子是，逻辑回归用于线性可分类问题时，如果权重 $w$ 可获得最佳分类结果，那么$2w$ 也可以，而且使用最大似然框架时，似然度更高。进行迭代优化如使用SGD(随机梯度下降)时将会导致 $w$ 不断增加，而算法不会停止（实际中会产生数值溢出）。</p>
<p>大多数正则化可以保证迭代方法收敛，比如权重衰减( $L^2$ 正则化)中似然函数梯度等于权重衰减系数时停止。</p>
<p><strong>（多元）线性回归问题的损失函数为：</strong></p>
<script type="math/tex; mode=display">
  (Xw-y)^T(Xw-y)</script><p>对应的解为:(参考附录)</p>
<script type="math/tex; mode=display">
w = (X^T X)^{-1} X^T y</script><p>（可以看到，如果 $X^TX$ 是奇异的，将无法求解）</p>
<p>添加 $L^2$ 正则项之后的损失函数变为:</p>
<script type="math/tex; mode=display">
  (Xw-y)^T(Xw-y)+\frac{1}{2}\alpha w^Tw</script><p>此时的正则化解为：</p>
<script type="math/tex; mode=display">
w = (X^T X + \alpha I)^{-1} X^T y</script><p>其中，$I$ 是 (n + 1) x (n + 1) 矩阵</p>
<script type="math/tex; mode=display">
Z =
\begin{bmatrix}
0 \\
& 1 \\
& & 1 \\
& & & \ddots \\
& & & & 1
\end{bmatrix}</script><p>正则化实际上解决了两个问题。一个是确保不发生过拟合，另外一个也解决了 $X^T X$ 的奇异矩阵问题。当 m &lt; n 时，$X^T X$ 将是一个奇异矩阵，从数学上可以证明，加上 $\alpha I$ 后，结果将是一个非奇异矩阵。</p>
<h2 id="7-5-数据增强"><a href="#7-5-数据增强" class="headerlink" title="7.5 数据增强"></a>7.5 数据增强</h2><p>理论上来说，数据越多，模型训练得越充分，模型泛化能力越强。但是现实情况是，数据量总是有限的，解决此问题的一个方法是生成一部分的模拟数据。</p>
<p>这对于分类问题最简单，它需要喂入复杂的高维度数据输入并映射到一个单一分类上。这说明，分类问题主要面临的是对于任意广度的输入其分类结果不变。我们可以简单的生成一个 $(x,y)$ 即可。但是对于很多其他问题，如密度估计问题，很难生成模拟数据，除非已经知道需要解决的密度估计问题。  </p>
<p> <strong>图像识别</strong>：数据增强用于特定领域分类问题，如图像识别很有效。但是切记，转换数据的时候不要改变图像的正确分类。比如不要将手写字识别图像中的<code>6</code> 垂直转换成了<code>9</code>，<code>b</code>水平翻转成了<code>d</code>。    </p>
<p> <strong>语音识别</strong>：语音识别问题中，网络输入数据中也会注入一些随机噪音干扰，这也是一种数据增强（现实生活中语音环境有噪音）。</p>
<p>  神经网络对噪音鲁棒性并不好，所以我们在可以有一种提升网络性能的方法，即在训练时加入随机干扰。</p>
<p>  由于数据增强的存在，我们在比较机器学习结果时应当考虑数据增强。手工设计的数据通常可以大幅度减少泛化错误。所以，在比较机器学习算法时，需要做对照试验，使用相同算法，一组使用没有应用数据增强的输入A，另一组使用应用了数据增强的B，如果A的性能很差，而B的性能很好，那么说明促使模型性能提升的不是算法而是数据。</p>
<h2 id="7-6-噪声鲁棒"><a href="#7-6-噪声鲁棒" class="headerlink" title="7.6 噪声鲁棒"></a>7.6 噪声鲁棒</h2><p> 对于使用了数据增强的模型，噪音其实等同于在权重分布上增加了惩罚机制。通常，噪音注入比简单的收缩参数(正则化)更有用，尤其是噪音作用于隐藏神经元时，dropout就是专门在方面的进展。<br> 在RNN中，权重上增加噪声被证明是一种很有效的正则策略（书中论文）。接下来分析下标准前馈神经网络中权重噪音的实际影响。<br> 在回归问题中，假设损失函数是最小平方误差，如下:</p>
<script type="math/tex; mode=display">
   J = E_{p(x,y)}[(\bar y(x)-y)^2]</script><p> 假设输入中包含随机扰动 $\epsilon_w \sim N(\epsilon ;0,\eta I)$ ，此时对应的目标函数变成:</p>
<script type="math/tex; mode=display">
   \bar J_W=E_{p(x,y,\epsilon _w)}[(\bar y_{\epsilon_W}(x)-y)^2] \\
   =E_{p(x,y,\epsilon_W)}[\bar y^2_{\epsilon_W}(x)-2y\bar y_{\epsilon_W}(x)+y^2]</script><p> 若 $\eta$ 很小,最小化 $J$ 等同于最小化 $J$ 外加一个正则项 $\eta E_{p(x,y)}[|\bigtriangledown_W\bar y(x)|^2]$<br>这种正则项使得模型对参数的轻微扰动不再敏感，此时的最优参数不在使得损失函数最小点而是在最小点附近。</p>
<h3 id="7-6-1-在输出目标上注入噪音"><a href="#7-6-1-在输出目标上注入噪音" class="headerlink" title="7.6.1 在输出目标上注入噪音"></a>7.6.1 在输出目标上注入噪音</h3><p> 模型的错误分类将导致最大似然框架求得的 $logP(y|x)$ 并不是真正最大点。一种办法是在标签上加入噪音，例如常量 $\theta$ ，此时训练数据集x，其输出被分类到标签y的概率为 $1-\theta$ ，这种方法很容易并入到惩罚函数，而不需要引入噪声数据。这是一种平滑机制，比如标签正则模型基于有k个输出的 <em>softmax</em> ，将分类<code>0</code>,<code>1</code>替换为 $\frac{\theta}{k}$ 和 $1-\frac{k-1}{k}\theta$</p>
<h2 id="7-7"><a href="#7-7" class="headerlink" title="7.7"></a>7.7</h2><p> 多任务学习</p>
<p> 多任务学习可以看做一种从多个模型中抽象出一个汇总模型以提高泛化能力的方法。模型的某个部分的参数在多个任务间共享时，该部分将获得更好的泛化能力。</p>
<p> 下图(图7.3)展示了一个多任务学习方法的常见形式，不同的监督学习任务(对于给定输入X预测输出Y)，共享了相同的输入X，以及一些中间表述层 $h^{shared}$ (捕获参数的一些平均特征)。</p>
<p> <img src="/images/blog/regular3.png" alt=""></p>
 <p align="Center">图7.3</p>

<p> 通过提升这些共享参数的统计特性（更稳定），可以提高模型泛化能力和泛化边界。与单任务学习相比，其实是按比例增加了共享参数的输入样本。当然前提是，多个模型之间可以共享参数。</p>
<p> 以深度学习的观点来看，这种方法的先验知识是：不同模型的输入数据有些解释了数据变动的参数是在多个任务中共享的。</p>
<h2 id="7-8-提前终止"><a href="#7-8-提前终止" class="headerlink" title="7.8 提前终止"></a>7.8 提前终止</h2><p> 看一张图(图7.4)：</p>
<p> <img src="/images/blog/regular4.png" alt=""></p>
 <p align="Center">图7.4</p>

<p> 我们可以看到随着时间或迭代次数的增加，训练误差不断减少，而验证误差最后会逐渐上升，呈U型。验证误差开始上升时，已经出现了过拟合。我们应当在验证误差有段时间没有下降时停止迭代，而不是等到验证误差达到某个极小值时。此策略即提前终止，是正则化策略中最常见，最有效的方法。</p>
<p> <strong>如何确定何时终止:</strong></p>
<ul>
<li>在算法开始之前，先确定训练次数。</li>
<li>在训练过程中定期地运行验证，验证集可以比训练集数据量小。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>几乎不改变算法过程，易于使用</li>
<li>提前终止可以很容易地与其他正则化方法结合使用。</li>
</ul>
<p><strong>代价</strong>：</p>
<ul>
<li>需要不断保存最优模型参数，这是可以接收的，可以直接存放在磁盘上。</li>
<li>需要一个验证数据集，验证数据集不用于训练。</li>
</ul>
<p><strong>最大化利用所有数据</strong></p>
<p>  为了更好的利用所有数据，可以在算法提前终止后再次训练。此时有两种策略</p>
<ul>
<li><p>重新初始化模型，在所有数据集上重新训练，但只训练提前终止训练的次数。此时可以增加一些参数，因为数据更多（个人认为，其实是分两步走，第一步是用提前终止找到最优训练次数，第二次再完全训练）</p>
</li>
<li><p>保存第一次训练时的所有参数，并在所有数据上继续训练。此时无法知晓算法何时停止，但是可以观察验证数据集上的平均Loss，当其低于第一次训练的loss时停止。此方法可以避免第一次的重复计算，但是表现一般。</p>
</li>
</ul>
<p><strong>提前终止的内在机制</strong>：一些论文认为，它能将参数搜索空间限制在较小区间，进而加快模型训练。实际上在使用均方差作为损失函数和梯度下降更新参数的简单线性回归模型中，$L^2$ 正则等同于提前终止。</p>
<h2 id="7-9-参数捆绑和参数共享"><a href="#7-9-参数捆绑和参数共享" class="headerlink" title="7.9 参数捆绑和参数共享"></a>7.9 参数捆绑和参数共享</h2><p>前面的部分讲的都是在固定区域或点对参数加约束或惩罚，例如 $L^2$ 从0点开始寻找最优参数。有时候我们的先验知识需要其他表现形式，或者有时候无法确定精确值，但是知道参数之间的依赖。</p>
<p>前面讲到的参数规范惩罚是一种让参数近似另外一个参数的正则化方法，一种更常见的形式是:<strong>强制让一群参数相等</strong>，此方法称为<strong>参数共享</strong>。其优势在于，可以大幅度减少需要存储和更新的参数。该方法在卷积神经网络中尤其有效。</p>
<p>一个图像处理的例子如下图所示:</p>
<p><img src="/images/blog/regular9.png" alt="卷积图像"></p>
<p>上图左图为全连接，右图为局部连接。在上右图中，假如每个神经元（隐藏层100万个神经元）只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的万分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。但其实这样的话参数仍然过多，再使用权值共享。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100（100万个神经元每个神经元与100个(10x10像素)参数相关，现在这100个参数一样了）了。</p>
<p>怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。</p>
<p>更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8x8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8x8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8x8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>
<h2 id="7-10-稀疏表述"><a href="#7-10-稀疏表述" class="headerlink" title="7.10 稀疏表述"></a>7.10 稀疏表述</h2><p> 权重衰减是通过在权重参数上增加惩罚项，另一种策略是在网络神经元上施加惩罚。</p>
<p> $L^1$ 正则化带来了稀疏项，是通过使得部分参数为0，而稀疏表述则直接让其中的表述元素直接为0.举例线性回归来说明这种差别:</p>
<p> <strong>参数稀疏的表述：</strong></p>
<script type="math/tex; mode=display">
   \begin{bmatrix}
   18\\
   5\\
   15\\
   -9\\
   -3
   \end{bmatrix}=\begin{bmatrix}
    4\quad0\quad0\quad-2\quad0\quad0\\
    0\quad0\quad-1\quad0\quad3\quad0\\
    0\quad5\quad0\quad0\quad0\quad0\\
    1\quad0\quad0\quad-1\quad0\quad-4
   \end{bmatrix}

   \quad\begin{bmatrix}
    2\\
    3\\
    -2\\
    -5\\
    1\\
    4
   \end{bmatrix}\\
   y\epsilon R^m \quad\quad\quad \quad \quad A\epsilon R^{m\times n}\quad\quad\quad\quad x \epsilon R^m</script><p> <strong>表述稀疏的</strong>:</p>
<script type="math/tex; mode=display">
   \begin{bmatrix}
   -14\\
   1\\
   19\\
   2\\
   23
   \end{bmatrix}=\begin{bmatrix}
    3\quad-1\quad2\quad-5\quad4\quad1\\
    4\quad2\quad-3\quad-1\quad1\quad3\\
    -1\quad5\quad4\quad2\quad-3\quad-2\\
    3\quad1\quad2\quad-3\quad0\quad-3\\
    -5\quad4\quad-2\quad2\quad-5\quad-1\\
   \end{bmatrix}

   \quad\begin{bmatrix}
    0\\
    2\\
    0\\
    0\\
    -3\\
    0
   \end{bmatrix}\\
   y\epsilon R^m \quad\quad\quad \quad \quad B\epsilon R^{m\times n}\quad\quad\quad\quad h \epsilon R^m</script><p> 稀疏表述的 规范惩罚是在损失函数 $J$ 上加一个惩罚项 $\Omega (h)$</p>
<script type="math/tex; mode=display">
   \bar J(\theta;X,y) = J(\theta;X,y)+\alpha \Omega (h) \\
   其中 \alpha \epsilon [0,\infty)</script><h2 id="7-11-集成学习方法"><a href="#7-11-集成学习方法" class="headerlink" title="7.11 集成学习方法"></a>7.11 集成学习方法</h2><p>  主要思想：独立训练多个模型然后所有模型对测试样本投票来减少泛化错误。依据是，不同模型不会在测试样本上犯同样错误。</p>
<p>  考虑k个回归模型集合，每个模型在每个(单个)样本上误差为 $\epsilon _i$，模型误差来自多变量正太分布，方差为 $E[\epsilon _i^2]=v$ ，协方差期望为 $E[\epsilon _i,\epsilon _j]=c$ ，则所有集成模型的平均误差是 $\frac{1}{k}\sum_i\epsilon _i$ ,期望方差为:</p>
<script type="math/tex; mode=display">
   E[(\frac{1}{k}\sum_i \epsilon _i)]=\frac{1}{k^2}E[\sum(\epsilon _i^2+\sum_{j\ne i}\epsilon _i\epsilon _j)] \\
   =\frac{1}{k}v+\frac{k-1}{k}c</script><p>有些情况下，误差完全相关，$c=v$ ，此时平均模型没用。<br>集成学习的期望均方误差与集成规模呈线性递减，也就是集成学习最终至少有其中一个模型的性能。</p>
<p>集成学习方法的简单示例:</p>
<p> <img src="/images/blog/regular5.png" alt="拟合情况"></p>
  <p align="Center">图7.5</p>

<p> 图中第一行是原始数据，进行一些随机替换和重复（如第二行9替换为8，第三行重复9）分别进行训练。其运行机制就是，每个模型使用的数据集大小相同，但是内容不一，会有部分替换和重复。这样来看，每个单独的模型是相对脆弱的，但是平均化输出，整个模型又是健壮的（上图中只有两个模型的输出都是8时，才会有最大置信度）。</p>
<p> 平均模型是极其有效可靠的减少泛化误差的方法，它经常被应用到机器学习竞赛中。BVLC的googlenet使用了6个模型，但是不鼓励在论文中使用，因为可以通过以存储空间换取模型泛化能力。</p>
<h2 id="7-12-dropout"><a href="#7-12-dropout" class="headerlink" title="7.12 dropout"></a>7.12 dropout</h2><p> dropout提供了一种计算量不大，但是强大的正则化方法。它是对一个<strong>模型</strong>族进行正则化处理。</p>
<p> 集成学习方法需要训练多个模型，如果模型巨大，无法实现。dropout训练的是从基础（原始）网络中移除非输出单元构成的全部子网的集合。下图（图7.6）展示了此过程。</p>
<p> <img src="/images/blog/regular6.png" alt="dropout训练过程"></p>
<p align="Center">图7.6</p>

<p>集成学习定义了k个模型，k个从训练数据集中抽样的自集。dropout的目标就是模拟这一过程。训练dropout过程中，我们使用了一个基于<em>mini-batch</em>的学习算法，如<strong>SGD</strong>(随机梯度下降)。每载入一个样本到<em>mini-batch</em>时，对网络中所有输入和隐藏神经元应用一个二进制掩码，决定每个神经元是否被纳入（二进制掩码为1时），每个神经元取掩码值得过程是独立抽样。掩码取值为1的概率在训练之前就固定的，一般取值是，输入神经元0.8，隐藏神经元0.5。下图是一个前馈网络示例:</p>
<p>   <img src="/images/blog/regular7.png" alt="dropout训练过程"><br>  <p align="Center">图7-7</p></p>
<p>集成学习的模型都是独立的(每个模型参数和训练数据)，dropout的每个模型的参数都是原网络参数的一个子集，这种共享机制使得dropout 网络有能力表述指数级的特征。<strong>集成学习</strong>每个子模型分别在其训练子集中收敛，而在dropout中，大部分模型并没有得到完全训练（不是每个模型都达到了收敛状态），其模型太大无法穷尽。dropout网络中，子网的某些部分在迭代中一步步训练，同时参数共享机制使得剩余子网的参数达到一个较好的状态。</p>
<p>使用集成学习模型进行预测时，需要所有模型对预测结果投票，我称此过程为<em>inference</em>。无论是集成学习或者dropout，我们都没有要求模型的精确概率,假设模型是输出一个概率分布，那么集成学习即输出所有概率分布的均值。</p>
<script type="math/tex; mode=display">
    \frac{1}{k}\sum_{i=1}^kp^{(i)}(y\|x)</script><p>  dropout模型中每个由掩码向量 $\mu$ 定义的子模型定义了一个概率分布 $p(y|x,\mu)$，其<em>inference</em>是所有掩码概率分布的均值。所有掩码的算术均值如下:</p>
<script type="math/tex; mode=display">
   \sum_{\mu}p(\mu)p(y|x,\mu)</script><p>  由于求和公式包含太多项（指数级），当模型经过一些简化之后很难估计其输出期望（目前为止，深度神经网络的都有不可知的简化）。我们可以通过抽样来模拟<em>inference</em>，即平均多个掩码的输出。一般10-20个掩码足以获得较好的表现。</p>
<p>  然而，有一种更好的方法，一次前向传播即可获得较好的模拟整个集成学习，即使用<strong>几何平均</strong>替换<strong>算术平均</strong>(所有子模型的)。（论文）</p>
<p>  多概率分布的几何均值并不一定是概率分布，为保证多概率的结果依然是概率分布，所有子模型不得使任何事件出现的概率为0，然后使结果呈正态分布。几何均值的非标准(<em>unnormalizalized</em>)概率分布如下:</p>
<script type="math/tex; mode=display">
    \widetilde p_{ensemble}(y|x) =\sqrt[2^d]{\prod_{\mu}p(y|x,\mu)} \\
    其中d为可能被dropout的神经元数</script><p>  此处使用一个 $\mu$ 的正太分布简化表述，其实非正太分布也是可能的。如果要预测输出，需要对模型集合重新标准化:</p>
<script type="math/tex; mode=display">
    P(y|x) =\frac{\widetilde P(y|x)}{\sum_{y^{'}}\widetilde P(y^{'}|x)}</script><p>  dropout中可以通过估计一个模型的 $p(y|x)$ 来近似 $p_{ensemble}$ ，该模型包含了所有的神经元，但是每个神经元输出要乘以该神经元被保留的概率。此方法可以获得该神经元的期望输出。我们称此方法为<em>scale inference rule</em>，此方法只是一种经验上的技巧，并没有学术论证，但是实际效果很好。</p>
<h3 id="7-12-1-dropout的优点和注意"><a href="#7-12-1-dropout的优点和注意" class="headerlink" title="7.12.1 dropout的优点和注意"></a>7.12.1 dropout的优点和注意</h3><p>  <strong>优点一:</strong> 计算量小，训练时每次更新每个样本的时间复杂度为 $O(n)$，其中 $n$ 为要生成的随机二进制数</p>
<p>  <strong>优点二:</strong> 对模型类型和训练过程没有太大限制。几乎对所有使用分布式表述(<em>distribute representation</em>)并使用SGD的模型都可以很好。</p>
<p>  <strong>注意:</strong> 尽管dropout的每一步代价不高，但是整体权衡下来还是会比较高，作为一种正则化技术，会削弱模型性能，增大模型可以一定程度上抵消这种削弱，但是切记勿得不偿失。一般标签分类任务中，如果样本较少，比如少于5000时，dropout性能一般。</p>
<h2 id="7-13-对抗训练"><a href="#7-13-对抗训练" class="headerlink" title="7.13 对抗训练"></a>7.13 对抗训练</h2><p>在独立同分布(<strong><em>i.i.d</em></strong>)测试中，神经网络已经基本达到人类的分辨能力。为了测试一个神经网络对于隐藏任务的理解能力，我们可以搜寻模型误分类的样本。论文(2010b)发现，即便是达到人类同层次理解能力的神经网络，对使用<strong>优化过程构造</strong>的临界样本（输入相近，但是输出不同）依然会100%犯错。在很多情况下，与输入<code>x</code>十分相近的点<code>x&#39;</code>，人都无法区分对照样本和原始样本的区别，但是网络可以得到完全不同的预测。下图(图7.8)展示了这种差异：</p>
<p>   <img src="/images/blog/regular8.png" alt="对照训练"></p>
<p>一组对照样本应用到GoogLeNet，测试ImageNet。在原始图上添加一个很小的向量，该向量的值是对应输入的损失函数梯度的符号，我们可以改变GoogLeNet对于图像的分类。</p>
<p> 产生这些临街对照样本的基本原因是过度线性化，由线性单元构建的神经网络，其结果也呈高度线性相关，这些线性函数很容易优化。但是，如果输入值是数值的话，线性函数的值可能会产生巨大变化，尤其是高维场合。假若每个维度的输入改变 $\epsilon$，那么权重为w的线性函数会被改变 $\epsilon ||w||_1$，高维场合此值会极大。对照训练可以使得训练数据集中相邻数据具有局部不变性，进而抑制这种高度敏感的局部线性行为。</p>
<p> 纯粹的线性模型无法应对对照训练样本的干扰的，因为它们被强行线性化。神经网络可以表述的函数范围从近线性到局部不变性，因而具有较好稳定性，即可以在学习到训练数据的线性趋势的同时能够有效对抗局部干扰。</p>
<p> <strong>其他用途：</strong> 对照样本可以用来完成半监督学习。数据集中点<code>x</code>没有打标签，模型可能会给它标签 $\widetilde y$,假若模型较好，那么数据点<code>x</code>标签即为 $\widetilde y$ 的概率较高。我们可以搜寻一个对照样本<code>x&#39;</code>使得分类器输出标签为<code>y&#39;</code>，其中 $y’\ne \widetilde y$。对照样本不是使用真实的标签，而是由一个称为<strong>虚拟对照样本</strong> 模型生成的。分类器可能会被训练为给<code>x</code>和<code>x&#39;</code>相同的标签。这会使得分类器逐渐具备对较小变动的未分类数据分类的能力。<strong>motivating</strong>:认为不同类别之间通常不连通，较小扰动不足以使得一种类别与另外一种类别产生关联。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>大数据：spark mllib python使用示例</title>
    <url>/2016/10/02/spark-python-example/</url>
    <content><![CDATA[<h2 id="机器学习的背景知识"><a href="#机器学习的背景知识" class="headerlink" title="机器学习的背景知识"></a>机器学习的背景知识</h2><blockquote>
<p>监督学习的中点是<strong> 在规则化参数的同时最小化误差</strong>。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但<strong>训练误差</strong>小并不是我们的最终目标，我们的目标是希望模型的<strong>测试误差</strong>小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。要知道，有时候人的先验是非常重要的。</p>
</blockquote>
<p>来源于：   <a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="noopener">http://blog.csdn.net/zouxy09/article/details/24971995</a></p>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="一-数学公式"><a href="#一-数学公式" class="headerlink" title="一  数学公式"></a>一  数学公式</h2><p>&emsp; &emsp; 许多机器学习方法都可以被转换为一个凸函数优化问题，比如查找凸函数f（自变量是w，在代码中称为权重，自变量有d维）最小值。通常，我们可以将这些写成 $ min_{w\epsilon R^d}f(w) $ ，其目标函数是以下形式</p>
<script type="math/tex; mode=display">f(w) := \lambda\, R(w) +\ frac1n \sum_{i=1}^n L(w;x_i,y_i) \label{eq:regPrimal}</script><p>&emsp;<br>&emsp;<br>此处向量$x<em>{i}\epsilon R^d$是训练数据，对于$1\leq i\leq n$ 和 $y</em>{i}\epsilon R$是我们需要预测的标签。如果$L(w;x,y)$可以被表示为 $W^T x$和$y$的函数，则可以调用 <strong><em>linear</em></strong>方法。</p>
<p>&emsp;<br>&emsp;<br>目标函数<strong><em>f</em></strong>分为两部分：控制模型复杂度的正则化部分，模型在训练数据集上误差评估的损失度量部分。损失度量函数$L(w;.)$是一个在域$w$上的凸函数。固定的正则化参数$\lambda \geq 0$(代码中是参数<strong><em>regParam</em></strong>)定义了权衡最小化损失（比如训练误差）和最小化模型复杂度（比如，防止过拟合）之间的平衡。</p>
<h2 id="二-误差函数"><a href="#二-误差函数" class="headerlink" title="二 误差函数"></a>二 误差函数</h2><p>下表概括了损失函数和它们在spark.mllib支持的的梯度和分梯度方法. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>损失</th>
<th>loss function $L(w;x,y)$$\;\;\;\;\;\;\;\;$</th>
<th>梯度或分梯度</th>
</tr>
</thead>
<tbody>
<tr>
<td>hinge loss(SVM)</td>
<td>$max${0,$1-yw^{T}x$},$y \epsilon$ {-1,+1}</td>
<td>$ -y\cdot x (if\, yw^Tx &lt; 1)\;\; 0 (otherwise))$</td>
</tr>
<tr>
<td>logstic loss(逻辑回归)</td>
<td>$log(1+exp(-yw^Tx)),y\epsilon {-1,+1}$</td>
<td>$-y(1-\frac {1}{1+exp(-yw^Tx)})\cdot x$</td>
</tr>
<tr>
<td>squared loss(最小二乘)</td>
<td>$\frac {1}{2}(w^Tx-y)^2, y\epsilon R$</td>
<td>$(w^Tx-y)\cdot x$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三-规则化"><a href="#三-规则化" class="headerlink" title="三 规则化"></a>三 规则化</h2><p>  &emsp;&emsp;规则化的目的是简化模型并避免过拟合，规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同<br>&emsp;&emsp;关于L1范式和L2范式:</p>
<ul>
<li>L0 范式：L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说，让参数W是稀疏的。</li>
<li>L1 范式:  L1范数是指向量中各个元素绝对值之和，也称叫“稀疏规则算子”（Lasso regularization）。<br>既然L0可以实现稀疏，为什么不用L0，而要用L1呢？一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。</li>
<li>L2范式：<br>在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。它的作用是改善过拟合。<br>L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。<br>&emsp;&emsp;目前在 spark.mllib中支持的正则化如下：<br>|范式|      regularizer $R(w)$      |            梯度或子梯度       |<br>|——|—————-|———-|<br>|zero(unregularized)|0|0|<br>|L2|$\frac{1}{2}$||w||$_2^2$|w|<br>|L1|$||w||_1$|$sign(w)$|<br>|elastic net|$\alpha||w||_1+(1-\alpha)\frac{1}{2}||w||_2^2$|$\alpha sign(w)+(1-\alpha)w$|</li>
</ul>
<p><em>此处$sign(w)$是由项$w$中所有$sign(-1,+1)$组成的向量</em></p>
<h2 id="三-优化"><a href="#三-优化" class="headerlink" title="三 优化"></a>三 优化</h2><p>&emsp;&emsp;线性方法使用凸函数来优化目标函数. spark.mllib使用两个方法，SGD和LBFGS（Limited-Memory Quasi-Newton Method）。当前，大多数算法API都支持Stochastic Gradient Descent（随机梯度下降），和少部分支持LBFGS。</p>
<h2 id="四-分类"><a href="#四-分类" class="headerlink" title="四  分类"></a>四  分类</h2><p>&emsp;&emsp;分类旨在将数据项切分到不同类别。<strong><em>spark.mllib</em></strong>提供了两个线性分类方法：线性SVM和逻辑回归。线性SVM只支持二分类，逻辑回归既支持二分类也支持多分类。这两种方法，<strong><em>spark.mllib</em></strong>都支持L1和L2范式规则化。在<strong>MLlib</strong>中训练数据集合以 <em>LabeledPoint</em>类型的RDD代表，其中label（标签）是从0开始0,1,2…的类别索引。<br>&emsp;&emsp;<strong>注意</strong>：指导手册中的，二分类标签$y$要么是 1 要么是 -1，这是为了方便在公式里，但是在<strong>*spark.mllib</strong>里面是以0代表公式中的-1的</p>
<h2 id="五-线性SVM"><a href="#五-线性SVM" class="headerlink" title="五 线性SVM"></a>五 线性SVM</h2><h3 id="5-1-线性SVM概要"><a href="#5-1-线性SVM概要" class="headerlink" title="5.1 线性SVM概要"></a>5.1 线性SVM概要</h3><p>&emsp;&amp;emsp线性SVM的误差函数是由hingle loss给出:<script type="math/tex">L(w;x,y) :=max\lbrace0,1-yw^Tx\rbrace</script><br>线性SVM默认使用L2范式规则化训练数据，同时是支持L1范式规则的，此时就变成一个线性问题。<br>线性SVM的输出是一个SVM模型。对于一个新数据点，以$x$表示，模型将会基于$w^Tx$的值预测。默认情况下，如果$w^Tx\geq 0$那么输出为1，否则输出为0。</p>
<h3 id="5-2-示例代码"><a href="#5-2-示例代码" class="headerlink" title="5.2 示例代码"></a>5.2 示例代码</h3><p>&emsp;&emsp;一下代码展示了如何载入数据，创建SVM模型，根据模型预测并计算训练误差。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">学习使用spark.mllib中 SVM模型。代码展示了如何载入数据，创建SVM模型，根据模型预测并计算训练误差。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from pyspark.mllib.classification import SVMWithSGD,SVMModel</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestSVM&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line">#载入数据并解析</span><br><span class="line">def parsePoint(line):</span><br><span class="line">  values &#x3D; [float(x) for x in line.split(&quot; &quot;)]</span><br><span class="line">    return LabeledPoint(values[0],values[1:])</span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;sample_svm_data.txt&quot;)</span><br><span class="line"></span><br><span class="line">parseData &#x3D; data.map(parsePoint)</span><br><span class="line">#创建SVM模型</span><br><span class="line">model &#x3D; SVMWithSGD.train(parseData,iterations&#x3D;100)</span><br><span class="line"># 评估模型</span><br><span class="line">labelsAndPoints &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">trainError &#x3D; labelsAndPoints.filter(lambda (v,p):v!&#x3D;p).count()&#x2F;float(parseData.count())</span><br><span class="line">mylog.append(&quot;SVM模型测试，训练误差是:&quot;)</span><br><span class="line">mylog.append(str(trainError))</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;log&#x2F;&quot;)</span><br><span class="line">#存储和载入模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;SVMModelSave&quot;)</span><br><span class="line">sameModel &#x3D; SVMModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;SVM&#x2F;SVMModelSave&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="六-逻辑回归"><a href="#六-逻辑回归" class="headerlink" title="六 逻辑回归"></a>六 逻辑回归</h2><h3 id="6-1-逻辑回归概要"><a href="#6-1-逻辑回归概要" class="headerlink" title="6.1 逻辑回归概要"></a>6.1 逻辑回归概要</h3><p>&emsp;&emsp;逻辑回归在二分类预测中广泛使用，其误差函数是下式 logistic loss:<script type="math/tex">L(w;x,y):=log(1+\exp(-yw^Tx))</script><br>对于二分类问题，算法的输出结果是一个二项式逻辑回归模型。对于给定新数据点，以$x$表示，使用logistic函数来预测:<script type="math/tex">f(z) =\frac{1}{1+e^{-z}}</script>。其中$z =w^Tx$，如果$f(w^Tx)&gt;0.5$输出为正，否则为负。与线性SVM不同的是，逻辑回归模型的原始输出有一个概率解释（即，x是正的概率）。<br>&emsp;&emsp;二项式逻辑回归可以生成多项式逻辑回归并用来训练和预测多分类问题。比如说，对于<strong>K</strong>个可能的输出结果，其中一个可以选定为轴，其余<strong>k-1</strong>则与此轴对立。在spark.mllib中第一个被选中的类0就是轴类。<br>&emsp;&emsp;对于多分类问题，算法将会输出一个多项式逻辑回归模型，包含了<strong>k-1</strong>个与第一个类对立的二项式逻辑回归模型。对于新数据点，<strong>k-1</strong>个模型将会运行，其中有最大概率的模型即预测的模型。<br>&emsp;&emsp;spark中实现了两个算法来解决逻辑回归问题：mini-batch gradient（梯度下降）和L-BFGS。参考<a href="http://www.bubuko.com/infodetail-898846.html" target="_blank" rel="noopener">batch-GD， SGD， Mini-batch-GD， Stochastic GD， Online-GD区别</a>  spark推荐L-BFGS梯度下降以获得更快的收敛。</p>
<h3 id="6-2-逻辑回归代码"><a href="#6-2-逻辑回归代码" class="headerlink" title="6.2 逻辑回归代码"></a>6.2 逻辑回归代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">测试逻辑回归代码</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from pyspark.mllib.classification import  LogisticRegressionWithSGD,LogisticRegressionModel</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(</span><br><span class="line">&quot;TestLogisticRegression&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#载入和解析数据</span><br><span class="line">def parsePoint(line):</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">values &#x3D; [float(x) for x in line.split(&quot; &quot;)]</span><br><span class="line">return LabeledPoint(values[0],values[1:])</span><br><span class="line"></span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;sample_svm_data.txt&quot;)</span><br><span class="line">parseData  &#x3D; data.map(parsePoint)</span><br><span class="line">#创建模型</span><br><span class="line">model &#x3D; LogisticRegressionWithSGD.train(parseData)</span><br><span class="line"></span><br><span class="line">#评估模型</span><br><span class="line">labelaAndPoints &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">trainError &#x3D; labelaAndPoints.filter(lambda (k,v):k!&#x3D;v).count()</span><br><span class="line">&#x2F;</span><br><span class="line">float(parseData.count())</span><br><span class="line">mylog.append(&quot;逻辑回归的误差是:&quot;)</span><br><span class="line">mylog.append(trainError)</span><br><span class="line"></span><br><span class="line"># 存储和载入模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;logisticregression_model&#x2F;&quot;)</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;log&#x2F;&quot;)</span><br><span class="line">logisticregression_model &#x3D; LogisticRegressionModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;logistic_regression&#x2F;logisticregression_model&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="7-回归"><a href="#7-回归" class="headerlink" title="7 回归"></a>7 回归</h2><p>&emsp;&emsp;<strong>Linear least squares, Lasso, and ridge regression</strong><br>&emsp;&emsp;Linear least squares是回归问题最常用的公式，其误差函数如下：<script type="math/tex">L(w;x,y):=\frac{1}{2}(w^Tx-y)^{2}</script><br>使用不同的规则参数将会派生出不同的相关回归方法；其中的 <strong>ordinary least squares</strong>和<strong>linear least squares</strong>不使用规则参数，ridge regression(岭回归)使用L2规则参数，Lasso使用L1规则参数。所有的这些模型，其平均误差或者训练误差<script type="math/tex">\frac{1}{n}\sum_{i=1}^n(w^Tx-y_i)^2</script> 即均方差。<br>&emsp;&emsp;一下代码展示了如何载入数据、转换为LabeledPoint类型的RDD。然后使用 LinearRegressionWithSGD来创建简单线性模型来预测标签值。最后再计算均方差来评估适应度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,LinearRegressionModel</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestSimpleLinearRegression&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line"></span><br><span class="line">#载入数据</span><br><span class="line">def parsePoint(line):</span><br><span class="line">  values &#x3D; [float(x) for x in line.replace(&#39;,&#39;,&#39; &#39;).split(&#39; &#39;)]</span><br><span class="line">    return LabeledPoint(values[0],values[1:])</span><br><span class="line"></span><br><span class="line">mylog &#x3D; []</span><br><span class="line">data &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;data&#x2F;lpsa.data&quot;)</span><br><span class="line">parseData &#x3D; data.map(parsePoint)</span><br><span class="line">#创建模型</span><br><span class="line">model &#x3D; LinearRegressionWithSGD.train(parseData,iterations &#x3D; 10,step&#x3D;0.000001)</span><br><span class="line">#评估模型误差</span><br><span class="line">valuesAndPres &#x3D; parseData.map(lambda p:(p.label,model.predict(p.features)))</span><br><span class="line">MSE &#x3D; valuesAndPres.map(lambda (v,p):(v-p)**2).reduce(lambda x,y:x+y)&#x2F;valuesAndPres.count()</span><br><span class="line">mylog.append(&quot;简单线性回归误差是：&quot;)</span><br><span class="line">mylog.append(MSE)</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;log&quot;)</span><br><span class="line">#存储 和使用模型</span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;SimpleLinearRegressionModel&quot;)</span><br><span class="line">sameMode &#x3D; LinearRegressionModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;linear_regression&#x2F;SimpleLinearRegressionModel&quot;)&lt;&#x2F;pre&gt;</span><br></pre></td></tr></table></figure>
<h3 id="流线性回归"><a href="#流线性回归" class="headerlink" title="流线性回归"></a>流线性回归</h3><p>如果数据是以流的形式到达，在线适配回归模型、新数据到达时跟更新模型参数是很有用的。<em>spark.mllib</em>当前支持使用 <strong>ordinary least squares</strong>的线性回归。适应过程类似于离线使用，一批新数据到达时预测适应值，以此来不断地更新流中的新数据的回应值（回归值）。<br>&emsp;&emsp;如下代码演示了如何训练和测试来自两个不同文本格式的输入流，将流解析成labeled point,使用第一个流来拟合回归模型，并在第二个流中作预测。注意，当训练目录 <em>“/home/xiatao/machine_learing/streaming_linear_regression/data</em> 新增数据时，相应的预测目录<em>/home/xiatao/machine_learing/streaming_linear_regression/predict</em>就会产生相应的结果。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># 流线性回归模型测试</span><br><span class="line">from pyspark.mllib.linalg import Vectors</span><br><span class="line">from pyspark.mllib.regression import StreamingLinearRegressionWithSGD</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestStreamLinearRegression&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">mylog &#x3D; []</span><br><span class="line"></span><br><span class="line">#第一步创建 StreamingContextssc &#x3D; StreamingContext(sc,1)</span><br><span class="line"></span><br><span class="line">#载入和解析数据</span><br><span class="line">def parse(lp):</span><br><span class="line">  label &#x3D; float(lp[ lp.find(&#39;(&#39;)+1:lp.find(&#39;,&#39;)  ])</span><br><span class="line">    vec &#x3D; Vectors.dense(lp[lp.find(&#39;[&#39;)+1:lp.find(&#39;,&#39;)].split(&#39;,&#39;))</span><br><span class="line">    return LabeledPoint(label,vec)</span><br><span class="line"># 训练集和测试集的数据每行的格式为 (y,[x1,x2,x3]),其中y是标签，x1,x2,x3是特征。</span><br><span class="line"># 训练集中数据更新时，测试集目录就会出现预测值。并且数据越多，预测越准确</span><br><span class="line">trainingData &#x3D; ssc.textFileStream(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;streaming_linear_regression&#x2F;data&quot;).map(parse).cache()</span><br><span class="line">testData &#x3D; ssc.textFileStream(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;streaming_linear_regression&#x2F;predict&quot;).map(parse)</span><br><span class="line">#创建 权值为0的初始化模型</span><br><span class="line">numFeatures&#x3D; 3</span><br><span class="line">model &#x3D; StreamingLinearRegressionWithSGD()</span><br><span class="line">model.setInitialWeights([0.0,0.0,0.0])</span><br><span class="line"># 为训练流和测试流登记，并启动作业</span><br><span class="line">model.trainOn(trainingData)</span><br><span class="line">mylog.append(model.predictOnValues(testData.map(lambda lp:(lp.label,lp.features))))</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()&lt;&#x2F;pre&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>大数据：spark mllib决策树</title>
    <url>/2016/10/02/spark-mllib-desciontree/</url>
    <content><![CDATA[<h1 id="一-基本算法"><a href="#一-基本算法" class="headerlink" title="一 基本算法"></a>一 基本算法</h1><p>&emsp;&emsp;决策树是一个在特征空间递归执行二分类的贪心算法。决策树预测所有叶子节点分区的标签。为了在树的每个节点最大化信息增益，其每个分区都是基于贪心策略从可能分裂集合里选择一个最佳分裂(split)。也即，每个数节点分裂的选择是从集合 $argmaxIG(D,s)$，其中$IG(D,s)$是信息增益，而s是应用到数据集D上的分裂。</p>
<h2 id="二-节点不纯度和信息增益"><a href="#二-节点不纯度和信息增益" class="headerlink" title="二 节点不纯度和信息增益"></a>二 节点不纯度和信息增益</h2><p>节点不纯度是用以衡量节点同质化标签的度量。当前为了分类提供两种不纯度方法（Gini impurity和信息熵），为回归提供了一个不纯度度量（方差）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>impurity(不纯度)</th>
<th>作业</th>
<th>公式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gini impurity</td>
<td>分类</td>
<td>$\sum_{i=1}^cf_i(1-f_i)$</td>
<td>$f_i$ 是某个节点上标签为i的频率,C是标签数据</td>
</tr>
<tr>
<td>信息熵</td>
<td>分类</td>
<td>$\sum_{i=1}^c-f_i\log(f_i)$</td>
<td>$f_i $是某个节点上标签为i的频率,C是标签数据</td>
</tr>
<tr>
<td>方差</td>
<td>回归</td>
<td>$\frac{1}{N}\sum_{i=1}^N(y_i-\mu)^2$</td>
<td>$y<em>i$是某个数据实例的标签，N是数据实例的总数，$\mu$是由$\frac{1}{N}\sum</em>{i=1}^Ny_i$均值</td>
</tr>
</tbody>
</table>
</div>
<p>信息增益是衡量父母节点的不纯度与两个孩子节点不纯度权值求和的差异。假设一个分裂$s$将数据集 D(包含N个元素)分裂成两个子集合 $D<em>{left}$（包含$N</em>{left}$个元素）和 $D<em>{right}$ (包含 $N</em>{right}$ )，相应的信息增益是:</p>
<script type="math/tex; mode=display">

IG(D,s) = Impurity(D)-\frac{N_{left}}{N}Impurity(D_{left})-\frac{N_{right}}{N}Impurity(D_{right})</script><h2 id="三-分裂候选"><a href="#三-分裂候选" class="headerlink" title="三 分裂候选"></a>三 分裂候选</h2><h3 id="3-1-连续特征"><a href="#3-1-连续特征" class="headerlink" title="3.1 连续特征"></a>3.1 连续特征</h3><p>对于小数据集在单机上的实现，对每个连续特征来说其分裂候选一般是该特征的唯一值。有些实现会将特征值排序然后使用排序后的唯一值作为分裂候选以达到更快的计算速度。<br>&emsp;&emsp;对于大规模分布式数据来说排序的特征值是代价高昂的。通过对部分抽样数据进行位数计算来近似计算其分裂候选，以此来实现排序。排序后的分裂会创建“分箱”，可以通过参数<strong><em>maxBins</em></strong>来指定最大分箱数。<br>&emsp;&emsp;注意，分箱数目可以比数据实例数目大（这种情况比较少见，由于默认的<strong><em>maxBins</em></strong>是32）。如果分裂时条件不满足了，决策树会自动减少分箱数目。</p>
<h3 id="3-2-分类特征"><a href="#3-2-分类特征" class="headerlink" title="3.2 分类特征"></a>3.2 分类特征</h3><p>对于一个分类特征，有M个可能的取值（类别），可能会有$2^{M-1}-1$个分裂候选。对于二分类(0/1)和回归，我们可以通过对类别特征排序（用平均标签）将分裂候选减少至<strong>M-1</strong>。例如对于某个二分类问题，1个类别特征，3个分类A,B,C，相应的标签为1的比例为0.2,0.6,0.4，类别特征排序为A,C,B。两个分裂候选是A|C,B和A,C|B，其中竖线代表分裂。<br>&emsp;&emsp;在多分类中，所有的$2^{M-1}-1$个可能的分裂无论何时都可能会被使用到。如果$2^{M-1}-1$比参数<strong><em>maxBins</em></strong>大，使用一个与二分类和回归分析中类似的启发式方法。<strong><em>M</em></strong>个类别特征都是根据不纯度排序的。</p>
<h2 id="四-停止规则"><a href="#四-停止规则" class="headerlink" title="四 停止规则"></a>四 停止规则</h2><p>递归的构建树过程会在某个节点满足以下条件时停止：</p>
<ol>
<li><p>树深度已经等于训练参数<strong><em>maxDepth</em></strong>。</p>
</li>
<li><p>分裂候选产生的信息增益都小于参数<strong><em>minInfoGain</em></strong>。</p>
</li>
<li><p>分裂候选已经不能产生孩子节点，满足每个孩子节点有至少<strong><em>minInstancePerNode</em></strong>训练集实例。</p>
</li>
</ol>
<h2 id="五-参数设置问题"><a href="#五-参数设置问题" class="headerlink" title="五 参数设置问题"></a>五 参数设置问题</h2><p>以下参数需要设置但不需要调节。</p>
<ol>
<li><p><strong>algo</strong>：分类还是回归。</p>
</li>
<li><p><strong>numClass</strong>:分类的类别数目（只对分类）</p>
</li>
<li><p><strong>categoricalFeaturesInfo</strong>：设置哪些特征是类别以及每个这些特征值可以取多少类别值。此参数以map的形式给出，所有不在这个map中的特征都会被视为连续的。map的取值示例如下:</p>
</li>
</ol>
<ul>
<li><p>Map(0-&gt;2,4-&gt;10….) 指明，特征0 是二分类（取值为0或1），特征4有10个类别（取值是0-9）</p>
</li>
<li><p><strong>注意</strong>：你并不需要配置 <em>categoricalFeaturesInfo</em>。算法依然会运行并给出不错的结果，然而如果可特征化的值设计得很好，算法可以有更好的性能。</p>
</li>
</ul>
<h2 id="六-停止标准"><a href="#六-停止标准" class="headerlink" title="六 停止标准"></a>六 停止标准</h2><p>&emsp;&emsp;这些参数决定算法何时停止（增加节点），调节以下参数时，注意在测试数据集上验证并避免过拟合。</p>
<ul>
<li><p><strong>maxDepth</strong>:树的最大深度。越深的树（可能会获取更高的准确率）计算代价越高，但是它们也更耗时同时更可能过拟合。</p>
</li>
<li><p><strong>minInstancesPerNode</strong>:对于一个可能会进一步分裂的节点，它的子节点必须有至少这么多个训练实例数据。此参数一般和随机森林一起使用，因为这些会比单独的树要训练得更深。</p>
</li>
<li><p><strong>minInfoGain</strong>:对于可能会进一步分裂的节点，分裂必须增加这么多信息增益。</p>
</li>
</ul>
<h2 id="七-调节参数"><a href="#七-调节参数" class="headerlink" title="七 调节参数"></a>七 调节参数</h2><p>&emsp;&emsp;这些参数可以调节，但是注意在测试数据集上验证并避免过拟合。</p>
<ul>
<li><strong>maxBins</strong>:离散化连续型变量时使用的分箱数。增加 <strong>maxBins</strong>使得算法考虑更多的分裂候选并产生更细粒度的分裂决策，然而会增加计算消耗和组件间沟通成本。注意：对于任何可类别话的特征，参数<strong>maxBins</strong>必须至少是类别<strong>M</strong>最大值。</li>
<li><strong>maxMemoryInMB</strong>:进行统计时使用的内存量。默认值保守取到256MB，足以使得决策树在大多数场景适用。增大此参数可以减少数据传输让训练过程更快。<br>实现细节：为了更快的处理速度，决策树算法收集每组会分裂的节点的统计数据（而不是一次一个节点）。能放入一个组中处理的节点是由内存需求决定的（不同的特征不同）。参数<strong>maxMemoryInMB</strong>配置了每个使用这些统计的worker的内存限制。</li>
<li><strong>subsamplingRate</strong>:学习决策树的训练数据集比例。这个参数大多用在训练树的集合（随机森林、GradientBoostedTrees（渐变提振树））中，用以在袁术数据集中抽样数据。在单个决策树中，此参数并没有那么重要，因为训练数据并不是最大的限制。</li>
<li><strong>impurity</strong>:在分裂候选中筛选衡量不纯度的参数，这个参数必须与<strong>algo</strong>参数相对应。</li>
</ul>
<h2 id="八-缓存和检查点"><a href="#八-缓存和检查点" class="headerlink" title="八 缓存和检查点"></a>八 缓存和检查点</h2><p>当参数<strong>maxDepth</strong>设置得很大时，有必要开启节点ID缓存和检查点。在随机森林中，如何参数<strong>numTrees</strong>设置得很大时，也比较有用。</p>
<ul>
<li><strong>useNodeIdCache</strong>：如何此参数设置为<em> ture</em>，算法将会避免在每次迭时传入当前模型（tree ,trees）。算法默认会让当前模型与executors交流，使得executors每个树节点能够达到训练实例要求。当开启此参数时，算法将会缓存这部分信息。</li>
</ul>
<p>节点ID缓存会生成一些RDD（每次迭代时生成一个）。这种很长的lineage(血缘)会导致性能问题，但是检查点中间RDD可以缓和这些问题，<strong>注意</strong>只有当<strong><em>useNodeIdCache</em></strong>设置为<strong><em>true</em></strong>检查点才可用。</p>
<ul>
<li><strong>checkpointDir</strong>:节点ID缓存RDD的检查点目录。</li>
<li><strong>checkpointInteral</strong>:节点ID缓存RDD的频率，设置的过小会导致过量的写入HDFS，设置得太大时会使得executors失败并需要重新计算时等待太长。</li>
</ul>
<h2 id="九-代码实例"><a href="#九-代码实例" class="headerlink" title="九 代码实例"></a>九 代码实例</h2><p>以下代码展示了如何载入一个<strong>LIBSVM</strong>数据文件，解析成一个<strong>LabeledPoint</strong>RDD，然后使用决策树，使用Gini不纯度作为不纯度衡量指标，最大树深度是5.测试误差用来计算算法准确率。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">测试决策树</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import logging</span><br><span class="line">from pyspark.mllib.tree import DecisionTree,DecisionTreeModel</span><br><span class="line">from pyspark.mllib.util import MLUtils</span><br><span class="line"></span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line"></span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"></span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br><span class="line"></span><br><span class="line">#conf.set(&quot;spark.executor.memory&quot;, &quot;1g&quot;)</span><br><span class="line">#conf.set(&quot;spark.python.worker.memory&quot;, &quot;1g&quot;)</span><br><span class="line">conf.setMaster(&quot;yarn-client&quot;)</span><br><span class="line">conf.setAppName(&quot;TestDecisionTree&quot;)</span><br><span class="line">logger &#x3D; logging.getLogger(&#39;pyspark&#39;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line"></span><br><span class="line">mylog &#x3D; []</span><br><span class="line">#载入和解析数据文件为 LabeledPoint RDDdata &#x3D; MLUtils.loadLibSVMFile(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;&quot;)</span><br><span class="line">#将数据拆分成训练集合测试集</span><br><span class="line">(trainingData,testData) &#x3D; data.randomSplit([0.7,0.3])</span><br><span class="line"></span><br><span class="line">##训练决策树模型</span><br><span class="line">#空的 categoricalFeauresInfo 代表了所有的特征都是连续的</span><br><span class="line">model &#x3D; DecisionTree.trainClassifier(trainingData, numClasses&#x3D;2,categoricalFeaturesInfo&#x3D;&#123;&#125;,impurity&#x3D;&#39;gini&#39;,maxDepth&#x3D;5,maxBins&#x3D;32)</span><br><span class="line"></span><br><span class="line"># 在测试实例上评估模型并计算测试误差</span><br><span class="line"></span><br><span class="line">predictions &#x3D; model.predict(testData.map(lambda x:x.features))</span><br><span class="line">labelsAndPoint &#x3D; testData.map(lambda lp:lp.label).zip(predictions)</span><br><span class="line">testMSE &#x3D; labelsAndPoint.map(lambda (v,p):(v-p)**2).sum()&#x2F;float(testData.count())</span><br><span class="line">mylog.append(&quot;测试误差是&quot;)</span><br><span class="line">mylog.append(testMSE)</span><br><span class="line"></span><br><span class="line">#存储模型</span><br><span class="line"></span><br><span class="line">model.save(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;&quot;)</span><br><span class="line">sc.parallelize(mylog).saveAsTextFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;log&quot;)</span><br><span class="line">sameModel &#x3D; DecisionTreeModel.load(sc,&quot;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;&quot;)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：参数正则化参考资料</title>
    <url>/2016/10/01/regular-attachment/</url>
    <content><![CDATA[<h2 id="Hessian矩阵"><a href="#Hessian矩阵" class="headerlink" title="Hessian矩阵"></a>Hessian矩阵</h2><p> 是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。若多元函数在临界点处一阶导数为0，此时仅仅通过一阶导数无法判断是极大值还是极小值，此时通过Hessian矩阵</p>
<ul>
<li><p>若$H(m)$是正定矩阵，则M处为局部极大值。</p>
</li>
<li><p>若$H(m)$是负定矩阵，则M处为局部极小值。</p>
</li>
<li><p>若$H(m)$是不定矩阵，则M处不是极值。</p>
</li>
</ul>
<h2 id="泰勒级数"><a href="#泰勒级数" class="headerlink" title="泰勒级数"></a>泰勒级数</h2><p>泰勒级数（英语：Taylor series）用无限项连加式——级数来表示一个函数，这些相加的项由函数在某一点的导数求得。如果  在点 $x=x_0$ 具有任意阶导数，则幂级数</p>
<script type="math/tex; mode=display">
\sum_{n=0}^{\infty}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0))}{2!}(x-x_0)^2+...</script><p>称为在点 $x_0$ 处的泰勒级数</p>
<h2 id="拉格朗日乘子和KKT条件"><a href="#拉格朗日乘子和KKT条件" class="headerlink" title="拉格朗日乘子和KKT条件"></a>拉格朗日乘子和KKT条件</h2><p>在求取有约束条件的优化问题时，拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是非常重要的两个求取方法，对于等式约束的优化问题，可以应用拉格朗日乘子法去求取最优值；如果含有不等式约束，可以应用KKT条件去求取。当然，这两个方法求得的结果只是必要条件，只有当是凸函数的情况下，才能保证是充分必要条件。KKT条件是拉格朗日乘子法的泛化。之前学习的时候，只知道直接应用两个方法，但是却不知道为什么拉格朗日乘子法（Lagrange Multiplier) 和KKT条件能够起作用，为什么要这样去求取最优值呢？</p>
<p>下面将首先把什么是拉格朗日乘子法（Lagrange Multiplier) 和KKT条件叙述一下；然后开始分别谈谈为什么要这样求最优值。</p>
<h3 id="一-拉格朗日乘子法（Lagrange-Multiplier-和KKT条件"><a href="#一-拉格朗日乘子法（Lagrange-Multiplier-和KKT条件" class="headerlink" title="一. 拉格朗日乘子法（Lagrange Multiplier) 和KKT条件"></a>一. 拉格朗日乘子法（Lagrange Multiplier) 和KKT条件</h3><p>通常我们需要求解的最优化问题有如下几类：</p>
<p>1.无约束优化问题，可以写为:</p>
<script type="math/tex; mode=display">
                    min f(x);</script><p>2.有等式约束的优化问题，可以写为:</p>
<script type="math/tex; mode=display">

                    min f(x), \\
                    s.t. h_i(x) = 0;i =1, ..., n</script><p>3.有不等式约束的优化问题，可以写为：</p>
<script type="math/tex; mode=display">
    min f(x), \\
    s.t. g_i(x) <= 0; i =1, ..., n\\
    h_j(x) = 0; j =1, ..., m</script><p>对于第(1)类的优化问题，常常使用的方法就是Fermat定理，即使用求取f(x)的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。</p>
<p>对于第(2)类的优化问题，常常使用的方法就是拉格朗日乘子法（Lagrange Multiplier) ，即把等式约束 $h_i(x)$ 用一个系数与 $f(x)$ 写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。</p>
<p>对于第(3)类的优化问题，常常使用的方法就是KKT条件。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的必要条件，这个条件称为KKT条件。</p>
<ul>
<li><p><strong>拉格朗日乘子法（Lagrange Multiplier):</strong> 对于等式约束，我们可以通过一个拉格朗日系数a 把等式约束和目标函数组合成为一个式子 $L(a, x) = f(x) + a*h(x)$, 这里把a和 $h(x)$ 视为向量形式，a是横向量，h(x)为列向量。然后求取最优值，可以通过对 $L(a,x)$ 对各个参数求导取零，联立等式进行求取，这个在高等数学里面有讲，但是没有讲为什么这么做就可以，在后面，将简要介绍其思想。</p>
</li>
<li><p><strong>KKT条件:</strong> 对于含有不等式约束的优化问题，如何求取最优值呢？常用的方法是KKT条件，同样地，把所有的不等式约束、等式约束和目标函数全部写为一个式子 $L(a,b,x)=f(x)+a<em>g(x)+b</em>h(x)$ ，KKT条件是说最优值必须满足以下条件：</p>
<ol>
<li><p>$L(a, b, x)$ 对 $x$ 求导为零</p>
</li>
<li><p>$h(x) =0$</p>
</li>
<li><p>$a*g(x) = 0$</p>
</li>
</ol>
<p>求取这三个等式之后就能得到候选最优值。其中第三个式子非常有趣，因为$g(x)&lt;=0$，如果要满足这个等式，必须$a=0$或者$g(x)=0$ . 这是SVM的很多重要性质的来源，如支持向量的概念。</p>
</li>
</ul>
<h3 id="二-为什么拉格朗日乘子法（Lagrange-Multiplier-和KKT条件能够得到最优值？"><a href="#二-为什么拉格朗日乘子法（Lagrange-Multiplier-和KKT条件能够得到最优值？" class="headerlink" title="二. 为什么拉格朗日乘子法（Lagrange Multiplier) 和KKT条件能够得到最优值？"></a>二. 为什么拉格朗日乘子法（Lagrange Multiplier) 和KKT条件能够得到最优值？</h3><p>为什么要这么求能得到最优值？先说拉格朗日乘子法，设想我们的目标函数 $z = f(x)$, x是向量, z取不同的值，相当于可以投影在x构成的平面（曲面）上，即成为等高线，如下图，目标函数是f(x, y)，这里x是标量，虚线是等高线，现在假设我们的约束 $g(x)=0$，x是向量，在x构成的平面或者曲面上是一条曲线，假设g(x)与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值，但肯定不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小，只有到等高线与目标函数的曲线相切的时候，可能取得最优值，如下图所示，即等高线和目标函数的曲线在该点的法向量必须有相同方向，所以最优值必须满足：$f(x)的梯度 = a* g(x)$ 的梯度，a是常数，表示左右两边同向。这个等式就是$L(a,x)$ 对参数求导的结果。</p>
<p><img src="/images/blog/拉格朗日1.jpg" alt="拉格朗日"></p>
<p>而KKT条件是满足强对偶条件的优化问题的必要条件，可以这样理解：我们要求</p>
<script type="math/tex; mode=display">
min f(x), L(a, b, x) = f(x) + a\times g(x) + b\times h(x)，a>=0，</script><p>我们可以把f(x)写为：$max<em>{a,b} L(a,b,x)$ 为什么呢？因为 $h(x)=0, g(x)&lt;=0$ ，现在是取 $L(a,b,x)$ 的最大值, $ag(x)\le0$ ,所以 $L(a,b,x)$ 只有在 $a\times g(x)=0$ 的情况下才能取得最大值，否则，就不满足约束条件，因此 $max</em>{a,b}L(a,b,x)$ 在满足约束条件的情况下就是f(x)，因此我们的目标函数可以写为 $min<em>xmax</em>{a,b}L(a,b,x)$ 。如果用对偶表达式： $max_{a,b}min_xL(a,b,x)$ ，由于我们的优化是满足强对偶的（强对偶就是说对偶式子的最优值是等于原问题的最优值的），所以在取得最优值x0的条件下，它满足</p>
<script type="math/tex; mode=display">
f(x0) = max_{a,b} min_x  L(a,b,x) = min_x max_{a,b} L(a,b,x) =f(x0)，</script><p>我们来看看中间两个式子发生了什么事情：</p>
<script type="math/tex; mode=display">
 f(x0) = max_{a,b} min_x  L(a,b,x) \\
 =  max_{a,b} min_x f(x) + a\times g(x) + b\times h(x) \\
 =  max_{a,b} f(x0)+a\times g(x0)+b\times h(x0) = f(x0)</script><p>可以看到上述本质上是说 $min_xf(x)+a\times g(x)+b\times h(x)$ 在 $x_0$ 取得了最小值，用fermat定理，即是说对于函数 $f(x)+a\times g(x)+b\times h(x)$，求取导数要等于零，即 $f(x)的梯度+a\times g(x)的梯度+b*h(x)的梯度 = 0$</p>
<p>这就是kkt条件中第一个条件：$L(a, b, x)$ 对 $x$ 求导为零。</p>
<p>而之前说明过，$a*g(x) = 0$ ，这是kkt条件的第2个条件，当然已知的条件 $h(x)=0$ 必须被满足，所有上述说明，满足强对偶条件的优化问题的最优值都必须满足KKT条件，即上述说明的三个条件。可以把KKT条件视为是拉格朗日乘子法的泛化。</p>
<h2 id="适定、超定和欠定方程的概念"><a href="#适定、超定和欠定方程的概念" class="headerlink" title="适定、超定和欠定方程的概念"></a>适定、超定和欠定方程的概念</h2><p>矩阵的每一行代表一个方程，m行代表m个线性联立方程。 n列代表n个变量。如果m是独立方程数，根据m<n、m=n、m>n确定方程是 <code>欠定</code>、<code>适定</code> 还是 <code>超定</code>。</p>
<h3 id="超定方程组"><a href="#超定方程组" class="headerlink" title="超定方程组"></a>超定方程组</h3><p> 即方程个数大于未知量个数的方程组。</p>
<p> 对于方程组Ra=y，R为n×m矩阵，如果R列满秩，且n&gt;m</p>
<p> 超定方程一般是不存在解的矛盾方程。</p>
<p> 例如，如果给定的三点不在一条直线上， 我们将无法得到这样一条直线，使得这条直线同时经过给定这三个点。 也就是说给定的条件（限制）过于严格， 导致解不存在。在实验数据处理和曲线拟合问题中，求解超定方程组非常普遍。比较常用的方法是最小二乘法。形象的说，就是在无法完全满足给定的这些条件的情况下，求一个最接近的解。</p>
<p> 曲线拟合的最小二乘法要解决的问题，实际上就是求以上超定方程组的最小二乘解的问题。</p>
<h3 id="欠定方程组"><a href="#欠定方程组" class="headerlink" title="欠定方程组:"></a>欠定方程组:</h3><p> 即方程个数小于未知量个数的方程组。<br> 对于方程组Ra=y，R为n×m矩阵，且n&lt;m。则方程组有无穷多组解，此时称方程组为欠定方程组。<br> 内点法和梯度投影法是目前解欠定方程组的常用方法。</p>
<h2 id="奇异矩阵"><a href="#奇异矩阵" class="headerlink" title="奇异矩阵"></a>奇异矩阵</h2><p><strong>概念</strong>：奇异矩阵是线性代数的概念，就是对应的行列式等于0的方阵</p>
<p><strong>判断方法</strong>：首先，看这个矩阵是不是方阵（即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上奇异矩阵和非奇异矩阵）。 然后，再看此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵；若不等于0，称矩阵A为非奇异矩阵。 同时，由|A|≠0可知矩阵A可逆，这样可以得出另外一个重要结论:可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵。　如果A为奇异矩阵，则AX=0有无穷解，AX=b有无穷解或者无解。如果A为非奇异矩阵，则AX=0有且只有唯一零解，AX=b有唯一解</p>
<h2 id="多元线性回归参数估计问题"><a href="#多元线性回归参数估计问题" class="headerlink" title="多元线性回归参数估计问题"></a>多元线性回归参数估计问题</h2><h3 id="回归参数的最小二乘估计"><a href="#回归参数的最小二乘估计" class="headerlink" title="回归参数的最小二乘估计"></a>回归参数的最小二乘估计</h3><p>对于含有k个解释变量的多元线性回归模型</p>
<script type="math/tex; mode=display">
 Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+...+\beta_kX_{ki}+\mu_i</script><p>设 $\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k$ 分别作为参数 $\beta_0,\beta_1,\beta_2,…\beta_k$ 的估计量，得样本回归方程为：</p>
<script type="math/tex; mode=display">
   Y_i=\bar \beta_0+\bar \beta_1X_{1i}+\bar \beta_2X_{2i}+...+\bar \beta_kX_{ki}</script><p>观测值 $Y_i$ 与回归值 $\bar Y_i$ 的残差 $e_i$ 为：</p>
<script type="math/tex; mode=display">
  e_i=Y_i-\bar Y_i=Y_i-(\bar \beta_0+\bar \beta_1X_{1i}+\bar \beta_2X_{2i}+...+\bar \beta_kX_{ki})</script><p>由最小二乘法可知 $\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k$ 应使全部观测值$Y_i$ 与回归值 $\bar Y_i$ 的残差 $e_i$ 的平方和最小，即使</p>
<script type="math/tex; mode=display">
  Q(\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k)=\sum e^2_i=\sum(Y-\bar Y_i)^2\\
  =\sum(Y_i-\bar \beta _0-\bar \beta_1X_{1i}-\bar \beta_2X_{2i}-...\bar \beta_kX_{ki})</script><p>取得最小值。令 $Y=(Y_i,Y_1,Y_2,..Y_n),X=(x_0,x_1,…x_n),\bar B=(\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k)$  ,上式可写为</p>
<script type="math/tex; mode=display">
Q((\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k)=(Y-\bar BX)^T(Y-\bar BX)\\
=(Y^TY-Y^TX\bar B-\bar BX^TY+\bar B^TX^TX\bar B)\\
=Y^TY-2\bar B^TX^TY+\bar B^TX^TX\bar B</script><p>根据多元函数的极值原理，$Q$ 分别对 $\bar \beta_0,\bar \beta_2,\bar \beta_2,..\bar \beta_k$ 求一阶偏导，并令其等于零，即</p>
<script type="math/tex; mode=display">
 \frac{\partial Q}{\partial \bar \beta _j}=0(j=0,1,2...k)</script><p>即 $-X^TY+X^TX\bar B=0$ 。则为向量 $B$ 的估计量为</p>
<script type="math/tex; mode=display">
  \bar B= (X^TX)^{-1}X^TY</script><h2 id="几何概率"><a href="#几何概率" class="headerlink" title="几何概率"></a>几何概率</h2><p>  几何概率(<em>geometric probability</em>)可以用几何方法向某一可度量的区域求得的概率。向区域内投一质点，如果所投的点落在域<em>O</em>中任意区域<em>g</em>内的可能性大小与<em>g</em>的度量成正比，而与<em>g</em>的具体形状无关，则称这个随机试验为几何型随机试验或几何概率。此处的度量就是测度，一维是长度，二维是面积，三维是体积。对于几何概率，若记 $A = 质点落在区域g内$ 这一事件，则其概率定义为</p>
<script type="math/tex; mode=display">
    P(A) = \frac{L(g)}{L(\Omega)}</script><p> 其中 $L(g)$ 和 $L(\Omega)$ 分别为<em>g</em>和<em>O</em>的度量。这一类概率是古典概率定义的推广，它保留了等可能性，但将有限个基本事件推广到了无限。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>使用pyspark做数据挖掘</title>
    <url>/2016/09/26/pyspark-moiverecommand/</url>
    <content><![CDATA[<h2 id="一-环境准备"><a href="#一-环境准备" class="headerlink" title="一 环境准备"></a>一 环境准备</h2><h3 id="1-1-编程环境"><a href="#1-1-编程环境" class="headerlink" title="1.1 编程环境"></a>1.1 编程环境</h3><pre><code>必须加入spark内容，将以下代码加入推荐逻辑之前   
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip&quot;)</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)</span><br></pre></td></tr></table></figure>
<p>各个参数视个人机器配置而定</p>
<h3 id="1-2-本地模式"><a href="#1-2-本地模式" class="headerlink" title="1.2  本地模式"></a>1.2  本地模式</h3><pre><code>在本地模式运行时，参数需要如下设定
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conf.setMaster(&quot;client&quot;)</span><br><span class="line">此时代码会在本地执行，对于小数据集（小于1MB的数据）可以正常执行，如果数据超过1MB将会遇到以下问题：</span><br><span class="line">ERROR PythonRDD: Python worker exited unexpectedly (crashed)</span><br><span class="line">java.net.SocketException: Connection reset by peer: socket write error</span><br><span class="line">        at java.net.SocketOutputStream.socketWrite0(Native Method)</span><br><span class="line">解决办法参考“pyspark处理大数据集” 这篇笔记</span><br></pre></td></tr></table></figure>
<h3 id="1-3-数据准备"><a href="#1-3-数据准备" class="headerlink" title="1. 3 数据准备"></a>1. 3 数据准备</h3><pre><code>数据都是存放在HDFS上，需要先将数据上传到个人目录。
</code></pre><h2 id="二-数据挖掘视角"><a href="#二-数据挖掘视角" class="headerlink" title="二 数据挖掘视角"></a>二 数据挖掘视角</h2><pre><code>接下来进入数据挖掘的思路，数据挖掘标准流程如下：
</code></pre><ul>
<li><p>数据收集（本案例中数据已经准备好）</p>
</li>
<li><p>数据清洗转换</p>
</li>
<li><p>根据数据选择算法模型（本案例以推荐算法为例）</p>
</li>
<li><p>训练模型：使用训练数据训练算法模型中一些参数。</p>
</li>
<li><p>使用模型：使用训练好的模型预测或者对检验数据分类、聚类</p>
</li>
<li><p>评估模型：验证模型预测结果与真实结果误差，评估准确率、召回率等指标</p>
</li>
</ul>
<h3 id="2-1-收集数据"><a href="#2-1-收集数据" class="headerlink" title="2.1 收集数据"></a>2.1 收集数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataset_path &#x3D; os.path.join(&#39;&#x2F;home&#x2F;xiatao&#x2F;machine_learing&#x2F;moive_recommend&#x2F;&#39;,&#39;&#39;)</span><br><span class="line">complete_dataset_path &#x3D;   os.path.join(dataset_path,&#39;ml-latest.zip&#39;)</span><br><span class="line">small_dataset_path &#x3D; os.path.join(dataset_path,&#39;ml-latest-small.zip&#39;)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-数据清洗转换"><a href="#2-2-数据清洗转换" class="headerlink" title="2.2 数据清洗转换"></a>2.2 数据清洗转换</h3><p>   本示例中数据清洗和转换比较简单，只是去掉数据头，并将数据封装成RDD。在其他数据中可能需要去除部分没用的列，数据降维，连续型数据转换为离散型等操作。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 载入数据，将数据的头过滤出来</span><br><span class="line">small_rating_file &#x3D; os.path.join(dataset_path,&#39;latest_small&#39;,&#39;ratings.csv&#39;)</span><br><span class="line">small_rating_raw_data &#x3D; sc.textFile(small_rating_file)</span><br><span class="line">small_rating_raw_data_header &#x3D; small_rating_raw_data.take(1)[0]</span><br><span class="line"># 将原始数据封装成新的RDD</span><br><span class="line">small_rating_data  &#x3D; small_rating_raw_data.filter(lambda line:line!&#x3D;small_rating_raw_data_header)\</span><br><span class="line">    .map(lambda line:line.split(&quot;,&quot;)).map(lambda tokens:(tokens[0],tokens[1],tokens[2])).cache()</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-根据数据选择算法模型"><a href="#2-3-根据数据选择算法模型" class="headerlink" title="2.3 根据数据选择算法模型"></a>2.3 根据数据选择算法模型</h3><pre><code>本示例以推荐算法中的ALS（最小交替二乘法）为例，关于交替二乘法参考 https://www.zhihu.com/question/31509438
</code></pre><h3 id="2-4-训练模型"><a href="#2-4-训练模型" class="headerlink" title="2.4  训练模型"></a>2.4  训练模型</h3><pre><code>在开始之前，我们先将数据分为三份，分别是training_rdd（训练数据） ,validation_rdd（验证数据）,test_rdd（测试数据）。使用training_rdd获得一个训练模型，然后去预测validation_rdd结果，并计算训练模型对validation_rdd预测结果与validation_rdd真实结果之间误差，以此来决定模型应该使用的参数。
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用小数据集选择 交叉最小二乘法参数</span><br><span class="line"># 首先将数据分为训练数据，校验数据，测试数据</span><br><span class="line">training_rdd ,validation_rdd,test_rdd &#x3D; small_rating_data.randomSplit([6,2,2],seed&#x3D;0L)</span><br><span class="line">validation_for_predict_rdd &#x3D; validation_rdd.map(lambda x:(x[0],x[1]))</span><br><span class="line">test_for_predict_rdd &#x3D;test_rdd.map(lambda x:(x[0],x[1]))</span><br></pre></td></tr></table></figure>
<p> 推荐算法ALS中最重要的比较重要的参数有如下：</p>
<ul>
<li><p>rank：特征向量秩大小，越大的秩会得到更好的模型，但是计算消耗也相应增加。默认是 10</p>
</li>
<li><p>iteration： 算法迭代次数（默认是10）</p>
</li>
<li><p>lambda：正则参数，默认是 0.01。详细解释参考</p>
</li>
<li><p>alpha：在隐式ALS中用于计算置信度的常量，默认为1.0</p>
</li>
<li><p>numUserBlocks,numProductBlocks：将用户和产品数据分解的块数目，用来控制并行度；你可以传入-1来让MLlib自动决定。</p>
<p>   本示例中，算法迭代次数固定为10（可以根据实际情况调整），lambda参数固定位 0.1 ，由于本文的电影评分数据是确信数据，使用的ALS是确定模型，因此不需要alpha参数，numUserBlocks和numProductBlocks参数使用默认参数。<br>  因此，本示例中需要训练的参数是 rank，不同的rank会影响模型的预测准确度，不同的模型其预测误差可以通过均方根误差（标准方差RMSE）来衡量优劣。选取均方根误差最小的rank作为预测模型的rank。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">seed &#x3D;5L</span><br><span class="line">iterations &#x3D;10</span><br><span class="line">regularization_parmeter &#x3D;0.1</span><br><span class="line">ranks &#x3D;[4,5,6,7,8,10,12]</span><br><span class="line">errors &#x3D; [0,0,0,0,0,0,0]</span><br><span class="line">err &#x3D;0</span><br><span class="line">min_error &#x3D; float(&#39;inf&#39;)</span><br><span class="line">best_rank &#x3D;-1</span><br><span class="line">best_interation &#x3D;-1</span><br><span class="line">for rank in ranks:</span><br><span class="line">    model &#x3D; ALS.train(training_rdd,rank,seed&#x3D;seed,iterations&#x3D;iterations,lambda_&#x3D;regularization_parmeter)</span><br><span class="line">    predictions &#x3D; model.predictAll(validation_for_predict_rdd).map(lambda r:((r[0],r[1],r[2])))</span><br><span class="line">    rates_and_preds &#x3D;validation_rdd.map(lambda r:(int(r[0]),int(r[1]),float(r[2]))).join(predictions)</span><br><span class="line">    error &#x3D; math.sqrt(rates_and_preds.map(lambda r:(r[1][0]-r[1][1])**2).mean())</span><br><span class="line">    errors[err] &#x3D; error</span><br><span class="line">    err+&#x3D;1</span><br><span class="line">    print &#39;For rank %s the RMSE is %s&#39;%(rank,error)</span><br><span class="line">    if error &lt; min_error:</span><br><span class="line">        min_error &#x3D;error</span><br><span class="line">        best_rank &#x3D; rank</span><br></pre></td></tr></table></figure>
<h2 id="2-5-使用模型"><a href="#2-5-使用模型" class="headerlink" title="2.5 使用模型"></a>2.5 使用模型</h2><pre><code>使用训练数据中获得的最佳参数来构建新的推荐模型，本示例中使用完整数据集ml-lates数据集检验模型预测结果,为了检验模型准确率，我们将数据分为训练数据和验证数据两份，分别为training_complete_rdd(70%),test_complete_rdd (30%)
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#现在开始使用完整数据集来构建最终模型</span><br><span class="line">complete_rating_file &#x3D; os.path.join(dataset_path,&#39;latest_all&#39;,&#39;ratings.csv&#39;)</span><br><span class="line">complete_rating_raw_data &#x3D;sc.textFile(complete_rating_file)</span><br><span class="line">complete_rating_raw_data_header &#x3D; complete_rating_raw_data.take(1)[0]</span><br><span class="line">complete_rating_data &#x3D; complete_rating_raw_data.filter(lambda line:line!&#x3D;complete_rating_raw_data_header)\</span><br><span class="line">    .map(lambda line:line.split(&quot;,&quot;)).map(lambda tokens:(int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()</span><br><span class="line">#现在开始训练推荐模型</span><br><span class="line">training_complete_rdd,test_complete_rdd &#x3D; complete_rating_data.randomSplit([7,3],seed &#x3D;0L)</span><br><span class="line">complete_model &#x3D; ALS.train(training_complete_rdd,best_rank,seed &#x3D; seed,iterations&#x3D;\</span><br><span class="line">    iterations,lambda_ &#x3D;regularization_parmeter)</span><br></pre></td></tr></table></figure>
<h3 id="2-6-评估验证模型"><a href="#2-6-评估验证模型" class="headerlink" title="2.6 评估验证模型"></a>2.6 评估验证模型</h3><pre><code> 使用完整数据集中30%的部分来测试模型预测结果准确率。 
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#在测试数据集上测试</span><br><span class="line">test_for_predict_rdd &#x3D; test_complete_rdd.map(lambda x:(x[0],x[1]))</span><br><span class="line">predictions_complete &#x3D;complete_model.predictAll(test_for_predict_rdd).map(lambda r:((r[0],r[1],r[2])))</span><br><span class="line">rates_and_preds_complete &#x3D; test_complete_rdd.map(lambda r:((int(r[0]),int(r[1])),float(r[2]))).join(predictions_complete)</span><br><span class="line">error_complete &#x3D; math.sqrt(rates_and_preds_complete.map(lambda r: (r[1][0]-r[1][1]) **2).mean())</span><br><span class="line">mylog.append( &quot;完整数据集的误差是RMSE   %s&quot;%(error_complete))</span><br></pre></td></tr></table></figure>
<pre><code>此示例中只使用了平方根误差来评估模型。
</code></pre><h3 id="2-7-模型后续使用"><a href="#2-7-模型后续使用" class="headerlink" title="2.7 模型后续使用"></a>2.7 模型后续使用</h3><h4 id="2-7-1-给老用户（对部分电影有评分）推荐"><a href="#2-7-1-给老用户（对部分电影有评分）推荐" class="headerlink" title="2.7.1 给老用户（对部分电影有评分）推荐"></a>2.7.1 给老用户（对部分电影有评分）推荐</h4><pre><code>添加新数据，每次添加新数据都需要重新训练模型，此时将新数据与原数据合并再训练并得到模型。
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#添加新的用户评分，</span><br><span class="line">new_user_ID &#x3D; 0</span><br><span class="line">new_user_rating &#x3D;[</span><br><span class="line">    (0,260,9),</span><br><span class="line">    (0,1,8),</span><br><span class="line">    (0,16,7),</span><br><span class="line">    (0,25,8),</span><br><span class="line">    (0,32,9),</span><br><span class="line">    (0,335,4),</span><br><span class="line">    (0,379,4),</span><br><span class="line">    (0,296,4),</span><br><span class="line">    (0,854,10),</span><br><span class="line">    (0,50,8)</span><br><span class="line">]</span><br><span class="line">new_user_rating_RDD &#x3D; sc.parallelize(new_user_rating)</span><br><span class="line">mylog.append( &quot;新用户的评分是 %s&quot;%new_user_rating_RDD.take(10))</span><br><span class="line"># 将数据加入到推荐模型将使用的训练数据中，</span><br><span class="line">complete_data_with_new_rating_RDD &#x3D; complete_rating_data.union(new_user_rating_RDD)</span><br><span class="line"># 最后，使用前面选择的最优参数来训练ALS模型</span><br><span class="line">from time import time</span><br><span class="line">new_rating_model &#x3D; ALS.train(complete_data_with_new_rating_RDD,best_rank,seed &#x3D; seed,iterations&#x3D;iterations,lambda_ &#x3D;regularization_parmeter)</span><br></pre></td></tr></table></figure>
<p>再利用此模型向老用户推荐电影   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取最好的推荐。鉴于我们将获得新用户没有评分的RDD</span><br><span class="line">#获得电影ID</span><br><span class="line">new_users_ratings_ids &#x3D; map(lambda x:x[1],new_user_rating)</span><br><span class="line">#获得不在ID列表中的</span><br><span class="line">new_user_unrated_moive_RDD &#x3D; (complete_moive_data.filter(lambda x:x[0] not in new_users_ratings_ids)\</span><br><span class="line">                              .map(lambda x:(new_user_ID,x[0])))</span><br><span class="line"># 使用输入的RDD和 new_user_unrated_moive_RDD，使用 new_rating_mode.predictAll() 来预测电影</span><br><span class="line">new_user_recommendations_RDD &#x3D; new_rating_model.predictAll(new_user_unrated_moive_RDD)</span><br><span class="line">new_user_recommendations_rating_RDD &#x3D; new_user_recommendations_RDD.map(lambda x:(x.product,x.rating))</span><br><span class="line">new_user_recommendations_rating_title_and_count_RDD &#x3D;new_user_recommendations_rating_RDD.join(complete_moive_titles)\</span><br><span class="line">    .join(moive_rating_counts_RDD)</span><br><span class="line">new_user_recommendations_rating_title_and_count_RDD.take(3)</span><br><span class="line">top_moives &#x3D; new_user_recommendations_rating_title_and_count_RDD.map(lambda r:(r[1][0][1],r[1][0][0],r[1][1]))\</span><br><span class="line">    .filter(lambda r:r[2]&gt;&#x3D;25).takeOrdered(25,key&#x3D;lambda x:-x[1])</span><br><span class="line">mylog.append( &quot;推荐的电影（浏览量超过25的）%s&quot;%&#39;\n&#39;.join(map(str,top_moives)))</span><br></pre></td></tr></table></figure>
<h4 id="2-7-2-预测新用户对某部电影评分"><a href="#2-7-2-预测新用户对某部电影评分" class="headerlink" title="2.7.2  预测新用户对某部电影评分"></a>2.7.2  预测新用户对某部电影评分</h4><h2 id="三-模型的保存于复用"><a href="#三-模型的保存于复用" class="headerlink" title="三  模型的保存于复用"></a>三  模型的保存于复用</h2><pre><code> 可以将我们的模型存储起来作为后续的在线推荐系统使用，尽管每次有新的用户评分数据时都会生成新的模型，为了节省服务启动时间。当前的模型也是值得存储的。我们可以通过存储那些RDD以节省时间，尤其是那些需要消耗极大时间的。
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.recommendation import MatrixFactorizationModel</span><br><span class="line">model_path &#x3D; os.path.join(dataset_path,&#39;models&#39;,&#39;moive_lens_als&#39;)</span><br><span class="line">model.save(sc,model_path)</span><br><span class="line">same_model &#x3D; MatrixFactorizationModel.load(sc,model_path)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>大数据：spark mllib集成学习</title>
    <url>/2016/09/25/sparkmllib/</url>
    <content><![CDATA[<p>此文可以参考<a href="http://blog.jobbole.com/85408/" target="_blank" rel="noopener">如何在MLlib中实现随机森林和梯度提升树（GBTs）</a>一起阅读</p>
<h1 id="一-梯度提升树和随机森林"><a href="#一-梯度提升树和随机森林" class="headerlink" title="一  梯度提升树和随机森林"></a>一  梯度提升树和随机森林</h1><p>&emsp;&emsp;梯度提升树(Gradient-Boosted Trees，GBTs)和随机森林都是决策树的集成学习方法，但是训练过程不一。以下是两种之间的一些利弊：</p>
<ul>
<li>GBTs一次训练一颗树，所以它会比随机森林耗时更长。随机森林可以并行训练多颗树。<ul>
<li>另一方面，给GBTs使用比随机森林更小的树比较合理，训练更小的树耗时更少。</li>
</ul>
</li>
<li>随机森林更不易过拟合。训练更多的决策树可以减少随机森林过拟合风险，但会增加GBTs过拟合风险。</li>
<li>由于增加随机森林使用的决策树数目可以单调的提升性能，因而森林更容易调节。但对于GBTs，决策树数目过大时可能会导致性能的减弱。</li>
</ul>
<h1 id="二-随机森林"><a href="#二-随机森林" class="headerlink" title="二 随机森林"></a>二 随机森林</h1><p>在分类和回归中，随机森林是最成功的机器学习方法。它结合多颗决策树以减少过拟合风险。比如决策树，随机森林可以处理类别特征，如果不需要数据规范化（关于数据规范化，可以参考<a href="http://blog.csdn.net/memray/article/details/9023737" target="_blank" rel="noopener">数据规范化</a>,数据规范化的好处参考<br><a href="https://www.zhihu.com/question/37129350/answer/70964527" target="_blank" rel="noopener">为什么feature scaling会使 gradient desent收敛更好</a>）的话可以拓展到多分类，并且可以处理非线性和特征交互问题。<br><strong>spark.mllib</strong> 可以同时使用连续型数据和类别特征，为分类和逻辑回归提供二分类和多分类。直接使用了现有的决策树实现了随机森林。</p>
<h2 id="2-1-基本算法"><a href="#2-1-基本算法" class="headerlink" title="2.1 基本算法"></a>2.1 基本算法</h2><p>随机森林单独的训练集合中每一课决策树，因而可以并行执行。算法在训练过程中引入了随机性，使得每个决策树都不一样。结合每棵树的决策可以减少最终决策偏差，提高算法最终表现。</p>
<h2 id="2-2-训练数据"><a href="#2-2-训练数据" class="headerlink" title="2.2 训练数据"></a>2.2 训练数据</h2><p>随机森林算法中加入的随机性包括以下：</p>
<ul>
<li>每次迭代时从原始数据集中抽样部分数据，以保证每次的数据不同。</li>
<li>每次切分树节点时会考虑特征的随机子集。</li>
</ul>
<h2 id="2-3-预测"><a href="#2-3-预测" class="headerlink" title="2.3 预测"></a>2.3 预测</h2><p>&emsp;&emsp;为了在新数据上作出预测，随机森林需要从其决策树集合中合计出预测，这个过程在分类和回归中是完全不同的。</p>
<ul>
<li>分类：多数表决，每棵树的预测都会为某一个分类投一票，得票最多的分类即预测分类。</li>
<li>回归： 平均主义，每棵树预测值是一个实数，预测的分类为所有预测值的均值。</li>
</ul>
<h2 id="2-4-小提示"><a href="#2-4-小提示" class="headerlink" title="2.4 小提示"></a>2.4 小提示</h2><p>&emsp;&emsp;以下两个参数微调可以提高算法性能</p>
<ul>
<li><strong>numTrees</strong>:森林中的决策树数目。<ul>
<li>增加数数目可以减少预测偏差，提高模型的测试时间准确率。</li>
<li>训练时长会随着树数目增加而大致线性增长</li>
</ul>
</li>
<li><strong>maxDepth</strong>:森林中每棵树的最大深度<ul>
<li>增加深度会更强大的模型，同时会增加消耗。但是更深的树，训练时间更长，同时更容易产生过拟合问题。</li>
<li>通常来说，与单一决策树相比，随机森林总更适合训练更深的树。单一决策树更容易产生过拟合问题。</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;以下连个参数通常不需要调节，但是可以用来加速训练过程</p>
<ul>
<li><strong>subsample</strong>: 此参数用来设置随机森林中每棵树训练时使用的数据集大小，值为原始数据集比例。推荐默认值1.0，但是减少此值可以加速训练过程。</li>
<li><strong>featureSubsetStraegy</strong>: 每个树节点分裂候选的特征数。该值设置为分数或者关于总特征数的函数。减少此值可以加速训练过程，但是太低的话可能会影响性能。</li>
</ul>
<h2 id="2-5-代码示例"><a href="#2-5-代码示例" class="headerlink" title="2.5 代码示例"></a>2.5 代码示例</h2><p>以下代码演示了如何载入 <strong>LIBSVM data file</strong> ，将其解析成<strong>LabeledPoint</strong>类型的RDD，然后使用随机森林来分类。使用测试误差来衡量算法准确率</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：后向传播网络中使用的一些小tricks</title>
    <url>/2016/09/21/bp_tricks/</url>
    <content><![CDATA[<h2 id="使用mini-batches"><a href="#使用mini-batches" class="headerlink" title="使用mini-batches"></a>使用mini-batches</h2><ul>
<li><strong>batch学习:</strong>  对权重的每次迭代更新中，都需要对数据库中所有的样本过一遍，然后求解平均或者真实的梯度</li>
<li><strong>随机或者在线stochastic (online)学习</strong>:  每次从训练集中选择（例如，随机的）一个样本{Zt, Dt}来计算梯度。这时候，梯度的估计只通过这一个样本进行估计获得。这时候的模型参数更新为:<script type="math/tex; mode=display">
W(t+1)=W(t)-\eta\frac{\partial E^t}{\partial W}</script></li>
</ul>
<p>Batch学习发现的极小值是依据参数初始化在代价函数表面的某个坑上面，所以如果参数一旦初始化了，因为梯度下降都是往低的地方走，它最后一定是掉到这个坑里面。</p>
<p>随机学习除了学习速度比batch学习速度快之外，还可以得到更好的解，这是因为它给我们的梯度更新带来了噪声。由于噪声的存在，有时候会使参数跳到另一个坑中，从而有可能找到更深的局部极小值。更深的局部极小值意味着更小的代价函数值，也就是更拟合数据的模型。</p>
<p><strong>噪声对找到更好的局部极小值很关键，但它也会阻止完全的收敛到局部极小值，它会让代价函数在极小值周围徘徊，此时，我们需要mini-batch。</strong></p>
<p><strong>mini-batch:</strong> 训练一开始，我们的参数刚初始化，离最小值还很远，这时候我们就要加快它前进的步伐，因此借助随机学习的收敛速度，我们采用一个很小的mini-batches，也就是每个batch包含的训练样本数不多。随着训练的进行，离最小值越来越近，我们就得减速了，因此我们增加mini-batches的大小，从而降低噪声。然而，每种方法的引入都会引入另外需要考虑的超参，在这里就是应该对mini-batches的大小选择怎样的增长率？这实际上和选择学习率是同样困难的。殊途同归，有效的调整学习率和有效的调整mini-batches的大小增长率效果差不多</p>
<h2 id="shuffle扰乱样本学习顺序"><a href="#shuffle扰乱样本学习顺序" class="headerlink" title="shuffle扰乱样本学习顺序"></a>shuffle扰乱样本学习顺序</h2><p>有一个原则是，网络从意料之外的样本中学习最快。所以思想就很简单了，为了加速学习，在每次迭代的时候我们挑选一个和系统最不相似、最不和谐的样本让网络去学习。很明显，这个方法只对随机学习有效，因为batch是不管顺序的（全量）。<br>没有很简单的方法可以知道到底哪个输入样本携带了对系统最丰富的信息量，下面是两个简略的方法</p>
<ul>
<li><p>有个的trick就是粗糙地选择来自不同类的样本，换句话来说，就是，如果在第t次迭代，我是用第i类的样本来学习的，那么在第t+1次迭代的时候，就选择除i类外的其他类的一个样本来学习。因为同一个类的训练样本很大可能携带的是相似的信息。</p>
</li>
<li><p>另一种启发式的判断到底一个训练样本携带了多少新信息的方法，就是测试当将这个样本输入到网络的时候，网络的输出值和目标输出值的误差大小。误差越大，那就表示网络还没学习到这个样本，因此它具有更多的新的信息。</p>
</li>
</ul>
<p>随着网络的训练，每个输入样本的这个误差都会变化，所以每个样本被输入网络训练的次数也会变化。有个修改每个样本的这个概率或者次数的方法叫emphasizing scheme：</p>
<ul>
<li>打乱训练集，使得邻近的样本几乎不会属于同一个类；</li>
<li>挑会使网络产生更大误差的样本输入网络学习。</li>
</ul>
<p><strong>注意</strong>：打乱输入样本被学习的正常频率，会改变每个样本对网络的重要程度，这可能不是那么好。如果训练集中有离群点outliers，那将带来灾难性的后果。因为离群点可以产生很大的误差，但很明显，不应该将它多次地送给网络去训练，这样会扰乱这个网络的正常学习。这个trick对一种情况非常有用，那就是可以对那些正常的但很少出现的输入模式进行性能的加速，例如在音素识别中/z/这个音素。如果这个样本是个正常的小众，那让网络多次学习它是有益的。</p>
<h2 id="对输入进行标准化-Normalize"><a href="#对输入进行标准化-Normalize" class="headerlink" title="对输入进行标准化 Normalize"></a>对输入进行标准化 Normalize</h2><p> <strong>如果训练样本中每个输入变量（特征维度）的均值接近于0，那收敛一般都会更快。</strong></p>
<p>考虑个极端的情况。也就是网络所有的输入都是正数。第一个隐层的神经元的参数更新值是和δx成比例的，δ是这个神经元的误差，x是输入的向量。当x所有的元素都是正数的时候，对这个神经元的参数的更新值都具有相同的符号（因为x是正数，所以更新值的符号和δ的符号一致，而δ是一个标量）。这就导致了，<strong>这些参数对一个给定的输入样本，要么全部增加（δ是正数），要么全部减小（δ是负数）</strong>。所以，如果一个参数向量到达到最优值是必须要改变方向的话，那么它就会沿着“之”形状的路径前进，这是非常低效的，所以会导致收敛非常慢。</p>
<p><strong>因此，将整个训练集每个样本的输入变量的均值偏移到0处是有好处的。而且，这种启发式的方法应该在网络的每一层都使用上，即我们希望每个节点的输出的均值都接近于0</strong></p>
<h2 id="对样本缩放"><a href="#对样本缩放" class="headerlink" title="对样本缩放"></a>对样本缩放</h2><p>有一个加速收敛的方法是对样本进行缩放，让每一个特征维度都具有相同的协方差。缩放为什么会加速学习？因为它可以平衡与输入节点连接的参数的学习率。什么意思呢？上面提到第一个隐层的神经元的参数更新值是和δx成比例的，那如果x中有些元素的值很大，而有些元素的值很小，那很明显，值大的会导致参数的更新值也很大，值小的更新值也小。这个值应该和sigmoid的选择相匹配。对下面给定的sigmoid函数，协方差取1是个不错的选择。</p>
<h2 id="去除输入的相关性"><a href="#去除输入的相关性" class="headerlink" title="去除输入的相关性"></a>去除输入的相关性</h2><p>考虑一种情况是，当一个输入变量总是另一个输入的两倍z2=2z1。那网络沿着线W2=v-(1/2)W1（v是个常数）的输出就都是常数。因此，在这个方向的梯度就都是0了。因此在这些线上移动对学习不会起到任何的效果。</p>
<h2 id="三次变换的总结"><a href="#三次变换的总结" class="headerlink" title="三次变换的总结"></a>三次变换的总结</h2><p>上述过程可以表达如下：1）平移输入让他们的均值为0；2）对输入解相关；3）均衡化协方差。如下图所示：</p>
<p>   <img src="/images/blog/bp_tricks.png" alt="三次变换"></p>
<h2 id="sigmod函数选择"><a href="#sigmod函数选择" class="headerlink" title="sigmod函数选择"></a>sigmod函数选择</h2><p>一般取标准的逻辑函数$f(x)=\frac{1}{(1+e^{-x})}$和双曲线正切函数$f(x)=\tan h(x)$。人们往往更喜欢关于原点对称版本的Sigmoid函数（双曲线正切函数），因为上面我们提到输入应该要满足标准化，所以这个函数的输出更有可能为下一层创造均值接近于0的输入。相反，Logistic函数因为输出总是正数，因此它的均值也总是正数。</p>
<p>对Sigmoids函数的Tricks如下：</p>
<ul>
<li><p>对称性的sigmoids函数例如双曲线正切函数往往比标准的Logistic函数收敛更快。</p>
</li>
<li><p>一个建议的激活函数是$f(x)=1.7159\tan h(\frac{2x}{3})$。因为tanh函数计算挺耗时的，所以一般可以用多项式的系数来近似。</p>
</li>
<li><p>有时候，增加一个线性项会很有用，例如f(x)=tanh(x)+ax，这样可以避免代价函数曲面flat的地方。</p>
</li>
</ul>
<p>上述激活函数，当你使用的是标准化的输入后，这个激活函数输出的方差也会接近于1，因为sigmoid的effective gain（有效增益？）在它的有效范围内大致为1。这个特别版本的sigmoid具有以下性质：</p>
<ul>
<li>$f(\pm1)=\pm1$</li>
<li>最大的二次导数出现在x=1的地方；c）有效增益接近于1。</li>
</ul>
<p>使用对称性sigmoid也有它的缺点，那就是它会使得误差表面在接近原点的地方会非常平flat。因为这个原因，所以最好可以避免将网络参数初始化为很小的值。因为sigmoids的饱和，误差表面在远离原点的时候也是flat的。在sigmoid中增加一个线性的项有时候可以避开这些flat的区域。</p>
<h2 id="目标值的选择"><a href="#目标值的选择" class="headerlink" title="目标值的选择"></a>目标值的选择</h2><p><strong>sigmod饱和问题</strong>：网络的训练会尽自己的最大努力让网络的输出尽可能的接近于目标值，当然了，只能渐进的接近。这样，网络的参数（输出层，甚至隐层）会变得越来越大，而在这些地方，sigmoid的导数值接近于0。这些非常大的参数会增加梯度的值，然而，这些梯度接下来会乘以非常小的sigmoid导数（除非增加一个twisting扭曲项，也就是之前说的增加个线性项ax）从而导致最后的参数更新值也接近于0。最终导致参数无法更新。当输出饱和时，网络无法给出置信度的指示</p>
<p><strong>方案：</strong>把目标值设置在sigmoid的有效范围内，而不是在渐进线的区域。还需要小心的是，为了保证节点不会只被限制在sigmoid的线性部分，可以把目标值设置在sigmoid的最大二阶导数的位置，这样不但可以利用非线性的优点，还可以避免sigmoid的饱和。这也是上图b中的sigmoid函数是个不错的选择的原因。它在正负1的地方具有最大的二阶导数，而正负1对应的恰好是分类问题的典型二值目标值。</p>
<h2 id="参数的初始化"><a href="#参数的初始化" class="headerlink" title="参数的初始化"></a>参数的初始化</h2><p>数初始化的原则是：参数应该随机初始化在能让sigmoid函数在线性区域激活的值。如果参数全部都很大，那sigmoid一开始就饱和了，这样就会得到一个非常小的梯度值，那参数更新就会很慢，训练也会很慢。如果参数太小了，那梯度也会很小，同样也会导致训练很慢。</p>
<p>参数处于sigmoid线性范围的那段区域有几个优点：</p>
<ul>
<li>梯度可以足够的大，从而使得学习能正常进行</li>
<li>网络可以在学习映射的非常困难的非线性部分之前学习映射的线性部分。</li>
</ul>
<p><strong>如何让参数能使得sigmoid函数在线性区域激活的值</strong>:<br>首先，要求每个节点的输出的标准差应该接近于1，这可以通过使用之前提到的数据标准化来对训练集进行变换获得。为了可以在第一个隐层的输出同样获得标准差为1的输出，我们只需要使用上面建议的sigmoid函数，同时要求sigmoid的输入的标准差也为1。假设一个结点的输入yi是不相关的，而且方差为1，那结点的标准差就是参数的加权和：</p>
<script type="math/tex; mode=display">
  \sigma _{y_i}=(\sum_{j}w^2_{ij})^{\frac{1}{2}}</script><p>因此，为了保证上述这个方差近似于1，参数就应该从一个均值为0，标准差为：σw=m-1/2的分布中随机采样得到（m是fan-in，也就是与这个结点连接的输入个数，也就是前一层的节点个数，如果是全连接网络的话）。</p>
<p>参数初始化的tricks：</p>
<p>假设：1）训练集已经被标准化；2）sigmoid是选择f(x)=1.7159tanh(2x/3)。<br>那参数就应该从一个均值为0，标准差为σw=m-1/2的分布（例如正态分布）中采样得到。</p>
<h2 id="学习率的选择"><a href="#学习率的选择" class="headerlink" title="学习率的选择"></a>学习率的选择</h2><p>大部分方法都是在参数发生震荡的时候减小学习率，而在参数相对稳定的朝着一个方向前进的时候增加学习率。这个方法的主要问题在于它对随机梯度或者在线学习是不合适的，因为参数在所有的训练过程中都是抖动的。</p>
<p><strong>tricks</strong></p>
<ul>
<li>给每个参数自己的学习率；</li>
<li>学习率应该和该节点的输入个数的平方根成比例；</li>
<li>低层参数的学习率应该比高层的大。</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>pyspark机器学习库使用</title>
    <url>/2016/09/20/pyspark-mllibuse/</url>
    <content><![CDATA[<h2 id="示例：垃圾邮件分类器"><a href="#示例：垃圾邮件分类器" class="headerlink" title="示例：垃圾邮件分类器"></a>示例：垃圾邮件分类器</h2><p> 以下代码使用两个 MLlib算法，HashingTF（从文本中构建词频特征向量的）和 LogisticRegressionWithSGD（使用随机梯度下降法来执行逻辑回归的算法）。</p>
<h2 id="数据："><a href="#数据：" class="headerlink" title="数据："></a>数据：</h2><p>  spam.txt和normal.txt。都包含了垃圾邮箱和非垃圾邮箱，每行一封邮箱。将两篇文档转换为词频向量模型，然后训练逻辑回归模型来区分垃圾和非垃圾邮箱。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">#以下代码使用两个 MLlib算法，HashingTF（从文本中构建词频特征向量的）和 LogisticRegressionWithSGD（使用随机梯度</span><br><span class="line"># 下降法来执行逻辑回归的算法）。</span><br><span class="line"># 数据：</span><br><span class="line">#spam.txt和normal.txt。都包含了垃圾邮箱和非垃圾邮箱，每行一封邮箱。将两篇文档转换为词频向量模型，</span><br><span class="line"># 然后训练逻辑回归模型来区分垃圾和非垃圾邮箱。</span><br><span class="line"> </span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line"> </span><br><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark.mllib.feature import HashingTF</span><br><span class="line">from pyspark.mllib.classification import LogisticRegressionWithSGD</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line"> </span><br><span class="line"># Path for spark source folder</span><br><span class="line">os.environ[&#39;SPARK_HOME&#39;]&#x3D;&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6&quot;</span><br><span class="line"># Append pyspark  to Python Path</span><br><span class="line">sys.path.append(&quot;D:\javaPackages\spark-1.6.0-bin-hadoop2.6\python&quot;)</span><br><span class="line"> </span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    print &quot;Program lanuch!&quot;</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.set(&quot;YARN_CONF_DIR &quot;, &quot;D:\javaPackages\hadoop_conf_dir\yarn-conf&quot;)</span><br><span class="line">    conf.set(&quot;spark.driver.memory&quot;, &quot;1gb&quot;)</span><br><span class="line">    conf.setMaster(&quot;local&quot;)</span><br><span class="line">    conf.setAppName(&quot;First_Remote_Spark_Program&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line"> </span><br><span class="line">    spam &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;spam.txt&quot;)</span><br><span class="line">    normal &#x3D; sc.textFile(&quot;&#x2F;home&#x2F;xiatao&#x2F;normal.txt&quot;)</span><br><span class="line"> </span><br><span class="line">    print &quot;读取文件结束了&quot;</span><br><span class="line">    #创建一个 HashingTF实例将邮件文本映射到包含了10000个features的向量</span><br><span class="line">    tf &#x3D; HashingTF(numFeatures&#x3D;10000)</span><br><span class="line">    # 将每封邮件都切成单词，每个词映射到一个 features</span><br><span class="line">    spamFeatures &#x3D; spam.map(lambda email:tf.transform(email.split(&quot; &quot;)))</span><br><span class="line">    normalFeatures &#x3D; spam.map(lambda email: tf.transform(email.split(&quot; &quot;)))</span><br><span class="line">    #分别给 正特征（垃圾邮件）和负特征（非垃圾邮件）创建 LabelPoint数据集</span><br><span class="line">    positiveExamples &#x3D; spamFeatures.map(lambda features:LabeledPoint(1,features))</span><br><span class="line">    negativeExamples &#x3D; normalFeatures . map(lambda features:LabeledPoint(0,features))</span><br><span class="line">    trainingData &#x3D; positiveExamples.union(negativeExamples)</span><br><span class="line">    # 由于逻辑回归是个迭代算法，，最好缓存下</span><br><span class="line">    trainingData.cache()</span><br><span class="line">    # 使用SGD算法 运行逻辑回归</span><br><span class="line">    print trainingData</span><br><span class="line">    print &quot;逻辑回归之前&quot;</span><br><span class="line">    model &#x3D; LogisticRegressionWithSGD.train(trainingData)</span><br><span class="line">    print &quot;使用逻辑回归算法之前&quot;</span><br><span class="line">    #  测试一个 正特征数据 和 负特征数据，我们首先 应用HashingTF特征转换来获得向量，然后应用到模型中</span><br><span class="line">    posTest &#x3D; tf.transform(&quot;O M G GET cheap stuff by sending money to ....&quot;.split(&quot; &quot;))</span><br><span class="line">    negTest &#x3D; tf .transform(&quot;Hi Dad,i am studing Spark now...&quot;.split(&quot; &quot;))</span><br><span class="line">    print &quot;预测结果是:%g&quot;%model.predict(posTest)</span><br><span class="line">    print &quot;预测结果是:%g&quot;%model.predict(negTest)</span><br><span class="line">    print &quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;end&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;</span><br></pre></td></tr></table></figure>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>MLlib包含了一些特殊的数据类型，位于 <code>org.apache.spark.mllib.package(Java 或者Scala)</code> 或者 <code>pyspark.mllib(Python)</code></p>
<ul>
<li><p><strong>向量：</strong>     一种数学向量，Spark支持稠密向量（每个位置都存储了值）和稀疏向量(只存储了非0值) 。可以通过 <code>mllib.linalg.Vector</code>类来创建向量</p>
</li>
<li><p><strong>LabeledPoint：</strong> 一个标签化的数据点用在监督学习的算法中，比如分类和回归算法。包括一个特征向量和标签（值类型时float）位于 <code>mllib.regression</code>包里面</p>
</li>
<li><p><strong>Rating：</strong> 用户产品评分，在 <code>mllib.recommendation</code>包中，用于产品推荐</p>
</li>
<li><p><strong>各种Model类：</strong>每个Model都是一个训练算法的结果，并且基本上都有一个 predict()方法用来将模型应用新的数据点或者新数据点的RDD<br>大部分算法可以直接在 向量、LabeledPoint或者Rating的RDD上运行。</p>
</li>
</ul>
<h2 id="使用向量"><a href="#使用向量" class="headerlink" title="使用向量"></a>使用向量</h2><p>首先：向量分两种，稀疏和稠密。对于10%左右元素非零的向量，推荐使用稀疏向量。既节省存储空间又提升速度。</p>
<p>其次：不同的语言构建向量时不同，在python可以简单的传入一个 NumPy数组到MLlib中创建一个稠密向量，或者使用<code>pyspark.mllib.linalg.Vectors</code>类来创建其他类型的向量。以下是python代码示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from numpy import array</span><br><span class="line">from pyspark.mllib.linalg import Vectors</span><br><span class="line"> </span><br><span class="line">#创建一个稠密向量 &lt;1.0,2.0,3.0&gt;</span><br><span class="line"># numpy可以直接传入到MLlib</span><br><span class="line">denseVec1 &#x3D; array([1.0,2.0,3.0]) </span><br><span class="line">#或者使用 Vectors类</span><br><span class="line">denseVec2 &#x3D; Vectors.dense([1.0,2.0,3.0])</span><br><span class="line">#创建稀疏向量 &lt;1.0,0.0,2.0,0.0&gt;,其中(4)为向量元素个数，其他的是非零元素位置</span><br><span class="line">#可以传入词典类型，也可以使用两个列表，分别是位置和值</span><br><span class="line">sparseVec1 &#x3D; Vectors.sparse(4,&#123;0:1.0,2:2.0&#125;)</span><br><span class="line">sparseVec2 &#x3D; Vectors.sparse(4,[0,2],[1.0,2.0])</span><br></pre></td></tr></table></figure>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>如何调用和配置算法</p>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p><code>mllib.feature</code> 包包含 了几个常用的特征转换类，其中有将文本转换为特征向量的算法以及规划化和尺度的路径。</p>
<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>词频-逆向文档模型，是从文本中生成特征向量的最简单的办法。MLlib有两个计算 TF-IDF的算法：HashingTF和IDF都在mllib.feature 包中。HashingTF从文本中根据给定大小计算出词频向量。为了将词频映射到向量序位，HashingTF将每个单词对向量大小取模的哈希码，因而每个单词都会被映射到 0到 (size-1)(向量大小)。尽管多个词可能会被映射到相同的哈希码。MLlib开发者建议的向量大小为 2^18到2^20。</p>
<p>在python中使用 HashingTF</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.feature import HashingTF</span><br><span class="line"> </span><br><span class="line">sentence &#x3D; &quot;Hello world,Hello&quot;</span><br><span class="line">words &#x3D; sentence.split(&quot; &quot;) #将语句切成词项列表</span><br><span class="line">tf &#x3D; HashingTF(10000)       #创建大小为10000的向量</span><br><span class="line">tf.transform(words)</span><br></pre></td></tr></table></figure>
<p>输出结果为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparseVector(10000,&#123;3065:1.0,6861:2.0&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="将整个RDD转换"><a href="#将整个RDD转换" class="headerlink" title="将整个RDD转换"></a>将整个RDD转换</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rdd &#x3D; sc.wholeTextFiles(&quot;data&quot;).map(lambda (name,text):text.split())</span><br><span class="line">tfVectors &#x3D; tf.transform(rdd)   #转换整个RDD</span><br></pre></td></tr></table></figure>
<p>一旦创建了词频向量，就可以使用 IDF来计算逆向文档词频，然后乘以词频来计算TF-IDF。首先在一个 IDF对象上使用 fit()来获得 IDFModel，该模型代表了语料库中的逆向文档频率，然后调用 transform()来转换 TF向量为一个 IDF向量。</p>
<p><strong>在python中使用 TF-IDF</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.feature import HashingTF,IDF</span><br><span class="line"> </span><br><span class="line"># 读取一些文档作为 TF向量</span><br><span class="line">rdd &#x3D; sc.wholeTextFiles(&quot;data&quot;).map(lambda (name,text):text.split(&quot; &quot;))</span><br><span class="line">tf &#x3D; HashingTF()</span><br><span class="line">tfVectors &#x3D; tf.transform(rdd).cache()</span><br><span class="line"> </span><br><span class="line">#计算 IDF，然后计算 TF-IDF</span><br><span class="line">idf &#x3D; IDF()</span><br><span class="line">idfModel &#x3D; idf.fit(tfVectors)</span><br><span class="line">tfidfVectors &#x3D; idfModel.transform(tfVectors)</span><br></pre></td></tr></table></figure>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><p>大部分机器学习算法会考虑特征向量中的每个元素的大小(尺度)，因而当特征都均衡时（比如都在范围 0-1之间）时算法表现最好。一旦建立好特征向量，可以使用 MLlib中的 StandardScaler类来解决尺度问题。先创建一个 StandardScaler，然后在数据集上调用 fit() 方法来获得一个 StandardScalerModel，然后在模型上调用 transform() 来均衡(尺度平衡)数据集。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.feature import StandardScaler</span><br><span class="line"> </span><br><span class="line">vectors &#x3D; [Vectors.dense([-2.0,5.0,1.0]),Vectors.dense([2.0,0.0,1.0])]</span><br><span class="line">dataset &#x3D; sc.parallize(vectors)</span><br><span class="line">scaler &#x3D;  StandardScaler(withMean &#x3D; True,withStd &#x3D; True)</span><br><span class="line">model &#x3D; scaler.fit(dataset)</span><br><span class="line">result &#x3D;model.transform(dataset)</span><br></pre></td></tr></table></figure>
<h2 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h2><p>Normalizer类允许用户将向量规范化到长度为1的空间内，使用 <code>Normalizer().transform(rdd)</code>即可。默认情况下是将数据按照欧几里得距离规范化，可以向Normalizer()中传入参数改变，如果传入的是3，将会被规范化到 L^3的空间上。</p>
<h2 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h2><p> Spark提供了一些直接应用到RDD上的统计函数，位于<code>mllib.stat.Statistics</code>类。</p>
<ul>
<li><p><strong>Statistics.colStats(rdd) :</strong>计算一个RDD向量的统计概要，保存向量集合每一列的最小值、最大值、平均值以及方差。</p>
</li>
<li><p><strong>Statistics.corr(rdd,method)：</strong>计算RDD向量列之间的相关性，使用Pearson 或者Spearman（方法必须是这两者中的一个）</p>
</li>
<li><p><strong>Statistics.corr(rdd1,rdd2,method)：</strong>计算两个RDD向量浮点值之间的相关性。method同上</p>
</li>
<li><p><strong>Statistics.chiSqTest(rdd)：</strong>计算有label标签的LabeledPoint对象的RDD的每个特征的皮埃尔独立性检测。</p>
</li>
</ul>
<h2 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h2><p>分类和回归两个常见的监督学习形式，算法尝试从打过标签的训练数据对象中预测变量。不同之处在于预测变量的类型：分类中所有分类是限定（离散）的，回归中变量预测是连续的。<br> 在MLlib中分类和回归都是用 LabeledPoint类，也即“数据类型”。一个LabeledPoint由标签(一般是double，但是也可以被设置成离散的)和特征向量组成。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><pre><code>  线性回归是是回归算法中最简单的回归算法，预测特征的线性组合变量输出。MLlib支持Lasso回归和ridge回归。通过 `mllib.regression.LineRegressionWithSGD`,`LassonWithSGD`和`RidgeRegressionWithSGD`类可以使用，在MLlib中遵从一致的命名模式，当问题牵扯到多个算法时，类名中&quot;With&quot;部分所使用的算法。此处SGD即 Stochastic Gradient Descent(随机梯度下降)。这些类都有几个参数来调整算法:
</code></pre><ul>
<li><strong>numIterations:</strong>算法迭代次数，默认是100</li>
<li><strong>stepSize：</strong> 梯度下降步长（默认是1.0）</li>
<li><strong>intercept:</strong> (截距)是否向数据中加入截距或者 偏置特征，也即特征值始终为1的。默认是不添加的</li>
<li><strong>regPram：</strong>Lasso和ridge回归的正则参数</li>
</ul>
<p>python中线性回归算法示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.mllib.regression import LabeledPoint</span><br><span class="line">from pyspark.mllib.regression import LinearRegressionWithSGD</span><br><span class="line"> </span><br><span class="line">points &#x3D; #创建一个 LabeledPoint的RDD</span><br><span class="line">model &#x3D; LinearRegressionWithSGD.train(point,iterations &#x3D; 200,intercept&#x3D;True)</span><br><span class="line">print &quot;weight:%s,intercept: %s&quot;%(model.weights,model.intercept)</span><br></pre></td></tr></table></figure>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><pre><code>  逻辑回归是一个将数据样例分为正、负的二分类平面。在MLlib中使用LabeledPoint 标签0 和标签1并返回LogisticRegressionModel来预测新的数据点。
</code></pre><p>逻辑回归有着与线性回归十分相似的API，不同之处在于逻辑回归使用的算法时SGD和LBFGS。通常选择LBFGS。可以在<code>mllib.classification.LogisticRegressionWithLBFGS</code> 和<code>WithSGD</code>类中找到。</p>
<pre><code>  这些逻辑回归算法中的`LogstisticRegressionModel`给每个点计算一个0到1之间的分值。然后给予给定的阈值返回0或1，可以通过设置 setThreshold来改变阈值，也可以通过 clearThreshold()方法清除阈值设置，清除之后 predict()将返回原始的分值。
</code></pre><h2 id="SVM支持向量机"><a href="#SVM支持向量机" class="headerlink" title="SVM支持向量机"></a>SVM支持向量机</h2><p>   SVM也是一个返回线性分类平面的二分类方法，</p>
<ul>
<li><p><strong>协同过滤和推荐算法</strong> </p>
<pre><code>   协同过滤是一种根据用户与物品的交互评分数据来推荐新物品的技术。仅需要一张 用户/产品 交互清单即可：可以是确定交互（直接在网站上给产品评分）或者隐式交互（用户浏览了某个产品，但是没有评分）。根据这些，协同过滤就知道哪些产品之间有相似性，以及哪些用户之间存在相似。
</code></pre></li>
<li><p><strong>交替最小二乘法</strong></p>
<pre><code>    产品和用户构成的M*N矩阵(产品有M个，用户有N个)，但这个矩阵是稀疏的，只有部分评分，ALS就是填满矩阵中缺失值得，填满的过程就是推荐过程。MLlib包含了一个ALS的实现，一个易于在集群中拓展的协同过滤算法，位于 mllib.recommendation.ALS
</code></pre><p>使用以下参数：</p>
</li>
<li><p><strong>rank：</strong>特征向量秩大小，越大的秩会得到更好的模型，但是计算消耗也相应增加。默认是 10</p>
</li>
<li><p><strong>iteration：</strong> 算法迭代次数（默认是10）</p>
</li>
<li><p><strong>lambda：</strong>正则参数，默认是 0.01。详细解释参考 <a href="https://www.zhihu.com/question/31509438" target="_blank" rel="noopener">https://www.zhihu.com/question/31509438</a></p>
</li>
<li><p><strong>alpha：</strong>在隐式ALS中用于计算置信度的常量，默认为1.0</p>
</li>
<li><p><strong>numUserBlocks,numProductBlocks：</strong>将用户和产品数据分解的块数目，用来控制并行度；你可以传入-1来让MLlib自动决定。</p>
</li>
</ul>
<p>要使用ALS，你需要给定一个 <code>mllib.recommendation.Rating</code>对象的RDD，每个都包含 用户ID，产品ID和评分。注意：每个ID都必须是是一个32位整型数据，如果你的ID是字符串或者比较大的数据，推荐使用哈希之后的数据。</p>
<p>ALS返回一个 <code>MatrixFactorizationModel</code>来代表结果，此结果可以用来给键值对RDD(userID,productID)使用predict()预测评分。另外，你可以使用 model.recommendProducts(userID,numProducts) 找到 top numProducts的产品给指定用户。切记，不像MLlib中其他模型，MatrixFactorizationModel是较大的，为每个用户和产品持有一个向量。这表明它不能存储在磁盘上然后再载入并用在另外部分代码，但是你可以存储它产生的特征向量RDD，比如<code>model.userFeatures</code>和<code>model.productFeatures</code>到分布式文件系统中。</p>
<p>最后，有两种类型的ALS：对于确定评分（默认，使用 ALS.train()）和隐式评分（使用 ALS.trainImplicit()）。对于确定评分，每个用户对产品的评分必须是分值（比如说1-5星），然后预测评分也是分值。对于隐式评分，评分代表了用户与给定产品项的交互置信度，然后预测项也是置信度。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：batch_size的设置与影响</title>
    <url>/2016/09/20/batch_sizesetup/</url>
    <content><![CDATA[<p>注意：本文根据知乎<strong>程引</strong>的<a href="https://www.zhihu.com/question/32673260" target="_blank" rel="noopener">回答</a>整理</p>
<h2 id="为何需要batch-size参数"><a href="#为何需要batch-size参数" class="headerlink" title="为何需要batch_size参数"></a>为何需要batch_size参数</h2><p> Batch的选择，<strong>首先决定的是下降的方向</strong>。如果数据集比较小，完全可以采用 <strong>全数据集(Full Batch Learning)</strong> 的形式。这样做有如下好处：</p>
<ul>
<li>全数据集确定的方向能够更好的代表样本总体，从而更准确的朝着极值所在方向。</li>
<li><p>由于不同权值的梯度值差别较大，因此选取一个全局的学习率很困难。</p>
<p>Full Batch Learning可以使用Rprop只基于梯度符号并且针对性单独更新各权值。<br>但是对于非常大的数据集，上述两个好处变成了两个坏处：</p>
</li>
<li><p>随着数据集的海量增加和内存限制，一次载入所有数据不现实。</p>
</li>
<li>以Rprop的方式迭代，会由于各个batch之间的采样差异性，各此梯度修正值相互抵消，无法修正。这才有了后来的<strong>RMSprop</strong>的妥协方案。</li>
</ul>
<h2 id="Full-Batch-Learning的另一个极端-Online-Learning"><a href="#Full-Batch-Learning的另一个极端-Online-Learning" class="headerlink" title="Full Batch Learning的另一个极端 Online Learning"></a>Full Batch Learning的另一个极端 Online Learning</h2><p> 既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？所谓另一个极端，就是每次只训练一个样本，即 Batch_Size = 1。这就是<strong>在线学习(Online Learning)</strong> 。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，<strong>难以达到收敛</strong></p>
<p> <img src="/images/blog/batch_size1.png" alt="batch_size"></p>
<h2 id="选取适中的batch-size"><a href="#选取适中的batch-size" class="headerlink" title="选取适中的batch_size"></a>选取适中的batch_size</h2><p>  可不可以选择一个适中的 Batch_Size 值呢？当然可以，这就是<strong>批梯度下降法（Mini-batches Learning）</strong>。因为如果数据集足够充分，那么用一半（<em>甚至少得多</em>）的数据训练算出来的梯度与用全部数据训练出来的梯度是<strong>几乎一样</strong>的。<br>  在合理范围内，增大 Batch_Size 有何好处？</p>
<ul>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li><p>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</p>
<p>盲目增大 Batch_Size 有何<u>坏处</p>
</li>
<li><p>内存利用率提高了，但是内存容量可能撑不住了。</p>
</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ul>
<h2 id="调节-Batch-Size-对训练效果影响到底如何？"><a href="#调节-Batch-Size-对训练效果影响到底如何？" class="headerlink" title="调节 Batch_Size 对训练效果影响到底如何？"></a>调节 Batch_Size 对训练效果影响到底如何？</h2><p>  这里跑一个 LeNet 在 MNIST 数据集上的效果。MNIST 是一个手写体标准库</p>
<p>   <img src="/images/blog/batch_size2.png" alt="batch_size"></p>
<p>  运行结果如上图所示，其中绝对时间做了标准化处理。运行结果与上文分析相印证：</p>
<ul>
<li>Batch_Size 太小，算法在 200 epoches 内不收敛。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到<u>某个</u>时候，达到<b>时间上</b>的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到<u>某些</u>时候，达到最终收敛<strong>精度上</strong>的最优。</li>
</ul>
<h2 id="caffe中batch-size影响"><a href="#caffe中batch-size影响" class="headerlink" title="caffe中batch size影响"></a>caffe中batch size影响</h2><p> caffe的代码实现上选取一个batch的时候似乎是按着数据库的图片顺序选取输入图片的，所以在生成数据库的时候切记要shuffle一下图片顺序。caffe中完成这一步的代码为</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$caffe_root&#x2F;build&#x2F;tools&#x2F;convert_imageset -shuffle -resize_height&#x3D;256 -resize_width&#x3D;256</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow：如何重新训练Inception模型的最后一层，以应对新分类</title>
    <url>/2016/09/09/tensorflow-retainmodel/</url>
    <content><![CDATA[<p>本文整理自 <a href="https://www.tensorflow.org/versions/r0.10/how_tos/image_retraining/index.html" target="_blank" rel="noopener">retrain network</a></p>
<h2 id="在Flowers数据集上重新训练"><a href="#在Flowers数据集上重新训练" class="headerlink" title="在Flowers数据集上重新训练"></a>在Flowers数据集上重新训练</h2><p><img src="/images/blog/retain_flowers.jpg" alt="flowers数据集"></p>
<p>训练开始之前你需要一些数据集，以告诉网络，你有哪些新分类需要学习。后面的部分会道明如何准备自己的数据，首先按照如下操作下载一些数据集</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">curl -O http:&#x2F;&#x2F;download.tensorflow.org&#x2F;example_images&#x2F;flower_photos.tgz</span><br><span class="line">tar xzf flower_photos.tgz</span><br></pre></td></tr></table></figure>
<p>等图片下载好，可以按照如下方式重新训练，在tensorflow源码的根目录下执行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bazel build tensorflow&#x2F;examples&#x2F;image_retraining:retrain</span><br></pre></td></tr></table></figure>
<p>然后继续执行如下操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bazel-bin&#x2F;tensorflow&#x2F;examples&#x2F;image_retraining&#x2F;retrain --image_dir ~&#x2F;flower_photos</span><br></pre></td></tr></table></figure>
<p>注意：<code>image_dir</code>参数需要指定你的flowers数据下载目录。<br>此脚本会载入预训练的Inception v3模型，移除原先模型中的最后一层，然后在flowers数据集上重新训练。flowers数据集的分类信息在原始的ImageNet网络中并不存在 ，转换学习的神奇之处在于网络的前几层用来训练识别物体之间的差别，这可以在做任何更新的情况下重用于新的分类任务。</p>
<h2 id="Bottlenecks"><a href="#Bottlenecks" class="headerlink" title="Bottlenecks"></a>Bottlenecks</h2><p>  脚本运行可能消耗半小时或者更多，这取决于你的硬件。脚本的第一阶段会分析所有图像并计算每张图像的Bottleneck.<code>bottleneck</code>是我们经常用于描述网络最后一层之前的那些实际完成分类任务的网络层的一种非正式称谓。倒数第二层的输出结果对于描述区分需要分类的类别已经足够.这意味着它必须有信息丰富并且关于图像信息也足够紧凑，因为它必须包含足够的信息来在一小撮数值（标签值）中完成分类。重新训练最后一层就可以完成新的分类任务，为何？因为在ImageNet数据上完成1000个分类任务的信息通常也可用于区分新的物体。</p>
<p>  由于每张图像在训练和计算bottleneck值的过程中重复使用多次，这极为耗时，将这些数据缓存在磁盘上有助于加速整个过程以避免重复计算。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p> 一旦bottleneck完成，网络最后一层的训练就开始了。你将会看到训练步骤的一系列的输出，每一个会输出训练准确率，验证准确率和交叉熵。训练准确率显示的当前批次的图像中正确分类的准确率。验证准确率显示的是从一组不同数据中随机选择的一组图像的精度。这其中的关键区别在于，训练的准确率是基于网络已经能够学习的图像集，因为网络</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow：理解tensorflow中的输入管道</title>
    <url>/2016/09/02/tensorflow-inputpipeline/</url>
    <content><![CDATA[<p>原文翻译整理自： <a href="https://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/" target="_blank" rel="noopener">理解tensorflow中输入管道</a></p>
<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><p>  本文旨在根据mnist数据集构建一个简单而有效的输入管道。</p>
<h2 id="使用tensorflow加载数据"><a href="#使用tensorflow加载数据" class="headerlink" title="使用tensorflow加载数据"></a>使用tensorflow加载数据</h2><p> 有两种方式来加载数据，其一是使用<strong>feeding</strong>方法并在每一步提供<strong>data</strong>和<strong>label</strong>给<strong>feed_dict</strong>对象。这种方式在数据集太大而无法在内存中存放时将无能为力，因此tensorflow的作者提出了使用 <em>input pipelines</em>。下一步将描述 <em>pipelines</em>，但是，注意：只有在session操作之前启动队列runners才能激活pipelines并载入数据。<br> <em>input pipeline</em>将会处理读取csv文件，解析文件格式，重构数据，混洗数据，数据增强以及其他数据处理，然后在批处理中使用线程载入数据。</p>
<h2 id="载入标签数据"><a href="#载入标签数据" class="headerlink" title="载入标签数据"></a>载入标签数据</h2><p> 假定我们有如下数据集：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> dataset_path      &#x3D; &quot;&#x2F;path&#x2F;to&#x2F;out&#x2F;dataset&#x2F;mnist&#x2F;&quot;</span><br><span class="line">test_labels_file  &#x3D; &quot;test-labels.csv&quot;</span><br><span class="line">train_labels_file &#x3D; &quot;train-labels.csv&quot;</span><br></pre></td></tr></table></figure>
<p>首先要做的事情就是从生成的文本文件中载入图像和标签信息。<strong>注意，我们并不是要训练模型,所以不需要进行one-hot编码</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def encode_label(label):</span><br><span class="line">  return int(label)</span><br><span class="line"></span><br><span class="line">def read_label_file(file):</span><br><span class="line">  f &#x3D; open(file, &quot;r&quot;)</span><br><span class="line">  filepaths &#x3D; []</span><br><span class="line">  labels &#x3D; []</span><br><span class="line">  for line in f:</span><br><span class="line">    filepath, label &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">    filepaths.append(filepath)</span><br><span class="line">    labels.append(encode_label(label))</span><br><span class="line">  return filepaths, labels</span><br><span class="line"></span><br><span class="line"># reading labels and file path</span><br><span class="line">train_filepaths, train_labels &#x3D; read_label_file(dataset_path + train_labels_file)</span><br><span class="line">test_filepaths, test_labels &#x3D; read_label_file(dataset_path + test_labels_file)</span><br></pre></td></tr></table></figure>
<h2 id="在字符串列表上选择性地做一些处理"><a href="#在字符串列表上选择性地做一些处理" class="headerlink" title="在字符串列表上选择性地做一些处理"></a>在字符串列表上选择性地做一些处理</h2><p>接下来，我们将图像数据的相对路径转换为绝对路径，同时将训练数据和测试数据拼接在一起。然后混洗数据并创建我们自己的训练和测试集合。为使脚本输出结果易于理解，我们将只从数据集中抽样20个样本。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># transform relative path into full path</span><br><span class="line">train_filepaths &#x3D; [ dataset_path + fp for fp in train_filepaths]</span><br><span class="line">test_filepaths &#x3D; [ dataset_path + fp for fp in test_filepaths]</span><br><span class="line"></span><br><span class="line"># for this example we will create or own test partition</span><br><span class="line">all_filepaths &#x3D; train_filepaths + test_filepaths</span><br><span class="line">all_labels &#x3D; train_labels + test_labels</span><br><span class="line"></span><br><span class="line"># we limit the number of files to 20 to make the output more clear!</span><br><span class="line">all_filepaths &#x3D; all_filepaths[:20]</span><br><span class="line">all_labels &#x3D; all_labels[:20]</span><br></pre></td></tr></table></figure>
<h2 id="开始构建pipelines"><a href="#开始构建pipelines" class="headerlink" title="开始构建pipelines"></a>开始构建pipelines</h2><p>确保我们所使用 <em>tensor</em>的数据类型<em>dtype</em>与列表中的已有的数据是一致的。载入以下包可以创建我们的tensorflow对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from tensorflow.python.framework import ops</span><br><span class="line">from tensorflow.python.framework import dtypes</span><br><span class="line"># convert string into tensors</span><br><span class="line">all_images &#x3D; ops.convert_to_tensor(all_filepaths, dtype&#x3D;dtypes.string)</span><br><span class="line">all_labels &#x3D; ops.convert_to_tensor(all_labels, dtype&#x3D;dtypes.int32)</span><br></pre></td></tr></table></figure>
<h2 id="开始对数据分区"><a href="#开始对数据分区" class="headerlink" title="开始对数据分区"></a>开始对数据分区</h2><p>这一步是可选的。鉴于我们已经将我们的20个样本置于一个大集合之中，我们需要执行一些<em>partition</em>操作来构建测试机和训练集。tensorflow可以在tensors上即时完成，所以不必预先做。如果对 partition操作感到困惑，可以参考<a href="https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#dynamic_partition" target="_blank" rel="noopener">tensorflow partition</a>.我们将 <em>test_set_size</em>设置为5个样本。下图显示了如何从数据集中随机选出训练集和测试集</p>
<p>   <img src="/images/blog/tensorflow_partition.png" alt="tensorflow partition操作示例图"></p>
<p>注意<em>partition</em>类似于位置因子或标签，数据某个位置上的不同标签将会将数据分成不同部分。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># create a partition vector</span><br><span class="line">partitions &#x3D; [0] * len(all_filepaths)</span><br><span class="line">partitions[:test_set_size] &#x3D; [1] * test_set_size</span><br><span class="line">random.shuffle(partitions)</span><br><span class="line"></span><br><span class="line"># partition our data into a test and train set according to our partition vector</span><br><span class="line">train_images, test_images &#x3D; tf.dynamic_partition(all_images, partitions, 2)</span><br><span class="line">train_labels, test_labels &#x3D; tf.dynamic_partition(all_labels, partitions, 2)</span><br></pre></td></tr></table></figure>
<h2 id="构建输入队列并定义如何载入图像"><a href="#构建输入队列并定义如何载入图像" class="headerlink" title="构建输入队列并定义如何载入图像"></a>构建输入队列并定义如何载入图像</h2><p> <em>slice_input_producer</em>将tensors切分成许许多多的单个实例，并使用多线程将它们入队列。关于进一步的参数，比如线程数和队列容量等需要参考API文档。然后，我们使用路径信息将文件读入到 <em>pipelines</em>，然后使用<em>jpg decoder</em>解码（也可以使用其他解码器）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> # create input queues</span><br><span class="line">train_input_queue &#x3D; tf.train.slice_input_producer(</span><br><span class="line">                                    [train_images, train_labels],</span><br><span class="line">                                    shuffle&#x3D;False)</span><br><span class="line">test_input_queue &#x3D; tf.train.slice_input_producer(</span><br><span class="line">                                    [test_images, test_labels],</span><br><span class="line">                                    shuffle&#x3D;False)</span><br><span class="line"></span><br><span class="line"># process path and string tensor into an image and a label</span><br><span class="line">file_content &#x3D; tf.read_file(train_input_queue[0])</span><br><span class="line">train_image &#x3D; tf.image.decode_jpeg(file_content, channels&#x3D;NUM_CHANNELS)</span><br><span class="line">train_label &#x3D; train_input_queue[1]</span><br><span class="line"></span><br><span class="line">file_content &#x3D; tf.read_file(test_input_queue[0])</span><br><span class="line">test_image &#x3D; tf.image.decode_jpeg(file_content, channels&#x3D;NUM_CHANNELS)</span><br><span class="line">test_label &#x3D; test_input_queue[1]</span><br></pre></td></tr></table></figure>
<h2 id="分组抽样并汇成一批批"><a href="#分组抽样并汇成一批批" class="headerlink" title="分组抽样并汇成一批批"></a>分组抽样并汇成一批批</h2><p> 如果在<em>session</em>中执行<em>train_image</em>，你将会得到一张图片信息（比如,(28,28,1)），这是我们的mnist图像的维度。在一张图片上训练模型是十分低效的，因此我们将图像汇入队列中称为一批，并在这一批批的数据上训练。目前为止，我们没有开始 <em>runners</em>来载入图像，只是描述了 <em>pipelines</em>初步形象，此时tensorflow 尚不了解图像的形状。使用<strong>tf.train_batch</strong>之前，需要先定义图像张量的<em>shape</em>，以便于将图像汇成一批批数据。此示例中，我们使用的是5个样本作为一批数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> # define tensor shape</span><br><span class="line">train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])</span><br><span class="line">test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># collect batches of images before processing</span><br><span class="line">train_image_batch, train_label_batch &#x3D; tf.train.batch(</span><br><span class="line">                                    [train_image, train_label],</span><br><span class="line">                                    batch_size&#x3D;BATCH_SIZE</span><br><span class="line">                                    #,num_threads&#x3D;1</span><br><span class="line">                                    )</span><br><span class="line">test_image_batch, test_label_batch &#x3D; tf.train.batch(</span><br><span class="line">                                    [test_image, test_label],</span><br><span class="line">                                    batch_size&#x3D;BATCH_SIZE</span><br><span class="line">                                    #,num_threads&#x3D;1</span><br><span class="line">                                    )</span><br></pre></td></tr></table></figure>
<h2 id="运行-Queue-Runners并启动session"><a href="#运行-Queue-Runners并启动session" class="headerlink" title="运行 Queue Runners并启动session"></a>运行 Queue Runners并启动session</h2><p> 上面的步骤已经完成 <em>input pipelines</em>的构建。但是若此时去访问比如<em>test_image_batch</em>，将不会有任何数据，因为我们并没有启动载入队列并将数据注入到 tensorflowd对象中的线程。完成这一步之后，接下来是两个循环，其一是处理训练数据，其二是吹测试数据。<br> 你可能留意到循环次数比每个数据的抽样数据大</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line"> # initialize the variables</span><br><span class="line"> sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"> # initialize the queue threads to start to shovel data</span><br><span class="line"> coord &#x3D; tf.train.Coordinator()</span><br><span class="line"> threads &#x3D; tf.train.start_queue_runners(coord&#x3D;coord)</span><br><span class="line"></span><br><span class="line"> print &quot;from the train set:&quot;</span><br><span class="line"> for i in range(20):</span><br><span class="line">   print sess.run(train_label_batch)</span><br><span class="line"></span><br><span class="line"> print &quot;from the test set:&quot;</span><br><span class="line"> for i in range(10):</span><br><span class="line">   print sess.run(test_label_batch)</span><br><span class="line"></span><br><span class="line"> # stop our queue threads and properly close the session</span><br><span class="line"> coord.request_stop()</span><br><span class="line"> coord.join(threads)</span><br><span class="line"> sess.close()</span><br></pre></td></tr></table></figure>
<p>但是从下面的输出结果看，你就会知道tensorflow不关心回合数(epochs)。我们不会混洗数据（查看input slicer的参数），同时 input pipelines只是在训练集上按照既定频率循环。你自己应该确保回合数(epochs)的准确性。尝试着调节 <em>batch size</em>和 <em>shuffle</em>并预测这将如何改变输出结果。你能预测到如果 <em>batch_size</em>改成4而不是5将会改变什么吗？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  tf-env)worker1:~$ python mnist_feed.py</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;dso_loader.cc:105] successfully opened CUDA library libcublas.so locally</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;dso_loader.cc:105] successfully opened CUDA library libcufft.so locally</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;dso_loader.cc:105] successfully opened CUDA library libcurand.so locally</span><br><span class="line">input pipeline ready</span><br><span class="line">I tensorflow&#x2F;stream_executor&#x2F;cuda&#x2F;cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_init.cc:102] Found device 0 with properties:</span><br><span class="line">name: GeForce GTX 960</span><br><span class="line">major: 5 minor: 2 memoryClockRate (GHz) 1.253</span><br><span class="line">pciBusID 0000:01:00.0</span><br><span class="line">Total memory: 2.00GiB</span><br><span class="line">Free memory: 1.77GiB</span><br><span class="line">I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_init.cc:126] DMA: 0</span><br><span class="line">I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_init.cc:136] 0:   Y</span><br><span class="line">I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:755] Creating TensorFlow device (&#x2F;gpu:0) -&gt; (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)</span><br><span class="line">from the train set:</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line">[1 7 2 6 9]</span><br><span class="line">[5 4 1 9 2]</span><br><span class="line">[1 3 1 3 6]</span><br><span class="line"></span><br><span class="line">from the test set:</span><br><span class="line"></span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br><span class="line">[0 4 5 3 8]</span><br></pre></td></tr></table></figure>
<p>由于我们混洗了 partition 向量，很显然你会得到不同的标签。但是注意，此处重点是理解tensorflow的载入机制是如何工作的。因为每我们的 <em>batch size</em>与测试集合的一样大。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><p>完整代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># Example on how to use the tensorflow input pipelines. The explanation can be found here ischlag.github.io.</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import random</span><br><span class="line">from tensorflow.python.framework import ops</span><br><span class="line">from tensorflow.python.framework import dtypes</span><br><span class="line"></span><br><span class="line">dataset_path      &#x3D; &quot;&#x2F;path&#x2F;to&#x2F;your&#x2F;dataset&#x2F;mnist&#x2F;&quot;</span><br><span class="line">test_labels_file  &#x3D; &quot;test-labels.csv&quot;</span><br><span class="line">train_labels_file &#x3D; &quot;train-labels.csv&quot;</span><br><span class="line"></span><br><span class="line">test_set_size &#x3D; 5</span><br><span class="line"></span><br><span class="line">IMAGE_HEIGHT  &#x3D; 28</span><br><span class="line">IMAGE_WIDTH   &#x3D; 28</span><br><span class="line">NUM_CHANNELS  &#x3D; 3</span><br><span class="line">BATCH_SIZE    &#x3D; 5</span><br><span class="line"></span><br><span class="line">def encode_label(label):</span><br><span class="line">  return int(label)</span><br><span class="line"></span><br><span class="line">def read_label_file(file):</span><br><span class="line">  f &#x3D; open(file, &quot;r&quot;)</span><br><span class="line">  filepaths &#x3D; []</span><br><span class="line">  labels &#x3D; []</span><br><span class="line">  for line in f:</span><br><span class="line">    filepath, label &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">    filepaths.append(filepath)</span><br><span class="line">    labels.append(encode_label(label))</span><br><span class="line">  return filepaths, labels</span><br><span class="line"></span><br><span class="line"># reading labels and file path</span><br><span class="line">train_filepaths, train_labels &#x3D; read_label_file(dataset_path + train_labels_file)</span><br><span class="line">test_filepaths, test_labels &#x3D; read_label_file(dataset_path + test_labels_file)</span><br><span class="line"></span><br><span class="line"># transform relative path into full path</span><br><span class="line">train_filepaths &#x3D; [ dataset_path + fp for fp in train_filepaths]</span><br><span class="line">test_filepaths &#x3D; [ dataset_path + fp for fp in test_filepaths]</span><br><span class="line"></span><br><span class="line"># for this example we will create or own test partition</span><br><span class="line">all_filepaths &#x3D; train_filepaths + test_filepaths</span><br><span class="line">all_labels &#x3D; train_labels + test_labels</span><br><span class="line"></span><br><span class="line">all_filepaths &#x3D; all_filepaths[:20]</span><br><span class="line">all_labels &#x3D; all_labels[:20]</span><br><span class="line"></span><br><span class="line"># convert string into tensors</span><br><span class="line">all_images &#x3D; ops.convert_to_tensor(all_filepaths, dtype&#x3D;dtypes.string)</span><br><span class="line">all_labels &#x3D; ops.convert_to_tensor(all_labels, dtype&#x3D;dtypes.int32)</span><br><span class="line"></span><br><span class="line"># create a partition vector</span><br><span class="line">partitions &#x3D; [0] * len(all_filepaths)</span><br><span class="line">partitions[:test_set_size] &#x3D; [1] * test_set_size</span><br><span class="line">random.shuffle(partitions)</span><br><span class="line"></span><br><span class="line"># partition our data into a test and train set according to our partition vector</span><br><span class="line">train_images, test_images &#x3D; tf.dynamic_partition(all_images, partitions, 2)</span><br><span class="line">train_labels, test_labels &#x3D; tf.dynamic_partition(all_labels, partitions, 2)</span><br><span class="line"></span><br><span class="line"># create input queues</span><br><span class="line">train_input_queue &#x3D; tf.train.slice_input_producer(</span><br><span class="line">                                    [train_images, train_labels],</span><br><span class="line">                                    shuffle&#x3D;False)</span><br><span class="line">test_input_queue &#x3D; tf.train.slice_input_producer(</span><br><span class="line">                                    [test_images, test_labels],</span><br><span class="line">                                    shuffle&#x3D;False)</span><br><span class="line"></span><br><span class="line"># process path and string tensor into an image and a label</span><br><span class="line">file_content &#x3D; tf.read_file(train_input_queue[0])</span><br><span class="line">train_image &#x3D; tf.image.decode_jpeg(file_content, channels&#x3D;NUM_CHANNELS)</span><br><span class="line">train_label &#x3D; train_input_queue[1]</span><br><span class="line"></span><br><span class="line">file_content &#x3D; tf.read_file(test_input_queue[0])</span><br><span class="line">test_image &#x3D; tf.image.decode_jpeg(file_content, channels&#x3D;NUM_CHANNELS)</span><br><span class="line">test_label &#x3D; test_input_queue[1]</span><br><span class="line"></span><br><span class="line"># define tensor shape</span><br><span class="line">train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])</span><br><span class="line">test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># collect batches of images before processing</span><br><span class="line">train_image_batch, train_label_batch &#x3D; tf.train.batch(</span><br><span class="line">                                    [train_image, train_label],</span><br><span class="line">                                    batch_size&#x3D;BATCH_SIZE</span><br><span class="line">                                    #,num_threads&#x3D;1</span><br><span class="line">                                    )</span><br><span class="line">test_image_batch, test_label_batch &#x3D; tf.train.batch(</span><br><span class="line">                                    [test_image, test_label],</span><br><span class="line">                                    batch_size&#x3D;BATCH_SIZE</span><br><span class="line">                                    #,num_threads&#x3D;1</span><br><span class="line">                                    )</span><br><span class="line"></span><br><span class="line">print &quot;input pipeline ready&quot;</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">  # initialize the variables</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line">  # initialize the queue threads to start to shovel data</span><br><span class="line">  coord &#x3D; tf.train.Coordinator()</span><br><span class="line">  threads &#x3D; tf.train.start_queue_runners(coord&#x3D;coord)</span><br><span class="line"></span><br><span class="line">  print &quot;from the train set:&quot;</span><br><span class="line">  for i in range(20):</span><br><span class="line">    print sess.run(train_label_batch)</span><br><span class="line"></span><br><span class="line">  print &quot;from the test set:&quot;</span><br><span class="line">  for i in range(10):</span><br><span class="line">    print sess.run(test_label_batch)</span><br><span class="line"></span><br><span class="line">  # stop our queue threads and properly close the session</span><br><span class="line">  coord.request_stop()</span><br><span class="line">  coord.join(threads)</span><br><span class="line">  sess.close()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习：卷积神经网络与图像识别基本概念</title>
    <url>/2016/08/24/network-imageclassify/</url>
    <content><![CDATA[<h2 id="一-卷积神经网络的组成"><a href="#一-卷积神经网络的组成" class="headerlink" title="一 卷积神经网络的组成"></a>一 卷积神经网络的组成</h2><p> 图像分类可以认为是给定一副测试图片作为输入 $I \epsilon R^{W×H×C}$，输出该图片<br>属于哪一类。参数 W 是图像的宽度，H 是高度，C 是通道的个数；彩色图像中 C = 3，灰度图像<br>中 C = 1。一般的会设定总共类别的个数，例如在ImageNet竞赛中总共有 1000 个类别；在CIFAR10 中有 10 个类别。卷积神经网络则可以看成这样的黑匣子。输入是原始图片 I，输出是 L 维的向量 $v \epsilon R^L$。L表示预先设定的类别个数。向量 v 的每一个维度代表图像属于对应类别的可能性的大小。如果是<br>单类别识别问题，也就是说每一幅图像只分配 L 个标签中的一个标签，那么可以对 v 中的元素进行比较，选取最大的值对应的标签作为分类的结果。v 可以是一个概率分布的形式，即每一个元素$0 ≤ vi ≤ 1$，并且 $\sum_iv_i=1$ 。其中 $v_i$ 表示 v 的第 i 个元素。也可以是从负无穷大到正无穷大的实数，越大代表属于对应类别的可能性越大。在卷积神经网络的内部，是由很多的层构成。每一个层可以认为是一个函数，输入是信号 x，输出是信号 $y = f(x)$ 。输出的 y 又可以作为其他层的输入。以下从网络的前段，中端，末端的角度调研常用的层的定义。前端主要考虑对于图像的处理过程，中端是各种神经元，末端主要考虑与训练网络有关的损失函数。</p>
<h2 id="二-网络的前段"><a href="#二-网络的前段" class="headerlink" title="二 网络的前段"></a>二 网络的前段</h2><p>  前段指 的是对图像数据的处理，可以称之为数据层。</p>
<h3 id="2-1-数据裁减"><a href="#2-1-数据裁减" class="headerlink" title="2.1 数据裁减"></a>2.1 数据裁减</h3><p> 输入的图像的大小可能各不相同，有一些图像的分辨率较大，有一些比较小。而且长宽比也不一定会一样。对于这样的不一致性，理论上而言，可以不予处理，但是这要求网络中其他的层次支持这样的输入。目前大部分情况下采用的是通过裁剪的方法使得输出的图像是固定分辨率的。<br> 在网络训练的阶段，裁剪的位置从原始的图像上随机选择，只需要满足裁剪<br>的子图完全落在图像中即可。通过随机的方式，是因为相当于增加了额外的数据，能够缓解过拟合的问题。</p>
<h3 id="2-2-颜色干扰"><a href="#2-2-颜色干扰" class="headerlink" title="2.2 颜色干扰"></a>2.2 颜色干扰</h3><p> 裁剪之后的原图，每一个像素的是 0 到 255 的固定的数值。进一步的处理，包括减去均值，以及等比例缩放像素值使得像素值的分部基本在 [−1, 1] 之间。除了这些常规的操作之外，也会对图像进行归一化，相当于图像增强，比如 [9, 18, 17] 中对 CIFAR10 的数据预处理中。比如，对于每一个像素，随机选择 RGB 三个通道中的一个，然后在原像素值的基础上，随机添加一个从 [-20,20] 之间的数值。</p>
<h2 id="三-网络的中段"><a href="#三-网络的中段" class="headerlink" title="三 网络的中段"></a>三 网络的中段</h2><p>  以下介绍在卷及神经网络中常用的层的定义，即输入的数据 x 是什么维度，输出 y 是什么维度以及如何从输入得到输出。</p>
<h3 id="3-1-卷积神经网络的基本组成"><a href="#3-1-卷积神经网络的基本组成" class="headerlink" title="3.1 卷积神经网络的基本组成"></a>3.1 卷积神经网络的基本组成</h3><p> 如下图：</p>
<p>  <img src="/images/blog/cnn_consist.png" alt="卷积神经网络基本组成"></p>
<h3 id="3-2-卷积层"><a href="#3-2-卷积层" class="headerlink" title="3.2 卷积层"></a>3.2 卷积层</h3><p>  卷积层输入表示为 $x \epsilon R^{W\times H\times C}$,是一个三维的数据。表示有C个矩阵,每个矩阵这里表示为 $x^c \epsilon R^{W\times H}$,也称之为特征图。输出 $y \epsilon R^{W_0\times H_0\times C_0}$，也是一个三维数据。特征图分辨率从$W\times H$变为$W_0\times H_0$,特征图的个数也从C变为$C_0$。<br>    从输入到输出的一般公式为：</p>
<script type="math/tex; mode=display">
     y^{c_1}=\sum_cX^c*W^{c,c_1}</script><p>  矩阵 $w<em>{c,c_1}\epsilon R</em>{w\times h}$ 称之为卷积核。属于卷积层的参数，一般通过随机梯度下降更新。$x^c$ 为输入数据的第 c 个特征图，但在一些情况下，也会在图像的周围补白。符号 ∗ 表示二维数据的卷积运算。卷积定义为</p>
<script type="math/tex; mode=display">
    (X^c*W^{c,c_1})=\sum_{m,n}x^c_{m,n}w^{c,c_1}_{u-m,v-n}</script><p>符号 $()_{u,v}$ 表示对应矩阵的 u 行 v 列的元素值。在有一些的网络结构中，并不是选择所有的 (u, v)，而是每隔一定数量选择一个。<br>直观而言，卷积层相当于对图像进行滤波，希望能够抽象出来局部信息。局部信息通过较小的卷积核在图像不同的局部位置上扫描而得。<br><img src="/images/blog/cnn_compute.png" alt="卷积层计算"></p>
<p>下图是是一个动态示例，来源于 <a href="https://cs231n.github.io/convolutional-networks/#conv" target="_blank" rel="noopener">convolutional-networks</a></p>
<h3 id="3-3-池化层"><a href="#3-3-池化层" class="headerlink" title="3.3 池化层"></a>3.3 池化层</h3><p>输入的信号表示为 $x\epsilon R^{W\times H\times C}$，具有 C 个通道，每一个通道是一个特征图。输出 $y\epsilon R^{W_0\times H_0\times C}$ 具有的通道个数与输入相同，但是特征图的分辨率一般是降低。</p>
<p>池化层是对每一个特征图单独进行操作并且输出一个对应的特征图。假设池化范围是 $w \times h$，那么输入的特征图提取出来 $w \times h$ 的小图，然后寻找子图的最大值，或者计算子图的均值，作为一个输出。签证一般称之为最大化池化，后者是均值池化。从图像中提出小图的方式可以是任意一个子图，也可以是每隔多个像素值得到一个子图。池化层的作用包括降低特征图的分辨率，从而减少计算量，以及增强网络的鲁棒性。比如对于最大化池化的方式，对于图像的平移具有一定的鲁棒性。</p>
<p>池化层的作用包括降低特征图的分辨率，从而减少计算量，以及增强网络的鲁棒性。比如对于最大化池化的方式，对于图像的平移具有一定的鲁棒性。</p>
<p>实例，对于如下特征图 4x4，使用最大池化效果如下：<br><img src="/images/blog/cnn_maxpool.png" alt="池化"><br>图中每个像素点的值是上面各个格子的数值，然后要对这张 4<em>4的图片进行池化；那么采用最大池化也就是对上面 4</em>4的图片分块，每块大小为2*2，然后统计每个块的最大值，作为下采样后图片的像素值。</p>
<h3 id="3-4-CCCP"><a href="#3-4-CCCP" class="headerlink" title="3.4 CCCP"></a>3.4 CCCP</h3><p>CCCP层的输入是$x\epsilon R^{W\times H\times C}$，输出是$y\epsilon W\times H\times C$。特征层的分辨率保持不变，但是通道数有所改变。其定义为：</p>
<script type="math/tex; mode=display">
   y^{c_0}_{u,v}=\sum_cx^c_{u,v}w^{c,c_0}</script><p>等效于卷积核为 1x1的卷积层。<br>CCCP 层相当于在多个全连接层，每一个全连接将信号从 C 维度映射为$C_0$维度。</p>
<h3 id="3-5-ReLU-层及相关变体"><a href="#3-5-ReLU-层及相关变体" class="headerlink" title="3.5 ReLU 层及相关变体"></a>3.5 ReLU 层及相关变体</h3><p>该层的输入认识是一个信号 x。ReLU 并不要求输入信号的维度必须是一维或者几维的，因为该层的操作是对输出的每一个元素单独操作。但依然可以认为输入的 $x\epsilon R^{W\times H\times C}$。输出是一个和输入维度一样的信号y。<br>   假设从输入到输出的一个示例为:</p>
<script type="math/tex; mode=display">
 y_i =
\begin{cases}
    x_i,  & \text{if $x_i\ge 0$ } \\
    0, & \text{if $x_i<0$ }  \\
\end{cases}</script><p>显然这是一个非线性操作，ReLU 的存在使得网络的表达更加丰富。同时从定义中容易得出，该操作非常简单，并且在不同的输入点之间进行并行。ReLU 在一定程度上也是 S 行函数的近似。</p>
<script type="math/tex; mode=display">
   y_i=\frac{1}{1+e^{-x_i}}</script><p>进一步将ReLU改进为：</p>
<script type="math/tex; mode=display">
y_i =
\begin{cases}
x_i,  & \text{if $x_i\ge 0$ } \\
0.01x_i, & \text{if $x_i<0$ }  \\
\end{cases}</script><p>当元素值为负数的时候，通过 $y_i = 0.01x_i$ 的方式，避免了导数为 0，无法传播的情况。<br> 进一步使用修正的ReLu为：</p>
<script type="math/tex; mode=display">
   y_i=
   \begin{cases}
   x_i,  & \text{if $x_i\ge 0$ } \\
   \alpha x_i, & \text{if $x_i<0$ }  \\
   \end{cases}</script><p> 其中斜率 $\alpha$ 不再是一个固定的数值，而是通过梯度下降的方式就行优化</p>
<h3 id="3-6-Dropout层"><a href="#3-6-Dropout层" class="headerlink" title="3.6 Dropout层"></a>3.6 Dropout层</h3><p> Dropout层的输入为$x\epsilon R^{W\times H\times C}$。这里并不要求输入是三维的信号，任意可能的维度都是可以。Dropout 同样是针对每一个数据进行操作。输出 y 与输入的大小一致。在网路进行训练的时候，对于输入的每一个数值 $x_i$，按照概率 p 设置为 0，否则保留。数学形式可以写为:</p>
<script type="math/tex; mode=display">y_i=\epsilon x_i</script><p>   其中 $\epsilon$ 是随机变量，并且满足 $\epsilon = 0$的概率为 p，$\epsilon = 1$ 的概率为 1 − p。实际中，概率 p 往往设置<br>为 1。</p>
<p>然而在进行测试的时候，计算公式更正为 $y_i=(1-p)x_i$相当于一个期望。</p>
<p>Dropout层的引入主要是为了减少过拟合的问题，减少不同参数的耦合性。</p>
<h3 id="3-7-全连接层"><a href="#3-7-全连接层" class="headerlink" title="3.7 全连接层"></a>3.7 全连接层</h3><p>输入时$x\epsilon R^D$。这里要求将输入认识是一个列向量。输出为 $y\epsilon R^P$。从输入到输出的关系是:</p>
<script type="math/tex; mode=display">
   y= Wx+b</script><p>其中$W\epsilon R^{P\times D},b\epsilon R^P$,是投影矩阵阵以及阈值，是该层的参数，通过随机梯度下降的方式更新优化。</p>
<p>全连接层是一个非常常用的层，然而该层在一定程度上会损失图像的空间信息，故而在有一些网络中，抛弃了全连接层。</p>
<h3 id="3-8-局部响应归一化-LRN"><a href="#3-8-局部响应归一化-LRN" class="headerlink" title="3.8 局部响应归一化(LRN)"></a>3.8 局部响应归一化(LRN)</h3><p>  LRN层做的事是对当前层的输出结果做平滑处理。下图是示例:</p>
<p>  <img src="/images/blog/lrn.jpg" alt="LRN示例"><br> 前后几层（对应位置的点）对中间这一层做一下平滑约束。</p>
<p>  输入时一个三维信号$x\epsilon R^{W\times H\times C}$，输出也是一个三维信号$y\epsilon R^{W\times H\times C}$ 局部响应一体化层（Local Response Normalization）通过如下公式计算：</p>
<script type="math/tex; mode=display">
   y^c_{u,v}=x^c_{u,v}/(k+\alpha\sum^{C-1,i+n/2}_{j=max(0,i-n/2)}(x^c_{u,v})^2)^\beta</script><p>  其中$x^c_{,v}$ 代表第 $c$ 个通道上位置是$(u,v)$的信号值。示例参数设置为: $k=2,n=5,\alpha =10^{-4},\beta =0.75$</p>
<h3 id="3-9-批归一化层-Batch-Normalization"><a href="#3-9-批归一化层-Batch-Normalization" class="headerlink" title="3.9 批归一化层(Batch Normalization)"></a>3.9 批归一化层(Batch Normalization)</h3><p>  详细理解参考:<a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">Batch Normalization</a></p>
<p>  批归一化层的输入依然是三维的信号 $x\epsilon R^{W\times H\times C}$，输<br>  出 y 与输入具有相同的大小。其归一化的基本思路是对输入的每一个元素按照如下方式归一化:</p>
<script type="math/tex; mode=display">
     y_i=\alpha x_i+b</script><p>  使得输出的 $y_i$ 均值尽量的为 0，以及方差尽量为 1。通过这样的方式，每一层的数据分布基本上一致。该方法能够提升优化的速度。</p>
<p> 其优点如下：</p>
<ul>
<li>可以选择比较大的初始学习率,此算法有快速训练收敛的特性。</li>
<li>可以不用考虑过拟合中的 Dropout ,L2正则项选择问题，采用BN之后可以移除这两个参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性。</li>
<li>不再需要局部响应归一化层，因为BN本身就是一个归一化网络层。</li>
<li>可以完全打乱训练数据，防止每批训练时某些样本经常被挑选到。</li>
</ul>
<h2 id="四-网络的末端"><a href="#四-网络的末端" class="headerlink" title="四  网络的末端"></a>四  网络的末端</h2><p>  从网络训练的角度，末端主要是损失函数。也就是将数据映射为一个标量。通过随机梯度下降的方式，使得损失函数逐渐的降低。目前使用比较广泛的是 Softmax 回归和 Hinge 损失函数。</p>
<h3 id="4-1-Softmax回归"><a href="#4-1-Softmax回归" class="headerlink" title="4.1  Softmax回归"></a>4.1  Softmax回归</h3><p>较通俗易懂的理解Softmax回归的一篇博客是 <a href="http://www.cnblogs.com/BYRans/p/4905420.html" target="_blank" rel="noopener">Softmax回归</a></p>
<p>输入时 $v \epsilon R^L$，表示输入图像在各个类别上的可能性；同时需要输入图像的标签 k。输出是损失值。首先将输入归一化到 [0, 1] 之间，通过 Softmax 函数：</p>
<script type="math/tex; mode=display">
   Z_i =\frac{exp\{v_i\}}{\sum_j exp\{v_j\}}</script><p>然后通过交叉熵定义损失值，也就是：</p>
<script type="math/tex; mode=display">
   y = -log(Z_k)</script><p>该损失函数主要应用与单类别分类问题中。<br>下图是$softmax$函数的坐标轴图像:<br><img src="/images/blog/softmax_function.png" alt="softmax_function"><br>从数学上来看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。<br>从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p>
<h3 id="4-2-近似神经激活函数-Softplus-amp-ReLu"><a href="#4-2-近似神经激活函数-Softplus-amp-ReLu" class="headerlink" title="4.2   近似神经激活函数:Softplus&amp;ReLu"></a>4.2   近似神经激活函数:Softplus&amp;ReLu</h3><p>2001年，神经科学家Dayan、Abott从生物学角度，模拟出了脑神经元接受信号更精确的激活模型，该模型如下图所示:<br><img src="/images/blog/Softplus_function1.png" alt="softplus"></p>
<hr>

<p>参考文章:</p>
<ul>
<li><a href="http://www.voidcn.com/blog/xbinworld/article/p-4966477.html" target="_blank" rel="noopener">卷积神经网络CNN经典模型整理</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">Batch Normalization 学习笔记</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/51761865" target="_blank" rel="noopener">卷积神经网络入门学习2.0</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow:CIFAR-10 图像处理源码.input.py</title>
    <url>/2016/08/22/tensorflow-sourcecode-input/</url>
    <content><![CDATA[<h1 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __future__ import absolute_import</span><br><span class="line">from __future__ import division</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">from six.moves import xrange  # pylint: disable&#x3D;redefined-builtin</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 处理当前尺寸大小的图像，注意这与CIFAR-10图像的32x32尺寸不同。如果更新了这个数，那么整个模型架构都需要改变，并且模型需要重新训练</span><br><span class="line">IMAGE_SIZE &#x3D; 24</span><br><span class="line"></span><br><span class="line"># 描述 CIFAR-10 数据集的全局常量</span><br><span class="line">NUM_CLASSES &#x3D; 10</span><br><span class="line">NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN &#x3D; 50000</span><br><span class="line">NUM_EXAMPLES_PER_EPOCH_FOR_EVAL &#x3D; 10000</span><br></pre></td></tr></table></figure>
<p>  ‘’’<br>   作用： 读取并解析CIFAR10数据文件抽样数据。<br>   注意:  如果需要N路并行读取，N次调用此函数即可。它会返回N个读取不同文件和位置的独立的reader</p>
<pre><code>@param  filename_queue  要读取的文件名队列
@return 某个对象，具有以下字段:
        height: 结果中的行数 (32)
        width:  结果中的列数 (32)
        depth:  结果中颜色通道数(3)
        key:    一个描述当前抽样数据的文件名和记录数的标量字符串
        label:  一个 int32类型的标签，取值范围 0..9.
        uint8image: 一个[height, width, depth]维度的图像数据
</code></pre><p>  ‘’’<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def read_cifar10(filename_queue):</span><br><span class="line">  class CIFAR10Record(object):</span><br><span class="line">    pass</span><br><span class="line">  result &#x3D; CIFAR10Record()</span><br><span class="line"></span><br><span class="line">  # CIFAR-10图像数据集维度</span><br><span class="line">  # 输入格式详见 http:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~kriz&#x2F;cifar.html</span><br><span class="line">  label_bytes &#x3D; 1  # 2 for CIFAR-100</span><br><span class="line">  result.height &#x3D; 32</span><br><span class="line">  result.width &#x3D; 32</span><br><span class="line">  result.depth &#x3D; 3</span><br><span class="line">  image_bytes &#x3D; result.height * result.width * result.depth</span><br><span class="line">  # 每行记录由 图像+标签组成 ，并且每行的长度固定</span><br><span class="line">  record_bytes &#x3D; label_bytes + image_bytes</span><br><span class="line">  # 读取一行记录，从filename_queue队列中获取文件名。CIFAR-10格式没有header和footer。默认设置为0</span><br><span class="line">  reader &#x3D; tf.FixedLengthRecordReader(record_bytes&#x3D;record_bytes)</span><br><span class="line">  result.key, value &#x3D; reader.read(filename_queue)</span><br><span class="line"></span><br><span class="line">  # 将一个长度为record_bytes的字符串转换为uint8的向量</span><br><span class="line">  record_bytes &#x3D; tf.decode_raw(value, tf.uint8)</span><br><span class="line">  # 第一个字节代表了标签，将其类型由 uint8转换为 int32</span><br><span class="line">  result.label &#x3D; tf.cast(tf.slice(record_bytes, [0], [label_bytes]), tf.int32)</span><br><span class="line">  # 标签之后的字节代表了图像数据，并且将其维度从[depth * height * width]转换为 [depth, height, width].</span><br><span class="line">  depth_major &#x3D; tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]),</span><br><span class="line">                           [result.depth, result.height, result.width])</span><br><span class="line">  # 将 [depth, height, width] 转换为[height, width, depth].</span><br><span class="line">  result.uint8image &#x3D; tf.transpose(depth_major, [1, 2, 0])</span><br><span class="line">  return result</span><br></pre></td></tr></table></figure></p>
<p>   作用： 构建一队列的批量图像和标签</p>
<p>   @param  image :             维度为[height, width, 3]的3D张量<br>   @param  label :             图像标签<br>   @param  min_queue_examples: int32类型,从提供批量样本的队列中最小抽样数量.<br>   @param  batch_size:         每一批量的图像数量Number of images per batch.<br>   @param  shuffle:            boolean类型，决定是否使用混排队列</p>
<p>   @return<br>          images:              图像. 维度为 [batch_size, height, width, 3] 的4D张量<br>          labels:              一维标签，尺寸为 [batch_size]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  def _generate_image_and_label_batch(image, label, min_queue_examples,</span><br><span class="line">                                    batch_size, shuffle):</span><br><span class="line"></span><br><span class="line">  # 创建一个混排样本的队列，然后从样本队列中读取 &#39;batch_size&#39;数量的 images + labels数据（每个样本都是由images + labels组成）</span><br><span class="line">  num_preprocess_threads &#x3D; 16</span><br><span class="line">  if shuffle:</span><br><span class="line">    images, label_batch &#x3D; tf.train.shuffle_batch(</span><br><span class="line">        [image, label],</span><br><span class="line">        batch_size&#x3D;batch_size,</span><br><span class="line">        num_threads&#x3D;num_preprocess_threads,</span><br><span class="line">        capacity&#x3D;min_queue_examples + 3 * batch_size,</span><br><span class="line">        min_after_dequeue&#x3D;min_queue_examples)</span><br><span class="line">  else:</span><br><span class="line">    images, label_batch &#x3D; tf.train.batch(</span><br><span class="line">        [image, label],</span><br><span class="line">        batch_size&#x3D;batch_size,</span><br><span class="line">        num_threads&#x3D;num_preprocess_threads,</span><br><span class="line">        capacity&#x3D;min_queue_examples + 3 * batch_size)</span><br><span class="line"></span><br><span class="line">  # Display the training images in the visualizer.</span><br><span class="line">  tf.image_summary(&#39;images&#39;, images)</span><br><span class="line"></span><br><span class="line">  return images, tf.reshape(label_batch, [batch_size])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">作用： 使用Reader操作构建扭曲的输入(图像)用作CIFAR训练</span><br><span class="line"></span><br><span class="line">@param  data_dir:   CIFAR-10数据目录</span><br><span class="line">        batch_size: 每一批量的图像数</span><br><span class="line">@Returns:</span><br><span class="line">      images: Images. 尺寸为 [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] 的4D张量</span><br><span class="line">      labels: Labels. 大小为[batch_size] 的一维张量</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">def distorted_inputs(data_dir, batch_size):</span><br><span class="line"></span><br><span class="line">  filenames &#x3D; [os.path.join(data_dir, &#39;data_batch_%d.bin&#39; % i)</span><br><span class="line">               for i in xrange(1, 6)]</span><br><span class="line">  for f in filenames:</span><br><span class="line">    if not tf.gfile.Exists(f):</span><br><span class="line">      raise ValueError(&#39;Failed to find file: &#39; + f)</span><br><span class="line"></span><br><span class="line">  #创建一个先进先出的文件名队列，文件阅读器需要它来读取数据</span><br><span class="line">  filename_queue &#x3D; tf.train.string_input_producer(filenames)</span><br><span class="line">  # 从文件名队列中读取样本</span><br><span class="line">  read_input &#x3D; read_cifar10(filename_queue)</span><br><span class="line">  reshaped_image &#x3D; tf.cast(read_input.uint8image, tf.float32)</span><br><span class="line"></span><br><span class="line">  height &#x3D; IMAGE_SIZE</span><br><span class="line">  width &#x3D; IMAGE_SIZE</span><br><span class="line">  # 用于训练神经网络的图像处理，注意对图像进行了很多随机扭曲处理</span><br><span class="line">  # 随机修建图像的某一块[height, width]区域</span><br><span class="line">  distorted_image &#x3D; tf.random_crop(reshaped_image, [height, width, 3])</span><br><span class="line">  #随机水平翻转图像</span><br><span class="line">  distorted_image &#x3D; tf.image.random_flip_left_right(distorted_image)</span><br><span class="line">  # 由于这些操作都是不可累积的，考虑随机这些操作的顺序</span><br><span class="line">  distorted_image &#x3D; tf.image.random_brightness(distorted_image,</span><br><span class="line">                                               max_delta&#x3D;63)</span><br><span class="line">  distorted_image &#x3D; tf.image.random_contrast(distorted_image,</span><br><span class="line">                                             lower&#x3D;0.2, upper&#x3D;1.8)</span><br><span class="line"></span><br><span class="line">  # 减去均值并处以像素的方差 (标准化)</span><br><span class="line">  float_image &#x3D; tf.image.per_image_whitening(distorted_image)</span><br><span class="line"></span><br><span class="line">  # 确保随机混排有很好的混合性</span><br><span class="line">  min_fraction_of_examples_in_queue &#x3D; 0.4</span><br><span class="line">  min_queue_examples &#x3D; int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *</span><br><span class="line">                           min_fraction_of_examples_in_queue)</span><br><span class="line">  print (&#39;Filling queue with %d CIFAR images before starting to train. &#39;</span><br><span class="line">         &#39;This will take a few minutes.&#39; % min_queue_examples)</span><br><span class="line"></span><br><span class="line">  # 通过构建一个样本队列来生成一批量的图像和标签</span><br><span class="line">  return _generate_image_and_label_batch(float_image, read_input.label,</span><br><span class="line">                                         min_queue_examples, batch_size,</span><br><span class="line">                                         shuffle&#x3D;True)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">  作用： 使用Reader ops操作构建CIFAR评估的输入</span><br><span class="line"></span><br><span class="line">  @param:</span><br><span class="line">         eval_data: boolean类型, 是否使用训练或评估数据集</span><br><span class="line">         data_dir: CIFAR-10数据目录.</span><br><span class="line">    batch_size: 每一批的图像数量</span><br><span class="line">  Returns:</span><br><span class="line">        images: Images. 尺寸为[batch_size, IMAGE_SIZE, IMAGE_SIZE, 3]的4D张量.</span><br><span class="line">        labels: Labels. 大小为[batch_size]的一维张量.</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">def inputs(eval_data, data_dir, batch_size):</span><br><span class="line"></span><br><span class="line">  if not eval_data:</span><br><span class="line">    filenames &#x3D; [os.path.join(data_dir, &#39;data_batch_%d.bin&#39; % i)</span><br><span class="line">                 for i in xrange(1, 6)]</span><br><span class="line">    num_examples_per_epoch &#x3D; NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN</span><br><span class="line">  else:</span><br><span class="line">    filenames &#x3D; [os.path.join(data_dir, &#39;test_batch.bin&#39;)]</span><br><span class="line">    num_examples_per_epoch &#x3D; NUM_EXAMPLES_PER_EPOCH_FOR_EVAL</span><br><span class="line"></span><br><span class="line">  for f in filenames:</span><br><span class="line">    if not tf.gfile.Exists(f):</span><br><span class="line">      raise ValueError(&#39;Failed to find file: &#39; + f)</span><br><span class="line"></span><br><span class="line">  #创建一个先进先出的文件名队列，文件阅读器需要它来读取数据</span><br><span class="line">  filename_queue &#x3D; tf.train.string_input_producer(filenames)</span><br><span class="line"></span><br><span class="line">  # 从文件名队列中读取抽样</span><br><span class="line">  read_input &#x3D; read_cifar10(filename_queue)</span><br><span class="line">  reshaped_image &#x3D; tf.cast(read_input.uint8image, tf.float32)</span><br><span class="line"></span><br><span class="line">  height &#x3D; IMAGE_SIZE</span><br><span class="line">  width &#x3D; IMAGE_SIZE</span><br><span class="line"></span><br><span class="line">  # 用于做评估的图像处理</span><br><span class="line">  # 裁减图像的中心 [height, width] Crop the central [height, width] of the image.</span><br><span class="line">  resized_image &#x3D; tf.image.resize_image_with_crop_or_pad(reshaped_image,</span><br><span class="line">                                                         width, height)</span><br><span class="line"></span><br><span class="line">  # 减去均值并处以像素的方差（标准化）</span><br><span class="line">  float_image &#x3D; tf.image.per_image_whitening(resized_image)</span><br><span class="line"></span><br><span class="line">  # 确保良好的随机性</span><br><span class="line">  min_fraction_of_examples_in_queue &#x3D; 0.4</span><br><span class="line">  min_queue_examples &#x3D; int(num_examples_per_epoch *</span><br><span class="line">                           min_fraction_of_examples_in_queue)</span><br><span class="line"></span><br><span class="line">  # Generate a batch of images and labels by building up a queue of examples.</span><br><span class="line">  return _generate_image_and_label_batch(float_image, read_input.label,</span><br><span class="line">                                         min_queue_examples, batch_size,</span><br><span class="line">                                         shuffle&#x3D;False)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow：卷积神经网络的理解</title>
    <url>/2016/08/22/tensorflow-cnn/</url>
    <content><![CDATA[<h1 id="基本概念理解"><a href="#基本概念理解" class="headerlink" title="基本概念理解"></a>基本概念理解</h1><ul>
<li>图像中每个像素都想象成一个神经元</li>
<li>padding: 填充的意思。使用卷积对图像某部分做运算时，总会有部分没有覆盖到，需要决定是否填充以及使用什么填充方式。</li>
<li>stride： 移动切片的步长，影响取样的数量。每次将卷积核（矩阵）在图像上移动时，一次移动的位置量。</li>
<li>局部感知：图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。</li>
<li><p>其他理解：</p>
<ul>
<li>卷积核心。一个卷积是一个二维矩阵，卷积核心就是矩阵中心</li>
<li>Patch/Kernel：一个局部切片</li>
<li>Depth: 数据的深度(通道数)，图像数据是三维的，长宽和RGB，神经网络的预测输出也属于一维</li>
<li>池化：就是在图像上采样，抽取部分数据。</li>
</ul>
<h2 id="误差函数"><a href="#误差函数" class="headerlink" title="误差函数"></a>误差函数</h2><p>  令$y_j(n)$记为在输出层第$j$个神经元输出产生的函数信号。相应的，神经元$j$输出所产生的误差信号定义为：</p>
<script type="math/tex; mode=display">
           e_j(n)=d_j(n)-y_j(n)</script><p>  其中$d_j(n)$是响应向量$d(n)$的第$j$个元素。那么<strong>瞬时误差能量</strong>(<em>instaneous error energy</em>)定义为:</p>
<script type="math/tex; mode=display">
      \Im _j(n)=\frac{1}{2}e^2_j(n)</script><p>  将所有输出层误差能量相加，得到整个网络的全部瞬时误差能量:</p>
<script type="math/tex; mode=display">
      \Im (n)=\sum _{j\epsilon C}\Im(n)=\frac{1}{2}\sum_{i\epsilon C}e^2_j(n)</script><p>其中集合C包括输出层的所有神经元。设训练样本中包含N个样例，训练样本上的<strong>平均误差能量(error energy averaged over the training sample)</strong>或者说经验风险(empirical risk)定义为:</p>
<script type="math/tex; mode=display">
    \Im_{av}(N) = \frac{1}{N}\sum^N_{n=1}\Im (n)=\frac{1}{2N}\sum^N_{n=1}\sum_{j\epsilon C}e^2_j(n)</script><h2 id="epoch-回合"><a href="#epoch-回合" class="headerlink" title="epoch 回合"></a>epoch 回合</h2><p>   epoch可以理解为回合，一回合代表全部输入数据都走了一遍训练过程，神经网络的训练可能需要经历m个回合。注意：虽然每个回合都是全部输入数据，但是每次输入的组合(即batch组合)是随机的，因为每个batch都是随机从输入样本中抽取固定数量的样本，因而每次的组合各不相同。</p>
</li>
</ul>
<h2 id="padding的类别"><a href="#padding的类别" class="headerlink" title="padding的类别"></a>padding的类别</h2><p><img src="/images/blog/padding-example.png" alt="padding"></p>
<p>用一个3x3的网格在一个28x28的图像上做切片并移动,移动到边缘上的时候，如果不超出边缘，3x3的中心就到不了边界，因此得到的内容就会缺乏边界的一圈像素点，只能得到26x26的结果。而可以越过边界的情况下，就可以让3x3的中心到达边界的像素点，超出部分的矩阵补零就行</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow:使用tensorflow构建简单卷积神经网络</title>
    <url>/2016/08/18/tensorflow-CIFAR10/</url>
    <content><![CDATA[<p>本文翻译自: <a href="https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html" target="_blank" rel="noopener">使用tensorflow构建简单卷积神经网络</a></p>
<h2 id="一-概要"><a href="#一-概要" class="headerlink" title="一 概要"></a>一 概要</h2><p>CIFAR-10分类问题是机器学习领域的一个通用基准，其问题是将32X32像素的RGB图像分类成10种类别:<code>飞机</code>，<code>手机</code>，<code>鸟</code>，<code>猫</code>，<code>鹿</code>，<code>狗</code>，<code>青蛙</code>，<code>马</code>，<code>船</code>和<code>卡车</code>。<br>更多信息请移步<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>和<a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank" rel="noopener">Alex Krizhevsky的演讲报告</a></p>
<h2 id="二-目标"><a href="#二-目标" class="headerlink" title="二 目标"></a>二 目标</h2><p>本教程的目标是建立一个相对简单的CNN卷积神经网络用以识别图像。在此过程中，本教程:</p>
<ol>
<li>高亮网络架构，训练和验证的典型组织。</li>
<li>为构建更大更复杂的模型提供模板。</li>
</ol>
<p>选择 CIFAR-10的原因是其复杂程度足以训练tensorFlow拓展成超大模型能力的大部分。同时，于训练而言，此模型也足够小，这对于想实现新想法和尝试新技术堪称完美。</p>
<h2 id="三-教程的高亮部分"><a href="#三-教程的高亮部分" class="headerlink" title="三 教程的高亮部分"></a>三 教程的高亮部分</h2><p>  CIFAR-10教程演示了使用TensorFlow构建更大更复杂模型几个重要结构：</p>
<ul>
<li>核心数学组件包括<code>卷积</code>,<code>修正的线性激活</code>,<code>最大池化</code>,<code>LRN即局部响应一体化</code>（AlexNet的论文3.3章）</li>
<li>训练期间网络活动的可视化，包括输入图像、损失函数值、激活函数值和梯度的分布。</li>
<li>计算学习参数的均线（moving average）的惯常做法，以及在评估期间使用这些均线来促进预测性能。</li>
<li>随时间系统递减的学习速率清单的实现</li>
<li><p>为消除从磁盘读取模型的延迟和代价极大的图像预处理的预取队列。</p>
<p>我们同时提供了一个<a href="https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html#training-a-model-using-multiple-gpu-cards" target="_blank" rel="noopener">多GPU版本</a>的模型演示：</p>
<ul>
<li>配置并行环境下跨多个GPU的训练的单个模型</li>
<li>在多GPU中共享和更新变量<br>我们期望此教程能为基于TensorFlow的可视化任务创建更大CNNs模型提供起点。</li>
</ul>
</li>
</ul>
<h2 id="四-模型架构"><a href="#四-模型架构" class="headerlink" title="四 模型架构"></a>四 模型架构</h2><p>  CIFAR-10教程中的模型是一个由可选的卷积和非线性层组成的多层架构。这些网络层之后是连接到一个softmax分类器的全连接的网络层。模型遵照Alxe Krizhevsky所描述的模型架设，只是最前面的几层有细微差别。<br>   此模型获得了一个极致的表现，在一个GPU上训练几个小时可以达到约86%的准确率。下文和代码部分有详细说明。模型由1068298个可学习的参数并且单张图片需要195000000个加乘操作以计算推理。</p>
<h2 id="五-代码组织"><a href="#五-代码组织" class="headerlink" title="五 代码组织"></a>五 代码组织</h2><p> 此教程的代码位于 <a href="https://www.tensorflow.org/code/tensorflow/models/image/cifar10/" target="_blank" rel="noopener">tensorflow/models/image/cifar10</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文件</th>
<th>目标</th>
</tr>
</thead>
<tbody>
<tr>
<td> cifar10_input.py</td>
<td>读取本地的CIFAR-10二进制文件</td>
</tr>
<tr>
<td> cifar10.py</td>
<td>创建CIFAR-10模型</td>
</tr>
<tr>
<td> cifar10_train.py</td>
<td>在CPU或者GPU上训练CIFAR-10模型</td>
</tr>
<tr>
<td> cifar10_multi_gpu_train.py</td>
<td>在多个GPU上训练CIFAR-10模型</td>
</tr>
<tr>
<td> cifar10_eval.py</td>
<td>评估CIFAR-10模型的预测性能</td>
</tr>
</tbody>
</table>
</div>
<h2 id="六-CIFAR-10-模型"><a href="#六-CIFAR-10-模型" class="headerlink" title="六 CIFAR-10 模型"></a>六 CIFAR-10 模型</h2><p>  CIFAR-10网络大部分代码在 <code>cifar10.py</code>。完整的训练图包括大概765个操作，我们发现通过使用以下模块来构建计算图，我们可以最大限度的重用代码:</p>
<ol>
<li><strong>模型输入</strong>: inputs() 和 distorted_inputs() 分别是评估和训练的读取并预处理CIFAR图像的加法操作</li>
<li><strong>模型预测</strong>: inference()是推理加法操作，即在提供的图像中进行分类。</li>
<li><strong>模型训练</strong>： loss() 和 train() 的加法操作是用来计算损失函数，梯度，变量更新和可视化摘要。</li>
</ol>
<h2 id="七-模型输入"><a href="#七-模型输入" class="headerlink" title="七 模型输入"></a>七 模型输入</h2><p>  模型的输入部分由从CIFAR-10的二进制文件读取函数 inputs()和distorted_inputs() 完成。这些文件包括固定字节长度的记录，因此我们使用<code>tf.FixedLengthRecordReader</code> 。查看<strong>读取数据</strong>来学习<em>Reader class</em>如何实现。<br>  图像将按照以下步骤进行处理：</p>
<ul>
<li>它们被裁减成24x24的像素，中央部分用来做评估或随机地用以训练。</li>
<li><p>它们是<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#per_image_whitening" target="_blank" rel="noopener"> approximately whitened</a> 用以使模型对动态变化不敏感。</p>
<p>对于训练部分，我们会额外应用一系列随机扭曲来人为增加数据集：</p>
</li>
<li><p>将图像随机的左右翻转<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#random_flip_left_right" target="_blank" rel="noopener">随机翻转</a></p>
</li>
<li>随机扰乱图像亮度<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#random_brightness" target="_blank" rel="noopener">随机亮度</a></li>
<li>随机扰乱图像对比度<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#random_contrast" target="_blank" rel="noopener">随机对比度</a></li>
</ul>
<p>在<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html" target="_blank" rel="noopener">图像页</a>可以查看可用的扭曲方法列表。为了在TensorBoard中可视化，我们也在图像中增加了一个 <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html" target="_blank" rel="noopener">缩略图</a>。这对于校验输入是否构建正确是个良好举措。<br><img src="/images/blog/cifar_image_summary.png" alt=""><br>从磁盘中读取图像并扰乱，可能会消耗不确定的时间。为防止这些操作拖慢训练过程，我们使用16个独立线程不断地从一个TensorFlow队列。</p>
<h2 id="八-模型预测"><a href="#八-模型预测" class="headerlink" title="八 模型预测"></a>八 模型预测</h2><p> 模型的预测部分由<em>inference()</em>函数构建，该操作会添加其他操作以计算预测逻辑。模型的此部分组织如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络层名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td> conv1</td>
<td>卷积和修正线性激活层</td>
</tr>
<tr>
<td> pool1</td>
<td>最大池化</td>
</tr>
<tr>
<td> norm1</td>
<td>局部响应一体化</td>
</tr>
<tr>
<td> conv2</td>
<td>卷积和修正线性激活层</td>
</tr>
<tr>
<td> norm2</td>
<td>局部响应一体化</td>
</tr>
<tr>
<td> pool2</td>
<td>最大池化</td>
</tr>
<tr>
<td> local3</td>
<td>使用修正线性激活的全连接层</td>
</tr>
<tr>
<td> local4</td>
<td>使用修正线性激活的全连接层</td>
</tr>
<tr>
<td> softmax_linear</td>
<td>线性转换以产生logits</td>
</tr>
</tbody>
</table>
</div>
<p> 下图由<strong>TensorBoard</strong>生成用以描述推理操作<br> <img src="/images/blog/cifar_graph.png" alt="推理操作"></p>
<p> <em>input()</em>和<em>inference()</em> 函数提供了在模型上进行评估的全部所需组件。我们先将注意力移到训练模型。</p>
<h2 id="九-模型训练"><a href="#九-模型训练" class="headerlink" title="九 模型训练"></a>九 模型训练</h2><p>  训练一个完成N类分类的网络的常用方法是多项式logstic回归,aka.softmax回归。softmax回归在网络输出上应用一个 <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#softmax" target="_blank" rel="noopener">softmax</a>非线性函数并计算标准预测和标签的<strong>1-hot</strong>编码的交叉熵。我们也将通常使用的权值衰减损益应用到所有学习变量上来完成正则化。  模型的目标函数是交叉熵损失之和，以及由 <em>loss()</em>函数返回的权值衰减项。<br>  我们在<em>TensorBoard</em>使用<strong>scalar_summary</strong>对其可视化：<br>  <img src="/images/blog/cifar_loss.png" alt=""><br>  我们使用标准的梯度下降算法（见对其他算法的<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html" target="_blank" rel="noopener">训练</a>方法），其学习速率随时间指数递减。<br>  <img src="/images/blog/cifar_lr_decay.png" alt=""></p>
<p>  <strong>train()</strong>函数添加必要的操作通过计算梯度和更新学习变量（详见<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#GradientDescentOptimizer" target="_blank" rel="noopener">GradientDescentOptimizer</a>）以最小化目标变量。<br>  该函数返回一个执行所有的训练和更新一批图像所需的计算操作。</p>
<h2 id="十-运行和训练模型"><a href="#十-运行和训练模型" class="headerlink" title="十 运行和训练模型"></a>十 运行和训练模型</h2><p>   通过运行脚本<em>cifar10_train.py</em>训练操作<br>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python cifar10_train.py</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意：首次运行该脚本时，CIFAR-10数据集会被自动下载。数据集大小为 160MB</p>
</blockquote>
<p>  如果成功，你将会看到如下输出:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</span><br><span class="line">2015-11-04 11:45:45.927302: step 0, loss &#x3D; 4.68 (2.0 examples&#x2F;sec; 64.221 sec&#x2F;batch)</span><br><span class="line">2015-11-04 11:45:49.133065: step 10, loss &#x3D; 4.66 (533.8 examples&#x2F;sec; 0.240 sec&#x2F;batch)</span><br><span class="line">2015-11-04 11:45:51.397710: step 20, loss &#x3D; 4.64 (597.4 examples&#x2F;sec; 0.214 sec&#x2F;batch)</span><br><span class="line">2015-11-04 11:45:54.446850: step 30, loss &#x3D; 4.62 (391.0 examples&#x2F;sec; 0.327 sec&#x2F;batch)</span><br><span class="line">2015-11-04 11:45:57.152676: step 40, loss &#x3D; 4.61 (430.2 examples&#x2F;sec; 0.298 sec&#x2F;batch)</span><br><span class="line">2015-11-04 11:46:00.437717: step 50, loss &#x3D; 4.59 (406.4 examples&#x2F;sec; 0.315 sec&#x2F;batch)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p> 脚本每运行10次输出一次总的损失函数值。注意：</p>
<ul>
<li>第一批数据可能会相当慢（例如：几分钟），因为预处理线程会将20万张处理过的图像数据填充混洗队列。</li>
<li>输出的损失函数值是最近一批数据的均值，要记得损失函数值是 交叉熵和权值递减项的总和。</li>
<li><p>密切关注批处理速度，以上数据由 <em>Tesla K40c</em>机器上输出，如果在CPU上运行可能会输出比这个更低的速率。</p>
<p><em>cifar10_train</em>会定期保存所有的模型参数到<em>checkpoint files</em>，但是它并不评估模型。checkpoint files将会被用于<em>cifar10_eval.py</em>来衡量预测性能。见下文的<strong>评估模型</strong></p>
<p>脚本<em>cifar10_train.py</em>的终端输出的文本只提供了模型如何训练的最小视角。我们可能需要更多的信息：</p>
</li>
<li><p>损失函数值真的在递减吗？还是说只是噪声</p>
</li>
<li>模型的输入训练图像是否合适？</li>
<li>其梯度，激活函数，权值是否合理？</li>
<li><p>当前的学习速率？</p>
<p><a href="https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html" target="_blank" rel="noopener">TensorBoard </a>提供了此类函数，展示了脚本<em>cifar10_train.py</em>通过<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#SummaryWriter" target="_blank" rel="noopener">SummaryWriter</a>阶段性输出的数据。<br>比如说，我们可以查看激活函数分布和训练过程中<em>local3</em>特性的稀疏度。<br><img src="/images/blog/cifar_sparsity.png" alt=""></p>
<p>损失函数值会随着时间呈现，但是由于训练过程中某些批量数据量过小而出现一些噪音数据。实际情况中，我们发现可视化原生数据之外的均线(moving averages)很有用。可以通过脚本<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#ExponentialMovingAverage" target="_blank" rel="noopener">ExponentialMovingAverage</a>查看。</p>
</li>
</ul>
<h2 id="十一-评估模型"><a href="#十一-评估模型" class="headerlink" title="十一 评估模型"></a>十一 评估模型</h2><p>接下来，我们需要评估我们的训练模型在hold-out数据集上的性能。模型由脚本<em>cifar10_eval.py</em>评估。它由<em>inference()</em>函数构建模型，并使用cifar-10数据集中的10000张图像。它会计算图像的真实标签的最高预测匹配的频率。<br> 为观察模型如何逐步在训练过程提高，评估脚本会阶段性地运行由<em>cifar10_train.py</em>脚本创建的checkpoint files。</p>
<pre><code>python cifar10_eval.py
</code></pre><p><strong>注意:不要在相同的GPU上运行训练和评估，否则会出现内存不足错误。可以考虑在分开的GPU上运行，或者在运行评估程序时挂起训练程序</strong></p>
<p>你将会看到如下输出：<br>    <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2015-11-06 08:30:44.391206: precision @ 1 &#x3D; 0.860</span><br><span class="line">...</span><br></pre></td></tr></table></figure><br>   脚本极少情况下会返回精确度为precision @ 1 ，当前这个例子返回的是86%的准确率。<em>cifar10_eval.py</em>也会导出一些可以在<em>TensorBoard</em>里可视化的概要信息。<br>  训练脚本计算所有的学习参数的变动均值(moving average version)。评估脚本则用变动均值(moving average version)替换所有的学习的模型参数，这些替换将有益于模型评估时的性能。</p>
<h2 id="十二-使用多GPU训练模型"><a href="#十二-使用多GPU训练模型" class="headerlink" title="十二 使用多GPU训练模型"></a>十二 使用多GPU训练模型</h2><p>   在分布式并行环境下训练模型需要一致的训练过程。接下来，我们将模型副本分配到训练数据集的每个子集上。<br>   简单地运行异步更新模型参数将导致局部最优化，这是因为单个模型的副本可能会在过时的模型参数上进行训练了。反过来说，完全的同步更新将会使整个过程与最慢的模型副本一样慢。<br>   工作站中的多GPU一般有相似的速度和内存。因此，我们按照如下步骤设计训练系统:</p>
<ul>
<li>每个GPU上放一份独立的模型副本。</li>
<li><p>通过等待所有的GPU完成某一批数据的处理来同步更新模型参数。</p>
<p>如下是模型的数据图:<br><img src="/images/blog/Parallelism.png" alt=""><br>注意到，每个GPU都会计算一份独一无二的数据的<em>inference</em>和梯度。这样的设置将极有效地在多GPU之间划分一个大批量数据。<br>这样的设置需要所有的GPU共享模型参数。众所周知，从GPU传输数据或者传输数据到GPU都相当慢。因此，我们决定在<strong>CPU</strong>上存储和更新所有的模型参数(见绿色盒图)。只有当一批数据已经被所有GPU处理之后，一份新的模型参数才会被传输到GPU。<br>GPU在操作上是同步的，所有的来自各个GPU的梯度都会被累加和均值化。模型参数会被所有模型的梯度均值更新。</p>
</li>
</ul>
<h2 id="十三-在设备上放置变量和操作"><a href="#十三-在设备上放置变量和操作" class="headerlink" title="十三 在设备上放置变量和操作"></a>十三 在设备上放置变量和操作</h2><p>在设备上放置操作和变量需要一些特殊抽象。<br>第一个抽象是我们需要一个函数来计算模型的单个副本的<em>inference</em>和梯度。在代码中，我们将这种抽象标为<strong>tower</strong>。我们必须为每个<strong>tower</strong>设置两个参数。</p>
<ul>
<li>一个<em>tower</em>内所有的操作都需要一个独一无二的名字。<em>tf.name_scope()</em>通过前置范围的形式提供了方法。比如说，第一个tower,tower_0内的所有操作都会有一个前置 <em>tower_0/conv1/Conv2D</em>。</li>
<li><p>某个tower的首选硬件设备。<em>tf.device()</em>可以来指定。比如说，第一个tower的所有的操作使用范围 <em>device(‘/gpu:0’)</em> 指明其应当在第一颗GPU上运行。</p>
<p>所有的变量都会被固定到<strong>CPU</strong>，并且可以通过<em>tf.get_variable()</em>在多GPU之间共享。关于如何<a href="https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html" target="_blank" rel="noopener">共享变量</a>请见下文。</p>
</li>
</ul>
<h2 id="十四-在多GPU上运行和训练模型"><a href="#十四-在多GPU上运行和训练模型" class="headerlink" title="十四 在多GPU上运行和训练模型"></a>十四 在多GPU上运行和训练模型</h2><p>对于有多颗GPU的机器，使用<em>cifar10_multi_gpu_train.py</em>脚本将会更快的训练模型。次代码在多颗GPU之间并行训练模型。</p>
<pre><code>python cifar10_multi_gpu_train.py --num_gpus=2
</code></pre><p>GPU数量默认为1.</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>谷歌TensorFlow基本概念</title>
    <url>/2016/06/13/GoogleTensorFlowBasicConcept/</url>
    <content><![CDATA[<h1 id="start-up"><a href="#start-up" class="headerlink" title="start up"></a>start up</h1><h2 id="1-1-谷歌深度学习工具历史"><a href="#1-1-谷歌深度学习工具历史" class="headerlink" title="1.1 谷歌深度学习工具历史:"></a>1.1 谷歌深度学习工具历史:</h2><ol>
<li>第一代：<strong>DistBelief</strong> 由 Dean于2011年发起，主要产品有：<ul>
<li>Inception (图像识别领域)</li>
<li>谷歌Search</li>
<li>谷歌翻译</li>
<li>谷歌照片</li>
</ul>
</li>
<li>第二代：<strong>TensorFlow</strong> 由Dean于2015年11月发起，大部分DistBelief都转向了TensorFlow</li>
</ol>
<h2 id="1-2-产品特性"><a href="#1-2-产品特性" class="headerlink" title="1.2 产品特性"></a>1.2 产品特性</h2><div class="table-container">
<table>
<thead>
<tr>
<th>概念</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>编程模型</td>
<td>类数据流的模型</td>
</tr>
<tr>
<td>语言</td>
<td>Python C++</td>
</tr>
<tr>
<td>部署</td>
<td>code once,run ererywhere</td>
</tr>
<tr>
<td>计算资源</td>
<td>cpu,gpu</td>
</tr>
<tr>
<td>分布式处理</td>
<td>本地实现，分布式实现</td>
</tr>
<tr>
<td>数学表达式</td>
<td>数学图表达式，自动分化</td>
</tr>
<tr>
<td>优化</td>
<td>自动消除，kernel 优化，通信优化，支持模式，数据并行</td>
</tr>
</tbody>
</table>
</div>
<h2 id="1-3-计算图"><a href="#1-3-计算图" class="headerlink" title="1.3 计算图"></a>1.3 计算图</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([100]))                   # 100维的向量，都初始化为0</span><br><span class="line">w &#x3D; tf.Variable(tf.random_uniform([784,100],-1,1)) # 784x100的矩阵</span><br><span class="line">x &#x3D; tf.placeholder(name&#x3D;&quot;x&quot;)                       # 输入的占位符placeholder</span><br><span class="line">relu &#x3D; tf.nn.relu(tf.matmul(w,x)+b)                # Relu(Wx+b)</span><br><span class="line">C &#x3D;[...]                                           # 使用relu的一个函数计算代价</span><br></pre></td></tr></table></figure>
<p>对应的计算图如下:<br><img src="/images/blog/tensorflow_basicconcept.png" alt="计算图"></p>
<h2 id="1-4-Tensorflow的代码样例"><a href="#1-4-Tensorflow的代码样例" class="headerlink" title="1.4 Tensorflow的代码样例"></a>1.4 Tensorflow的代码样例</h2><ol>
<li>构建数据流图的第一部分代码</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"># 创建100个numpy的 x,y 假数据点，y &#x3D; x*0.1+0.3</span><br><span class="line">x_data &#x3D; np.random.rand(100).astype(&quot;float32&quot;)</span><br><span class="line">y_data &#x3D; x_data*0.1+0.3</span><br><span class="line"># 找出计算 y_data &#x3D;W*x_data+b的w和b的值，虽然我们知道w&#x3D;0.1,b&#x3D;0.3,但是tensorflow会找到并计算出来</span><br><span class="line">w &#x3D; tf.Variable(tf.random_uniform)</span><br></pre></td></tr></table></figure>
<h1 id="2-tensorflow概览"><a href="#2-tensorflow概览" class="headerlink" title="2 tensorflow概览"></a>2 tensorflow概览</h1><p>要使用tensorflow的话，你需要理解以下概念:</p>
<ul>
<li>图代表了计算</li>
<li>图需要在会话(Sessions)中执行</li>
<li>张量(tensor)代表数据</li>
<li>使用Variables来持有状态</li>
<li>使用<strong>feeds</strong> 和 <strong>fetches</strong>来获得任何操作的输入输出数据</li>
</ul>
<p>tensorflow的概览</p>
<ul>
<li>一个将计算转化为图的编程系统</li>
<li>图中的节点是：<ul>
<li>操作(op):执行某些计算</li>
<li>输入(input):一个或多个张量(tensorflow)</li>
<li>Tensor张量：一个有类型的多维数组</li>
</ul>
</li>
</ul>
<h1 id="3-两个计算阶段"><a href="#3-两个计算阶段" class="headerlink" title="3 两个计算阶段"></a>3 两个计算阶段</h1><h2 id="3-1-在图中计算"><a href="#3-1-在图中计算" class="headerlink" title="3.1 在图中计算"></a>3.1 在图中计算</h2><ul>
<li>图必须在Session中运行</li>
<li>会话(Session)<ul>
<li>将图操作放入到设备上，比如CPUs和GPUs</li>
<li>提供执行方法</li>
<li>返回操作产生的张量，比如python中的<strong>numpy ndarray对象</strong>，以及C和C++<strong>tensorflow::Tensor</strong>实例。</li>
</ul>
</li>
</ul>
<h2 id="3-2-图中的两个计算阶段"><a href="#3-2-图中的两个计算阶段" class="headerlink" title="3.2 图中的两个计算阶段"></a>3.2 图中的两个计算阶段</h2><ol>
<li><p>构建阶段</p>
<ul>
<li>形成图</li>
<li>创建图来代表神经网络并训练这个神经网络</li>
</ul>
</li>
<li><p>执行阶段</p>
<ul>
<li>使用会话执行途中的操作</li>
<li>重复执行图中训练操作集合</li>
</ul>
</li>
<li><p>构建图</p>
</li>
</ol>
<ul>
<li>开始那些不需要任何输入(source ops)的操作(op)，常量</li>
<li>将它们的输出传入到其他做计算的操作</li>
<li>操作构建者返回对象<ul>
<li>代表了结构化操作的输出</li>
<li>将这些输出传入其他操作构建者作为输入</li>
</ul>
</li>
</ul>
<ol>
<li><p>默认图</p>
<p>将节点加入此图的操作构建者</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 创建一个产生1x2的矩阵的常量操作，操作被作为节点加入到默认图</span><br><span class="line"># 构建者的返回值代表了常量操作的输出</span><br><span class="line">matrix1 &#x3D; tf.constant([[3,3.]])</span><br><span class="line"># 创建另外一个产生 2x1矩阵的常量操作</span><br><span class="line">matrix2 &#x3D; tf.constant([[2.0],[2.]])</span><br></pre></td></tr></table></figure>
<p>   有三个节点：两个<strong>constant</strong>操作(ops)以及一个<strong>matmul</strong>操作<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个Matmul操作，将 matrix1和matrix2作为输入</span><br><span class="line"># 返回值，‘product’，代表了矩阵相乘的结果</span><br><span class="line">product &#x3D; tf.matmul(matrix1,matrix2)</span><br></pre></td></tr></table></figure></p>
<ol>
<li>在会话Session中运行图</li>
</ol>
<ul>
<li>创建一个Session对象：应该在被关闭以释放资源</li>
<li>没有参数，session构建者运行默认图</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 运行默认图</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line"># 要运行matmul操作，我们调用了session的‘run()’方法，传入&#39;producr&#39;代表了matmul操作的输出。这即回调了matmul操作的输出结果</span><br><span class="line"># 操作的输出以一个numpy的&#39;ndarray&#39;对象返回&#39;result&#39;</span><br><span class="line">result &#x3D; sess.run(product)</span><br><span class="line">print result</span><br><span class="line"># &#x3D;&#x3D;&gt;[[12.]]</span><br><span class="line">#关闭会话</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<ol>
<li>Session运行图，Session.run()方法执行操作</li>
<li>一个Session块(block)<ul>
<li>在块的结尾自动关闭<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    result &#x3D; sess.run([product])</span><br><span class="line">    print result</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>GPU的使用</li>
</ol>
<ul>
<li>将图定义转换为分布在各种计算资源，比如CPU和GPU之间的可执行操作</li>
<li>如果有GPU，tensorflow会有限使用GPU</li>
</ul>
<h1 id="4-交互使用"><a href="#4-交互使用" class="headerlink" title="4 交互使用"></a>4 交互使用</h1><ul>
<li>在python环境中，比如Ipython,<strong>InteractiveSession</strong>类会被使用</li>
<li><strong>Tensor.eval()</strong>和<strong>Operation.run()</strong></li>
<li>这可以避免必须用一个变量来保持一个session</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 进入一个交互的Tensorflow Session</span><br><span class="line">import tensorflow as tf</span><br><span class="line">sess &#x3D; tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x &#x3D; tf.Variable([1.0,2.0])</span><br><span class="line">y &#x3D; tf.constant([3.0,3.0])</span><br><span class="line">#使用&#39;x&#39;的initializer的 run() 方法初始化</span><br><span class="line">x.initializer.run()</span><br><span class="line"></span><br><span class="line"># 添加一个操作从&#39;x&#39;中抽取&#39;a&#39;，执行并打印结果</span><br><span class="line">sub &#x3D; tf.sub(x,a)</span><br><span class="line">print sub.eval()</span><br><span class="line"># &#x3D;&#x3D;&gt;[-2,-1.]</span><br><span class="line"></span><br><span class="line"># 关闭session</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<h1 id="5-张量-Tensors"><a href="#5-张量-Tensors" class="headerlink" title="5 张量(Tensors)"></a>5 张量(Tensors)</h1><ul>
<li>Tensor(张量)数据结构代表了所有数据</li>
<li>在计算图中只有张量在操作之间传递</li>
<li>n维数组或者列表<ul>
<li>静态类型，秩，或者 shape</li>
</ul>
</li>
</ul>
<p><strong>rank(秩)</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>rank</th>
<th>数学实体</th>
<th>python示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Scalar(大小)</td>
<td>s =483</td>
</tr>
<tr>
<td>1</td>
<td>Vector(大小和方向)</td>
<td>v=[1.1,2.2,3.3]</td>
</tr>
<tr>
<td>2</td>
<td>Matrix(数据表)</td>
<td>m=[[1,2,3],[4,5,6],[7,8,9]]</td>
</tr>
<tr>
<td>3</td>
<td>3-Tensor(立方(cube)的数量)</td>
<td>t=[[[2],[4],[6],[8]],[[10],[12]]]</td>
</tr>
<tr>
<td>n</td>
<td>n-Tensor</td>
<td>同上</td>
</tr>
</tbody>
</table>
</div>
<p><strong>shape</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rank</th>
<th>Shape</th>
<th>维数</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[]</td>
<td>0-D</td>
<td>一个0-D张量，一个标量</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>一个1-D张量，shape是[5]</td>
</tr>
<tr>
<td>2</td>
<td>[D0,D1]</td>
<td>2-D</td>
<td>一个2-D张量，shape是[3,4]</td>
</tr>
<tr>
<td>3</td>
<td>[D0,D1,D2]</td>
<td>3-D</td>
<td>一个3-D张量，shape[1,4,3]</td>
</tr>
<tr>
<td>n</td>
<td>[D0,D1,D2,…Dn]</td>
<td>n-D</td>
<td>一个n-D张量，shape是[D0,D1,…Dn]</td>
</tr>
</tbody>
</table>
</div>
<p><strong>数据类型</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Data type</th>
<th>python类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DT_FLOAT</td>
<td>tf.float32</td>
<td>32位浮点类型</td>
</tr>
<tr>
<td>DT_DOUBLE</td>
<td>tf.float64</td>
<td>64位浮点类型</td>
</tr>
<tr>
<td>DT_INT64</td>
<td>tf.int64</td>
<td>64位有符号整型</td>
</tr>
<tr>
<td>DT_INT32</td>
<td>tf.int32</td>
<td>32位有符号整型</td>
</tr>
<tr>
<td>DT_INT16</td>
<td>tf.int16</td>
<td>16位有符号整型</td>
</tr>
<tr>
<td>DT_INt8</td>
<td>tf.int8</td>
<td>8位有符号整型</td>
</tr>
<tr>
<td>DT_UINT</td>
<td>tf.unit8</td>
<td>8位无符号整型</td>
</tr>
<tr>
<td>DT_STRING</td>
<td>tf.string</td>
<td>变量长度的字节数组，Tensor每个元素是一个字节数组</td>
</tr>
<tr>
<td>DT_BOOL</td>
<td>tf.bool</td>
<td>Boolean</td>
</tr>
<tr>
<td>DT_COMPLEX64</td>
<td>tf.complex64</td>
<td>由两个32位浮点数组成的复数，实数和大小部分</td>
</tr>
<tr>
<td>DT_QINT32</td>
<td>tf.qint32</td>
<td>量化操作中32位有符号整型</td>
</tr>
<tr>
<td>DT_QINT8</td>
<td>tf.qint8</td>
<td>量化操作中8位有符号整型</td>
</tr>
<tr>
<td>DT_QUINT8</td>
<td>tf.quint8</td>
<td>量化操作中8位无符号整型</td>
</tr>
</tbody>
</table>
</div>
<h1 id="6-变量"><a href="#6-变量" class="headerlink" title="6 变量"></a>6 变量</h1><p>变量的创建、初始化、存储和载入</p>
<ul>
<li>为了持有和更新参数，在图中保持状态可以通过调用 <strong>run()</strong>方法</li>
<li>内存buffer包含张量</li>
<li>必须是明确初始化并且在训练期间和训练之后存储到磁盘上的</li>
<li>类 <strong>tf.Variable</strong><ul>
<li>构造器：变量的初始化值，一个任意类型和shape的张量</li>
<li>构造之后，类型和shape都会固定</li>
<li>使用<strong>assign</strong>操作op， validate_shape = False<h2 id="6-1-创建"><a href="#6-1-创建" class="headerlink" title="6.1 创建"></a>6.1 创建</h2></li>
</ul>
</li>
<li>传入一个张量作为初始值到 Variable构造方法中</li>
<li>初始值：常量constants,序列化和随机值<ul>
<li>tf.zeros(),tf.linspace(),tf.random_normal()</li>
</ul>
</li>
<li>固定shape：与操作的shape相同</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建两个变量</span><br><span class="line">weight &#x3D; tf.Variable(tf.random_normal([784,200],stddev&#x3D;0.35),name &#x3D;&quot;weights&quot;)</span><br><span class="line">biases &#x3D; tf.Variable(tf.zeros([200]),name &#x3D;&quot;biases&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>调用 tf.Variable() 加入操作到图中</li>
</ul>
<h2 id="6-2-初始化"><a href="#6-2-初始化" class="headerlink" title="6.2 初始化"></a>6.2 初始化</h2><ul>
<li>添加一个操作并执行</li>
<li>tf.initialize_all_variables()</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加一个操作来初始化变量</span><br><span class="line">init_op &#x3D; tf.initialize_all_variables()</span><br><span class="line"># 过后，执行model</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #运行初始化操作</span><br><span class="line">    sess.run（init_op）</span><br></pre></td></tr></table></figure>
<h2 id="6-3-存储和恢复"><a href="#6-3-存储和恢复" class="headerlink" title="6.3 存储和恢复"></a>6.3 存储和恢复</h2><ul>
<li><strong>tf.saver</strong></li>
<li>检查点文件：Variables都存储在二进制文件中，该文件包含了一个变量名到张量值得map</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一些变量</span><br><span class="line">v1 &#x3D; tf.Variables(...,name &#x3D;&quot;v1&quot;)</span><br><span class="line">v2 &#x3D; tf.Variables(...,name&#x3D;&quot;v2&quot;)</span><br><span class="line">...</span><br><span class="line">#添加一个操作来初始化变量</span><br><span class="line">init_op &#x3D; tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"># 添加操作来保存和恢复所有变量</span><br><span class="line">saver &#x3D; tf.train.Saver()</span><br><span class="line"># 然后，运行模型，初始化变量，做一些操作，保存变量到磁盘中</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">     sess.run(init_op)</span><br><span class="line">     # 对模型做一些操作</span><br><span class="line">    .....</span><br><span class="line">    #存储变量到磁盘中</span><br><span class="line">    save_path &#x3D; saver.save(sess,&quot;&#x2F;tmp&#x2F;model.ckpt&quot;)</span><br><span class="line">    print (&quot;Model saved in file: %s&quot;%save_path)</span><br></pre></td></tr></table></figure>
<p><strong> 恢复</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 从磁盘中恢复变量</span><br><span class="line">    saver.restore(sess,&quot;&#x2F;tmp&#x2F;model.ckpt&quot;)</span><br><span class="line">    print (&quot;Model restored&quot;)</span><br><span class="line">    # 做一些操作</span><br></pre></td></tr></table></figure>
<h2 id="6-4-选择哪些变量来存储和恢复"><a href="#6-4-选择哪些变量来存储和恢复" class="headerlink" title="6.4 选择哪些变量来存储和恢复"></a>6.4 选择哪些变量来存储和恢复</h2><ul>
<li><p>在 <strong> tf.train.Saver()</strong>中没有参数</p>
<ul>
<li>处理图中所有变量，每个变量都会被保存在该名字之下</li>
</ul>
</li>
<li><p>存储和恢复变量的子集</p>
<ul>
<li>训练5层神经网络，想训练一个新的6层神经网络，从5层圣经网络中恢复参数</li>
</ul>
</li>
<li><p>向<strong>tf.train.Saver()</strong>构造方法中传入一个Python词典:keys</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一些变量</span><br><span class="line">v1 &#x3D; tf.Variables(...,name &#x3D;&quot;v1&quot;)</span><br><span class="line">v2 &#x3D; tf.Variables(...,name &#x3D;&quot;v2&quot;)</span><br><span class="line"># 添加操作存储和恢复变量 v2,使用名字 &quot;my_v2&quot;</span><br><span class="line">saver &#x3D; tf.train.Saver(&#123;&quot;my_v2&quot;:v2&#125;)</span><br><span class="line"># 使用saver对象</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h2 id="6-5-简单计数器的示例代码"><a href="#6-5-简单计数器的示例代码" class="headerlink" title="6.5 简单计数器的示例代码"></a>6.5 简单计数器的示例代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个变量，初始化为标量0</span><br><span class="line">state &#x3D; tf.Variables(0,name&#x3D;&quot;Counter&quot;)</span><br><span class="line"># 创建一个操作来给&quot;state&quot;加1</span><br><span class="line">one &#x3D; tf.constant(1)</span><br><span class="line">new_value &#x3D; tf.add(state,one)</span><br><span class="line">update &#x3D; tf.assign(state,new_value)</span><br><span class="line"></span><br><span class="line"># 在图被运行，变量必须是通过运行一个&quot;init&quot;操作被初始化。</span><br><span class="line"># 我们首先要将&quot;init&quot;操作加入到图中</span><br><span class="line">init_op &#x3D; tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"># 运行图，和操作</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #运行 &#39;init&#39;操作</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    # 打印&#39;state&#39;的初始化值</span><br><span class="line">    print (sess.run(state))</span><br><span class="line">    # 运行更新&#39;state&#39;的操作，并打印&#39;state&#39;</span><br><span class="line">    for _ in range(3):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print (sess.run(state))</span><br><span class="line"># 输出</span><br><span class="line">#0</span><br><span class="line">#1</span><br><span class="line">#2</span><br><span class="line">#3</span><br></pre></td></tr></table></figure>
<h2 id="6-6-取数据Fetches"><a href="#6-6-取数据Fetches" class="headerlink" title="6.6 取数据Fetches"></a>6.6 取数据Fetches</h2><ul>
<li>在Session对象中调用<strong>run()</strong>方法来执行图，并传入张量来取回数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input1 &#x3D; tf.constant(3.0)</span><br><span class="line">input2 &#x3D; tf.constant(2.0)</span><br><span class="line">input3 &#x3D; tf.constant(5.0)</span><br><span class="line">intermed &#x3D; tf.add(input2,input3)</span><br><span class="line">mul &#x3D; tf.mul(input1,intermed)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">     result &#x3D; sess.run([mul,intermed])</span><br><span class="line">     print (result)</span><br><span class="line"># 输出</span><br><span class="line"># [array([21.],dtype &#x3D; float32),array([7.],dtype &#x3D; float32)]</span><br></pre></td></tr></table></figure>
<h2 id="6-7-Feeds"><a href="#6-7-Feeds" class="headerlink" title="6.7 Feeds"></a>6.7 Feeds</h2><ul>
<li>直接打包一个张量到图中的任何操作</li>
<li>使用一个张量值临时替换一个操作的输出值</li>
<li>feed数据作为<strong>run()</strong>方法的一个参数</li>
<li>仅仅用来运行调用被传入值</li>
<li><strong>tf.placeholder()</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input1 &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">input2 &#x3D;tf.placeholder(tf.float32)</span><br><span class="line">output &#x3D; tf.mul(input1,input2)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print (sess.run([output],feed_dict &#x3D; &#123;input1:[7.],input2:[2.]&#125;))</span><br><span class="line"></span><br><span class="line">#输出</span><br><span class="line">#[array([14.],dtype&#x3D;float32)]</span><br></pre></td></tr></table></figure>
<h1 id="7-操作"><a href="#7-操作" class="headerlink" title="7 操作"></a>7 操作</h1><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>逐元素数学运算</td>
<td>Add,Sub,Mul,Div,Exp,Log,Greater,Less,Equal…</td>
</tr>
<tr>
<td>数组操作</td>
<td>Concat,Slice,Split,Constant,Rank,Shape,Shuffle..</td>
</tr>
<tr>
<td>矩阵运算</td>
<td>MatMul,MatrixInverse,MatrixDeterminant…</td>
</tr>
<tr>
<td>状态操作</td>
<td>Variable,Assign,AssignAdd…</td>
</tr>
<tr>
<td>神经元构建块</td>
<td>SoftMax,Sigmoid,ReLU,Convolution2D,MaxPool…</td>
</tr>
<tr>
<td>检查点操作</td>
<td>Save,Restore</td>
</tr>
<tr>
<td>队列和同步操作</td>
<td>Enqueue,Dequeue,MutexAcquire,MutexRelease,…</td>
</tr>
<tr>
<td>控制流操作</td>
<td>Merge,Switch,Enter,Leave,NextIteration</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>hadoop lzo问题</title>
    <url>/2016/03/16/bigdata-hadooplzo/</url>
    <content><![CDATA[<h2 id="一-重要问题"><a href="#一-重要问题" class="headerlink" title="一 重要问题"></a>一 重要问题</h2><h2 id="1-1-hadoop-gpl-compression还是hadoop-lzo"><a href="#1-1-hadoop-gpl-compression还是hadoop-lzo" class="headerlink" title="1.1  hadoop-gpl-compression还是hadoop-lzo"></a>1.1  hadoop-gpl-compression还是hadoop-lzo</h2><p>  <strong>hadoop-lzo-xxx</strong> 的前身是<strong>hadoop-gpl-compression-xxx</strong>,之前是放在googlecode下管理,<a href="http://code.google.com/p/hadoop-gpl-compression/" target="_blank" rel="noopener">地址</a>但由于协议问题后来移植到github上,也就是现在的hadoop-lzo-xxx,github,<a href="https://github.com/kevinweil/hadoop-lzo" target="_blank" rel="noopener">链接地址</a>.<br>    网上介绍hadoop lzo压缩都是基于hadoop-gpl-compression的介绍.而hadoop-gpl-compression还是09年开发的,跟现在hadoop版本已经无法再完全兼容,会发生一些问题.而按照网上的方法,为了兼容hadoop,使用hadoop-lzo-xxx。</p>
<p>  <strong>原理：</strong>因为hadoop lzo实际上得依赖C/C++开发的lzo去压缩,而他们通过JNI去调用.如果使用hadoop-gpl-compression下的Native,但使用hadoop-lzo-xxx的话,会导致版本不一致问题.所以正确的做法是,将hadoop-lzo-xxx下的Native放入到/usr/local/lib下.而你每升级一个hadoop-lzo-xxx版本,或许就得重复将新lzo版本下的native目录放入/usr/local/lib下.具体需要测试.<br>同时这里说下,hadoop-lzo-xxx的验证原理,让我们更系统的了解为什么使用hadoop-lzo会报的一系列错误.     </p>
<ol>
<li>首先Hadoop-lzo会通过JNI调用gplcompression,如果调取不到会报Could not load native gpl library异常.具体代码如下:    </li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static &#123;   </span><br><span class="line">   try &#123;  </span><br><span class="line">       &#x2F;&#x2F;try to load the lib      </span><br><span class="line">         System.loadLibrary(&quot;gplcompression&quot;);</span><br><span class="line">         nativeLibraryLoaded &#x3D; true;  </span><br><span class="line">         LOG.info(&quot;Loaded native gpl library&quot;);  </span><br><span class="line">      &#125; catch (Throwable t) &#123;  </span><br><span class="line">	  LOG.error(&quot;Could not load native gpl library&quot;, t);  </span><br><span class="line">	  nativeLibraryLoaded &#x3D; false;  </span><br><span class="line">	 &#125;  </span><br><span class="line">&#96;&#96;&#96;      </span><br><span class="line">2. 获取了gplcompression后需要初始化加载以便可以调用,如果加载不成功,如我刚才说的版本冲突等也会报一系列错误.同时这里的加载和初始化分成两步,一步是压缩,对应Java的类是LzoCompressor.另一步解压缩,对应Java的类是LzoDecompressor.先看下LzoCompressor是如何加载初始化的,代码如下:          </span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;   </span><br><span class="line">	static &#123;  </span><br><span class="line">	  if (GPLNativeCodeLoader.isNativeCodeLoaded()) &#123;  </span><br><span class="line">	    &#x2F;&#x2F; Initialize the native library  </span><br><span class="line">	    try &#123;  </span><br><span class="line">	      initIDs();  </span><br><span class="line">	      nativeLzoLoaded &#x3D; true;  </span><br><span class="line">	    &#125; catch (Throwable t) &#123;  </span><br><span class="line">	      &#x2F;&#x2F; Ignore failure to load&#x2F;initialize native-lzo  </span><br><span class="line">	      LOG.warn(t.toString());  </span><br><span class="line">	      nativeLzoLoaded &#x3D; false;  </span><br><span class="line">	    &#125;  </span><br><span class="line">	    LZO_LIBRARY_VERSION &#x3D; (nativeLzoLoaded) ? 0xFFFF &amp; getLzoLibraryVersion()  </span><br><span class="line">	        : -1;  </span><br><span class="line">	  &#125; else &#123;  </span><br><span class="line">	    LOG.error(&quot;Cannot load &quot; + LzoCompressor.class.getName() +   </span><br><span class="line">	    &quot; without native-hadoop library!&quot;);  </span><br><span class="line">	    nativeLzoLoaded &#x3D; false;  </span><br><span class="line">	    LZO_LIBRARY_VERSION &#x3D; -1;  </span><br><span class="line">	  &#125;  </span><br><span class="line">	&#125;</span><br><span class="line">&#96;&#96;&#96;      </span><br><span class="line"></span><br><span class="line">   如我这里所报的警告    </span><br><span class="line">	&#96;WARN lzo.LzoCompressor: java.lang.NoSuchFieldError: workingMemoryBuf&#96;     </span><br><span class="line">  就是由这里的 **LOG.warn(t.toString())**所抛出.同时这里也会先加载gplcompression,加载不成功同样会报    </span><br><span class="line">	&#96;without native-hadoop library!&#96;    </span><br><span class="line">  错误.再看看解压缩LzoDecompressor,原理差不多,不再阐述,代码如下:</span><br></pre></td></tr></table></figure>
<p>   static {<br>      if (GPLNativeCodeLoader.isNativeCodeLoaded()) {<br>        // Initialize the native library<br>        try {<br>          initIDs();<br>          nativeLzoLoaded = true;<br>        } catch (Throwable t) {<br>          // Ignore failure to load/initialize native-lzo<br>          LOG.warn(t.toString());<br>          nativeLzoLoaded = false;<br>        }<br>        LZO_LIBRARY_VERSION = (nativeLzoLoaded) ? 0xFFFF &amp; getLzoLibraryVersion()<br>            : -1;<br>      } else {<br>        LOG.error(“Cannot load “ + LzoDecompressor.class.getName() +<br>        “ without native-hadoop library!”);<br>        nativeLzoLoaded = false;<br>        LZO_LIBRARY_VERSION = -1;<br>      }<br>    }<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#  二 如何安装LZO    </span><br><span class="line"></span><br><span class="line">1.首先下载https:&#x2F;&#x2F;github.com&#x2F;kevinweil&#x2F;hadoop-lzo&#x2F;,我这里下载到</span><br><span class="line">			**&#x2F;home&#x2F;guoyun&#x2F;Downloads&#x2F;&#x2F;home&#x2F;guoyun&#x2F;hadoop&#x2F;kevinweil-hadoop-lzo-2dd49ec**    </span><br><span class="line">2. 去lzo源码根目录下执行</span><br></pre></td></tr></table></figure><br>    wget <a href="https://download.github.com/kevinweil-hadoop-lzo-2ad6654.tar.gz" target="_blank" rel="noopener">https://download.github.com/kevinweil-hadoop-lzo-2ad6654.tar.gz</a><br>    tar -zxvf kevinweil-hadoop-lzo-2ad6654.tar.gz<br>    cd kevinweil-hadoop-lzo-2ad6654<br>    export CFLAGS=-m64<br>    export CXXFLAGS=-m64<br>    ant compile-native tar<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2. 通过ant生成native和jar,命令如下:    </span><br><span class="line"></span><br><span class="line">  在build目录下生成对应的tar包,解压缩后,进入该目录可以看到对应的jar包hadoop-lzo-0.4.14.jar.同时将lib&#x2F;native&#x2F;Linux-amd64-64&#x2F;目录下所有文件拷贝到$HADOOP_HOME&#x2F;lib和&#x2F;usr&#x2F;local&#x2F;lib两个目录下.    </span><br><span class="line"></span><br><span class="line">  **注明:**拷贝到&#x2F;usr&#x2F;local&#x2F;lib是便于调试,如是生产环境则无需拷贝.    </span><br><span class="line">  **注意：**如果 Hadoop&#x2F;lib&#x2F;目录下没有native&#x2F;Linux-amd64-64&#x2F; 目录，需要手工创建。或者下载hadoop-gpl-compression。参考(http:&#x2F;&#x2F;guoyunsky.iteye.com&#x2F;blog&#x2F;1237327),安装步骤中的第四步，复制库文件到hadoop&#x2F;lib目录下的操作。     </span><br><span class="line"></span><br><span class="line">  &#96;&#96;&#96;mv hadoop-gpl-compression-0.1.0&#x2F;lib&#x2F;native&#x2F;Linux-amd64-64&#x2F;* $HADOOP_HOME&#x2F;lib&#x2F;native&#x2F;Linux-amd64-64&#x2F;</span><br></pre></td></tr></table></figure></p>
<h1 id="三-如何确定是否已经安装好LZO"><a href="#三-如何确定是否已经安装好LZO" class="headerlink" title="三 如何确定是否已经安装好LZO"></a>三 如何确定是否已经安装好LZO</h1><p>  <a href="https://code.google.com/a/apache-extras.org/p/hadoop-gpl-compression/wiki/FAQ?redir=1" target="_blank" rel="noopener">参考</a><br> 执行命令:        </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> % ls -l &#x2F;usr&#x2F;lib*&#x2F;liblzo2*</span><br><span class="line">-rw-r--r--  1 root root 171056 Mar 20  2006 &#x2F;usr&#x2F;lib&#x2F;liblzo2.a</span><br><span class="line">lrwxrwxrwx  1 root root     16 Feb 17  2007 &#x2F;usr&#x2F;lib&#x2F;liblzo2.so -&gt; liblzo2.so.2.0.0*</span><br><span class="line">lrwxrwxrwx  1 root root     16 Feb 17  2007 &#x2F;usr&#x2F;lib&#x2F;liblzo2.so.2 -&gt; liblzo2.so.2.0.0*</span><br><span class="line">-rwxr-xr-x  1 root root 129067 Mar 20  2006 &#x2F;usr&#x2F;lib&#x2F;liblzo2.so.2.0.0*</span><br><span class="line">-rw-r--r--  1 root root 208494 Mar 20  2006 &#x2F;usr&#x2F;lib64&#x2F;liblzo2.a</span><br><span class="line">lrwxrwxrwx  1 root root     16 Feb 17  2007 &#x2F;usr&#x2F;lib64&#x2F;liblzo2.so -&gt; liblzo2.so.2.0.0*</span><br><span class="line">lrwxrwxrwx  1 root root     16 Feb 17  2007 &#x2F;usr&#x2F;lib64&#x2F;liblzo2.so.2 -&gt; liblzo2.so.2.0.0*</span><br><span class="line">-rwxr-xr-x  1 root root 126572 Mar 20  2006 &#x2F;usr&#x2F;lib64&#x2F;liblzo2.so.2.0.0*</span><br></pre></td></tr></table></figure>
<p> lzo压缩已经广泛用于Hadoop中,至于为什么要在Hadoop中使用Lzo.这里不再重述.其中很重要的一点就是由于分布式计算,所以需要支持对压缩数据进行分片,也就是Hadoop的InputSplit,这样才能分配给多台机器并行处理.所以这里花了一天的时间,看了下Hadoop lzo的源码,了解下Hadooplzo是如何做到的.    </p>
<p> 其实一直有种误解,就是以为lzo本身是支持分布式的,也就是支持压缩后的数据可以分片.我们提供给它分片的逻辑,由lzo本身控制.但看了Hadoop lzo源码才发现,lzo只是个压缩和解压缩的工具,如何分片,是由Hadooplzo(Javad代码里)控制.具体的分片算法写得也很简单,就是在内存中开一块大大的缓存,默认是256K,缓存可以在通过io.compression.codec.lzo.buffersize参数指定.数据读入缓存(实际上要更小一些),如果缓存满了,则交给lzo压缩,获取压缩后的数据,同时在lzo文件中写入压缩前后的大小以及压缩后的数据.所以这里,一个分片,其实就是&lt;=缓存大小.具体lzo文件格式(这里针对Lzop):</p>
<p> 1.lzo文件头</p>
<ul>
<li>写入lzo文件标识： 此时长度9</li>
<li><p>写入版本    </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LZOP_VERSION		lzo版本，short，此时长度11</span><br><span class="line">LZO_VERSION_LIBRARY	lzo压缩库版本，short，此时长度13</span><br><span class="line">LZOP_COMPAT_VERSION	最后lzo应该一直的版本，short，此时长度15</span><br></pre></td></tr></table></figure>
</li>
<li><p>写入压缩策略    </p>
</li>
<li>LZO1X_1的话writeByte写入1和5，此时长度17</li>
<li>writeInt写入flag(标识)，此时长度21    </li>
<li>writeInt写入mode(模式)，此时长度25    </li>
<li>writeInt写入当前时间秒，此时长度29    </li>
<li>writeInt写入0,不知道做何用途，此时长度33   </li>
<li>writeBye写入0，不知道做何用途，此时长度34    </li>
<li>writeInt写入之前数据的checksum，此时长度38</li>
</ul>
<ol>
<li>写入多个块,会有多个.循环处理,直到压缩完成<br>写入压缩前的数据长度,此时长度为39如果压缩前的长度小于压缩后的长度,则写入未压缩的数据长度,再写入未压缩的数据.反之则写入压缩后的数据长度,以及压缩后的数据    </li>
<li>lzo文件尾,只是写入4个0,不知道做什么用途               同时如果你指定索引文件路径的话,则一个缓存写完后便会将写入的数据长度写到索引文件中.如此在Hadoop分布式时只要根据索引文件的各个长度,读取该长度的数据 ,便可交给map处理.<br>以上是hadoop lzo大概原理,同时LzopCodec支持在压缩时又生成对应的索引文件.而LzoCodec不支持.具体代码看下来,还不明确LzoCodec为何没法做到,也是一样的切片逻辑.具体待测试.</li>
</ol>
<h1 id="4-hadoop中使用lzo的压缩"><a href="#4-hadoop中使用lzo的压缩" class="headerlink" title="4 hadoop中使用lzo的压缩"></a>4 hadoop中使用lzo的压缩</h1><p> 在hadoop中使用lzo的压缩算法可以减小数据的大小和数据的磁盘读写时间，不仅如此，lzo是基于block分块的，这样他就允许数据被分解成chunk，并行的被hadoop处理。这样的特点，就可以让lzo在hadoop上成为一种非常好用的压缩格式。    </p>
<p>   lzo本身不是splitable的，所以当数据为text格式时，用lzo压缩出来的数据当做job的输入是一个文件作为一个map。但是sequencefile本身是分块的，所以sequencefile格式的文件，再配上lzo的压缩格式，就可实现lzo文件方式的splitable。    </p>
<p>   由于压缩的数据通常只有原始数据的1/4，在HDFS中存储压缩数据，可以使集群能保存更多的数据，延长集群的使用寿命。不仅如此，由于mapreduce作业通常瓶颈都在IO上，存储压缩数据就意味这更少的IO操作，job运行更加的高效。但是，在hadoop上使用压缩也有两个比较麻烦的地方：   </p>
<ul>
<li>第一，有些压缩格式不能被分块，并行的处理，比如gzip。    </li>
<li><p>第二，另外的一些压缩格式虽然支持分块处理，但是解压的过程非常的缓慢，使job的瓶颈转移到了cpu上，例如bzip2。比如我们有一个1.1GB的gzip文件，该文件 被分成128MB/chunk存储在hdfs上，那么它就会被分成9块。为了能够在mapreduce中并行的处理各个chunk，那么各个mapper之间就有了依赖。而第二个mapper就会在文件的某个随机的byte出进行处理。那么gzip解压时要用到的上下文字典就会为空，这就意味这gzip的压缩文件无法在hadoop上进行正确的并行处理。也就因此在hadoop上大的gzip压缩文件只能被一个mapper来单个的处理，这样就很不高效，跟不用mapreduce没有什么区别了。而另一种bzip2压缩格式，虽然bzip2的压缩非常的快，并且甚至可以被分块，但是其解压过程非常非常的缓慢，并且不能被用streaming来读取，这样也无法在hadoop中高效的使用这种压缩。即使使用，由于其解压的低效，也会使得job的瓶颈转移到cpu上去。    </p>
<p>如果能够拥有一种压缩算法，即能够被分块，并行的处理，速度也非常的快，那就非常的理想。这种方式就是lzo。lzo的压缩文件是由许多的小的blocks组成（约256K），使的hadoop的job可以根据block的划分来splitjob。不仅如此，lzo在设计时就考虑到了效率问题，它的解压速度是gzip的两倍，这就让它能够节省很多的磁盘读写，它的压缩比的不如gzip，大约压缩出来的文件比gzip压缩的大一半，但是这样仍然比没有经过压缩的文件要节省20%-50%的存储空间，这样就可以在效率上大大的提高job执行的速度。以下是一组压缩对比数据，使用一个8.0GB的未经过压缩的数据来进行对比：    </p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">压缩格式</th>
<th style="text-align:center">文件大小(GB)</th>
<th style="text-align:center">压缩时间</th>
<th style="text-align:center">解压时间</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">None</td>
<td style="text-align:center">some_logs</td>
<td style="text-align:center">8.0</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:center">Gzip</td>
<td style="text-align:center">some_logs.gz</td>
<td style="text-align:center">1.3</td>
<td style="text-align:center">241</td>
<td>72</td>
</tr>
<tr>
<td style="text-align:center">LZO</td>
<td style="text-align:center">some_logs.lzo</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">55</td>
<td>35</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，lzo压缩文件会比gzip压缩文件稍微大一些，但是仍然比原始文件要小很多倍，并且lzo文件压缩的速度几乎相当于gzip的5倍，而解压的速度相当于gzip的两倍。lzo文件可以根据blockboundaries来进行分块，比如一个1.1G的lzo压缩文件，那么处理第二个128MBblock的mapper就必须能够确认下一个block的boundary，以便进行解压操作。lzo并没有写什么数据头来做到这一点，而是实现了一个lzoindex文件，将这个文件（foo.lzo.index）写在每个foo.lzo文件中。这个index文件只是简单的包含了每个block在数据中的offset，这样由于offset已知的缘故，对数据的读写就变得非常的快。通常能达到90-100MB/秒，也就是10-12秒就能读完一个GB的文件。一旦该index文件被创建，任何基于lzo的压缩文件就能通过load该index文件而进行相应的分块，并且一个block接一个block的被读取。也因此，各个mapper都能够得到正确的block，这就是说，可以只需要进行一个LzopInputStream的封装，就可以在hadoop的mapreduce中并行高效的使用lzo。如果现在有一个job的InputFormat是TextInputFormat，那么就可以用lzop来压缩文件，确保它正确的创建了index，将TextInputFormat换成LzoTextInputFormat，然后job就能像以前一样正确的运行，并且更加的快。有时候，一个大的文件被lzo压缩过之后，甚至都不用分块就能被单个mapper高效的处理了。<br>在hadoop集群中安装lzo<br>要在hadoop中搭建lzo使用环境非常简单：    </p>
<ol>
<li><p>安装lzop native libraries<br>例如：<code>sudo yum install lzop lzo2</code></p>
</li>
<li><p>从如下地址下载 hadooplzo支持到源代码：<a href="http://github.com/kevinweil/hadoop-lzo" target="_blank" rel="noopener">http://github.com/kevinweil/hadoop-lzo</a></p>
</li>
<li>编译从以上链接checkout下来到代码，通常为：ant compile-native tar</li>
<li>将编译出来到hadoop-lzo-*.jar部署到hadoop集群到各个slave到某个有效目录下，如$HADOOOP_HOME/lib</li>
<li>将以上编译所得到hadoop-lzo native libbinary部署到集群到某个有效目录下，如$HADOOP_HOME/lib/native/Linux-amd64-64。</li>
<li>将如下配置到 core-site.xml 中：    </li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codec.lzo.class&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<ol>
<li>将如下配置到mapred-site.xml中：        <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">	&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapred.child.env&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;JAVA_LIBRARY_PATH&#x3D;&#x2F;path&#x2F;to&#x2F;your&#x2F;native&#x2F;hadoop-lzo&#x2F;libs&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">	如果想要mapreduce再写中间结果时也使用压缩，可以将如下配置也写入到mapred-site.xml中。</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">	&lt;name&gt;mapred.map.output.compression.codec&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">&#96;&#96;&#96;    </span><br><span class="line"></span><br><span class="line">如果以上所有操作都成功，那么现在就可以尝试使用lzo了。比如打包一个lzo都压缩文件，如lzo_log文件，上传到hdfs中，然后用以下命令进行测试：</span><br></pre></td></tr></table></figure>
hadoop jar /path/to/hadoop-lzo.jarcom.hadoop.compression.lzo.LzoIndexerhdfs://namenode:9000/lzo_logs<br>```    </li>
</ol>
<p>如果要写一个job来使用lzo，可以找一个job，例如wordcount，将当中到TextInputFormat修改为LzoTextInputForma，其他都不用修改，job就能从hdfs上读入lzo压缩文件，进行分布式都分块并行处理。</p>
<blockquote>
<p>Using Hadoop and LZO<br>Reading and Writing LZO Data<br>The project provides LzoInputStream and LzoOutputStream wrapping regular streams, to allow you to easily read and write compressed LZO data.<br>Indexing LZO Files<br>At this point, you should also be able to use the indexer to index lzo files in Hadoop (recall: this makes them splittable, so that they can be analyzed in parallel in a mapreduce job). Imagine that big_file.lzo is a 1 GB LZO file. You have two options:<br>•    index it in-process via:<br>•    hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer big_file.lzo<br>•    index it in a map-reduce job via:<br>•    hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo<br>Either way, after 10-20 seconds there will be a file named big_file.lzo.index. The newly-created index file tells the LzoTextInputFormat’s getSplits function how to break the LZO file into splits that can be decompressed and processed in parallel. Alternatively, if you specify a directory instead of a filename, both indexers will recursively walk the directory structure looking for .lzo files, indexing any that do not already have corresponding .lzo.index files.<br>Running MR Jobs over Indexed Files<br>Now run any job, say wordcount, over the new file. In Java-based M/R jobs, just replace any uses of TextInputFormat by LzoTextInputFormat. In streaming jobs, add “-inputformat com.hadoop.mapred.DeprecatedLzoTextInputFormat” (streaming still uses the old APIs, and needs a class that inherits from org.apache.hadoop.mapred.InputFormat). Note that to use the DeprecatedLzoTextInputFormat properly with hadoop-streaming, you should also set the jobconf propertystream.map.input.ignoreKey=true. That will replicate the behavior of the default TextInputFormat by stripping off the byte offset keys from the input lines that get piped to the mapper process. For Pig jobs, email me or check the pig list — I have custom LZO loader classes that work but are not (yet) contributed back.<br>Note that if you forget to index an .lzo file, the job will work but will process the entire file in a single split, which will be less efficient.</p>
</blockquote>
<p>参考资料    </p>
<p><a href="http://blog.csdn.net/scorpiohjx2/article/details/18423529" target="_blank" rel="noopener">lzo本地压缩与解压缩实例</a><br><a href="http://share.blog.51cto.com/278008/549393/" target="_blank" rel="noopener">hadoop集群内lzo的安装与配置</a><br><a href="http://www.tuicool.com/articles/VVj6rm" target="_blank" rel="noopener">安装 Hadoop 2.0.0-cdh4.3.0 LZO 成功</a><br><a href="https://code.google.com/a/apache-extras.org/p/hadoop-gpl-compression/wiki/FAQ?redir=1" target="_blank" rel="noopener">hadoop-lzo源代码</a><br><a href="http://guoyunsky.iteye.com/blog/1237327" target="_blank" rel="noopener">Hadoop Could not load native gpl library异常解决</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>spark环境部署</title>
    <url>/2016/03/12/spark-envirnoment/</url>
    <content><![CDATA[<h2 id="一-spark安装和使用"><a href="#一-spark安装和使用" class="headerlink" title="一 spark安装和使用"></a>一 spark安装和使用</h2><h2 id="1-1-安装spark"><a href="#1-1-安装spark" class="headerlink" title="1.1 安装spark"></a>1.1 安装spark</h2><p>  我们主要以Windows环境为例介绍Spark的安装。<br>  整个安装过程主要分为四个步骤：安装JDK、安装Scala、安装Spark、安装WinUtil。在Linux和Mac OS X下<br>  安装Spark只需要完成前三步即可。</p>
<h3 id="1-1-1-安装JDK"><a href="#1-1-1-安装JDK" class="headerlink" title="1.1.1 安装JDK"></a>1.1.1 安装JDK</h3><p>Spark采用Scala语言编写，而Scala程序是以JVM为运行环境的，因此需先安装JDK以支持Spark的运行。<br>Spark通常需要JDK 6.0以上版本，你可以在Oracle的JDK<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">官网</a> 下载相应版本的JDK安装包，如<br>。需要注意的是，应选择下载“JDK”安装包，而不是“JRE”。在我们这个示例中，我们选择的是JDK 7.</p>
<h3 id="1-1-2-安装scala"><a href="#1-1-2-安装scala" class="headerlink" title="1.1.2 安装scala"></a>1.1.2 安装scala</h3><p>刚才我们提到，Spark是采用Scala语言编写的，因此第二步是要安装Scala。Scala官网的<a href="http://www.scala-lang.org/download/" target="_blank" rel="noopener">下载页面</a>提供了多个版本的Scala下载，<br>但由于Scala各个版本之间兼容性并不好，因此在下载的时候一定要注意你要安装的Spark版本所依赖的Scala版本，以免遇到一些难以预知的问题。在我们的例子中，是要安装目前最新的Spark 1.3.0版本，因此<br>我们选择下载所需的Scala 2.10.4版本。选择之前的历史版本下载，需要先从如图2-2所示的下载页面中点击“All previous Scala Releases”链接，进入历史版本列表，然后选择“2.10.4”版本<a href="http://www.scala-lang.org/f iles/archive/scala-2.10.4.msi" target="_blank" rel="noopener">下载</a><br>。下载后按照提示一步一步执行安装即可。</p>
<p>  Scala安装后，要进行一个验证的过程以确认安装成功，其方法如下：          </p>
<ul>
<li>在Windows中执行命令cmd，启动Windows命令行环境。        </li>
<li>在命令行环境中，输入scala，然后敲回车。        </li>
<li>如果看到如图2-3所示成功启动Scala Shell环境，则说明安装成功，然后输入exit，退出Scala Shell环境。    </li>
<li>如果启动Scala Shell环境失败，一般只需要在Windows环境变量设置界面配置SCALA_HOME环境变量为Scala的安装路径即可。<br><img src="/images/blog/sparkenvirnoment1.png" alt="windows启动scala界面">    </li>
</ul>
<h3 id="1-1-3-安装spark"><a href="#1-1-3-安装spark" class="headerlink" title="1.1.3 安装spark"></a>1.1.3 安装spark</h3><p> <a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark官网</a>提供了各个版本的安装包。为搭建学习试验环境，我们选择下载下载预编译好的包，例<br>如spark1.3.0binhadoop2.4.tgz<br> <img src="/images/blog/sparkenvirnoment2.png" alt="spark下载">    </p>
<h3 id="1-1-4-安装winutils"><a href="#1-1-4-安装winutils" class="headerlink" title="1.1.4 安装winutils"></a>1.1.4 安装winutils</h3><p>由于Spark的设计和开发目标是在Linux环境下运行，因此在Windows单机环境（没有Hadoop集群的支撑）时运行会遇到winutils的问题（一个相关的Issue可以参见<br><a href="https://issues.apache.org/jira/browse/SPARK-2356" target="_blank" rel="noopener">参考</a> 。为了解决这一问题，我们需要安装winutils.exe，具体方法如下：    </p>
<ol>
<li>从一个可靠的网站下载winutils.exe（我们选择从Hadoop商业发行版Hortonworks提供的下载<a href="http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe" target="_blank" rel="noopener">链接</a></li>
<li>将winutil.exe拷贝到一个目录，例如：E:\LearnSpark\win\bin。</li>
<li>按照如图2-4、2-5的步骤，设置Windows系统的环境变量HADOOP_HOME为E:\LearnSpark\win（注意没有bin）<br><img src="/images/blog/sparkenvirnoment3.png" alt="设置环境变量"><br><img src="/images/blog/sparkenvirnoment4.png" alt="设置环境变量"><br>至此，Windows下安装Spark的过程全部完成。</li>
</ol>
<h2 id="1-2-使用spark-shell"><a href="#1-2-使用spark-shell" class="headerlink" title="1.2 使用spark shell"></a>1.2 使用spark shell</h2><p>  就像HelloWorld程序基本已成为学习某一门开发语言的第一个入门程序一样，WordCount程序就是试用大数据处理技术的HelloWorld。下面我们就以使用Spark统计一个文件中的单词出现次数为例，快速体验一下便捷的Spark使用方式。</p>
<ul>
<li><p>启动Spark Shell环境<br>在Windows文件管理器中，切换目录到Spark安装后生成的spark1.3.0binhadoop2.4目录下，按住Shift键的同时点击鼠标右键，然后使用左键点击在此处打开命令窗口。在打开一个命令行的窗口中，输入bin\sparkshell，就可以启动spark-shell环境，如图2-6所示。<br><img src="/images/blog/sparkenvirnoment5.png" alt="sparkshel"><br>如果不希望这么麻烦地切换目录，而是希望在打开一个命令行窗口中直接运行spark-shell，那么只需要在Windows环境变量中将上面的spark-shell所在的路径加入环境变量PATH中即可。</p>
</li>
<li><p>建立待统计的单词文件    </p>
</li>
</ul>
<p>选择一个已存在的文本文件，或新建一个文本文件，作为待统计的单词文件E:\LearnSpark\word.txt，在这里我们新建一个文件,内容为：    </p>
<figure class="highlight plain"><figcaption><span>banana</span></figcaption><table><tr><td class="code"><pre><span class="line">banana banana&#96;&#96;&#96;     </span><br><span class="line"></span><br><span class="line">+ 加载单词文件    </span><br><span class="line">执行Spark程序需要一个SparkContext类实例，在SparkShell中已经默认将SparkContext类初始化为对象实例sc。因此我们不需要再去初始化一个新的sc，直接输入以下命令使用即可。该行命令使用SparkContext类的textFile函数，加载待统计的单词文件，结果如图2-7所示。    </span><br><span class="line">  &#96;&#96;&#96;val file &#x3D; sc.textFile(&quot;E:\\LearnSpark\\word.txt&quot;)&#96;&#96;&#96;    </span><br><span class="line"></span><br><span class="line">  ![加载单词文件](&#x2F;images&#x2F;blog&#x2F;sparkenvirnoment6.png)     </span><br><span class="line"></span><br><span class="line">+ 统计单词出现次数    </span><br><span class="line">  如果你用MapReduce计算框架编写过WordCount程序，那你一定能体会到执行一个简单的单词统计功能需要数十行代码的不便。而利用Spark的函数式编程模式，我们只需要一行Scala语句即可完成单词统计功能，结果如图2-8所示。在这里我们暂时先不解释这行代码的具体含义，留待在后面的章节中慢慢</span><br><span class="line">学习。你只需要体会到Spark是如何大幅简化数据处理的工作的难度即可。    </span><br><span class="line">  &#96;&#96;&#96;val counts &#x3D; file.flatMap(line &#x3D;&gt; line.split(&quot; &quot;)).map(word &#x3D;&gt; (word, 1)).reduceByKey(_+_)&#96;&#96;&#96;    </span><br><span class="line">![统计单词次数](&#x2F;images&#x2F;blog&#x2F;sparkenvirnoment7.png)     </span><br><span class="line">+ 保存结果文件    </span><br><span class="line">在这里我们使用E:\LearnSpark\counts.txt作为输出文件。需要注意的是，要保证没有和输出文件同名的文件或者是文件夹，如果存在则需要手动删除</span><br><span class="line">该文件夹，否则会出错。保存结果文件的命令如下所示，运行过程如图2-9所示，在运行完成后打开E:\LearnSpark\counts.txt文件即可看到如图所示的单词统计结果。    </span><br><span class="line"></span><br><span class="line">  &#96;&#96;&#96;counts.saveAsTextFile(&quot;E:\\LearnSpark\\counts.txt&quot;)&#96;&#96;&#96;    </span><br><span class="line">  </span><br><span class="line">  ![保存结果](&#x2F;images&#x2F;blog&#x2F;sparkenvirnoment8.png)    </span><br><span class="line">  </span><br><span class="line">下面我们来看一下最后的输出结果，count.txt其实是个目录，在该目录下有好多个文件，其中part-00000和part-00001是我们需要的结果。</span><br></pre></td></tr></table></figure>
<p>part00000<br>(apple,1)</p>
<p>part00001<br>(banana,3)<br>```    </p>
<h2 id="1-3-了解Spark目录结构"><a href="#1-3-了解Spark目录结构" class="headerlink" title="1.3 了解Spark目录结构"></a>1.3 了解Spark目录结构</h2><p>Spark安装后，会在安装目录下生成一系列的目录，其结构如下:    </p>
<ul>
<li>bin目录下是使用Spark时常用的一些执行程序，例如我们进行Spark命令交互环境使用的spark-shell。    </li>
<li>conf目录下存放的是运行Spark环境所需的配置文件。    </li>
<li>data目录mllib需要的一些测试数据    </li>
<li>ec2目录是在AWS上部署使用的一些相关文件    </li>
<li>examples目录中有一些例子的源代码和测试文件    </li>
<li>lib目录下存放的是Spark使用的一些库，我们之后开发spark应用，也是需要使用这些库的。    </li>
<li>python目录是使用python相关的一些资源    </li>
<li>sbin目录中是搭建spark集群所需要使用的一些脚本。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>大数据：kafka常见问题</title>
    <url>/2016/03/10/kafkaquestion/</url>
    <content><![CDATA[<h1 id="一-kafka如何处理消费过的数据"><a href="#一-kafka如何处理消费过的数据" class="headerlink" title="一 kafka如何处理消费过的数据"></a>一 kafka如何处理消费过的数据</h1><h2 id="1-1-如果想消费已经被消费过的数据"><a href="#1-1-如果想消费已经被消费过的数据" class="headerlink" title="1.1     如果想消费已经被消费过的数据"></a>1.1     如果想消费已经被消费过的数据</h2><ul>
<li><p>consumer是底层采用的是一个阻塞队列，只要一有producer生产数据，那consumer就会将数据消费。当然这里会产生一个很严重的问题，如果你重启一消费者程序，那你连一条数据都抓不到，但是log文件中明明可以看到所有数据都好好的存在。换句话说，一旦你消费过这些数据，那你就无法再次用同一个groupid消费同一组数据了。    </p>
</li>
<li><p><strong>原因:</strong> 消费者消费了数据并不从队列中移除，只是记录了offset偏移量。同一个consumer group的所有consumer合起来消费一个topic，并且他们每次消费的时候都会保存一个offset参数在zookeeper的root上。如果此时某个consumer挂了或者新增一个consumer进程，将会触发kafka的负载均衡，暂时性的重启所有consumer，重新分配哪个consumer去消费哪个partition，然后再继续通过保存在zookeeper上的offset参数继续读取数据。注意:offset保存的是consumer 组消费的消息偏移。    </p>
</li>
<li>如何消费同一组数据：<ol>
<li>采用不同的group</li>
<li>通过一些配置，就可以将线上产生的数据同步到镜像中去，然后再由特定的集群区处理大批量的数据。详见<a href="http://my.oschina.net/ielts0909/blog/110280" target="_blank" rel="noopener">详细</a><br><img src="/images/blog/kafka-question1.jpg" alt="图片"></li>
</ol>
</li>
</ul>
<h2 id="1-2-如何自定义去消费已经消费过的数据"><a href="#1-2-如何自定义去消费已经消费过的数据" class="headerlink" title="1.2    如何自定义去消费已经消费过的数据"></a>1.2    如何自定义去消费已经消费过的数据</h2><h3 id="1-2-1-Conosumer-properties配置文件中有两个重要参数"><a href="#1-2-1-Conosumer-properties配置文件中有两个重要参数" class="headerlink" title="1.2.1 Conosumer.properties配置文件中有两个重要参数:"></a>1.2.1 Conosumer.properties配置文件中有两个重要参数:</h3><ul>
<li><strong>auto.commit.enable</strong>:如果为true，则consumer的消费偏移offset会被记录到zookeeper。下次consumer启动时会从此位置继续消费。</li>
<li><strong>auto.offset.reset</strong>: 该参数只接受两个常量largest和Smallest,分别表示将当前offset指到日志文件的最开始位置和最近的位置。<br>如果进一步想控制时间，则需要调用Simple Consumer，自己去设置相关参数。比较重要的参数是 kafka.api.OffsetRequest.EarliestTime()和kafka.api.OffsetRequest.LatestTime()分别表示从日志（数据）的开始位置读取和只读取最新日志。    </li>
</ul>
<h3 id="1-2-2-如何使用SimpleConsumer"><a href="#1-2-2-如何使用SimpleConsumer" class="headerlink" title="1.2.2 如何使用SimpleConsumer"></a>1.2.2 如何使用SimpleConsumer</h3><ul>
<li>首先，你必须知道读哪个topic的哪个partition<br>然后，找到负责该partition的broker leader，从而找到存有该partition副本的那个broker    </li>
<li>再者，自己去写request并fetch数据.      </li>
<li><p>最终，还要注意需要识别和处理broker leader的改变.    </p>
<p><a href="http://stackoverflow.com/questions/14935755/how-to-get-data-from-old-offset-point-in-kafka" target="_blank" rel="noopener">参考1</a><br><a href="https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka" target="_blank" rel="noopener">参考2</a><br><a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="noopener">完整代码</a>        </p>
</li>
</ul>
<h2 id="2-kafka-partition和consumer数目关系"><a href="#2-kafka-partition和consumer数目关系" class="headerlink" title="2    kafka partition和consumer数目关系"></a>2    kafka partition和consumer数目关系</h2><ol>
<li>如果consumer比partition多，是浪费，因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数 。</li>
<li>如果consumer比partition少，一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀 。最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目 。</li>
<li>如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同 </li>
<li><p>增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化    </p>
<p><a href="http://www.cnblogs.com/fxjwind/p/3794255.html" target="_blank" rel="noopener">详见</a>     </p>
</li>
</ol>
<h2 id="3-kafka副本问题"><a href="#3-kafka副本问题" class="headerlink" title="3    kafka副本问题"></a>3    kafka副本问题</h2><p>   kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。    </p>
<h2 id="3-1-如何分配副本"><a href="#3-1-如何分配副本" class="headerlink" title="3.1     如何分配副本"></a>3.1     如何分配副本</h2><p>   Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致.    </p>
<h2 id="3-2-Kafka分配Replica的算法如下"><a href="#3-2-Kafka分配Replica的算法如下" class="headerlink" title="3.2 Kafka分配Replica的算法如下"></a>3.2 Kafka分配Replica的算法如下</h2><p>   1.将所有Broker（假设共n个Broker）和待分配的Partition排序.    </p>
<ol>
<li>将第i个Partition分配到第（i mod n）个Broker上.</li>
<li><p>将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上.</p>
<p><a href="http://www.haokoo.com/internet/2877400.html" target="_blank" rel="noopener">算法详细</a>    </p>
</li>
</ol>
<h2 id="4-kafka如何设置生存周期与清理数据"><a href="#4-kafka如何设置生存周期与清理数据" class="headerlink" title="4    kafka如何设置生存周期与清理数据"></a>4    kafka如何设置生存周期与清理数据</h2><p>   日志文件的删除策略非常简单:启动一个后台线程定期扫描log file列表,把保存时间超过阀值的文件直接删除(根据文件的创建时间).清理参数在server.properties文件中：<br>  <img src="/images/blog/kafka-question2.jpg" alt=""><br>  <a href="http://blog.csdn.net/lizhitao/article/details/25667831" target="_blank" rel="noopener">详见</a>或<a href="http://kafka.apache.org/documentation.html" target="_blank" rel="noopener">官网说明</a>    </p>
<h2 id="5-zookeeper如何管理kafka"><a href="#5-zookeeper如何管理kafka" class="headerlink" title="5    zookeeper如何管理kafka"></a>5    zookeeper如何管理kafka</h2><ol>
<li>Producer端使用zookeeper用来”发现”broker列表,以及和Topic下每个partition leader建立socket连接并发送消息.</li>
<li>Broker端使用zookeeper用来注册broker信息,以及监测partition leader存活性.</li>
<li>Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息.    </li>
</ol>
<h2 id="6-补充问题，kafka能否自动创建topics"><a href="#6-补充问题，kafka能否自动创建topics" class="headerlink" title="6    补充问题，kafka能否自动创建topics"></a>6    补充问题，kafka能否自动创建topics</h2><p>  producer.properties配置文件中的一个参数:<strong><em>auto.create.topics.enable=true</em></strong><br>  是否自动创建<br>  如果broker中没有topic的信息,当producer/consumer操作topic时,是否自动创建.<br>  如果为false,则只能通过API或者command创建topic  </p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>大数据：集群优化</title>
    <url>/2016/03/09/bigdata-cluster-opt/</url>
    <content><![CDATA[<h1 id="0-集群优化"><a href="#0-集群优化" class="headerlink" title="0 集群优化"></a>0 集群优化</h1><p>  一个小集群:1个master,10个datanode。<br>  最开始使用pig脚本分析作业，后面作业运行时观察发现，pig脚本执行的小作业太多导致任务调度频繁，集群效率低。<br>  小作业太多的影响:    </p>
<ol>
<li>频繁新建和关闭task，频繁分配container会消耗资源。</li>
<li>一个oozie action先会启动一个oozie laucher作业消耗一个container，然后再启动实际的job，启动的job首先会用个container启动application master，然后在启动计算的task<br>现在同时最多会有29个job，至少会有50个container不是在计算。</li>
</ol>
<h1 id="1-代码优化"><a href="#1-代码优化" class="headerlink" title="1 代码优化"></a>1 代码优化</h1><ol>
<li><p>增加5分钟基础作业时间粒度，5分钟-&gt;15分钟，减少Job数</p>
</li>
<li><p>合并15分钟粒度作业，Pig-&gt;MR，grouping comparator，减少基础数据重复读取次数，减少Job数</p>
</li>
<li>合并5分钟基础作业，一个作业处理三种话单，去除冗余字段（各粒度时间），减少Job数，减少数据量</li>
</ol>
<h1 id="2-集群参数配置"><a href="#2-集群参数配置" class="headerlink" title="2 集群参数配置"></a>2 集群参数配置</h1><h2 id="2-1-HDFS"><a href="#2-1-HDFS" class="headerlink" title="2.1 HDFS"></a>2.1 HDFS</h2><ul>
<li><strong>HDFS块大小</strong>:从默认的128MB调整为256MB，更大的块大小(block)意味着更少的job，由于当前作业计算并不复杂，可以使用更大块。</li>
<li><strong>复制因子</strong>:默认是3，现在修改为2。减少数据存储量，可以减小话单上传的时间消耗</li>
<li><strong>DataNode处理程序计数</strong>:参数是<strong><em>dfs.datanode.handler.count</em></strong> 默认值是3，调整为30。datanode上用于处理RPC的线程数。默认为3，较大集群，可适当调大些，比如8。需要注意的是，每添加一个线程，需要的内存增加。</li>
<li><strong>NameNode处理程序计数</strong>:参数是<strong><em>dfs.namenode.handler.count</em></strong> 默认是30，建议值是47，现在调整为60.namenode或者jobtracker中用于处理RPC的线程数，默认是10，较大集群，可调大些，比如64。<br><strong>NameNode服务处理程序计数</strong>:参数是 <strong><em>dfs.namenode.service.handler.count</em></strong>，默认值是30，建议值是47，现在调整为60。NameNode 用于服务调用的服务器线程数量。</li>
<li><strong>最大传输线程数</strong>:参数是一起配置的为:<strong><em>dfs.datanode.max.xcievers, dfs.datanode.max.transfer.threads</em></strong>对于datanode来说，就如同linux上的文件句柄的限制，当datanode 上面的连接数操作配置中的设置时，datanode就会拒绝连接。<br>一般都会将此参数调的很大，40000+左右。</li>
</ul>
<h2 id="2-2-YARN"><a href="#2-2-YARN" class="headerlink" title="2.2 YARN"></a>2.2 YARN</h2><ul>
<li><strong>每个作业的 Reduce 任务的默认数量</strong>:参数为<strong><em>mapreduce.job.reduces</em></strong>默认值为1，现在调整为30。通过观察当前运行的job实例，观察其reduce执行时间，发现时间消耗不足1秒，故不必启用过多reduce。</li>
<li><strong>启用 Ubertask 优化</strong>:Uber模式是Hadoop2.0针对MR小作业的优化机制。通过<strong><em>mapreduce.job.ubertask.enable</em></strong>来设置是否开启小作业优化，默认为false。<br>如果用Job足够小，则串行在的一个JVM完成该JOB，即MRAppMaster进程中，这样比为每一个任务分配Container性能更好。关于Ubertask的详细可以参考<a href="http://qianshangding.iteye.com/blog/2259421" target="_blank" rel="noopener">Ubertask模式</a>。</li>
<li><strong>Map任务内存</strong>：参数为<strong><em>mapreduce.map.memory.mb</em></strong>，保持默认值1GB。</li>
<li><strong>Reduce任务内存</strong>:参数为<strong><em>mapreduce.reduce.memory.mb</em></strong>，保持默认值1GB。</li>
<li><strong>Map任务CPU虚拟内核</strong>：参数为<strong><em>mapreduce.map.cpu.vcores</em></strong>，为作业的每个 Map 任务分配的虚拟 CPU 内核数。默认每个map一个CPU，用户提交应用程序时，可以指定每个任务需要的虚拟CPU个数。在MRAppMaster中，每个Map Task和Reduce Task默认情况下需要的虚拟CPU个数为1。    </li>
<li><strong>Reduce任务CPU虚拟内核</strong>:参数为<strong><em>mapreduce.reduce.cpu.vcores</em></strong>，说明 与Map任务CPU虚拟内核一致。</li>
<li><strong>Map 任务最大堆栈</strong>:参数是<strong><em>mapreduce.map.java.opts.max.heap</em></strong>，Map 进程的最大 Java 堆栈（字节）。该参数与<strong><em>mapreduce.reduce.java.opts.max.heap</em></strong>一样，都是ClouderManager独有的，标准的hadoop参数是<strong><em>mapreduce.map.java.opts</em></strong>和<strong><em>mapreduce.reduce.java.opts</em></strong></li>
<li><strong>Reduce 任务最大堆栈</strong>: 同Map 任务最大堆栈。</li>
<li><strong>容器内存</strong>:参数是<strong><em>yarn.nodemanager.resource.memory-mb</em></strong>。表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。当前配置为24GB。</li>
<li><strong>容器虚拟 CPU 内核</strong>:参数是<strong><em>yarn.nodemanager.resource.cpu-vcores</em></strong>可以为容器分配的虚拟CPU内核的数量。集群中每台服务器只有24个虚核，所以容器内存配24G内存就行，现在作业都小map、reduce都用不了太多内存，默认是1GB。多了也没用，因为每个container至少要1个核。</li>
</ul>
<h2 id="2-3-Oozie"><a href="#2-3-Oozie" class="headerlink" title="2.3 Oozie"></a>2.3 Oozie</h2><p>  <strong>Oozie Server 的 Java 堆栈大小</strong><br>    默认值为1GB，现在修改为4GB。</p>
<h2 id="2-4-HBase"><a href="#2-4-HBase" class="headerlink" title="2.4 HBase"></a>2.4 HBase</h2><ul>
<li><p><strong>HBaseMaster的Java堆栈大小</strong>:暂无调整。</p>
</li>
<li><p><strong>HBase Region Server处理程序计数</strong>:参数为<strong><em>hbase.regionserver.handler.count</em></strong>,默认值为30，调节至150.是RegionServer的请求处理IO线程数。较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。<br>较多的IO线程，适用于单次请求内存消耗低，TPS要求非常高的场景。设置该值的时候，以监控内存为主要参考。<br>这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。<br>压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。</p>
</li>
<li><p><strong>HBase RegionServer的Java堆栈大小(字节）</strong>:HBase regionserver堆栈能多大就多大，计算方式是RegionServer java堆大小= 服务器总内存-已分配内存 （注意：此配置为优化索引入库）</p>
</li>
</ul>
<h2 id="2-5-服务器参数"><a href="#2-5-服务器参数" class="headerlink" title="2.5 服务器参数"></a>2.5 服务器参数</h2><ul>
<li>服务器时钟同步</li>
<li>修改swappiness值<br> 在所有服务器上，使用root用户执行     <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sysctl vm.swappiness&#x3D;0</span><br><span class="line"># echo &#39;vm.swappiness&#x3D;0&#39;&gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"># sysctl -p</span><br></pre></td></tr></table></figure></li>
<li>禁用透明巨页     <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># echo never &gt;&#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;redhat_transparent_hugepage&#x2F;enabled</span><br></pre></td></tr></table></figure>
 关于透明巨页，参考<a href="http://blog.chinaunix.net/uid-26489617-id-3205109.html" target="_blank" rel="noopener">透明巨页</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>个人简历</title>
    <url>/2016/03/05/me/</url>
    <content><![CDATA[<h2 id="this-is-my-profile"><a href="#this-is-my-profile" class="headerlink" title="this is my profile"></a>this is my profile</h2><h2 id="Well-this-is-not-the-right-time-to-introduce-myself"><a href="#Well-this-is-not-the-right-time-to-introduce-myself" class="headerlink" title="Well,this is not the right time to introduce myself."></a>Well,this is not the right time to introduce myself.</h2><h3 id="基本情况"><a href="#基本情况" class="headerlink" title="基本情况"></a>基本情况</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>姓名</strong></td>
<td>&nbsp;&nbsp; 夏涛 &nbsp;&nbsp;</td>
<td><strong>性别</strong></td>
<td>男 &nbsp;&nbsp;</td>
<td><strong>出生年月</strong></td>
<td>1989-05&nbsp;&nbsp;</td>
</tr>
<tr>
<td><strong>民族</strong></td>
<td>汉</td>
<td><strong>籍贯</strong></td>
<td>湖北&nbsp;黄冈&nbsp;&nbsp;</td>
<td><strong>联系方式</strong></td>
<td>17705694468&nbsp;&nbsp;</td>
<td><strong>邮件</strong></td>
<td>shartoo518@gmail.com</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
  </entry>
  <entry>
    <title>spark 测试</title>
    <url>/2015/09/25/mapreduce-introduce/</url>
    <content><![CDATA[<h2 id="spark-overview"><a href="#spark-overview" class="headerlink" title="spark overview"></a>spark overview</h2><h3 id="UC-Berkeley-的spark数据分析栈"><a href="#UC-Berkeley-的spark数据分析栈" class="headerlink" title="UC Berkeley 的spark数据分析栈"></a>UC Berkeley 的spark数据分析栈</h3><p><img src="http://i.imgur.com/OWyTF1M.png" alt=""></p>
<p>按使用方式划分</p>
<ul>
<li>离线批处理（Mlib，Graphs）</li>
<li>交互式查询（spark SQL）</li>
<li>时实计算（spark streaming）</li>
</ul>
<h3 id="spark资源调度"><a href="#spark资源调度" class="headerlink" title="spark资源调度"></a>spark资源调度</h3><p><img src="http://i.imgur.com/rtY99ub.png" alt=""></p>
<ul>
<li>stanalone</li>
<li>mesos</li>
<li>yarn</li>
</ul>
<p>  其中我们使用的是yarn资源调度，也就是运行spark job向集群申请资源的方式与hadoop是一样的，先向resourcemanger，然后在nodemanager，申请container启动applicationMaster,运行excutor</p>
<p>  yarn的提交job方式client和cluster</p>
<ul>
<li>client提交方式，driver program运行在提交机器上</li>
<li>cluster方式，driver program是运行在集群中的某个worker中</li>
</ul>
<h3 id="spark-VS-hadoop"><a href="#spark-VS-hadoop" class="headerlink" title="spark VS hadoop"></a>spark VS hadoop</h3><ul>
<li><p>应用场景</p>
<ul>
<li>hadoop的mapreduce适合做大数据集的离线批处理，</li>
<li>hadoop不是万能的，小数据集（单机能处理的小数据集杀鸡用牛刀），以及复杂的迭代运算，实时计算，在线分析等无能为力，而spark的出现很好的弥补了hadoop的不足之处，因为spark是基于内存的计算框架，适合复杂的迭代计算，spark streaming弥补实时计算的空缺（storm实时性更高，吞吐量，容错方面缺不如spark，稍后介绍spark的容错机制lineage和实时计算与storm的对比）</li>
</ul>
</li>
<li><p>运行效率</p>
<ul>
<li>spark官网效率比较<br><img src="http://i.imgur.com/nTBmin9.png" alt=""></li>
<li>咱门研究中心同事实际的测试报告</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   Spark性性能能与与MR相相比比较较提提高高了了13.6% </span><br><span class="line">    结果分析 </span><br><span class="line">    之前的Hadoop版本的批处理作业，共有23个作业，作业之间的关联方式为，</span><br><span class="line"> 前一个作业的输出结果保存在特定目录中，作为之后作业的输入数据。其中有一定量的计算结果是仅作为中间的临时数据存在，</span><br><span class="line">所有作业结束后将会被清理。这是由于每个Hadoop作业仅能执行一个MapReduce过程，</span><br><span class="line">这个问 题通过Spark的编程结构可以改善为按功能模块进行作业划分，每个作业中实现多个原来MapReduce的功能，</span><br><span class="line">将中间数据输出到磁盘并在下一次作业中重新读入的过程简化为Spark中的中间缓存变量保存在内存中。</span><br><span class="line">这里性能的提升主要来自于此，即优化了中间数据的冗余磁盘IO时间。此外，对于省网的分析作业而言，</span><br><span class="line">有着如下的特点，导致了性能提升不能达到理论上提及的一个数量级的改善效果。 </span><br><span class="line">    第一，原始数据量大，输入的基础数据量过于巨大，导致大量的时间花费在第一次的磁盘数据读取上，</span><br><span class="line">这个时间只取决于磁盘IO速率和文件大 小，而与分布式计算模式无关。省网分析作业内容大多属于磁盘密集型，</span><br><span class="line">与数据读取的时间相比，计算的时间耗费比重较轻，使得Hadoop和Spark的性能表现差异不大。 </span><br><span class="line">    第二，省网数据分析的内容，大多数属于单次的计算分析，即统计次数和汇总的工作，</span><br><span class="line">这方面Hadoop的性能可以极好的发挥出来。Spark更优势 于对一组小规模输入数据的，反复迭代计算，输入文件的读取时间较小，</span><br><span class="line">而计算过程十分复杂，这样其基于内存的计算方法可以更充分的展现优势。这在前一阶段中，</span><br><span class="line">使用分布式对矩阵进行计算的过程中体现的尤为明显，效果可以接近理论中提及的一个数量级提升。</span><br></pre></td></tr></table></figure>
<ul>
<li>开发效率比较<ul>
<li>spark基于rdd的操作，是mapreduce的超集，提供我们基于rdd丰富的接口，如filter，disinct，reducebykey等等，而hadoop这些操作需要用户在map或reduce，combine自己编码实现，</li>
<li>咱门写mapreduce程序，每个job都要写maper类，reducer类（当然有些job可以不写reducer类，如sqoop导入数据库就只需maper），可能还要写partition，combiner类，而且写完job后，需要构建job与job之间执行的顺序和依赖关系，输入输出的键值类型等；<ul>
<li>而spark是不需要这么琐碎，对rdd执行多个transform后，当执行一个action动作后（后面将介绍rdd的操作），自动构建一个基于rdd的DAG有向无环执行作业图，使用过pig的同事有所体会，这点类似pig，pig的解释器会将基于数据集的流处理过程，转换为DAG的job链，但spark又优于pig，可以做到过程控制，pig作为一个数据流语言，缺乏过程控制，粗糙的过程控制需要一门动态的脚本语言如python，javascript来实现，而且pig，hive只适合做统计分析作业，面对复杂的处理，如dougelas参数区线的压缩，需要用mapreduce或spark处理。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="开发语言支持"><a href="#开发语言支持" class="headerlink" title="开发语言支持"></a>开发语言支持</h3><ul>
<li>原生语言scala</li>
<li>java</li>
<li>python</li>
<li>spark1.4后支持R语言</li>
</ul>
<h3 id="spark的核心RDD"><a href="#spark的核心RDD" class="headerlink" title="spark的核心RDD"></a>spark的核心RDD</h3><p>  大家可以理解弹性分布式集合就是一个数据集合，这个集合有多个partition组成，而这些partition分布到集群中各节点的worker</p>
<ul>
<li><p>创建RDD的方式</p>
<ul>
<li>基于内存集合<br>如1到100数字Range作为rdd，val data = sc.parallelize(1 to 100)</li>
<li>外部存储系统，如hbase，cassandra，hdfs等， 如val data = sc.textfile(“dataPath”)</li>
</ul>
</li>
<li><p>基于rdd的操作</p>
<ul>
<li><p>Transformations操作<br>如map，filter，groupbykey等等，更多操作可参考<a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations" target="_blank" rel="noopener">spark官网</a></p>
</li>
<li><p>action操作<br>top，count，reducebykey，saveastexrfile等等，更多操作可参考<a href="http://spark.apache.org/docs/latest/programming-guide.html#actions" target="_blank" rel="noopener">spark官网</a></p>
</li>
</ul>
</li>
</ul>
<p>  transform是lazy执行的，也就是说直到遇到该rdd链执行action操作，才会启动job，执行计算，这种思想跟scala语言的lazy十分相似，下面通过一个简单的scala例子体会下这种思想</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">package com.haohandata.dnsApp</span><br><span class="line">import scala.io.Source._</span><br><span class="line">import scala.io.Source</span><br><span class="line">&#x2F;**</span><br><span class="line"> * @author xizhououyang@163.com</span><br><span class="line"> * @desription lazy deamon</span><br><span class="line"> *&#x2F;</span><br><span class="line">object LazyDeamon &#123;</span><br><span class="line">  &#x2F;*</span><br><span class="line">代码解释：当我们输入一个不存在的文件，如果不执行for循环对文件进行读取，program并不会抛异常，也就是说定义一个变量为lazy</span><br><span class="line">后，当我们对其引用求值时候，才会加载运行，这点类似于java的反射机制，动态加载</span><br><span class="line">*&#x2F;</span><br><span class="line">  def  main(args:Array[String])&#123;</span><br><span class="line">  lazy  val  file &#x3D; Source.fromFile(&quot;&#x2F;home&#x2F;osun&#x2F;pgadmin.logxx&quot;)</span><br><span class="line">  for(line&lt;- file.getLines)</span><br><span class="line">    println(line)</span><br><span class="line">  val word &#x3D;&quot;learning spark&quot;</span><br><span class="line">   println(word)</span><br><span class="line">  </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量是分发到每个worker的只读变量不能修改，功能与hadoop的分布式缓存类似，</p>
<p>  目前的dns项目实战使用到是做资源表关联（大数据集与小数据集的关联），存放广播变量中，通过map转换操作做关联，注意广播变量是一个只读变量，不能做修改。</p>
<h3 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h3><p>作业中全局的一个计数器，与hadoop的计数器类似，并不陌生，我们平时跑完mr或者pig的时候会有三种类型计数器的统计，<br>Framkework计数器，job计数器，hdfs文件系统计数器，注意spark中的计数器是不能在task中求值，只能在driver program中求值</p>
<p>  在dns项目中统计各用户群，各运营商，top10的icp，每个icp下统计top10 的host，可先在每个partition中统计top10的icp和top10的host，然后保存到计数器变量中，然后将聚合后结果话单过滤只保留掉计数器中的host和icp，这样可以避免多次迭代调用rdd.top（10）产生N<em>N个job；取五分钟小片数据，采用n</em>n迭代调用rdd.top方式生成库表需要两个小时，并产生了1800多个小job，跑了两个多小时，采用计数器过滤方式，4分多钟就能跑完库表实现入库postgresql</p>
<h3 id="rdd依赖"><a href="#rdd依赖" class="headerlink" title="rdd依赖"></a>rdd依赖</h3><ul>
<li>narrow依赖（父rdd的同一个partion最多只给子rdd一个partion依赖）</li>
<li>wide依赖（父rdd的同一个partion被子rdd多个partion依赖）<br><img src="http://i.imgur.com/UE5Od8S.png" alt=""></li>
</ul>
<h3 id="小结，从计算，存储，容错谈谈rdd"><a href="#小结，从计算，存储，容错谈谈rdd" class="headerlink" title="小结，从计算，存储，容错谈谈rdd"></a>小结，从计算，存储，容错谈谈rdd</h3><ul>
<li>计算</li>
</ul>
<p><img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/ba527855ed3b360d8c82840c62b0b3ab/spark计算code.png" alt="spark计算code"></p>
<p>注意：由于时间关系，直接截了他人画的图，deamon中存在一点error，正确的代码应该是map(parts=&gt;(parts(0),parts(1).toInt)),第一次map的transform得到的是RDD[Array[String]],不是RDD[List[String]]</p>
<p>code.png)<img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/e82db4ea22a47be62bc7355505d06ba2/spark计算code依赖.png" alt="spark计算code依赖"></p>
<p>每个job划分不同的stage，每个stage就是一个Set[task]集合  </p>
<p><img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/8f0a48da38c9da65b84ed5af5262562f/spark提交作业流程.png" alt="spark提交作业流程"></p>
<p>  spark的作业调度，分DAGshedule，和taskshedule二级，跟hadoop的jobtraker，tasktracker两级调度类似</p>
<ul>
<li><p>存储</p>
<ul>
<li>MEMORY_ONLY</li>
<li>MEMORY_AND_DISK</li>
<li>MEMORY_ONLY_SER</li>
<li>MEMORY_AND_DISK_SER</li>
<li>DISK_ONLY</li>
<li>MEMORY_ONLY_2, MEMORY_AND_DISK_2</li>
</ul>
<p>上面是spark是rdd的各种存储策略，是spark计算框架中，默认认为重复计算rdd需要的时间会比从磁盘中读取数据进行的io操作效率高，<br>因此默认所有的rdd的persist方式都是存在内存中，当内存不足后，会丢弃掉这个rdd，需要时候再根据lineage<br>机制从新计算，实际开发中那如果认为计算出来的rdd代价远比进行io大，这时可根据情况选择其他持久化策略，如在dns项目中，需要关联ppp的result和record话单后的rdd，采取MEMORY_AND_DISK_SER方式的持久化</p>
</li>
<li>容错（lineage）</li>
</ul>
<p>穿插一个小故事：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  高帅富小明家有一个家传宝，祖训这个宝物得一代代往下传，每代人可能对这个传家宝，</span><br><span class="line">每代人需对这宝物进行雕塑改造，如嵌入宝石，或者砖石，某天小明炒股亏空了，于是他要变卖这个传家宝，</span><br><span class="line">可是造化弄人，当他要变卖时候，发现传家宝不见了，聪明的小明，首先会确认他爸爸是否已经把这件宝物传了给他，</span><br><span class="line">如果确定是，他会将在翻遍自己房子找，如果他父亲没传给他，直接去他父亲的住处找，按照这个步骤，</span><br><span class="line">如果祖父那还没找到，他会一直回溯到他曾祖父那，直到找到传家宝，然后再一代代地传给小明，</span><br><span class="line">小明得到宝物后最终把它变卖</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">分析情景:</span><br><span class="line">- rdd就好比传家宝</span><br><span class="line">- 情景中的每个人物就好比不同时候集群中的计算节点中的worker</span><br><span class="line">-  小明变卖宝物，就好比执行了一个action，触发提交job</span><br><span class="line">-  而每代人对宝物加入一个宝石，就好比rdd的transform操作</span><br><span class="line">-  rdd的容错是lineage机制，如果当向spark提交job的时候，会构造基于rdd操作的DAG的作业流，这时会有基于rdd依赖链，如果计算过程中某个rdd丢失了，它会从父rdd那重新计算，如果父rdd不存在，会一直回溯上去直到找到父的rdd，然后再依照依赖链重新执行计算，最后执行action操作</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## spark在项目的实战应用</span><br><span class="line">### 架构图</span><br><span class="line">![](http:&#x2F;&#x2F;i.imgur.com&#x2F;VdB2LPU.png)</span><br><span class="line">### 项目代码</span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;gitlab.hudoumiao.com&#x2F;applications&#x2F;User_Mobility_Analysis&#x2F;tree&#x2F;master&#x2F;sparkcode&#x2F;src&#x2F;main&#x2F;scala&#x2F;com&#x2F;haohandata</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## spark streaming</span><br><span class="line"></span><br><span class="line">### spark streaming vs Storm（下面是引用研究中心同事的给出的两者对比的报告内容）</span><br></pre></td></tr></table></figure>
<p> Storm和Spark Streaming都是分布式流处理的开源框架。虽然二者功能类似，但是也有着一定的区别。 </p>
<ul>
<li>处理模型 </li>
</ul>
<p>虽然这两个框架都提供可扩展性和容错性,它们根本的区别在于他们的处理模型。<br>Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark，<br>也就是把Spark Streaming的输入数据按照batch size  （如1秒）分成一段一段的数据 （Discretized Stream），<br>每一段数据都 转换成Spark中的RDD  （Resilient Distributed Dataset ），然后将Spark Streaming 中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加，或者存储到外部设备。 </p>
<p>在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑 （topology ）。<br>这个拓扑将会被提交给集群，由集群中的主控节点 （masternode ）分发代码，将任务分配给工作节点 （worker node ）执行。<br>一个拓扑中包括spout和bolt两种角色，其中spout发送消息，负责将数据流以tuple元 组的形式发送出去；<br>而bolt则负责转发数据流，在bolt 中可以完成计算、过滤等操作，bolt 自身也可以随机将数据发送给其他bolt 。<br>在storm中，每个都是tuple是不可变数组，对应着固定的键值对。 简而言之，Storm是让数据面向计算，<br>而Spark Streaming是使计算面向数据。 </p>
<ul>
<li><p>延迟，storm更高<br>Spark Streaming，最小的Batch Size的选取在0.5~2秒钟之间，而Storm 目前最小的延迟是100ms左右，<br>所以Spark Streaming能，够满足除对实时性要求非常高 （如高频实时交易）之外的所有流式准实时计算场景，<br>而高实时性要求的场景则应该交给Storm来完成。 </p>
</li>
<li><p>容错，spark streaming更好 </p>
</li>
</ul>
<p>在容错数据保证方面的权衡是，Spark Streaming提供了更好的支持容错状态计算。<br>在Storm中,每个单独的记录当它通过系统时必须被跟踪，所以 Storm能够至少保证每个记录将被处理一次，<br>但是在从错误中恢复过来时候允许出现重复记录。这意味着可变状态可能不正确地被更新两次。<br>另一方 面，Spark Streaming只需要在批级别进行跟踪处理，因此可以有效地保证每个mini-batch将完全被处理一次，<br>即便一个节点发生故障。 </p>
<ul>
<li>吞吐量，spark streaming更强<br> Spark 目前在EC2上已能够线性扩展到100个节点 （每个节点4Core ），可以以数秒的延迟处理6GB/s的数据量 （60M records/s ），其吞吐量也比流行的Storm高2～5倍。 </li>
</ul>
<p>使用选择 </p>
<p>如果你想要的是一个允许增量计算的高速事件处理系统，Storm会是最佳选择。<br>它可以应对你在客户端等待结果的同时，进一步进行分布式计算的需求，使用开箱即用的分布式RPC  （DRPC）就可以了。<br>最后但同样重要的原因：Storm使用Apache Thrift ，你可以用任何编程语言来编写拓扑结构。<br>如果你需要状态持续，同时/或者达到恰好一次的传递效果，应当看看更高层面的Trdent API，它同时也提供了微批处理的方式。<br>如果你必须有状态的计算，恰好一次的递送，并且不介意高延迟的话，那么可以考虑Spark Streaming，<br>特别如果你还计划图形操作、机器学习或者访问SQL的话，ApacheSpark的stack允许你将一些library与数据流相结合<br>（Spark SQL，Mllib，GraphX），它们会提供便捷的一体化编程模型。<br>尤其是数据流算法 （例如：K均值流媒体）允许Spark实时决策的促进。 </p>
<p>```</p>
<h3 id="核心DStream"><a href="#核心DStream" class="headerlink" title="核心DStream"></a>核心DStream</h3><ul>
<li>Dstream简介<br>Dstream是一组以时间为轴连续的一组rdd<br><img src="http://i.imgur.com/H5GA2XL.png" alt=""></li>
<li>Dstream的输入源</li>
</ul>
<p><img src="http://i.imgur.com/ya40qiL.png" alt=""></p>
<ul>
<li>DStream的transformations操作</li>
<li>DSstream的action操作</li>
</ul>
<h3 id="使用场景划分"><a href="#使用场景划分" class="headerlink" title="使用场景划分"></a>使用场景划分</h3><ul>
<li>无状态</li>
</ul>
<p>每次批处理，receiver接收的数据都作为数据Dstream操作</p>
<ul>
<li><p>有状态updateStateByKey(func)</p>
<p>本次计算，需要用到上次批处理的结果。<br>比如spark streaming的批处理时间是五分钟，但业务中，我需要统计话单中haohandata.com.cn从程序运行后，每五分钟后haohandata.com.cn这个域名的累加的访问数，这时我们会以上次批处理为key的访问次数，加上本次五分钟批处理得到结果</p>
</li>
<li><p>windowns</p>
</li>
</ul>
<p>基于窗口的操作，批处理时间，滑动窗口，窗口大小<br>DNS实时计算实验项目中，统计五分钟粒度各rcode的次分布，<br>由于存在边界数据，解决的办法采取五分钟为批处理时间，滑动窗口为五分钟，窗口大小为10分钟，每次进行reduceByKeyAndWindow后，会进行过滤，只存这个windown中的中间五分钟数据，再入库cassandra</p>
<h2 id="dns项目的spark-streaming实时计算（实验性项目）"><a href="#dns项目的spark-streaming实时计算（实验性项目）" class="headerlink" title="dns项目的spark streaming实时计算（实验性项目）"></a>dns项目的spark streaming实时计算（实验性项目）</h2><h3 id="DNS项目处理流程图"><a href="#DNS项目处理流程图" class="headerlink" title="DNS项目处理流程图"></a>DNS项目处理流程图</h3><p><img src="http://i.imgur.com/7EatbDK.png" alt=""></p>
<h3 id="项目代码"><a href="#项目代码" class="headerlink" title="项目代码"></a>项目代码</h3>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>大数据之流数据挖掘</title>
    <url>/2014/05/17/bigdata-streamdata-introduce/</url>
    <content><![CDATA[<h2 id="一-流数据的特征"><a href="#一-流数据的特征" class="headerlink" title="一 流数据的特征"></a>一 流数据的特征</h2><ol>
<li>分发速度非常快，必须及时处理，否则将永远丢失。</li>
<li>即使分发速度较慢，同时多个数据流一起则超过了内存最大容量。</li>
</ol>

<h2 id="二-流数据的处理方法"><a href="#二-流数据的处理方法" class="headerlink" title="二 流数据的处理方法"></a>二 流数据的处理方法</h2><h3 id="2-1-流数据抽样"><a href="#2-1-流数据抽样" class="headerlink" title="2.1 流数据抽样"></a>2.1 流数据抽样</h3><p><B>问题描述：</B>过去的一个月中典型用户提交的重复查询比率是多少。假设我们只存储其中十分之一的流元素。<br></p>
<p><B>典型做法：</B>对每个搜索查询产生一个随机数（比如0-9中间的一个随机数），并当且仅当为0时才存储。如果用户提交的查询足够多，大数定律会保证大部分用户所存储的比例非常接近1/10.</p>
<h4 id="2-1-1-误区"><a href="#2-1-1-误区" class="headerlink" title="2.1.1 误区"></a>2.1.1 误区</h4><p>&nbsp;&nbsp;&nbsp;如果想得到用户提交的平均重复查询数目，上述抽样会得到错误结果。<br><br>&nbsp;&nbsp;&nbsp;假设某个用户在你过去一个月中有s个查询只提交过一次，d个查询提交两次，不存在超过两次的提交。那么提交过一次查询数目达到我们所期望的s/10，而在出现过两次的d个查询中，只有d/100会在样本中出现2次，该值等于d乘以该查询两次出现在1/10样本中的概率。于是在整个中出现2次的d个查询中，有18d/100个查询样本在样本中出现一次。<br>   本来，在所有搜索查询中重复搜索查询的比率正确答案是d/(s+d).但是，如果采用上述方法，我们得到的值为 d/10+18d/100个查询出现一次。</p>
<h4 id="2-1-2-正确思路"><a href="#2-1-2-正确思路" class="headerlink" title="2.1.2 正确思路"></a>2.1.2 正确思路</h4><p>&nbsp;&nbsp;&nbsp;我们不能从每个用户的搜索查询的抽样样本中得到正确答案。因此，必须要挑出1/10的用户并将它们所有的查询放入样本，而不考虑其他用户的搜索查询。每当一个新的查询到达流中时，我们会查找用户以判断其是否在已有样本中出现，若出现则放入样本，否则丢弃。如果没有出现该用户，我们产生一个0-9随机数，若为0则加入用户列表，并将其标记为”in”，否则，也加入用户列表，但是标记为”out”。<br><br>&nbsp;&nbsp;&nbsp;<font color="blue">注意：</font>引入哈希函数将每个用户哈希到编号0-9的10个桶中之一。但是桶中并不保存真正用户，事实上桶中没有任何数据。只是将哈希函数作为随机数生成器来使用。该哈希函数的一个重要特点就是，即使在相同用户上应用多次，其生成的随机数也相同。即，对任何用户都不需要存储其in/out决策，因为任何查询到来时都可以重构该决策。</p>
<h3 id="2-2-流过滤"><a href="#2-2-流过滤" class="headerlink" title="2.2 流过滤"></a>2.2 流过滤</h3><p>&nbsp;&nbsp;主要讨论的是使用布隆过滤器。</p>
<h4 id="2-2-1-布隆过滤器简介"><a href="#2-2-1-布隆过滤器简介" class="headerlink" title="2.2.1 布隆过滤器简介"></a>2.2.1 布隆过滤器简介</h4><p> 布隆过滤器也即Bloom Filter算法  一个布隆过滤器由以下几个部分组成</p>
<ul>
<li>n个位组成的数组，每个位初始值都是0。</li>
<li>一系列哈希哈书 $h_1,h_2,h_3…..h_k$ 组成的集合。每个哈希函数将“键”值映射到上述n个桶（对应于位数组的n个位）中。</li>
<li><p>m个键值组成的集合S</p>
<p>布隆过滤器的目的是让所有键值在S中的流元素通过，而阻挡大部分键值不再S中的流元素，哈希函数hi及S中的键值K，将每个 $h_i(K)$对应的位置为1。</p>
</li>
</ul>
<p>当键值为K的流元素到达时，检查所有的 $h_1(k)， h_2(k) ，h_3(k)….h_k(k)$ 对应的位是否全部都是1.如果是则允许该元素通过，如果有一位或多位为0，则认为K不可能在S中。则拒绝该元素通过。如果元素键值在S中出现一定会通过布隆过滤器，但是元素键值不在S中的元素也有可能会通过。我们需要了解如何基于位数组长度n，集合S的元素数目m及哈希函数的数目k来计算false positive概率。</p>
<h4 id="2-2-1-Bloom-Filter算法思路"><a href="#2-2-1-Bloom-Filter算法思路" class="headerlink" title="2.2.1  Bloom Filter算法思路"></a>2.2.1  Bloom Filter算法思路</h4><ol>
<li>我们有一个长度为n的比特数组，开始的时候将这个比特数组里所有的元素都初始化为0。<br><br>

00000000000000000000<br><br>

上面的比特数组n为20。</li>
<li>然后选取k个哈希函数，这k个哈希函数产生的结果的值的范围在0到n-1之间（对于上面的比特数组，即0到19）。对每个要添加进集合的对象进行哈希运算，然后将哈希计算结果作为数组的索引，将索引位置的比特位设置为1（不管该比特位原先为0还是为1）。<br>
比如我们选取三个哈希函数，对于对象A哈希值为0，5，7。那么比特数组就为：
<br><br>10000101000000000000<br><br>

对象B的值为2，8，13，那么添加B后的比特数组为：<br><br>

10100101100001000000<br><br>

对象C为0，4，7（对象C的第一个哈希函数的值与对象A的相同了，没关系我们还是设置为1就可以了）：

<br><br>10101101100001000000<br><br>

现在我们的Bloom Filter里已经有3个元素了。现在我们要判断某元素X是否在该集合中。就相当于我们要实现一个contains方法。<br>
对元素X采用相同的三个哈希函数哈希，然后以这三个哈希值为索引去比特数组里找。如果三个索引位置的比特位都为1我们就认为该元素在集合中，否则不是。</li>
</ol>

<h4 id="2-2-3-Bloom-Filter算法应用"><a href="#2-2-3-Bloom-Filter算法应用" class="headerlink" title="2.2.3 Bloom Filter算法应用"></a>2.2.3 Bloom Filter算法应用</h4><p>&nbsp;&nbsp;&nbsp;比如假设我们有一个缓存服务器集群，集群里的不同的服务器承担的缓存也不尽相同。如果一个用户请求过来了，我们如何能快速的判断出用户请求的这个url在集群里哪台服务器上呢？因为每台服务器上缓存的url对应的页面非常庞大，我们全部弄到内存里代价也很高。我们就可以在每台服务器上放一个Bloom Filter，里面添加的都是本服务器上有缓存的那些url。这样即使Bloom Filter误报了，那就是把一个url发到了一个并不持有该url对应的缓存的服务器上，结果就是缓存未命中，缓存服务器只需要将该url打到后端的上游服务器就好了。</p>
<h2 id="三-独立元素数目估计"><a href="#三-独立元素数目估计" class="headerlink" title="三 独立元素数目估计"></a>三 独立元素数目估计</h2><h3 id="3-1-FM算法（Flajolet-Martin）"><a href="#3-1-FM算法（Flajolet-Martin）" class="headerlink" title="3.1 FM算法（Flajolet-Martin）"></a>3.1 FM算法（Flajolet-Martin）</h3><p>&nbsp;&nbsp;&nbsp;基本思想是：如果流中看到的不同元素越多，那么我们看到的不同的哈希值也越多。我们看到的不同哈希值越多时，哈希函数的性质是对同一个数哈希结果都是一样的。<br><br>&nbsp;&nbsp;&nbsp;理想中的是：对同一批数据使用多个哈希函数，每个哈希函数上得到不同的 $2^R$ 的值（对流元素a应用哈希函数h,h(a)的尾部将以一些0结束，尾部0的数目成为a和h的尾长，假设目前所有已有元素a的最大尾长为R， $2^R$  用来估计流中独立元素数目），然后求它们的平均值即可得到真实的m的近似值。</p>
<h3 id="3-2-FM算法的问题"><a href="#3-2-FM算法的问题" class="headerlink" title="3.2 FM算法的问题"></a>3.2 FM算法的问题</h3><p>&nbsp;&nbsp;&nbsp;假设一个r，使得2^远大于m。存在某个概率p发现r是流中最大尾长，于是发现r+1是流中最大尾长的概率至少为p/2.因此，随着R的增长，每个可能的R对2^R的期望贡献也越大。2^R的期望值实际是无限大。</p>
<h3 id="3-3-完美解决方案"><a href="#3-3-完美解决方案" class="headerlink" title="3.3 完美解决方案"></a>3.3 完美解决方案</h3><p>&nbsp;&nbsp;&nbsp;取所有估计值得中位数，由于中位数不会受到偶然极大的2^R影响。<B>缺陷是：</B>它永远都是2的幂值,不论用多少哈希函数，都是在两个2 的幂之间，那么小至少是log2(m)的一个小的倍数。<br><br>我们可以：首先将哈希函数分成小组，每个小组内取平均值。然后再所有平均值中取中位数，组间取中位数可以将中位数的缺陷的影响降低到几乎没有的地步。每个组的大小至少是log2(m)的一个小的倍数。</p>
<h2 id="四-矩估计"><a href="#四-矩估计" class="headerlink" title="四 矩估计"></a>四 矩估计</h2><p>&nbsp;&nbsp;&nbsp;上述独立流元素计数推广到一般的问题，该问题称为矩计算，包括不同流元素出现的频率分布的计算。</p>
<h3 id="4-1-矩定义"><a href="#4-1-矩定义" class="headerlink" title="4.1 矩定义"></a>4.1 矩定义</h3><p>&nbsp;&nbsp;&nbsp;假定一个流由选自某个全集的元素够成，并假定该全集中所有元素都排好序，这样我们通过整数 i 来标记该序列中的第i 个元素，假设该元素出现的次数为mi,则流的k 阶矩是所有 i 上的(mi)^k  之和。<br><br>&nbsp;&nbsp;&nbsp;流的一阶矩是所有元素mi之和，也即整个流的长度，当前流所有元素个数；二阶矩是所有元素mi的平方和。</p>
<h3 id="4-2-二阶矩的AMS算法"><a href="#4-2-二阶矩的AMS算法" class="headerlink" title="4.2 二阶矩的AMS算法"></a>4.2 二阶矩的AMS算法</h3><p>&nbsp;&nbsp;&nbsp;假设没有足够空间来计算流中所有元素的mi。我们仍然可以使用有限空间来估计流的二阶矩，空间越多结果越精确。对每个变量X 我们保存一下内容。</p>
<ol>
<li>全集当中的一个特定元素，记为X.element。</li>
<li>一个整数，记为X.value，它是变量X的值。在流中均匀的随机选择1到n之间的一个位置。将X.element置为该位置上的元素，X.value初始为1，每再看到一个X.element 就将其对应的X.value 值加1。</li>
</ol>
&nbsp;&nbsp;&nbsp;假定流  a,b,c,b,d,a,c,d,a,b,d,c,a,a,b,流长度为15。由于a  出现5次，b 出现4次，c和d各出现3次。<img src="/images/blog/bigdata-streamdata1.png">因此二阶矩为   $5^2+4^2+3^2+3^2=59$ .假定维护3个变量 $X_1,X_2,X_3$ .假定随机位置为3,8,13。当到达位置3时，对应的元素为c，于是   $X_1.element=c$ ,此时 $X_1.value=1$ ,而位置4为b，5为d，6为a，X1的值均不改变，在位置7元素c再次出现，继续往后的话，位置12再出现c，因此 $X_1.value=3$ 。据此可以得到 $X_2.value=2$ 和 $X_3.value=2$ (注意，他们都是从该元素第一次出现之后，往后出现的才算)。
<br>基于任意一个变量X，我们可以导出二阶矩的一个估计值为： n*(2*X.value-1)<br>
根据本例中的值，我们可以通过二阶矩估算值得平均值为：(15*(2*3-1)+15*(2*2-1)+15*(2*2-1))/3=55 可知与精确值 <font color="blue">59</font>相当接近了。

### 4.3 无限流的处理

&nbsp;&nbsp;&nbsp;对于二阶矩以及多阶矩的估计当中，我们是假定流长度n 是一个常数。实际应用当中，n 会不断随着时间增长。因此在变量位置选择的时候需要谨慎。
<ol><li>一方面，如果只对所有元素做一次选择，流不断增长时，计算会偏向早期的元素</li><li>另一发面，如果选择的等待时间太久，那么早期的元素位置上变量不多，从而造成估算的可靠性不高。</li><li>比较合理的选择是，任何时候都尽可能保持足够多的变量，并在流增长时丢弃某些变量（在选择某个位置的概率和其他位置的概率必须相等）。</li></ol>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>Java对象创建过程与垃圾回收机制</title>
    <url>/2014/05/15/javafoundamention-newObjectAndRubCollection/</url>
    <content><![CDATA[<h2 id="一-创建对象的步骤"><a href="#一-创建对象的步骤" class="headerlink" title="一 创建对象的步骤"></a>一 创建对象的步骤</h2><ol>
<li>虚拟机遇到一条<B>new</B>指令<br></li>
<li>检查指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过<br></li>
<li>若没有，则必须首先执行相应的类加载过程<br></li>
<li>类加载完成，虚拟机为新生对象分配内存。分配内存相当于从Java堆中划分出一块内存大小确定的块，分两种情况<br>(1)Java堆内存，属于绝对规整的那种。只需指针向空闲空间挪动一段举例即可<br>(2)不规整。空闲区和已分配区交错，需要一张“空闲列表”记录哪些区域是分配了的<br></li>
<li>虚拟机对对象进行必要的设置。如，这个对象是哪个类的实例，如何才能找到类的元数据信息，对象的哈希码。此步骤为止<font color="blue">虚拟机步骤完成</font><br></li>
<li><font color="blue">Java程序开始：</font>init方法还没有执行，所有字段仍然为0，new指令后，执行 init 方法。</li>
</ol>

<h2 id="二-垃圾回收的目标"><a href="#二-垃圾回收的目标" class="headerlink" title="二 垃圾回收的目标"></a>二 垃圾回收的目标</h2><p>Java运行期间的各个部分：程序计数器、虚拟机栈、本地方法栈，这三个区域随着线程而存亡。栈中的线帧随着方法的进入和退出而入栈、出栈。每个线帧分配多少内存在类的结构确定时也确定。这几个区域的内存分配和回收都是确定的，方法结束或线程结束时，内存就回收了。<br></p>
<p>而Java堆和方法区就不一样了，一个接口的多个实现类所需要的内存都不一样，一个方法的各个分支所需内存也不一样，程序运行期间才能动态确认。<font color="blue">垃圾收集器关注的也是此部分</font>    <br></p>
<h2 id="三-判断对象是否存活-堆中"><a href="#三-判断对象是否存活-堆中" class="headerlink" title="三 判断对象是否存活(堆中)"></a>三 判断对象是否存活(堆中)</h2><p>有以下方法：</p>
<ol>
<li><B>引用记数法：</B>给对象添加一个引育弄个计数器，有一个地方引用时该计数器加1，一个引用失效时，减一<br>
<B>缺点：</B>无法应对循环引用，例如【objA.instance=objB和obj.instance=objA】</li>
<li><B>可达性分析算法：</B>以一个称为"GC Root"的对象作为起始点，从这些节点往下搜索。当一个对象到"GC Root"的引用路径不可达时，证明该对象不可用。<font color="blue">java采用此方法</font><img src="/images/blog/java-jvm-obj-rubcollect1.png"></li>
</ol>

<h2 id="四-回收方法区"><a href="#四-回收方法区" class="headerlink" title="四 回收方法区"></a>四 回收方法区</h2><p>主要有两部分：(1)废弃的常量（2）无用的类</p>
<ol>
<li><B>废弃常量:</B>没有任何对象引用它</li>
<li><B>无用的类：</B><br>(1)该类所有的实例都已被回收，也即Java堆中不存在该类的任何实例<br>(2)加载该类的classLoader已被回收<br>(3)该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法</li>
</ol>

<h2 id="五-对象的访问定位"><a href="#五-对象的访问定位" class="headerlink" title="五 对象的访问定位"></a>五 对象的访问定位</h2><p>Java程序，需要通过栈上的reference数据来操作堆行的具体对象。reference类型在Java中只规定了一个指向该对象的引用，如何去定位，访问堆中的对象的具体对象。不同的虚拟机实现，分为使用句柄和直接指针两种。<br><br></p>
<h3 id="5-1-句柄访问"><a href="#5-1-句柄访问" class="headerlink" title="5.1 句柄访问"></a>5.1 句柄访问</h3><p>Java堆中划分一块内存作为句柄池，reference中存储的即对象的句柄地址，而句柄包含了对象实例数据与类型数据各自的具体地址信息。<br><img src="/images/blog/java-jvm-obj-rubcollect2.png"><br><br></p>
<p><B>优点：</B>reference中存储的是稳定的句柄地址，对象被移动（垃圾回收时），只会改变句柄中的实例数据指针，而reference本身不需要移动</p>
<h3 id="5-2-直接指针访问"><a href="#5-2-直接指针访问" class="headerlink" title="5.2 直接指针访问"></a>5.2 直接指针访问</h3><p>Java堆中对象的布局中就必须考虑如何放置访问类型数据相关信息，reference中存储的直接是对象地址<br><img src="/images/blog/java-jvm-obj-rubcollect3.png"><br></p>
<p><B>优点：</B>速度更快，节省了一次指针定位的时间开销。虚拟机Sun HotSpot即采用此方法。</p>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
  </entry>
  <entry>
    <title>Java基础笔记-Java内存区域</title>
    <url>/2014/05/11/javafoundamention1/</url>
    <content><![CDATA[<h2 id="一-运行时的数据区组成"><a href="#一-运行时的数据区组成" class="headerlink" title="一 运行时的数据区组成"></a>一 运行时的数据区组成</h2><p><img src="/images/blog/java-jvm-store-model.png" alt="图示1">   </p>
<ol>
<li><p>程序计数器<br>当前线程执行的字节码的行号指示器。自己吗解释器通过改变程序计数器(PC)的值来选取下一条需要执行的字节码指令。分支、循环、跳转、线程回复等基础功能都依赖于它。<br>例如：Java多线程机制。线程轮流切换，分配CPU执行时间，任一时刻，一个CPU只会执行一条线程指令，每个线程都需要一个独立PC，以保证线程切换后能正确恢复。      </p>
</li>
<li><p><strong>虚拟机栈</strong>四点说明:</p>
<ol>
<li>生命周期随着线程存亡</li>
<li>Java方法执行的内存模型：每个方法在执行的同时会创建一个线帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。</li>
<li>每个方法从调用直至执行完成的过程，就对应着一个线帧在虚拟机栈中从入栈到出栈的过程。</li>
<li>能发生的两类异常:<ul>
<li>I：线程的请求深度大于虚拟机所允许的深度，会抛出StackOverflowError。</li>
<li>II：虚拟机栈可动态扩展时无法申请到足够的内存，就会抛出OutOfMemoryError。</li>
</ul>
</li>
</ol>
</li>
<li><p>本地方法栈<br>作用与虚拟机相同，不同的是虚拟机栈为虚拟机执行Java方法（即字节码）服务，而本地方法栈为虚拟机执行本地方法（Native）服务。    </p>
</li>
<li><p>Java堆<br>Java Heap是Java虚拟机所管理的内存中最大的一块，是所有线程共享的一块内存区域。在虚拟机启动时创建，此区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。<br>说明：</p>
<ul>
<li>是垃圾收集器管理的主要目标(GC Garbage Collected).</li>
<li>可以处于物理上不连续的内存空间中，只要逻辑上连续即可。</li>
</ul>
</li>
</ol>
<p>5.方法区<br>   与堆一样，是所有线程共享的内存区域。它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。<br>  运行时常量池(Runtime Constannt Pool)：是方法区的一部分。Class文件中除了有类的版本、方法、字段、接口等描述信息外，还有一项是常量池，用于存放编译期间生成的各种字面两和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。<br><br>&nbsp;&nbsp;&nbsp;&nbsp;运行时常量池相对于class文件常量池的另外一个特征是动态性，Java语言并不要求常量一定是编译器才能产生。运行期间可将新的常量放入池中，比如String的intern()方法就是这种特性的直接应用。</p>
<ol>
<li>直接内存<br>非虚拟机运行时数据区的一部分，但频繁使用<br>来源：JDK 1.4之后引入NIO类，引入一种基于通道（Channel）与缓冲区（Buffer）的I/O方式。它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。由于避免在Java堆和Native堆中来回复制数据，在某些场合可显著提高性能。</li>
</ol>
<h2 id="二-Java内存模型与线程"><a href="#二-Java内存模型与线程" class="headerlink" title="二 Java内存模型与线程"></a>二 Java内存模型与线程</h2><ol>
<li><strong>主要目标</strong><br>定义程序各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量的底层细节。注意：</li>
</ol>
<ul>
<li>此处的主内存、工作内存与上面的Java堆、栈、方法区等不是同一层次的划分，两者没有关系</li>
</ul>
<p>2.内存模型与线程的对应关系如下图：<br><img src="/images/blog/java-jvm-store-model2.png" alt="图示2">  </p>
<ol>
<li>说明<ul>
<li>Java内存模型规定所有变量都存储在主内存中，此处主内存仅是虚拟机内存的一部分.</li>
<li>每个线程还有自己的工作内存（作用类似于处理器告诉缓冲），线程的工作内存保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之分类</title>
    <url>/2013/05/24/datamining-classfy-main/</url>
    <content><![CDATA[<p>博文参考《数据挖掘概念与技术》：韩家威著（机械工业出版社）</p>
<h2 id="一-分类的概念"><a href="#一-分类的概念" class="headerlink" title="一 分类的概念"></a>一 分类的概念</h2><p>在面向对象编程（OOP）中我们说“一切皆对象”，在数据挖掘中，我们应该认为“一切皆数据”。而分类就是按照您的选择评估标准将数据进行分离，使得具有某些相同特性的数据属于一个类，不相同的数据不在一个类。</p>
<h2 id="二-分类的一般过程"><a href="#二-分类的一般过程" class="headerlink" title="二 分类的一般过程"></a>二 分类的一般过程</h2><p>分类一般分为两个阶段：<code>学习阶段(构建模型)</code>和<code>分类阶段（使用分类模型给测试数据的赋予类标号）</code>。</p>
<ul>
<li>学习阶段：通过分析或从训练集“学习”来构造分类器。训练集由数据元组和相应的类标号组成。<ul>
<li><strong>元组</strong>：是用n维向量 $X=(x_1,x_2,x_3…)$ 表示的一条数据记录，其中n维向量表示的是元组X在n个属性上的度量。例如下图中黑边框标记的一条记录的元组表示为：<em>X=(“Ricky Field”,”Middle_aged”,”Low”)</em> 该数据有4个属性，分别是 <em>“name”,”age”,”income”,”Loan_descision”</em> ，其中属性”Loan_descision”也是分类属性，类标号为”Risky”。<br><img src="/images/blog/classfymain1.png"></li>
<li>分类阶段，如下图所示，它属于一种映射过程。根据分类模型中的规则，给予测试数据元组X特定的类标号<br><img src="/images/blog/classfymain2.png"></li>
</ul>
</li>
</ul>
<h2 id="三-分类的评估"><a href="#三-分类的评估" class="headerlink" title="三 分类的评估"></a>三 分类的评估</h2><h3 id="3-1-度量的基础术语"><a href="#3-1-度量的基础术语" class="headerlink" title="3.1 度量的基础术语"></a>3.1 度量的基础术语</h3><ul>
<li><strong>正元组</strong>：感兴趣的主要元组，即我们要在第二章图中的数据中找到类标号”Loan_edscision”为”safe”的记录。</li>
<li><strong>负元组</strong>：除去正元组以外的其他元组（或称为记录）。</li>
</ul>
<h3 id="3-2-度量的四个构件"><a href="#3-2-度量的四个构件" class="headerlink" title="3.2 度量的四个构件"></a>3.2 度量的四个构件</h3><ul>
<li><strong>真正例/真阳例（True Positive ,TP）</strong>：指的是被分类器正确分类的我们”感兴趣”的元组。</li>
<li><strong>真负例/真阴例（True Negative ,TN）</strong>：指的是被分类器正确分类的我们”不感兴趣”的元组。</li>
<li><strong>假正例/假阳例（False Positive,FP）</strong>：指的是被分类器错误的分类的元组，即将我们”不感兴趣”的元组分成了”感兴趣”的元组。</li>
<li><strong>假负例/假阳例（False Negative,FN）</strong>：指的是被分类器错误的分类的元组，即将我们”感兴趣”的元组分成了”不感兴趣”的元组。</li>
</ul>
<h3 id="3-3-评估度量"><a href="#3-3-评估度量" class="headerlink" title="3.3 评估度量"></a>3.3 评估度量</h3><p>有了度量的四个构件，我们可以得到常用的评估度量公式。如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>度量</th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率(识别率)</td>
<td>$\frac{TP+TN}{P+N}$</td>
</tr>
<tr>
<td>错误率(误分类率)</td>
<td>$\frac{FP+FN}{P+N}$</td>
</tr>
<tr>
<td>敏感率(真正例率、召回率)</td>
<td>$\frac{TP}{P}$</td>
</tr>
<tr>
<td>特效性(真负例率)</td>
<td>$\frac{TN}{N}$</td>
</tr>
<tr>
<td>精度</td>
<td>$\frac{TP}{TP+FP}$</td>
</tr>
<tr>
<td>F分数(精度和召回率的调和均值)</td>
<td>$\frac{2<em>精度</em>召回率}{精度+召回率}$</td>
</tr>
</tbody>
</table>
</div>
<p>如何理解：</p>
<ul>
<li>准确率和错误率是相对的：前者计算的是<B>全部记录</B>中，分类器正确分类的元组数量，包括正确分类的”感兴趣”元组和”不感兴趣”元组；而后者计算的是<B>全部记录</B>中分类器错误分类的元组，也包括错误分类的”感兴趣”元组和”不感兴趣”元组。</li>
<li>敏感度和特效性是相对的：前者计算的是<B>“感兴趣”元组</B>中，被正确分类的元组数量，可以理解为“我们得到的”感兴趣”元组，有多少是真正的”感兴趣”元组”；特效性计算的是<B>“不感兴趣”元组</B>中被正确分类的元组数量，可以理解为“我们的得到的”不感兴趣”元组，有多少真正是”不感兴趣”元组”。</li>
<li>精度：是一个完全关乎”感兴趣”元组的统计项，一般情况下与敏感度等同。</li>
</ul>
<h2 id="四-对模型的评估"><a href="#四-对模型的评估" class="headerlink" title="四 对模型的评估"></a>四 对模型的评估</h2><h3 id="4-1-保持-holdout-方法和随机二次抽样"><a href="#4-1-保持-holdout-方法和随机二次抽样" class="headerlink" title="4.1 保持(holdout)方法和随机二次抽样"></a>4.1 保持(holdout)方法和随机二次抽样</h3><ul>
<li><strong>保持(holdout)方法</strong> 是我们在讨论准确率时默认使用的方法。此方法中，数据会被随机地划分为两个独立的集合：<br><code>训练数据集合</code>和<code>检验数据集合</code>。通常,2/3的数据分配到训练集，其余1/3分配到检验集。</li>
<li><strong>随机二次抽样方法</strong> ，是保持方法的一种变形，只是将保持方法重复k次，总准确率取每次迭代准确率的平均值。</li>
</ul>
<h3 id="4-2-k折交叉验证"><a href="#4-2-k折交叉验证" class="headerlink" title="4.2 k折交叉验证"></a>4.2 k折交叉验证</h3><p>将数据分成互不相交的k等份 $D_1,D_2,D_3,…D_k$，训练和校验进行k次。第i次迭代时，将第i个等份(“折”)作为校验集，而其他等份(“折”)全体作为训练集合。注意，在保持方法中数据是随机分的，而此处是均分，并且每份数据集合都有一次机会作为校验集。下图显示的是第四次迭代时的一个示例：<br><img src="/images/blog/classfymain4.png"></p>
<h3 id="4-3-自助法"><a href="#4-3-自助法" class="headerlink" title="4.3 自助法"></a>4.3 自助法</h3>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之六：解读逻辑回归</title>
    <url>/2013/05/16/datamining-logicregressionsample/</url>
    <content><![CDATA[<h2 id="一-使用数据"><a href="#一-使用数据" class="headerlink" title="一 使用数据"></a>一 使用数据</h2><p>本文着重示例如何使用逻辑回归<br><br><a herf="http://download.csdn.net/detail/huangxia73/7059709">数据来源:电信数据集合</a><br></p>
<B>描述：</B>电信数据，有多个属性，用来预测客户流失。<br>
载入数据：

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">call_consumer&lt;-read.table(file&#x3D;&quot;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&quot;,header&#x3D;TRUE,sep&#x3D;&quot;,&quot;)  </span><br><span class="line">   Warning message:  </span><br><span class="line">   In read.table(file &#x3D; &quot;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&quot;, header &#x3D; TRUE,  :  </span><br><span class="line">    incomplete final line found by readTableHeader on &#39;d:&#x2F;LabData&#x2F;RData&#x2F;churn.txt&#39;  </span><br><span class="line"> &gt; edit(call_consumer)</span><br></pre></td></tr></table></figure>

<img src="/images/blog/loginregressionsample1.png">

## 二 .解读逻辑回归模型

分三种：

+ 一个两分预测变量的模型
+ 多分预测变量
+ 连续的预测变量

### 2.1 两分预测变量模型

假定唯一的预测变量是语音邮箱套餐（Intl.plan），这是一个表示是否为套餐会员的标记变量。下表显示了语音邮箱套餐会员流失情况。

||语音邮箱=否(x=0)|语音邮箱=是(x=1)|合计|
|---|---|---|---|
|流失=假(y=0)|2008|842|2850|
|流失=真(y=1)|403|80|483|
|合计|2411|922|3333|


似然函数可以表示为：

$$
 L(b\|x)=[\pi(0)]^{403}*[1-\pi(0)]^{2008}*[\pi(1)]^{80}[1-\pi(1)]^{842}
$$

使用语音邮箱套餐的客户流失的发生比＝ $\frac{\pi(1)}{1-\pi(1)} =\frac{80}{842}=0.095$

没有使用语音邮箱套餐的客户流失的发生比＝ $\frac{\pi(0)}{1-\pi(0)}=\frac{403}{2008}=0.2007$  

 OR=0.095/0.2007=0.47

也即　使用语音邮箱套餐的客户与没有使用语音邮箱套餐的客户相比，流失概率只有47%

下图显示了语音套餐会员流失的逻辑回归结果
<img src="/images/blog/loginregressionsample4.png">
可以得到 $b_0$＝-1.60596和 $b_1$=-0.747795。所以用于语音邮箱套餐（x=1）的客户或者没有语音套餐（x=0）的客户流失的估计值为：

$$
 \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}=\frac{e^{-1.60596-0.747795}}{1+e^{-1.60596-0.747795}}
$$

+ 对于一个拥有此套餐的客户，估计他的流失概率为： $\pi(1)=0.0868$（也可以直接计算 P(流失\|语音邮箱计划)=80/922=0.0868)，这一概率比数据集中给出的客户流失的总比例14.5%要小，说明开通语音邮箱套餐有利于减少客户流失。

+ 对于一个没有拥有此套餐的客户，估计他的流失概率为：$\pi(0)=0.16715$ （也可以直接计算 P(流失\｜语音邮箱计划)=403/2411=0.16715，这一概率比数据集中给出的客户流失的总比例14.5%要高，说明没有开通语音邮箱套餐对于客户流失不大。
+ 进一步地，可以利用Wald检验法检验语音邮箱套餐参数的显著性。这里, $b_1$ =-0.747795, $SE(b_1)$ =0.129101得 $Z_{wald}=-0.747795/0.129101=-5.79$ P值为P(\|Z\|>-5.79)趋近于0


### 2.2 多分预测变量模型

假定将客户服务电话数（customers services calls)看做一个新的变量<font color="red">"-CSC"</font>，分类如下：

+ 0个或1个客户服务电话：CSC＝低
+ 2个或3个客户服务电话：CSC＝中
+ 4个以上客户服务电话：CSC＝高

此时，分析人员需要用指示变量（虚拟变量）和参考单元编码法来给数据集编码，假定选择“ＣＳＣ＝低”作为参考单元，则可把指示变量值分配给另外两个变量。使用指示变量之后：

<p align="center">使用参考单元编码的客户电话指示变量</p>

||CSC-中|csc-高|
|---|---|---|
|低(0个或1个电话)|0|0|
|中(2个或3个电话)|1|0|
|高( $\ge4个电话$ )|0|1|

使用CSC展示客户流失情况列表汇总如下：

||CSC-低|CSC-中|CSC-高|合计|
|---|---|---|---|---|
|流失=假(y=0)|1664|1057|129|2850|
|流失=真(y=1)|214|131|138|483|
|合计|1878|1188|267|3333|

此时再对数据进行逻辑回归分析，得到的结果如下（<font color="blue">注意：没有CSC－低</font>）：
<img src="/images/blog/loginregressionsample8.png">

+ 对于CSC－中：$\bar {OR}＝ｅ^{b1}＝ｅ^{-0.03698}=0.96$
+ 对于CSC－高：$\bar {OR}＝ｅ^{b2}＝ｅ^{2.11844}=8.32$

这里， $b_0＝-2.501,b_1=-0.03698，b_2=2.11844$ 所以客户流失概率的估计量为：

$$
  \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}
  \\ 其中g(x)=e^{-2.051-0.036989(csc-中)+2.11844(csc-高)}
$$

有：

+ 对于那些很少拨打客服电话的客户：$g(x)=e^{-2.051-0.036989(0)+2.11844(0)}=e^{-2.501}$ 概率为：$\pi(x)=\frac{e^{-2.501}}{1+e^{-2.501}}=0.114$
。此概率比全部数据样本集中客户流失的概率14.5%要小。这表明这一类客户的流失率一定程度上比总体客　　户的流失率要小。
+ 对于拨打客服电话处于中等水平的客户，同上，此时
$g(x)=e^{-2.051-0.036989(1)+2.11844(0)}=e^{-2.088}$ <font color="blue">注意系数的差别,上一条中的系数是0，0，这个是1，0
+ 对于经常拨打客服电话的客户，同上，此时
  $g(x)=e^{-2.051-0.036989(0)+2.11844(1)}=e^{-2.501}$ 注意系数的差别,上一条中的系数是1，0，这个是0，1</font>

#### Wald检验   

如下：
+ 对于<font color="blue">CSC－中</font> 的参数进行Wald检验，$b_1=-0.036989,SE(b_1)=0.11771$
　故而，
$$
  Z_{wald}＝-0.036989/0.117701=-0.31426
$$

此时，P值P(\|Z\|>0.31426)=0.753，不显著，所以没有证据表明<font color="blue">CSC－中</font>与<font color="blue">CSC－低</font>的差异能有效预测客户流失。
+ 对于<font color="blue">CSC－高</font>的参数进行Wald检验，$b_1=2.11844,SE(b_1)=0.142380$故而<font align="center">$Z_{wald}=2.11844/0.142380=14.88$</font><br>此时，P值P(|Z|>14.88)=0.000，显著，表明<font color="blue">CSC－高</font>与<font color="blue">CSC－低</font>的差异能有效预测客户流失。

<B>所以，对于多分预测变量模型，关键是指示变量和参照单元编码</B>   

<h3 id="2-3-解读连续预测变量模型"><a href="#2-3-解读连续预测变量模型" class="headerlink" title="2.3　解读连续预测变量模型"></a>2.3　解读连续预测变量模型</h3><p>假定我们考虑以客户日使用分钟数作为预测变量，则相应的逻辑回归分析结果如下：<br><img src="/images/blog/loginregressionsample14.png"><br>因此对于一个给定日使用分钟数的顾客，流失概率：</p>
<script type="math/tex; mode=display">
  \pi(x)=\frac{e^{g(x)}}{1+e^{g(x)}}=\frac{e^{-3.929-0.112717(日分钟数)}}{1+e^{-3.929-0.112717(日分钟数)}}</script><ul>
<li>对于一个日使用分钟数为100的顾客流失的概率估计为：</li>
</ul>
<script type="math/tex; mode=display">
ｇ(x)＝-3.9292+0.112717(100)=-2.80212</script><p>概率π(100)＝0.0572,比数据集中总比例14.5%要小，表明低的日使用分钟数会在一定程度上防止顾客流失</p>
<ul>
<li>对于一个日使用分钟数为300的顾客流失的概率估计为：</li>
</ul>
<script type="math/tex; mode=display">
ｇ(x)=-3.9292+0.0112717(300)＝-0.054778</script><p>概率π(300)＝0.3664，比数据集中总比例14.5%要大，表明日使用分钟数越多顾客流失越多</p>
<p>“日使用分钟数”，这一实例的<strong>偏差Ｇ</strong>为：</p>
<script type="math/tex; mode=display">
  G=偏差(没有预测变量的模型)-偏差(有预测变量的模型)
  \\=-2ln\frac{没有预测变量的似然值}{有预测变量的似然值}
  \\=2{-1307.129-[483ln(483)+2850ln(2850)-3333ln(3333)]}
  \\=144.035</script><p>对Ｇ进行卡方检验，</p>
<script type="math/tex; mode=display">
  P(x^2)\gt G_{观测值}即P(x^2)\gt 144.035=0.0000</script><p>因此强有力的证据表明日使用分钟数有助于预测顾客的流失情况。</p>
<p>对“日使用分钟数”进行Ｗａｌｄ检验，可以得到同样的结论。</p>
<h2 id="三-多元逻辑回归"><a href="#三-多元逻辑回归" class="headerlink" title="三.多元逻辑回归"></a>三.多元逻辑回归</h2><p>多元逻辑回归与简单逻辑回归十分相似，需要注意的是选择恰当的预测变量，其方法主要有</p>
<ul>
<li>针对单个变量的挑选：Wald检验某个变量是否有助于预测</li>
<li>针对多个变量总体挑选：总体显著性Ｇ</li>
</ul>
<p>下图一个简单示例：<br><img src="/images/blog/loginregressionsample18.png"><br><img src="/images/blog/loginregressionsample19.png"><br>由上面两幅图可以看出，其中的“账户时长”变量其Wald检验的Ｐ值没有拒绝零假设检验，因而需要从全体预测变量中剔除。最后的Ｇ偏差，卡方检验虽然两幅图中都能表明，多元预测变量能显著预测结果（Ｇ检验的Ｐ值＝０），但是剔除账户长度后更好。</p>
<h2 id="四-逻辑回归中引入高阶项"><a href="#四-逻辑回归中引入高阶项" class="headerlink" title="四 逻辑回归中引入高阶项"></a>四 逻辑回归中引入高阶项</h2><h4 id="为何需要高阶项"><a href="#为何需要高阶项" class="headerlink" title="为何需要高阶项"></a>为何需要高阶项</h4><p>如果逻辑回归转换函数在连续变量中不是线性的，让步比的估计和置信区间的应用可能会有问题。原因在与估计的让步比在预测变量取值域上是一个常数。例如，不论是第23分钟还是第323分钟，日使用分钟数每增加1个单位，让步比都是1.01.这种让步比为常数的假设并不总是成立。<br><br>此时，分析人员需要做一些非线性的调整，如使用指示变量（见多分预测变量模型）和高阶项（如：$x^2，x^3．．$）。<br></p>
<h4 id="高阶项的作用"><a href="#高阶项的作用" class="headerlink" title="高阶项的作用"></a>高阶项的作用</h4><p>高阶项的引入可以作为惩罚函数，减少该变量不正常的分布。使用高阶项（和起始变量一起运用）的优势在于，高阶项可以是连续的并且可以提供更严格的估计。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之五：逻辑回归</title>
    <url>/2013/05/12/datamining-logicregression/</url>
    <content><![CDATA[<h2 id="一-为何要有逻辑回归"><a href="#一-为何要有逻辑回归" class="headerlink" title="一 为何要有逻辑回归"></a>一 为何要有逻辑回归</h2><p>假设有如下关于患者年龄与患病情况的数据集：<br><img src="/images/blog/loginregression1.png"><br>我们查看数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; edit(patient)  </span><br><span class="line">        patient_id age if_sick  </span><br><span class="line">   [1,]          1  25       0  </span><br><span class="line">   [2,]          2  29       0  </span><br><span class="line">   [3,]          3  30       0  </span><br><span class="line">   [4,]          4  31       0  </span><br><span class="line">   [5,]          5  32       0  </span><br><span class="line">   [6,]          6  41       0  </span><br><span class="line">   [7,]          7  41       0  </span><br><span class="line">   [8,]          8  42       0  </span><br><span class="line">   [9,]          9  44       1  </span><br><span class="line">   [10,]         10  49       1  </span><br><span class="line">   [11,]         11  50       0  </span><br><span class="line">   [12,]         12  59       1  </span><br><span class="line">   [13,]         13  60       0  </span><br><span class="line">   [14,]         14  62       0  </span><br><span class="line">   [15,]         15  68       1  </span><br><span class="line">   [16,]         16  72       0  </span><br><span class="line">   [17,]         17  79       1  </span><br><span class="line">   [18,]         18  80       0  </span><br><span class="line">   [19,]         19  81       1  </span><br><span class="line">   [20,]         20  84       1  </span><br><span class="line">   &gt; p&lt;-as.data.frame(patient)  </span><br><span class="line">   &gt; plot(p$if_sick~p$age,main&#x3D;&quot;20位患者年龄与患病情况&quot;,xlab&#x3D;&quot;年龄&quot;,ylab&#x3D;&quot;患病情况&quot;)</span><br></pre></td></tr></table></figure>
<p>画出对照图看数据分布：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; plot(p$if_sick~p$age,main&#x3D;&quot;20位患者年龄与患病情况&quot;,xlab&#x3D;&quot;年龄&quot;,ylab&#x3D;&quot;患病情况&quot;)  </span><br><span class="line">&gt; lm&lt;-lm(if_sick~age,data&#x3D;p)  </span><br><span class="line">&gt; abline(lm)  </span><br><span class="line">&gt; legend(x&#x3D;65,y&#x3D;0.2,legend&#x3D;&quot;线性拟合&quot;,lty&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>结果图如下：<br><img src="/images/blog/loginregression2.png"><br>此时，我们发现线性拟合完全偏离了数据分布，即使使用如对数变换也难以取得理想结果，如上图所示数据分布（不是0就是1）提醒我们应该使用逻辑回归。</p>
<h2 id="二-逻辑回归概念及认识"><a href="#二-逻辑回归概念及认识" class="headerlink" title="二 .  逻辑回归概念及认识"></a>二 .  逻辑回归概念及认识</h2><h3 id="2-1-对比线性回归"><a href="#2-1-对比线性回归" class="headerlink" title="2.1  对比线性回归"></a>2.1  对比线性回归</h3><p>线性回归是用来估计<font color="red">连续型</font>回应变量与一组预测变量之间关系的方法。<br><br>逻辑回归用来估计<font color="red">非连续型（分类型）</font>回应变量与一组预测变量之间关系的方法。</p>
<h3 id="2-2-公式"><a href="#2-2-公式" class="headerlink" title="2.2 公式"></a>2.2 公式</h3><p>逻辑回归的条件均值（给定X=x的情况下Y的条件均值表示为：E(Y|x)，以下用π(x)表示）具体如下:</p>
<script type="math/tex; mode=display">
  \pi(x)= \frac{e^{b_0+b_1}}{1+e^{b_0+b_1}}</script><p>上式形成的图形成为反曲线，是非线性的S型。它的一种逻辑转换如下，它是一种有效的逻辑转换方法：</p>
<script type="math/tex; mode=display">
    g(x)=ln\frac{\pi(x)}{1-\pi(x)}=b_0+b_1x</script><p>转换函数g(x)表现了线性逻辑回归模型的几个很好的性质，如线性、连续性、取值范围无限性。</p>
<p><strong>取值范围</strong>：$\pi(x)在 b_0+b_1x\backsim -\infty 时取最小值0，π(x)在 b_0+b_1x\backsim\infty时取最大值1$ 。</p>
<p><strong>性质</strong>：π(x)可以看做一种概率形式，其取值范围为 (0,1).其中</p>
<ul>
<li>π(x) 可以看做X=x条件下的正效应（如疾病）发生的概率</li>
<li>1-π(x) 可以看做是在这种条件下正效应没有发生的概率。</li>
</ul>
<h3 id="2-3-误差"><a href="#2-3-误差" class="headerlink" title="2.3 误差"></a>2.3 误差</h3><p>线性回归模型中，误差e服从均值为0、方差为常数的正态分布，而逻辑回归由于其回应变量的取值是二分的，其误差只有两种形式：</p>
<p><ol></p>
<p><li>X=x时，如果出现Y=1的情况，其概率为π(x)，误差为 e=1-π(x)</li></p>
<p><li>X=x时， 如果Y=0，其概率为1-π(x),误差为 e=0-π(x)=-π(x)</li><br>&lt;/ol&gt;<br>因而，逻辑回归的误差服从二项分布，其方差为π(x)(1-π(x))，这样逻辑回归的回应变量Y=π(x)+e也服从概率为π(x)的二项分布。</p>
<h3 id="2-4-估值（最大似然估计）"><a href="#2-4-估值（最大似然估计）" class="headerlink" title="2.4 估值（最大似然估计）"></a>2.4 估值（最大似然估计）</h3><p>最大似然估计：在已经得到试验结果的情况下，我们应该寻找使这个结果出现 的可能性最大的那个  作为真的估计。</p>
<p>线性回归中使用最小二乘法有可能得到回归系数最优值的闭合形式解，但在逻辑回归中不存在，我们采用的最大似然估计法，得到的观测数据的似然参数估计值是最大的。似然函数 $l(b|x)$ 是一个参数为 $b=b_0,b_1,….$ ，用来表示被观测数据x的概率的函数。在回应变量为正相关的情况下$(X=x_i,Y_i=1)$ ，观测值会影响概率π(x)的值，在回应变量为负相关的情况下 $(X=x_i,Y_i=0)$，观测值会影响概率1-π(x)的值。因此 $Y_i=0或1$，对第i个观测值概率的影响可以表示为 :$[\pi(x_i)]^{y_i}[1-\pi(x)^{1-y_i}]$</p>
<p>假设观测值是独立的，可以把似然函数 $l(b|x)$ 表示为单个项的乘积：$(b|x)=\prod^n_{i=1}$<br>通过对 $l(b|x)$ 每个参数求微分，并令其微分等于零，可以得到最大似然估计。</p>
<h3 id="2-5-衡量回归模型显著性"><a href="#2-5-衡量回归模型显著性" class="headerlink" title="2.5 衡量回归模型显著性"></a>2.5 衡量回归模型显著性</h3><p>先见下表,患病情况与年龄的逻辑回归分析结果<br><img src="/images/blog/loginregression7.png"></p>
<h4 id="2-5-1-统计量G"><a href="#2-5-1-统计量G" class="headerlink" title="2.5.1 统计量G"></a>2.5.1 统计量G</h4><p>在简单线性回归模型中，检验统计量F=MSR/MSE 可以来判断回归模型的显著性。在逻辑回归模型中，检验的是带有某个预测变量的模型比不带该预测变量的模型是否能更好的回应变量匹配。</p>
<ul>
<li>饱和模型：包含了和数据点个数一样多的参数的模型（能完全正确估计回应变量，没有预测误差）</li>
<li>拟合模型：带有少于数据点个数的参数</li>
</ul>
<p>偏差定义如下：</p>
<script type="math/tex; mode=display">
   偏差 D= -2ln[\frac{拟合模型似然值}{饱和模型似然值}]</script><p>上述检验称为似然比值检验，其中-2ln 部分是为了方便计算。将拟合模型中对π(x)的估计值表示为π(x)’，则偏差公式变为：</p>
<script type="math/tex; mode=display">
   偏差 D= -2ln\sum^n_{i=1}[y_iln\frac{\pi(x)'}{y_i}+(1-y_i)ln\frac{1-\pi(x)'}{1-y_i}]</script><p>该偏差表示考虑了预测变量后模型的误差，它类似于线性回归中的平方和误差。</p>
<p>决定某个特定的预测变量是否重要的程序是计算出不带此预测变量模型的偏差，减去带有此预测变量模型的偏差，即:</p>
<script type="math/tex; mode=display">
   G= 偏差(非预测模型)-偏差(预测模型)= -ln[\frac{非预测似然值}{预测似然值}]</script><p><font color="blue">统计量G服从自由度为1的卡方分布</font><br><br>在患病情况例子中，从表格可以看到似然对数比是 -10.101，那么:</p>
<p>G=2{-10.101-[7ln(7)+13ln(13)-20ln(20)]}=5.696</p>
<h4 id="2-5-2-Wald检验"><a href="#2-5-2-Wald检验" class="headerlink" title="2.5.2 Wald检验"></a>2.5.2 Wald检验</h4><p>该比率为：</p>
<script type="math/tex; mode=display">
  Z_{wald}=\frac{b_1}{SE(b_1)}</script><p>服从标准正态分布，由表1提供的系数估计值及标准差：b1=0.6696和 $SE(b<em>1)=0.03223$，于是有: $Z</em>{wald}=0.6696/0.3223=2.08$ <br><br>表中P值即为P(|Z|&gt;2.08)=0.038<br>通常可以为逻辑回归系数构建一个100(1-a)%的置信区间：</p>
<script type="math/tex; mode=display">
   [b_0-Z*SE(b_0),b_0*SE(b_0)]和[b_1-Z*SE(b_1),b_1+Z*SE(b_1)]</script><h4 id="2-5-3-发生比和让步比"><a href="#2-5-3-发生比和让步比" class="headerlink" title="2.5.3 发生比和让步比"></a>2.5.3 发生比和让步比</h4><ul>
<li><strong>发生比</strong>：事件发生的概率与不发生的概率的比值。<ul>
<li>发生比告诉我们一件事情发生或者不发生哪种情况更有可能.一件 事情发生的可能性大于不发生的可能性时，发生比大于1。</li>
<li>一件事情发生的可能性小于不发生的可能性时，发生比大于1</li>
<li>一件事情很有可能发生时，发生比等于1</li>
</ul>
</li>
</ul>
<p>例如，预测的一个72岁病人换用特定病例的概率为61%，不患此病的概率为39%。因此一个72岁病人患此病的发生比=0.61/0.39=1.56。</p>
<p><strong>让步比</strong>：x=1时回应变量发生的发生比除以x=0时回应变量发生的发生比。它很简单的表达了让步比和斜率系数之间的关系<br><br>在二分预测变量的二元逻辑回归中，当x=1时，回应变量发生(y=1)的发生比为：$\frac{\pi(1)}{1-\pi(1)}=e^{b_0+b_1}$</p>
<p>相应的，当x=0时，回应变量发生的发生比为：$\frac{\pi(0)}{1-\pi(0)}=e^{b_0}$</p>
<p>则让步比公式如下：</p>
<script type="math/tex; mode=display">
  OR= \frac{\pi(1)/[1-\pi(1)]}{\pi(0)/[1-\pi(0)]}=e^{b_0+b_1}/e^{b_0}=e^{b_1}</script><p>例如：一个临床试验报告称，曾经使用过与从没有使用过雌性激素替换疗法的人中患子宫癌的让步比为5.0，这可以解释为使用雌性激素替换疗法的人得子宫癌的概率是未使用者的5倍。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之四：多重共线性及变量选择方法</title>
    <url>/2013/05/04/datamining-mutillinerandselect/</url>
    <content><![CDATA[<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>  前多重共线性： 也即使用的多个预测变量之间存在线性相关。多重共线性会导致解的不稳定，进而可能导致意外的结果。在线性代数中，基坐标必须是相互正交的，也即不相关的，此处在做多元回归预测时，必须保证预测变量之间是不相关的。</p>
<h3 id="避免不正交的方法"><a href="#避免不正交的方法" class="headerlink" title="避免不正交的方法"></a>避免不正交的方法</h3><h3 id="1分析之前"><a href="#1分析之前" class="headerlink" title="1分析之前"></a>1分析之前</h3><h4 id="a-逐个计算预测变量之间的相关系数"><a href="#a-逐个计算预测变量之间的相关系数" class="headerlink" title="a.逐个计算预测变量之间的相关系数"></a>a.逐个计算预测变量之间的相关系数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; cor(sugar$sugars,sugar$shelf)  </span><br><span class="line">[1] 0.1004379  </span><br><span class="line">&gt; cor(sugar$fiber,sugar$potass)  </span><br><span class="line">[1] 0.9033737</span><br></pre></td></tr></table></figure>
<p>可以看到纤维和钾含量存在高度相关性，需要注意</p>
<h4 id="b-为预测变量建立矩阵图"><a href="#b-为预测变量建立矩阵图" class="headerlink" title="b.为预测变量建立矩阵图"></a>b.为预测变量建立矩阵图</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; #同时画多个变量的对照图需要使用 car包中的 scatterplotMatrix函数  </span><br><span class="line">&gt;install.packages(&quot;car&quot;)  </span><br><span class="line">&gt;library(car)  </span><br><span class="line">&gt;#使用谷物数据集的 “糖”，“纤维”，“钾”三列数据  </span><br><span class="line">&gt; sugar_frame&lt;-as.data.frame(sugar[,c(&quot;糖&quot;,&quot;纤维&quot;,&quot;钾&quot;)])  </span><br><span class="line">&gt;#画出对照图  </span><br><span class="line">&gt; scatterplotMatrix(sugar_frame,spread&#x3D;F,lty.smooth&#x3D;2,var.labels&#x3D;c(&quot;糖&quot;,&quot;纤维&quot;,&quot;钾&quot;))</span><br></pre></td></tr></table></figure>
<p>结果如下图：<br><img src="/images/blog/muitllinerandselect1.png"><br>可以看到第四张和第六张是纤维和钾的相关图，可以看出他们之间有很强相关性。</p>
<h3 id="1-2-分析之后：方差膨胀因子-variance-inflation-factors-VIFs"><a href="#1-2-分析之后：方差膨胀因子-variance-inflation-factors-VIFs" class="headerlink" title="1.2 分析之后：方差膨胀因子(variance inflation factors,VIFs)"></a>1.2 分析之后：方差膨胀因子(variance inflation factors,VIFs)</h3><script type="math/tex; mode=display">
  VIF=\frac{1}{1-R^2_i}</script><p>其中 $R_i^2$ 表示 $R^2$ 的值是通过在其他预测变量上回归分析 $x_i$ 得到的。假设xi和其他变量没有任何关系,那么 $R_i^2=0$ ，于是可以得到 $VIFi=\frac{1}{1-0}=1$ 。也即VIF最小值为1，没有最大值.</p>
<p> $VIF_i$ 的变化对第i个系数的变化率Sbi如何产生影响，有如下公式：</p>
<script type="math/tex; mode=display">
   Sb_i=Sc_i=S\sqrt{\frac{1}{(n-1)S^2_i}\frac{1}{1-R^2_i}}=S\sqrt{\frac{VIF_i}{(n-1)S^2_i}}</script><p>如果 $x_i$ 与其他预测变量不想管，那么 $VIF_i=1$ ，而且相关系数的标准差 $Sb_i$ 没有增大。然而如果xi与其他变量相关，那么较大的 $VIF_i$ 值会使得相关系数的标准差 $Sb_i$过度膨胀。因此，方差估计的膨胀会导致估计精度的下降。</p>
<p>粗略的经验法则如下:</p>
<ul>
<li>VIF&gt;=5  模型有中度的多重共线性（相当于 $R^2=0.08$ ）</li>
<li>VIF&gt;=10  模型中有严重多重共线性(相当于 $R^2=0.90$ )</li>
</ul>
<p>下面来查看谷物数据集中 糖、纤维、钾的膨胀因子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; #回归拟合  </span><br><span class="line">&gt; fit&lt;-lm(data&#x3D;sugar,rating~sugars+fiber+potass)  </span><br><span class="line">&gt;#注意，我们只是用了sugar数据集中包含“糖”，“纤维”，“钾”三列数据的sugar_frame  </span><br><span class="line">&gt;#进行膨胀因子计算时，需要使用gvlma包中的vif函数，因此需要先安装  </span><br><span class="line">&gt; install.packages(&quot;gvlma&quot;)  </span><br><span class="line">&gt; library(gvlma)  </span><br><span class="line">Warning message:  </span><br><span class="line">程辑包‘gvlma’是用R版本3.0.2 来建造的   </span><br><span class="line">&gt;#线性模型的综合验证  </span><br><span class="line">&gt; gvlma(fit)  </span><br><span class="line">  Call:  </span><br><span class="line">  lm(formula &#x3D; rating ~ sugars + fiber + potass, data &#x3D; sugar)  </span><br><span class="line"></span><br><span class="line">    Coefficients:  </span><br><span class="line">    (Intercept)       sugars        fiber       potass    </span><br><span class="line">        52.6762      -2.0510       4.3701      -0.0543    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS  </span><br><span class="line">    USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:  </span><br><span class="line">    Level of Significance &#x3D;  0.05   </span><br><span class="line"></span><br><span class="line">    Call:  </span><br><span class="line">     gvlma(x &#x3D; fit)   </span><br><span class="line"></span><br><span class="line">                         Value p-value                   Decision  </span><br><span class="line">    Global Stat        7.24415 0.12353    Assumptions acceptable.  </span><br><span class="line">    Skewness           5.61716 0.01779 Assumptions NOT satisfied!  </span><br><span class="line">    Kurtsis           0.02125 0.88411    Assumptions acceptable.  </span><br><span class="line">    Link Function      0.40164 0.52624    Assumptions acceptable.  </span><br><span class="line">    Heteroscedasticity 1.20410 0.27250    Assumptions acceptable.  </span><br><span class="line">    &gt;#查看膨胀因子 vif  </span><br><span class="line">    &gt; vif(fit)  </span><br><span class="line">      sugars    fiber   potass   </span><br><span class="line">     1.164237 6.327286 6.204047</span><br></pre></td></tr></table></figure>
<h2 id="二-变量选择方法"><a href="#二-变量选择方法" class="headerlink" title="二.  变量选择方法"></a>二.  变量选择方法</h2><p>为帮助数据分析人员确定在多元回归模型中应该包含哪些变量，下面是几种变量选择方法</p>
<ul>
<li>向前选择</li>
<li>向后排除</li>
<li>逐步选择</li>
<li>最优子集</li>
</ul>
<p>注意四种选择方法所使用的数据集都是 “谷物数据集”。</p>
<h3 id="2-1-向前选择程序"><a href="#2-1-向前选择程序" class="headerlink" title="2.1   向前选择程序"></a>2.1   向前选择程序</h3><p>1.对于第一个加入模型的变量，选择与回应变量相关度最高的预测变量（假设为 $x_1$ ）如果所有变量对模型都不重要，则停止,否则执行2</p>
<ol>
<li>对其余的每个变量，F统计序列式 $F(x_2|x_1),F(x_3|x_1),F(x_4|x_1)$ .第二次通过此算法时是, $F(x_3|x_1,x_2),F(x_4|x_1,x_2)$ 。选择具有最大F统计序列的变量</li>
<li>对 2 选择出来的变量，进行F统计序列的显著性检验。如果结果模型没有重大意义，则停止，否咋将从 2 得到的变量加入到模型中，然后返回2</li>
</ol>
<p><strong>初始</strong>：模型中没有变量。</p>
<p><strong>过程</strong>：把与回应变量（营养级别） 密切相关的变量选出来，如果是显著的就加入到模型中。变量糖在所有预测变量中与营养级别有最高的相关系数（r=0.762）。然后进行序列F检验，例如F(纤维|糖)和F(钠|糖)等，然后看到，F(纤维|糖)显著性检验具有最高的F统计序列值，这样变量纤维作为第二个变量加入到模型中。再进行一次序列F检验，比如F(钠|糖，纤维)和F(脂肪|糖，纤维)，等等。F(钠|糖，纤维)具有最高的序列F统计值。因而钠作为第三个变量加入到模型中。</p>
<p><strong>结束</strong>：一次按照第二步进行，得到如下变量加入顺序：脂肪，蛋白质，碳水化合物，卡里路，维生素和钾。此时再也找不到其他显著的变量加入模型中才中断，此时的多元回归模型如下：<br><img src="/images/blog/muitllinerandselect4.png"><br>下图显示了一个顺序选择的模型概览:<br><img src="/images/blog/muitllinerandselect5.png"></p>
<h3 id="2-2-向后排除程序"><a href="#2-2-向后排除程序" class="headerlink" title="2.2 向后排除程序"></a>2.2 向后排除程序</h3><p>向后排除程序是从模型中所有变量或者所有用户自定义变量集开始的。步骤如下:</p>
<ol>
<li>在全模型中执行向后排除，即使用所有变量的模型。例如，可能全模型中有4个变量 $x_1,x_2,x_3,x_4$</li>
<li>对于当前模型中的每个变量，计算出它的偏F统计量。第一次是：$F(x<em>1,x_2,x_3,x_4)、F(x_2|x_1,x_3,x_4)、F(x_3|x_1,x_2,x_4)和F(x_4|x_1,x_2,x_3)$ 。选择具有最小偏F统计量的比那辆，其值用 $F</em>{min}$表示</li>
<li>检验 $F<em>{min}$ 的显著性。如果 $F</em>{min}$ 不显著，从模型中删除与Fmin对应的变量，然后返回执行2，如果 $F_{min}$ 显著，停止这个过程。</li>
</ol>
<p><strong>实例</strong>：<br>起始时模型包含了所有变量，然后计算该模型中每个变量的偏F统计量。例如，这些统计量分别是F(重量|糖，纤维，….杯子)，F(杯子|糖，纤维,…..重量|)。找到最小偏F统计量（ $F<em>{min}$ ）对应的变量。第一次是重量，此时 $F</em>{min}$ 不显著，因而从模型中去掉，接下来变量具有最小偏F统计是杯子，也是不显著的，因而需要被剔除。第三次具有最小偏F统计量的是货架2的指标变量，但是Fmin对应的p值并没有大道可以从模型中剔除，因而保留并中断。得到的模型为：</p>
<script type="math/tex; mode=display">
  y =b_0+b_1(糖)+b_2(纤维)+b_3(钠)+b_4(脂肪)+b_5(蛋白质)+b_6(碳水化合物)+b_7(卡里路)+b_8(维生素)+b_9(钾)+b_10(货架2)+e</script><p><img src="/images/blog/muitllinerandselect7.png"><br>模型1表示包含所有预测变量，模型2中剔除了重量之外所有预测变量，于是有：</p>
<script type="math/tex; mode=display">
  SS_{重量\|所有其他变量}=SS_{所有变量}-SS_{重量以外所有变量}=12980.078-14980.005=0.073</script><p>上表信息中显示，偏F统计量的结果为：</p>
<script type="math/tex; mode=display">
  F(重量\|所有其他变量)= \frac{SS_{重量|所有其他变量}}{MSE_{所有变量}}=0.073/0.261=0.0280</script><p>F统计量的值0.28 落在 $F<em>{1,n-p-2}=F</em>{1,72}$ 分布的40%点处，对应的p值是0.60，因而重量不应该包含在模型中。</p>
<h3 id="2-3-逐步选择程序"><a href="#2-3-逐步选择程序" class="headerlink" title="2.3 逐步选择程序"></a>2.3 逐步选择程序</h3><p> 逐步选择程序是向前选择方法的一种改进。在向前选择中会出现这种情况，当新加入的变量加入到模型时，向前选择过程中已经加入的变量可能就显得不重要了，这在向前选择方法中是没有考虑的。逐步选择过程可以检验这种情况，方法是每一步在现有变量的基础上计算每个变量的部分平方和，执行偏F检验。如果模型中有一个变量不再是显著的，这个含有最小偏F统计的变狼就会被移出模型。当不再有变量加入或者移出模型时，结束过程并得到最终模型。</p>
<h3 id="2-4-最优子集程序"><a href="#2-4-最优子集程序" class="headerlink" title="2.4  最优子集程序"></a>2.4  最优子集程序</h3><p>对于预测变量集不是太大的数据集，最优子集是一种较好方法。但是如果预测变量超过30个，最优子集方法就会产生组合爆炸，难以控制。步骤如下：</p>
<ol>
<li>分析人员需要指定需要多少个（假设为m）供筛选的模型，以及在一个模型中含有最大预测变量个数（假设为n）</li>
<li>对于含有一个预测变量的所有模型，例如：$y=b_0+b_1(糖),y=b_0+b_1(纤维)$,….等。计算对应的 $R^2$ ,修正 $R^2$ 和S值都计算出来，最优的m个模型是基于这些统计值得到。</li>
<li>对于含有两个最优的m个模型是基于这些统计值得到。</li>
<li>重复以上，直到达到最大的预测变量（n）个数，然后分析人员把预测变量个数为1,2,,..n的最优模型罗列，以选择最佳总体模型</li>
</ol>
<p><strong>实例，下图是最优子集程序用于谷物数据集的省略概览</strong></p>
<p><font color="blue">[注意，整个过程比下图要复杂，例如变量数为1时，本应该有12行结果，下图中只简要用了两行，其他的也是]</font><br><img src="/images/blog/muitllinerandselect10.png"><br>图中，每一行代表一个不同的模型，某模型中包含了哪个变量，该变量对应的方格被涂成黑色。如，第一个模型（第一行）仅包含了变量糖；第四个模型（第四行）包含了糖和钾。其中的最优模型子集被红色覆盖的那个模型（也即那一行）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之三：多元回归模型</title>
    <url>/2013/04/24/datamining-mutilregression/</url>
    <content><![CDATA[<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>  前面介绍了一个预测变量和一个回应变量的回归，但数据挖掘通常对一个回应变量和多个预测变量之间的关系更感兴趣，数据中可能有很多变量都与目标（回应）变量有线性关系，多元回归模型可以更加精确的预测这些关联。<br><br>多元回归模型如下：</p>
<script type="math/tex; mode=display">
y= b_0+b_1x_1+b_2x_2+....+e</script><p>其中 $b0，b1，b2…..$ 是模型参数，为常数，可以通过最小二乘法估计。关于误差项e和回应变量y的假设与简单线性回归模型一样。</p>
<h2 id="二-多元回归的推断"><a href="#二-多元回归的推断" class="headerlink" title="二 多元回归的推断"></a>二 多元回归的推断</h2><p>主要有：</p>
<ol>
<li><p>t检验，用来对预测变量xi和回应变量y之间的关系进行推断</p>
</li>
<li><p>F检验，用来对整个回归模型的显著性进行检验</p>
</li>
<li><p>$b_i$ ,第i个预测变量系数的置信区间</p>
</li>
<li><p>回应变量y的均值的置信区间，用于预测变量 $x1,x2,x3,..$ 取特定值时，对回应变量y的均值进行估计</p>
</li>
</ol>
<h3 id="2-1-y和-x-i-之间关系的t检验"><a href="#2-1-y和-x-i-之间关系的t检验" class="headerlink" title="2.1 y和 $x_i$之间关系的t检验"></a>2.1 y和 $x_i$之间关系的t检验</h3><p>假设检验如下：</p>
<ul>
<li>H0: bi=0</li>
<li>H1:  bi!=0</li>
</ul>
<p>这些假设的模型的唯一区别是第i项是否存在，其他项都是相同的。</p>
<p><strong>实例</strong>： 营养级别和糖之间关系的t检验</p>
<ul>
<li>H0: b1=0;模型 $y=b_0+b_2(纤维)+e$</li>
<li>H1:b1!=0;模型：$y=b_0+b_1(糖)+b_2(纤维)+e$</li>
</ul>
<p>还是使用数据集：谷物(在本系列文章第二篇中有下载地址)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#数据集存储在sugar中  </span><br><span class="line">#线性拟合  </span><br><span class="line">&gt;mutil_regre&lt;-lm(data&#x3D;sugar,rating~sugars+fiber)  </span><br><span class="line">#查看  </span><br><span class="line">&gt;summary(mutil_regre)    </span><br><span class="line">Call:  </span><br><span class="line">  lm(formula &#x3D; rating ~ sugars + fiber, data &#x3D; sugar)  </span><br><span class="line">  Residuals:  </span><br><span class="line">        Min      1Q  Median      3Q     Max   </span><br><span class="line">    -12.133  -4.247  -1.031   2.620  16.398   </span><br><span class="line"></span><br><span class="line">    Coefficients:  </span><br><span class="line">                Estimate Std. Error t value  </span><br><span class="line">    (Intercept)  51.6097     1.5463  33.376  </span><br><span class="line">    sugars       -2.1837     0.1621 -13.470  </span><br><span class="line">    fiber         2.8679     0.3023   9.486  </span><br><span class="line">                Pr(&gt;|t|)      </span><br><span class="line">    (Intercept)  &lt; 2e-16 ***  </span><br><span class="line">    sugars       &lt; 2e-16 ***  </span><br><span class="line">    fiber       2.02e-14 ***  </span><br><span class="line">    ---  </span><br><span class="line">    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  </span><br><span class="line"></span><br><span class="line">    Residual standard error: 6.219 on 74 degrees of freedom  </span><br><span class="line">    Multiple R-squared:  0.8092,    Adjusted R-squared:  0.804   </span><br><span class="line">    F-statistic: 156.9 on 2 and 74 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>
<p>可以得到，糖对应的稀疏为 $b<em>1=-2.1837$ ,对应的标准差为 $S</em>{bi}=0.1621$</p>
<p>T对应的t统计量，也即检验的统计量为:<br> $t=\frac{b<em>1}{S</em>{b1}}=\frac{-2.1837}{0.1621}=-13.4713$</p>
<p>P值对应的是t统计量的p值。也即:  $p=P(|t|&gt;tobs)=P(|t|&gt;-13.4713)$ 约等于0. 使用p值来检验假设，当p值很小时就可以拒绝原假设。</p>
<h3 id="2-2-整体回归模型的显著性水平检验：F检验"><a href="#2-2-整体回归模型的显著性水平检验：F检验" class="headerlink" title="2.2 整体回归模型的显著性水平检验：F检验"></a>2.2 整体回归模型的显著性水平检验：F检验</h3><p><strong>检验</strong> 是分别对每个变量，糖，纤维，….逐个检验与回应变量线性关系。即{营养级别|糖}，{营养级别|纤维}，….</p>
<p><strong>F检验</strong> 是对所有变量一起检测与回应变量关系，即{营养级别|糖，纤维，……}</p>
<p>F检验的前提是</p>
<ul>
<li><p>H0: b0=b1=……=0   也即模型为：$y=b_0+e$</p>
</li>
<li><p>H1:至少存在一个bi不等于零</p>
</li>
</ul>
<p>备选假设H1并不要求任何回归系数都不是零，而是当备选假设为真时，存在一个回归系数不是零。因此，F检验的备选假设并没有唯一确定一个模型，当一个、几个或者所有回归系数都不是零时，备选假设都是成立的。</p>
<p>F统计量为：</p>
<script type="math/tex; mode=display">
    F= F_{obs}=\frac{MSR}{MSE} 服从F_{m,n-m-1}分布</script><p> <strong>如何理解</strong>：MSE（误差平方和均值）能很好的估计总体变异 $σ^2$（不论原假设是否为真），而MSR只有当原假设为真时才是 $σ^2$ 的优良统计量，因而只有在原假设为真的情况下MSR和MSE才会比较接近，也即F很小的时候，有足够的争取表明原假设为真。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;#查看方差分析表  </span><br><span class="line">&gt;anova(mutil_regre)  </span><br><span class="line">Analysis of Variance Table  </span><br><span class="line"></span><br><span class="line">Response: rating  </span><br><span class="line">          Df Sum Sq Mean Sq F value    Pr(&gt;F)      </span><br><span class="line">sugars     1 8654.7  8654.7 223.774 &lt; 2.2e-16 ***  </span><br><span class="line">fiber      1 3480.0  3480.0  89.978 2.023e-14 ***  </span><br><span class="line">Residuals 74 2862.0    38.7                        </span><br><span class="line">---  </span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br></pre></td></tr></table></figure>
<p><font color="blue">注意：</font>方差分析表只给出了均方误差(MSE=Mean Sq=38.7) 而第一步中t检验中查看线性拟合时已经直接给出了F统计量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">F-statistic: 156.9 on 2 and 74 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>
<p>其中的p值小于任何合理的显著性水平所要求的值，因而拒绝原假设。</p>
<h3 id="2-3-特定回归系数的置信区间"><a href="#2-3-特定回归系数的置信区间" class="headerlink" title="2.3 特定回归系数的置信区间"></a>2.3 特定回归系数的置信区间</h3><p>可以为某个回归系数构造一个 100(1-a)%的置信区间，与简单线性回归无异。</p>
<h3 id="2-4-给定x1-x2-x3-…-下，y均值的置信区间"><a href="#2-4-给定x1-x2-x3-…-下，y均值的置信区间" class="headerlink" title="2.4 给定x1,x2,x3,….下，y均值的置信区间"></a>2.4 给定x1,x2,x3,….下，y均值的置信区间</h3><p> 与简单线性回归类似，只不过变量增多。变为如：谷物为5.00克的糖和5.00克纤维时，营养级别的均值分布。</p>
<h2 id="三-多元回归的三个重要参数"><a href="#三-多元回归的三个重要参数" class="headerlink" title="三 多元回归的三个重要参数"></a>三 多元回归的三个重要参数</h2><h3 id="3-1-调整-R-2-：对包含无用预测变量的惩罚模式"><a href="#3-1-调整-R-2-：对包含无用预测变量的惩罚模式" class="headerlink" title="3.1 调整 $R^2$ ：对包含无用预测变量的惩罚模式"></a>3.1 调整 $R^2$ ：对包含无用预测变量的惩罚模式</h3><p>往模型里增加一个变量会增加决定系数$R^2$ 的值，不管这个变量是否有用。为了模型的简洁性，需要找到某种方法来惩罚包含无用预测变量模型 $R^2$ 的值，此即通常所说的[调整 $R^2$ ],其表达式如下：</p>
<script type="math/tex; mode=display">
   R^2_{adj}= 1-(1-R^2)\frac{n-1}{n-m-1}</script><p>如果[<em>调整</em> $R^2$ ]比 $R^2$ 小很多，则表明模型中至少有一个变量是多余的，分析人员需要考虑剔除。</p>
<h3 id="3-2-序贯的误差平方和-（sequential-sums-of-squares）"><a href="#3-2-序贯的误差平方和-（sequential-sums-of-squares）" class="headerlink" title="3.2 序贯的误差平方和 （sequential sums of squares）"></a>3.2 序贯的误差平方和 <em>（sequential sums of squares）</em></h3><p>序贯的误差平方和代表SSR中回归平方和的部分，SSR代表通过回应变量和一组预测变量的线性关系对总体变异解释的部分。序贯的误差平方和把SSR划分成各个唯一的SSR部分，分别由某个特定的预测变量来描述。因此，序贯的误差平方和的值取决于变量输入模型中的次序。下表是某种次序的序贯的武昌平方和，可以看出其实是对糖含量和营养级别的简单回归分析得到的SSR值。</p>
<p>模型：</p>
<script type="math/tex; mode=display">
    y = b_0+b_1(糖)+b_2(纤维)+b_3(货架1)+b_4(货架4)+e</script><p>的序贯平方和</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>来源</th>
<th>DF</th>
<th>Seq SS</th>
</tr>
</thead>
<tbody>
<tr>
<td>糖</td>
<td>1</td>
<td>8701.7</td>
</tr>
<tr>
<td>纤维</td>
<td>1</td>
<td>3416.1</td>
</tr>
<tr>
<td>货架1</td>
<td>1</td>
<td>0.3</td>
</tr>
<tr>
<td>货架2</td>
<td>1</td>
<td>152.4</td>
</tr>
</tbody>
</table>
</div>
<p>从表中可以看到第三个序贯误差平方和是对货架1的指示标量，值为0.3 ，它代表了营养级别中，在位置因素作用下货架1的变化率，这里糖和纤维的变化率已经被提取出来了，表示货架1的序贯平方和的值很小，表明这个变量很有可能是对估计营养级别是没有用的。</p>
<h3 id="3-3-偏F检验"><a href="#3-3-偏F检验" class="headerlink" title="3.3 偏F检验"></a>3.3 偏F检验</h3><p>假设模型中已经有了p个变量， $x_1,x_2,x_3,….x_p,$ 一个新的变量x<em>是否应该包含在此模型中？应该计算将x</em>加入到给定含有p个变量的模型中所产生的额外序列平方和，这个值表示为:</p>
<script type="math/tex; mode=display">
 SS_{extra}=SS(x*|x_1,x_2,x_3,....x_p)。</script><p> 现在,额外序列平方和通过在全模型（包括 $x<em>1,x_2,x_3,….x_p$ 和x*）中的回归平方和计算得到，表示为 $SS</em>{full}=SS(x<em>1,x_2,x_3,….x_p,x*)$ ,从全模型的回归平方和中减速缩减模型（仅包含 $x_1,x_2,x_3,….x_p$）的回归平方和（表示为：$SS</em>{reduced}=SS(x_1,x_2,x_3,….x_p)$ ），也即：</p>
<script type="math/tex; mode=display">
      SS_{extra}=SS_{full}-SS_{reduced}</script><p>即：</p>
<script type="math/tex; mode=display">
   SS(x*|x_1,x_2,....x_p)= SS(x_1,x_2,....x_p,x^*)-SS(x_1,x_2,..x_p)</script><p>偏F检验的原假设如下：</p>
<ul>
<li><p>H0:否定 $SS_{extra}$与 $x^<em>$ 是相关的，对已经包含 $x_1,x_2,x_3,….x_p$ 的模型的回归平方和没有显著的共享。因此，模型中不应该包含 $x^</em>$ 。</p>
</li>
<li><p>H1:肯定SSextra与x<em>是相关的，对已经包含x1,x2,x3,….xp的模型的回归平方和有显著的贡献。因此，模型中应该包含x</em></p>
</li>
</ul>
<p>偏F检验的测试统计量是:</p>
<script type="math/tex; mode=display">
  F(x^*|x_1,x_2,...x_p)=\frac{SS_{extra}}{MSE_{full}}

  MSE_{full}代表全模型的均方误差，包括x_1,x_2,...x_p和x^*</script><p>当假设为真时，这个统计量服从 $F<em>{1,n-p-2}$ 的分布。因此，当 $F(x^*|x_1,x_2,x_3,….x_p)$ 值太大或者它像对应的p值太小时，有理由拒绝原假设。而偏F检验的一个可替代的方法是t检验。一个自由度为1和n-p-2的F检验等价于一个自由度为n-p-2的t检验。这是由他们之间的概率分布关系 $(F</em>{1,n-p-2}=(t_{n-p-2})^2)$</p>
<p><font color="blue">注意：</font>序贯平方和与部分平方和的区别如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>序贯平方和</th>
<th>部分平方和</th>
</tr>
</thead>
<tbody>
<tr>
<td>$x_1$</td>
<td>$SS(x_1)$</td>
<td>$SS(x_1\</td>
<td>x_2,x_3,x_4)$</td>
</tr>
<tr>
<td>$x_2$</td>
<td>$SS(x_2\</td>
<td>x_1)$</td>
<td>$SS(x_2\</td>
<td>x_1,x_3,x_4)$</td>
</tr>
<tr>
<td>$x_3$</td>
<td>$SS(x_3\</td>
<td>x_1,x_2)$</td>
<td>$SS(x_2\</td>
<td>x_1,x_2,x_4)$</td>
</tr>
<tr>
<td>$x_4$</td>
<td>$SS(x_4\</td>
<td>x_1,x_2,x_3)$</td>
<td>$SS(x_2\</td>
<td>x_1,x_2,x_3)$</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之二：回归模型（简单线性回归）</title>
    <url>/2013/04/20/datamining-regression/</url>
    <content><![CDATA[<h2 id="注：文中所使用代码为R"><a href="#注：文中所使用代码为R" class="headerlink" title="注：文中所使用代码为R"></a>注：文中所使用代码为R</h2><h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>简单线性回归模型是用于估计一个连续预测变量和一个连续回应变量的线性关系。回归方程或估计回归方程(estimated regression equation,ERE)：</p>
<script type="math/tex; mode=display">\bar y=b_0+b_1x</script><ul>
<li>$\bar y$是回应变量的估计值</li>
<li>$b_0$是回归线在y轴上的截距</li>
<li>$b_1$是回归线的斜率</li>
<li>$b_0$和$b_1$称为回归系数</li>
</ul>
<h2 id="二-实例"><a href="#二-实例" class="headerlink" title="二 实例"></a>二 实例</h2><p>数据来源: <a href="http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html" target="_blank" rel="noopener">谷物数据集</a><br>数据描述：谷物数据集,包含了77种早餐谷物的16个属性对应的营养信息<br>首先导入数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sugar&lt;-read.table(file&#x3D;&quot;&#x2F;LabData&#x2F;RData&#x2F;regression&#x2F;nutrition.txt&quot;,header&#x3D;TRUE)</span><br></pre></td></tr></table></figure>
<p>部分数据概览如下：</p>
<figure class="highlight plain"><figcaption><span>```</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![数据集](&#x2F;images&#x2F;blog&#x2F;regression1.png)</span><br><span class="line"></span><br><span class="line">就给定谷物的含糖量对该谷物的营养成分进行评价，77种谷物的营养级别与含糖量的散点图和拟合回归线如下:</span><br></pre></td></tr></table></figure>
<pre><code>plot(data=sugar,rating~sugars,main=&quot;营养级别和含糖量的散点图及拟合线&quot;,xlab=&quot;含糖量&quot;,ylab=&quot;营养级别&quot;)  

lm.reg&lt;-lm(data=sugar,rating~sugars)  

abline(lm.reg,lty=4,lwd=3)   
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![拟合](&#x2F;images&#x2F;blog&#x2F;regression2.png)</span><br><span class="line"></span><br><span class="line">使用线性回归模型拟合结果如下：</span><br></pre></td></tr></table></figure>
<pre><code>lm(data=sugar,rating~sugars)       
Call:      
lm(formula = rating ~ sugars, data = sugar)      
Coefficients:      
  (Intercept)       sugars        
   59.284       -2.401       
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这里给定ERE为 ： $\bar y&#x3D;59.284-2.401*sugars$， 所以$b_0$&#x3D;59.284,$b_1$&#x3D;-2.401   </span><br><span class="line"></span><br><span class="line">### 误差残留   </span><br><span class="line"></span><br><span class="line"> **问题**：数据集中包含了一个含糖量(sugars&#x3D;1)为1的谷物 cheerios(数据概览中，黑色框部分)，其营养价值是50.765,而非估计值的56.98.</span><br><span class="line">  二者之差也即</span><br><span class="line"></span><br><span class="line">$$y-\bar y&#x3D;56.98-50.765&#x3D;6.215,$$</span><br><span class="line"></span><br><span class="line">称为预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)。</span><br><span class="line"></span><br><span class="line">为寻求这种预测误差总体尽可能小，最小二乘回归法会选择一条唯一的回归线，满足使得数据集的整体残差平方和达到最小值。有多重方法可以选择，如中位数回归方法，但最小二乘法回归是最常见的。   </span><br><span class="line">&lt;br&gt;</span><br><span class="line"></span><br><span class="line">## 三 误差评估</span><br><span class="line"></span><br><span class="line">### 1 最小二乘法估计   </span><br><span class="line"></span><br><span class="line">公式如下：</span><br><span class="line"></span><br><span class="line">$$y&#x3D;m_0+m_1x+e$$</span><br><span class="line">   &lt;font color&#x3D;&quot;blue&quot;&gt;其中误差项e引入用以解释不确定性的因素。&lt;&#x2F;font&gt;</span><br><span class="line"></span><br><span class="line">**基本假设**</span><br><span class="line"></span><br><span class="line">1. 零均值假设：误差项是期望为零的随机变量，即$E(e)&#x3D;0$</span><br><span class="line">2. 不变方差假设：误差项e的方差（用$σ^2$表示）是常数且与 $x_1,x_2,....$ 的值无关</span><br><span class="line">3. 独立性假设：e的变量是相互独立的</span><br><span class="line">4. 正态性假设：误差项e是正态随机变量,也即：误差项e的值是独立的正态分布随机变量，带有均值0和不变方差$σ^2$</span><br><span class="line"></span><br><span class="line">回应变量y的分布:</span><br><span class="line"></span><br><span class="line">(1)根据零假设，回应变量y的值均落在回归线上</span><br><span class="line">(2)根据不变方差假设，不论预测变量x1,x2,..取什么值，y的方差不变</span><br><span class="line">(3)根据独立性假设，对任意的 $x_1,x_2,..$ 取值，y的值都是相互独立的</span><br><span class="line">(4)根据正态性假设，回应变量y也是正态分布的随机变量。也即回应变量y也是独立正态变量，均值不变，方差不变。</span><br><span class="line">最小二乘回归线(least-square line)将误差的平方和最小化，总的预测误差用SSEp表示，则总的误差平方和为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SSE_p&#x3D;\sum^n_&#123;i-1&#125;\epsilon^2&#x3D;\sum^n_&#123;i-1&#125;(y_i-\beta_0-\beta_1x_i)^2</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">利用微积分，在以下微积分方程结果为0的时候, $b_0$ 和 $b_1$ 的取值会让总的误差平方和最小。关于 $b_0$ 和 $b_1$ 的偏微分方程为:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac &#123;\partial SSE_p&#125;&#123;\partial \beta_0&#125;&#x3D;-2\sum^n_&#123;i-1&#125;(y_i-\beta_0-\beta_1x_i)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac&#123;\partial SSE_p&#125;&#123;\partial \beta_1&#125;&#x3D;-2\sum^n_&#123;i-1&#125;x_i(y_i-\beta_0-\beta_1x_i)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">令上式为0，则有:&lt;br&gt;</span><br><span class="line">$$</span><br><span class="line">(y_i-\beta_0-\beta_1x_i)&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">(y_i-\beta_0-\beta_1x_i) &#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">分别求和，得到</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  \sum^n_&#123;i-1&#125;y_i-nb_0-b_1\sum^a_&#123;i-1&#125;x_i&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \sum^n_&#123;i-1&#125;x_iy_i-b_0\sum^n_&#123;i-1&#125;x_i-b_1\sum^n_&#123;i-1&#125;x^2_i&#x3D;0</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">重新表示为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_0n+b_1\sum^n_&#123;i-1&#125;x_i&#x3D;\sum^n_&#123;i-1&#125;y_i</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   b_0\sum^n_&#123;i-1&#125;x_i+b_i\sum^n_&#123;i-1&#125;x_i^2&#x3D;\sum^n_&#123;i-1&#125;x_iy_i</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">求出 $b_0$ 和 $b_1$ 的值：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_1&#x3D;\frac&#123;\sum x_iy_i-[(\sum x_i)(\sum y_i)]&#x2F;n&#125;&#123;\sum x^2_i-(\sum x_i)^2&#x2F;n&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  b_0 &#x3D; \bar y-b_1\bar x (\bar x为x 的均值)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 2 决定系数</span><br><span class="line"></span><br><span class="line">$r^2$ 称为决定系数（coefficient of determination）用来衡量回归线的拟合度，也即最小二乘回归线产生的线性估计与实际观测数据的拟合程度。前面提到y^代表回应变量的估计值，$y-\bar y$ 代表预测误差或残差。</span><br><span class="line"></span><br><span class="line">#### 引子</span><br><span class="line"></span><br><span class="line">1.想象一下，如果不考虑数据集中含糖量而直接预测其营养价值，我们直观的做法是求其平均值作为预测值。假设开始为数据集里的每个记录计算(y-y&#39;)（其中y&#39;为回应变量的平均值），然后计算其平方和，这与计算误差(y-y^)，然后计算误差平方和类似。这时统计量总体误差平方和SST为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SST &#x3D;(y_1-y&#39;)^2+(y_2-y&#39;)^2+(y_3-y&#39;)^2....</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">SST，也称为总体平方和(sum of squares total，SST)是在没有考虑预测变量的情况下，衡量回应变量总体变异的统计量。</span><br><span class="line"></span><br><span class="line">2. 接下来是衡量估计回归方程能多大程度提高估计的准确度。运用回归线时的估计误差为： $y-\bar y$  ,当忽略含糖量信息时，估计误差是 $y-y&#39;$ 。因此改进量是：$\bar y-y&#39;$ .进一步基于 $\bar y-y&#39;$ 构造一个平方和的统计量，这样的统计量被称为回归平方和 *(sum of squares of regression,SSR)* ，是相对于忽略预测信息，衡量在使用回归线后预测精度提高的统计量，即：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   SSR &#x3D; (\bar y_1-y&#39;)^2+(\bar y_2-y&#39;)^2+....</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">由: $y-y&#39;&#x3D;(\bar y-y&#39;)+(y-\bar y)$  两边都进行平方，然后进行总和运算，有：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  SST &#x3D;SSR +SSE</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">#### 结论   </span><br><span class="line"></span><br><span class="line">SST衡量了回应变量变异的一部分，这部分是被回应变量和预测变量之间的线性关系所解释的。然而不是所有的数据点都正好落在回归线上，这意味着还有一部分y变量的变异不能被回归线所解释。SSE可以被认为是衡量不能被x和y之间的回归线所解释的其他变异，包括随机变异。</span><br><span class="line">决定系数 $r^2$ ，它衡量了用回归线来描述预测变量和回应变量之间线性关系的符合程度</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   r^2 &#x3D;\frac&#123;SSR&#125;&#123;SST&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">### 3 估计值的标准误差</span><br><span class="line"></span><br><span class="line">**符号**：*S*</span><br><span class="line"></span><br><span class="line">**概念**：用于衡量由回归线产生估计值的精度的统计量。</span><br><span class="line">为介绍s，首先引入均方误差 *(mean squares error，MSE)* ：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    MSE &#x3D; \frac&#123;SSE&#125;&#123;n-m-1&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中，m标示预测变量的个数，简单线性回归是m&#x3D;1,多元线性回归时m大于1。与 *SSE* 一样， *MSE* 用于衡量在回应变量中没有被回归分析所解释的变异。</span><br><span class="line">标准误差的估计由下式给出:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   S &#x3D;\sqrt &#123;MSE&#125;&#x3D;\sqrt &#123;SSE&#x2F;(n-m-1)&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"> *S* 值为“典型”残差的估计，*s* 是衡量估计中的典型误差，即回应预测值与实际值之间的差异。也即标准误差能反应估计回归方程做出预测的精确度，因此 *s* 越小越好。</span><br><span class="line"></span><br><span class="line">### 4 其他评估   </span><br><span class="line"></span><br><span class="line">#### 1. 相关系数</span><br><span class="line"></span><br><span class="line">用来定义两个变量线性关系的统计量称为相关系数 *(correlation coefficient，也称皮尔森相关系数)* ，用来衡量变量之间线性关系强弱。计算公式如下：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   r &#x3D;\frac&#123;\sum (x-x&#39;)(y-y&#39;)&#125;&#123;(n-1)S_xS_y&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">+ 其中 $S_x$和 $S_y$分别代表样本x和y的标准差</span><br><span class="line">+ 相关系数r的取值范围为：(-1,1)</span><br><span class="line">+ 变量r的值越接近于1，表明二者正向相关性越大，随着x增大y也会增大。</span><br><span class="line">+ 变量r的值越接近于-1，表明二者负向相关性越大，随着x增大y会减小。</span><br><span class="line"></span><br><span class="line">#### 2. 方差分析表(ANOVA table)   </span><br><span class="line"></span><br><span class="line">一般形式如下：</span><br><span class="line"></span><br><span class="line">|变异源|平方和( *SS* )|自由度|均方差( *MS* )|F|</span><br><span class="line">|---|---|---|---|---|</span><br><span class="line">|回归| *SSR* | *m* | $MSR &#x3D;\frac&#123;SSR&#125;&#123;m&#125;$ | $F&#x3D; \frac&#123;MSR&#125;&#123;MSE&#125;$ |</span><br><span class="line">|误差| *SSE* | *n-m-1*| $MSE &#x3D;\frac&#123;SSE&#125;&#123;n-m-1&#125;$ ||</span><br><span class="line">|合计| *SST&#x3D;SSR+SSE* | *n-1* |||</span><br><span class="line"></span><br><span class="line">下面展示了 糖含量营养级别回归结果:</span><br></pre></td></tr></table></figure>
<p>anova&lt;-aov(data=sugar,rating~sugars)<br>summary(anova)<br>                 Df Sum Sq Mean Sq  F value   Pr(&gt;F)<br>    sugars       1   8655    8655   102.3    1.15e-15<br>    Residuals   75   6342      85                   </p>
<pre><code>sugars      ***  
Residuals        
           ---  
</code></pre><p>  Signif. codes:  0 ‘<strong>*’ 0.001 ‘</strong>’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3. 异常点、高杠杠点和强影响观测值   </span><br><span class="line"></span><br><span class="line"> **高杠杆点** *(High leverage point)*：可以被认为是一个观测值在预测空间中的极限，也即一个高杠杆值可以被认为是不考虑y值得x变量的极限。杠杆第i个观察值hi可以被标示如下（x&#39;为平均数）：</span><br><span class="line"></span><br><span class="line"> $$</span><br><span class="line">   h_i &#x3D;\frac&#123;1&#125;&#123;h&#125;+\frac&#123;(x-x&#39;)^2&#125;&#123;\sum (x_i-x&#39;)^2&#125;</span><br><span class="line"> $$</span><br><span class="line"></span><br><span class="line">对于给定数据集,1&#x2F;n和右边分式分母都是常数，所以第i个观察的杠杆只依赖于 $(x_i-x&#39;)^2$。</span><br><span class="line">一个拥有大于 $\frac&#123;2*(m+1)&#125;&#123;n&#125;$ 和 $\frac&#123;3*(m+1)&#125;&#123;n&#125;$ 的观察点被认为是高杠杆点。</span><br><span class="line"></span><br><span class="line">**异常点** 观测到的偏离回归直（曲）线的点。一种粗略的评价观察值的方法是使用标准残留值 *(standardized residuals)*一般用:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   S_&#123;i,resid&#125;&#x3D;s\sqrt&#123;1-h_i&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">来标示第i个残留数的标准差，则hi代表第i个观测值的杠杆影响，那么标准残留值可以表示为:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    r_&#123;i,stand &#125;&#x3D; \frac&#123;(y-y&#39;_i)&#125;&#123;s_&#123;i,resid&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">如果标准残留值得绝对值超过了2，就可以认为是一个异常点,上图中观测点1和4应该是异常点。</span><br><span class="line"></span><br><span class="line"> **强影响力点** ：对数据集的分析造成较大影响的观测点。通常强影响力观测值同时有较大的残留值和较高的杠杆，但也有可能它既不是异常点也没有较高的杠杆，但两者特点组合成一个具有影响力的点。粗略估算一个观察点是否是强影响力点的方法是看它的Cook距离 *(Cook&#39;s distance)* 是否大于1.0，更确切的说，用Cook距离与F分布 *(m,n-m)* 来比较，若观测值落在分布的第一部分（低于25个百分点），就说它对整体分布只有一点点影响，若落在中点以后就说明该点是有影响力的。Cook距离将残留值和刚刚都考虑进去的，第i个观察点的距离可以为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \frac&#123;(y_i-y&#39;_i)^2&#125;&#123;(m+1)S^2&#125;\frac&#123;h_i&#125;&#123;1-h_i&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中 $y_i-y&#39;_i$表示第i个残留值，m表示预测变量的个数，s为标准误差的估计，hi为第i个观察点的杠杆。左边的比率含有一个元素代表了残留值，右边的函数代表了杠杆值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 四 回归推断</span><br><span class="line"></span><br><span class="line">最小二乘法回归是建立在一个假设基础上的线性回归模型，我们需要一个系统地框架来评估两个变量之间是否存在线性关系。对于最小二乘法的公式：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">     y &#x3D; m_0+m_1x+e</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">主要有以下四种方法：</span><br><span class="line"></span><br><span class="line">1.用来推断回应变量与预测变量之间关系的t检验法</span><br><span class="line">2. 斜率m1的置信区间</span><br><span class="line">3. 在给定一个特定的预测值条件下，回应变量&lt;B&gt;均值&lt;&#x2F;B&gt;的置信区间</span><br><span class="line">4. 在给定一个特定的预测值条件下，回应变量&lt;B&gt;随机值&lt;&#x2F;B&gt;的预测区间</span><br><span class="line"></span><br><span class="line">### 4.1 x和y之间线性关系的t检验</span><br><span class="line"></span><br><span class="line">对于简单线性回归t检验与F检验是等价的。&lt;br&gt;</span><br><span class="line"></span><br><span class="line">#### 对斜率的估计</span><br><span class="line"></span><br><span class="line">用最小二乘法估计的斜率m&#39;(注意m1是真实斜率)是一个统计量，像所有统计量一样服从一个特定均值和标准差的样本分布.斜率的回归推断是基于m&#39;的样本方差的点估计 Sm&#39;,Sm&#39;被解释为对斜率变异性的衡量指标，较大的Sm&#39;预示着斜率m*的估计是不稳定的。t检验基于统计量 $t&#x3D;(m&#39;-m_1)&#x2F;Sm&#39;$ ,它服从一个自由度为 $n-2$ 的t分布，当零假设为真（变量x和y之间不存在线性关系）时，检验统计量 $t&#x3D;m&#39;&#x2F;Sm&#39;$ 服从一个自由度为 *n-2* 的 t 分布。</span><br><span class="line"></span><br><span class="line">我们重新概览下77种谷物数据中营养级别与含糖量的线性回归结果：</span><br></pre></td></tr></table></figure>
<pre><code> lm.reg&lt;-lm(data=sugar,rating~sugars)  
 summary(lm.reg)  

Call:  
lm(formula = rating ~ sugars, data = sugar)  

Residuals:  
    Min      1Q  Median      3Q     Max   
-17.853  -5.677  -1.439   5.160  34.421   

Coefficients:  
            Estimate Std. Error t value Pr(&gt;|t|)      
(Intercept)  59.2844     1.9485   30.43  &lt; 2e-16 ***  
sugars       -2.4008     0.2373  -10.12 1.15e-15 ***  
---  
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  

Residual standard error: 9.196 on 75 degrees of freedom  
Multiple R-squared:  0.5771,    Adjusted R-squared:  0.5715   
F-statistic: 102.3 on 1 and 75 DF,  p-value: 1.153e-15
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">我们可以得到：</span><br><span class="line">  + 在系数列下面，找到斜率m&#39;的估计值为 -2.4008。</span><br><span class="line">  + 在SE系数列里找到斜率m&#39;的标准差Sm&#39;为 0.2373</span><br><span class="line">  + 在T值列找到t的统计量，即t检验的值 , $t&#x3D;\frac&#123;m&#39;&#125;&#123;Sm&#39;&#125;&#x3D;\frac&#123;-2.4008&#125;&#123;0.2373&#125;&#x3D;-10.1171$</span><br><span class="line">  + 在最后一列可以找到t检验的p值，是一个双尾检验，形式为：p值&#x3D; $P(|t|&gt;tobs)$ ，其中的tobs代表观测值。此处P值&#x3D; $P(|t|&gt;tobs)&#x3D;P(|t|&gt;-10.1171)$ 近似为0，小于任何显著性要求的合理界限，因此可以拒绝零假设也即认为含糖量和营养级别之前存在线性关系。</span><br><span class="line"></span><br><span class="line">### 4.2 回归直线斜率的置信区间</span><br><span class="line"></span><br><span class="line">置信区间也即在一定概率P下保证变量落在某区间 [a,b]内。对于回归直线的真实斜率 $m_1$来说，*100\*(1-c)%* 的置信区间也即有 100*(1-c)%的把握保证回归线的真实斜率位于 [ $m&#39;-(t_&#123;n-2&#125;)(Sm&#39;),m&#39;+(t_&#123;n-2&#125;)(Sm&#39;)$ ]区间。其中 $t_&#123;n-2&#125;$ 是自由度为n-2的 t 分布。</span><br><span class="line"></span><br><span class="line">例如构建一个回归直线的真实斜率 $m_1$的95%的置信区间。有一个对m1的点估计值 m&#39;&#x3D;-2.4008,对95%的之心去和自由度为 $n-2&#x3D;77-2&#x3D;75$ 的t临界值为 2.0(查表t75.95%&#x3D;2.0),从表中得到 Sm&#39;&#x3D;0.2373，因此置信区间为：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">    [ $m&#39;-(t_&#123;n-2&#125;)(Sm&#39;),m&#39;+(t_&#123;n-2&#125;)(Sm&#39;)$ ]</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">相关代码及结果如下：&lt;br&gt;</span><br></pre></td></tr></table></figure>
<p>lm.reg&lt;-lm(data=sugar,rating~sugars)  </p>
<h1 id="level-0-95为置信度"><a href="#level-0-95为置信度" class="headerlink" title="level=0.95为置信度"></a>level=0.95为置信度</h1><p>confint(lm.reg,level=0.95)<br>                    2.5 %    97.5 %<br>    (Intercept) 55.402783 63.165952<br>    sugars      -2.873567 -1.92807<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 4.3 给定x条件下，y均值的置信区间和y随机选择值的预测区间</span><br><span class="line"></span><br><span class="line">给定x条件下，y&lt;B&gt;均值&lt;&#x2F;B&gt;的置信区间由如下公式判定：</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">  \bar y\pm t_&#123;n-2&#125;(s)\sqrt &#123;\frac&#123;1&#125;&#123;n&#125;+\frac&#123;(x_p-\bar x)^2&#125;&#123;\sum(x_i-\bar x)^2&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">其中 $\bar y$ 表示给定x值后，y的点估计值，$t_&#123;n-2&#125;$ 是与样本大小和置信水平相关联的乘数，s是估计的标准误差，$\bar x$ 是产生预测值所对应的x专指变量。</span><br><span class="line"></span><br><span class="line">给定x条件下，y随机选择值的预测区间:</span><br><span class="line"></span><br><span class="line">$$</span><br><span class="line">   \bar y\pm t_&#123;n-2&#125;(s)\sqrt &#123;1+\frac&#123;1&#125;&#123;n&#125;+\frac&#123;(x_p-\bar x)^2&#125;&#123;\sum(x_i-\bar x)^2&#125;&#125;</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line"> **注意**：第二张图中表达式与第一张图中表达式相比，除了在平方根李出现了 **&quot;1+&quot;** 外完全一样，这说明比起均值估计来，对于单个y值得估计会有更大的变化范围，这也说明了预测区间总是比类似的置信区间要宽</span><br><span class="line"></span><br><span class="line">我们希望预测含糖量为 sugars&#x3D;10时该谷物的营养级别范围，实例代码如下：</span><br></pre></td></tr></table></figure></p>
<pre><code>&gt; point&lt;-data.frame(sugars=10)  
&gt; point  
  sugars  
1     10  
&gt; lm.reg  

Call:  
lm(formula = rating ~ sugars, data = sugar)  

Coefficients:  
(Intercept)       sugars    
     59.284       -2.401    

&gt; lm.pred&lt;-predict(lm.reg,point,interval=&quot;prediction&quot;,level=0.95)  
&gt; lm.pred  
       fit     lwr      upr  
 1 35.27617 16.7815 53.77083
</code></pre><p>```</p>
<p>可以看到在95%的置信度下，含糖量为10的谷物其营养级别介于 16.7815和53.77083之间</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>数据挖掘方法之一：主成份分析</title>
    <url>/2013/04/01/dataming-PCA/</url>
    <content><![CDATA[<h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一 概念"></a>一 概念</h2><p>主成分分析（principle componentsanalysis,PCA）是指将多个变量通过线性组合，选出较少个数的重要变量集合来描述相关结构的额一种统计分析方法，这些线性组合被称为“成分”。由m个变量组成的数据集的总变异，可以由k个线性组合变量组成的子集来标示（k&lt;m）。这意味着k个变量与原来的变量反应了几乎同样多的信息  </p>
<h2 id="二-步骤"><a href="#二-步骤" class="headerlink" title="二 步骤"></a>二 步骤</h2><ol>
<li><p>原始指标数据的标准化采集p维随机向量</p>
<script type="math/tex; mode=display">x = (x_1,x_2,...x_p)^T</script><p>$n$个样本<br>$x<em>i= (x</em>{i1},x<em>{i2},x</em>{i3},…x_{ip})^T,i=1,2..n, n&gt;p$</p>
<p>构造样本阵，对样本元素进行如下标准化变换</p>
</li>
</ol>
<script type="math/tex; mode=display">Z_{ij}=\frac{x_{ij}-\bar x_j}{s_J},i =1,2,..\pi, j =1,2,...p</script><p>其中<script type="math/tex">\bar x_j =\frac{\sum^n_{i-1}x_{ij}}{n},s^2_j=\frac{\sum^n_{i-1}(x_{ij}-\bar x_j)^2}{n-1}</script></p>
<p>   得标准化矩阵Z。</p>
<ol>
<li><p>对标准化矩阵Z求相关系数矩阵    </p>
<script type="math/tex; mode=display">R = [r_{ij}] xp =\frac{Z^TZ}{n-1}其中r_{ij}=\frac{\sum z_{kj}\cdot z_{kj}}{n-1}, i,j = 1,2...p</script></li>
<li><p>求解样本相关矩阵R的特征方程</p>
<p>$R-\lambda I_p =0$ 得 $p$ 个特征根，按</p>
<script type="math/tex; mode=display">\frac{sum^m_{j-1}\lambda _j}{\sum^p_{j-1}\lambda _j}\geq 0.85
确定n的值，使信息的利用率达85%以上。对每个 $\lambda_j,i=1,2,..m$ 解方程组$R_b= \lambda_jb$得单位特征向量$b^0_j</script></li>
<li><p>将标准化后的指标变量转换成主成分。</p>
<p>$U_{ij}=z^Tb^0_j,j=1,2,…m$</p>
<p>$U_1$称为第一主成分，$U_2$称为第二主成分，$U_3$称为第三主成分，…</p>
<p>对n个主成份进行综合评价。对n个主成份进行加权求和，即得最终评价值，权数为每个主成份的方差贡献率。</p>
</li>
</ol>
<h2 id="三-关键性结论"><a href="#三-关键性结论" class="headerlink" title="三 关键性结论"></a>三 关键性结论</h2><p>以下结论对主成份分析非常重要</p>
<p><strong>结论1</strong>：标准化数据集州农工的总体变动性等于所有Z向量方差之和，等于每个成分方差之和，等于特征权值之和，等于变量的个数。即</p>
<script type="math/tex; mode=display">
  \sum^m_{i-1}Var(Y_i)=\sum^m_{i-1}Var(Z_i)=\sum^m_{i-1}\lambda _i=m</script><p><strong>结论2</strong>：给定成分与给定变量间的偏相关性是特征向量与特征值的函数。</p>
<p>具体来说</p>
<script type="math/tex; mode=display">Corr(Y_i,Z_i)=e_{ii}\sqrt\lambda _i,i,j=1,2,..m,\lambda _1\gt\lambda _2\geq\lambda _3...\geq\lambda _m</script><p>其中 $(\lambda _1,e_1),(\lambda _2,e_2),(\lambda _3,e_3),..(\lambda _m,e_m)$<br>是相关系数矩阵p的<font color="gray">特征值-特征向量</font>对，并且偏相关系数包括了所有变量之间的影响。</p>
<p><strong>结论3</strong>：Z中第i个主成份解释了变量的总体变异的百分比，等于第i个特征根与变量个数之间的比率 $\frac{\lambda _i}{m}$</p>
<h2 id="四-应用于实际数据"><a href="#四-应用于实际数据" class="headerlink" title="四 应用于实际数据"></a>四 应用于实际数据</h2><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>最小值</th>
<th>最大值</th>
<th>均值</th>
<th>标准差</th>
</tr>
</thead>
<tbody>
<tr>
<td>房屋价值中位数</td>
<td>Range</td>
<td>14999</td>
<td>500001</td>
<td>206918.067</td>
<td>115485.040</td>
</tr>
<tr>
<td>收入中位数</td>
<td>Range</td>
<td>0.500</td>
<td>15.000</td>
<td>3.873</td>
<td>1.906</td>
</tr>
<tr>
<td>平均房龄中位数</td>
<td>Range</td>
<td>1</td>
<td>52</td>
<td>28.656</td>
<td>12.582</td>
</tr>
<tr>
<td>总房间数</td>
<td>Range</td>
<td>2</td>
<td>37937</td>
<td>2621.653</td>
<td>2131.644</td>
</tr>
<tr>
<td>总卧室数</td>
<td>Range</td>
<td>1</td>
<td>6445</td>
<td>535.096</td>
<td>413.541</td>
</tr>
<tr>
<td>人口数</td>
<td>Range</td>
<td>3</td>
<td>35682</td>
<td>1418.971</td>
<td>1122.534</td>
</tr>
<tr>
<td>家庭数</td>
<td>Range</td>
<td>1</td>
<td>6082</td>
<td>479.332</td>
<td>377.378</td>
</tr>
<tr>
<td>北纬</td>
<td>Range</td>
<td>32.540</td>
<td>41.950</td>
<td>35.630</td>
<td>2.137</td>
</tr>
<tr>
<td>西经</td>
<td>Range</td>
<td>-124.350</td>
<td>-114.310</td>
<td>-119.567</td>
<td>2.003</td>
</tr>
</tbody>
</table>
</div>
<p>1.使用上图的均值和标准差对变量进行标准化，得到Z向量。<br>2.研究下图中变量矩阵图以检验变量间是否存在相关性。<br> 可以看到 总房数、卧室数、人口数和家庭数之间表现出正相关性，西经和北纬表现出负相关性。再来观察变量的相关性矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>平均收入</th>
<th>平均房龄</th>
<th>总房数</th>
<th>总卧室数</th>
<th>人口数</th>
<th>家庭数</th>
<th>北纬</th>
<th>西经</th>
</tr>
</thead>
<tbody>
<tr>
<td>平均收入</td>
<td>1.000</td>
<td>-0.117</td>
<td>0.199</td>
<td>-0.012</td>
<td>0.002</td>
<td>0.010</td>
<td>-0.083</td>
<td>-0.012</td>
</tr>
<tr>
<td>平均房龄</td>
<td>-0.117</td>
<td>1.000</td>
<td>-0.360</td>
<td>-0.318</td>
<td>-0.292</td>
<td>-0.300</td>
<td>0.011</td>
<td>-0.107</td>
</tr>
<tr>
<td>总房数</td>
<td>0.199</td>
<td>-0.360</td>
<td>1.000</td>
<td>0.928</td>
<td>0.856</td>
<td>0.919</td>
<td>-0.035</td>
<td>0.041</td>
</tr>
<tr>
<td>总卧室数</td>
<td>-0.012</td>
<td>-0.318</td>
<td>0.928</td>
<td>1.000</td>
<td>0.878</td>
<td>0.981</td>
<td>-0.064</td>
<td>0.064</td>
</tr>
<tr>
<td>人口数</td>
<td>0.002</td>
<td>-0.292</td>
<td>0.856</td>
<td>0.878</td>
<td>1.000</td>
<td>0.907</td>
<td>-0.107</td>
<td>0.097</td>
</tr>
<tr>
<td>家庭数</td>
<td>0.010</td>
<td>-0.300</td>
<td>0.919</td>
<td>0.981</td>
<td>0.907</td>
<td>1.000</td>
<td>-0.069</td>
<td>0.051</td>
</tr>
<tr>
<td>北纬</td>
<td>-0.083</td>
<td>0.011</td>
<td>-0.035</td>
<td>-0.064</td>
<td>-0.107</td>
<td>-0.069</td>
<td>1.000</td>
<td>-0.925</td>
</tr>
<tr>
<td>西经</td>
<td>-0.012</td>
<td>-0.107</td>
<td>0.041</td>
<td>0.064</td>
<td>0.097</td>
<td>0.051</td>
<td>-0.925</td>
<td>1.000</td>
</tr>
</tbody>
</table>
</div>
<p> 矩阵图和相关矩阵式常用的两种方法，用来观察预测变量之间的相关性结构。<br>  如果完成一次住房价预测的多元回归分析，但不考虑数据集中的多重共线性将导致回归结果非常不稳定，预测值的微小变化将会导致回归系数的极大变化，而得不到任何结论。此时需要用主成分分析，其可以通过相关化结构，确定相关变量的基本组成部分</p>
<ol>
<li><p>采用主成分分析对房屋数据集进行分析，该要素矩阵（下图）中每个栏目代表成分Yi=e’Z中的一项。栏目中元素为成分的权重，代表了变量与成分的偏相关。结论2 表明这些成分的权重等于Corr(Yi,Zi),成分涉及第i个特征向量和特征值</p>
<p>||1|2|3|4|5|6|7|8|<br>|—-|—-|—-|—-|—-|—-|—-|—-|—-|<br>|平均收入|0.086|-0.058|0.922|0.370|-0.02|-0.018|0.037|-0.004|<br>|平均房龄|-0.429|0.025|-0.407|0.806|0.014|0.026|0.009|-0.001|<br>|总房数|0.956|0.100|0.102|0.104|0.120|0.162|-0.119|0.015|<br>|总卧室数|0.970|0.083|-0.121|0.056|0.144|-0.068|0.051|-0.083|<br>|人口数|0.933|0.034|-0.121|0.076|-0.327|0.034|0.006|-0.015|<br>|家庭数|0.972|0.086|-0.113|0.087|0.058|-0.112|0.061|0.083|<br>|北纬|-0.140|0.970|0.017|-0.088|0.017|0.132|0.113|0.005|<br>|西经|0.144|-0.969|-0.062|-0.063|0.037|0.136|0.109|0.007|</p>
</li>
<li><p>结论3表明，Z的总变异种第i个主成分所占的比例是ri/m(其中ri是特征值)，即第i个特征值与变量数的比例。由下图可以看出，第一特征值是 3.091,因为有8个预测变量，第一主成分解释 $\frac{3.091}{8}=48.767$ 的变异。</p>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>成分</th>
<th>合计</th>
<th>变化百分比</th>
<th>累计百分比%</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3.901</td>
<td>48.767</td>
<td>48.767</td>
</tr>
<tr>
<td>2</td>
<td>1.910</td>
<td>23.881</td>
<td>72.648</td>
</tr>
<tr>
<td>3</td>
<td>1.073</td>
<td>13.409</td>
<td>86.057</td>
</tr>
<tr>
<td>4</td>
<td>0.825</td>
<td>10.311</td>
<td>96.368</td>
</tr>
<tr>
<td>5</td>
<td>0.148</td>
<td>1.847</td>
<td>98.215</td>
</tr>
<tr>
<td>6</td>
<td>0.082</td>
<td>1.020</td>
<td>99.235</td>
</tr>
<tr>
<td>7</td>
<td>0.047</td>
<td>0.586</td>
<td>99.821</td>
</tr>
<tr>
<td>8</td>
<td>0.014</td>
<td>0.179</td>
<td>100.00</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
