<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://shartoo.github.com/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="shartoo">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shartoo.github.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-2018-10-11-merlin-mandarin-fronted-process" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-10-11-merlin-mandarin-fronted-process/" class="article-date">
  <time datetime="2019-12-23T10:45:59.660Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-10-11-merlin-mandarin-fronted-process/">merlin语音合成过程中 中文前端处理过程详细</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="0-概述"><a href="#0-概述" class="headerlink" title="0 概述"></a>0 概述</h2><p>语音合成过程，需要处理两部分内容，分别是：</p>
<ul>
<li><p>文本(Text)处理： 假设我们的输入是<code>你好看啊</code></p>
</li>
<li><p>音频(speech)处理： 对应<code>你好看啊.wav</code></p>
</li>
</ul>
<h2 id="1-文本处理"><a href="#1-文本处理" class="headerlink" title="1 文本处理"></a>1 文本处理</h2><h3 id="1-1-规范化"><a href="#1-1-规范化" class="headerlink" title="1.1 规范化"></a>1.1 规范化</h3><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等</p>
<p>有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音[7]。例如</p>
<ul>
<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”；</li>
<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>
</ul>
<p>对于英文而言，如：</p>
<ul>
<li><strong>类别为年份（NYER）</strong>： 2011 $\rightarrow$ twenty eleven</li>
<li><strong>类别为货币(MONEY)</strong>: £100 $\rightarrow$  one hundred pounds</li>
<li><strong>类别为非单词，需要拟音(ASWD)</strong>:  IKEA $\rightarrow$  apply letter-to-sound</li>
<li><strong>类别为数字(NUM)</strong> : 100 NUM $\rightarrow$ one hundred</li>
<li><strong>类别为字母(LSEQ)</strong> :  DVD  $\rightarrow$ dee vee dee</li>
</ul>
<h3 id="1-2-转化为拼音"><a href="#1-2-转化为拼音" class="headerlink" title="1.2 转化为拼音"></a>1.2 转化为拼音</h3><p>参考<a href="http://www.moe.edu.cn/s78/A19/yxs_left/moe_810/s230/195802/t19580201_186000.html" target="_blank" rel="noopener">国家汉语拼音方案</a></p>
<p>使用一个汉语拼音词典，将<code>你好看啊</code>转换为： <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>。此过程需要注意有些多音词需要处理，可以只是使用python的<strong>pypinyin</strong></p>
<h3 id="1-3-拼音转换为音调表示"><a href="#1-3-拼音转换为音调表示" class="headerlink" title="1.3 拼音转换为音调表示"></a>1.3 拼音转换为音调表示</h3><p>目前支持将汉语拼音中的<code>一</code>,<code>二</code>,<code>三</code>,<code>四</code>声转换为 <code>1</code>,<code>2</code>,<code>3</code>,<code>4</code>,<code>5</code>（5代表轻声）</p>
<p> <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>$\rightarrow$ <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<p>事实上<strong>pypinyin</strong>可以一步从<code>你好看啊</code>转换为 <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<h3 id="1-4-将音节分解为音素"><a href="#1-4-将音节分解为音素" class="headerlink" title="1.4  将音节分解为音素"></a>1.4  将音节分解为音素</h3><p>音素为汉语拼音的最小单元。包括<code>声母</code>,<code>韵母</code>,但是其中还会有一些整体认读音节。(注意：下面所列并非官方标准版，不同情形可以采取不同取舍，参考<a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">MTTS文本分析</a>)</p>
<p><strong>整体认读音节</strong></p>
<p>16个整体认读音节分别是：<code>zhi 、chi、shi、ri、zi、ci、si、yi、wu、yu、ye、yue、yuan、yin 、yun、ying</code>，但是要注意没有yan，因为yan并不发作an音</p>
<p><strong>声母（23个）</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b p m f d t n l g k h j q x zh ch sh r z c s y w</span><br></pre></td></tr></table></figure>

<p><strong>韵母（39个）</strong></p>
<ul>
<li>单韵母 a、o、e、 ê、i、u、ü、-i（前）、-i（后）、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 üe、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 üan 、en、in、uen、 ün 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<p><strong>韵母（39个）（转换标注后）</strong></p>
<ul>
<li>单韵母 a、o、e、ea、i、u、v、ic、ih、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 ve、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 van 、en、in、uen、 vn 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<h3 id="1-5-结果"><a href="#1-5-结果" class="headerlink" title="1.5 结果"></a>1.5 结果</h3><p>此步骤的结果为 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#39;n&#39;, &#39;i3&#39;), (&#39;h&#39;, &#39;ao3&#39;), (&#39;k&#39;, &#39;an4&#39;), (&#39;a5&#39;,)]</span><br></pre></td></tr></table></figure>

<h2 id="2-合成基元选取"><a href="#2-合成基元选取" class="headerlink" title="2 合成基元选取"></a>2 合成基元选取</h2><p>合成基元就是合成语音所需的最小单元。由大到小来说：</p>
<ol>
<li>可以选择每个汉字，一共有6万多，会导致需要很大的训练集</li>
<li>可以选择所有拼音，数量会比汉字少很多</li>
<li>也可以选择声韵母，声韵母是组成音节的单元，21个声母+39个韵母，数据量大幅度减少。</li>
</ol>
<p>在实际语音中除了这些文本上的内容之外，还会存在开始和结束的<strong>静音</strong>，标点符号之间存在的<strong>短暂停顿</strong>。所以我们可以采取以下这套合成基元方案。</p>
<ul>
<li><strong>声母</strong>：  21个声母+wy（共23个）</li>
<li><strong>韵母</strong>： 39个韵母</li>
<li><strong>静音</strong>：<code>sil</code>, <code>pau</code>, <code>sp</code>。sil(silence) 表示句首和句尾的静音，pau(pause) 表示由逗号，顿号造成的停顿，句中其他的短停顿为sp(short pause)</li>
</ul>
<h2 id="3-上下文相关标注"><a href="#3-上下文相关标注" class="headerlink" title="3  上下文相关标注"></a>3  上下文相关标注</h2><p>上下文相关标注的规则要综合考虑有哪些上下文对当前音素发音的影响，总的来说，需要考虑发音基元及其前后基元的信息，以及发音基元所在的音节、词、韵律词、韵律短语、语句相关的信息。</p>
<p>此类标注对于不同任务可以自由设计，一种参考是<a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/mandarin_example_label.md" target="_blank" rel="noopener">MTTS普通话标注示例</a>。这里将参考中的一些内容作出一些解释：</p>
<table>
<thead>
<tr>
<th>层级（由小到达）</th>
<th>标注格式</th>
</tr>
</thead>
<tbody><tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6_p7</td>
</tr>
<tr>
<td>.</td>
<td>/A:a1_a2-a3_a4#a5</td>
</tr>
<tr>
<td>音节层</td>
<td>/B:b1_b2!b3_b4#b5@b6!b7+b8@b9#b10_b11</td>
</tr>
<tr>
<td>.</td>
<td>/C:c1+c2-c3=c4#c5</td>
</tr>
<tr>
<td>词层</td>
<td>/D:d1-d2 /E:e1&amp;e2^e3_e4 /F:f1-f2</td>
</tr>
<tr>
<td>韵律层</td>
<td>/G:g1-g2 /H:h1-h2@h3+h4 /I:i1-i2</td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/J:j1^j2=j3-j4 /K:k1=k2_k3^k4&amp;k5_k6 /L:l1^l2#l3-l4</td>
</tr>
<tr>
<td>语句层</td>
<td>/M:m1#m2+m3+m4!m5</td>
</tr>
</tbody></table>
<p>下面的（发音）基元指的是声韵母，HMM建模选用的单元是音节</p>
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前基元在当前音节的位置（正序）</td>
</tr>
<tr>
<td>p7</td>
<td>当前基元在当前音节的位置（倒序）</td>
</tr>
<tr>
<td>a1</td>
<td>前一音节的首基元</td>
</tr>
<tr>
<td>a2</td>
<td>前一音节的末基元</td>
</tr>
<tr>
<td>a3，a4</td>
<td>前一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>前一音节的基元数目</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节的首基元</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节的末基元</td>
</tr>
<tr>
<td>b3，b4</td>
<td>当前音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>当前音节的基元数目</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节在词中的位置（正序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节在词中的位置（倒序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b9</td>
<td>当前音节在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b10</td>
<td>当前音节在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b11</td>
<td>当前音节在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>c1</td>
<td>后一音节的首基元</td>
</tr>
<tr>
<td>c2</td>
<td>后一音节的末基元</td>
</tr>
<tr>
<td>c3，c4</td>
<td>后一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>c5</td>
<td>后一音节的基元数目</td>
</tr>
<tr>
<td>d1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>d2</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>e1</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>e2</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>当前词在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>e4</td>
<td>当前词在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>f1</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>f2</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>g1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>g2</td>
<td>前一个韵律词的词数目</td>
</tr>
<tr>
<td>—</td>
<td>—-</td>
</tr>
<tr>
<td>h1</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>h2</td>
<td>当前韵律词的词数目</td>
</tr>
<tr>
<td>h3</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>h4</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td>—</td>
<td>–</td>
</tr>
<tr>
<td>i1</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>i2</td>
<td>后一个韵律词的词数目</td>
</tr>
<tr>
<td>–</td>
<td>—</td>
</tr>
<tr>
<td>j1</td>
<td>前一韵律短语的语调类型</td>
</tr>
<tr>
<td>j2</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>j3</td>
<td>前一韵律短语的词数目</td>
</tr>
<tr>
<td>j4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>—</td>
<td>—-</td>
</tr>
<tr>
<td>k1</td>
<td>当前韵律短语的语调类型</td>
</tr>
<tr>
<td>k2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>k3</td>
<td>当前韵律短语的词数目</td>
</tr>
<tr>
<td>k4</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>k5</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>k6</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>l1</td>
<td>后一韵律短语的语调类型</td>
</tr>
<tr>
<td>l2</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>l3</td>
<td>后一韵律短语的词数目</td>
</tr>
<tr>
<td>l4</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>m1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>m2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>m3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>m4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>m5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody></table>
<h2 id="4-问题集设计"><a href="#4-问题集设计" class="headerlink" title="4 问题集设计"></a>4 问题集设计</h2><p>问题集(Question Set)即是决策树中条件判断的设计。问题集通常很大，由几百个判断条件组成。</p>
<p>问题集的设计依赖于不同语言的语言学知识，而且<strong>与上下文标注文件相匹配，改变上下文标注方法也需要相应地改变问题集</strong>，对于中文语音合成而言，问题集的设计的规则有:</p>
<ul>
<li><strong>前前个，前个，当前，下个，下下个声韵母分别是某个合成基元吗</strong>，合成基元共有65个(23声母+39韵母+3静音)，例如判断是否是元音a QS “LL-a” QS “L-a” QS “C-a” QS “R-a” QS “RR-a”</li>
<li><strong>声母特征划分</strong>，例如声母可以划分成塞音，擦音，鼻音，唇音等，声母特征划分24个</li>
<li><strong>韵母特征划分</strong>，例如韵母可以划分成单韵母，复合韵母，分别包含aeiouv的韵母，韵母特征划分8个</li>
<li><strong>其他信息划分</strong>，词性划分，26个词性; 声调类型，5个; 是否是声母或者韵母或者静音，3个</li>
<li><strong>韵律特征划分</strong>，如是否是重音，重音和韵律词/短语的位置数量</li>
<li><strong>位置和数量特征划分</strong></li>
</ul>
<p>对于三音素模型而言，对于每个划分的特征，都会产生3个判断条件，该音素是否满足条件，它的左音素（声韵母）和右音素（声韵母）是否满足条件，有时会扩展到左左音素和右右音素的情况，这样就有5个问题。其中，每个问题都是以 QS 命令开头，问题集的答案可以有多个，中间以逗号隔开，答案是一个包含通配符的字符串。当问题表达式为真时，该字符串成功匹配标注文件中的某一行标注。格式如：</p>
<p>QS 问题表达式 {答案 1，答案 2，答案 3，……}</p>
<p>QS “LL==Fricative” {f^<em>,s^</em>,sh^<em>,x^</em>,h^<em>,lh^</em>,hy^<em>,hh^</em>}</p>
<p>对于3音素上下文相关的基元模型的3个问题，例如： * 判断当前，前接，后接音素/单元是否为擦音 * QS ‘C_Fricative’ * QS ‘L_Fricative’ * QS ‘R_Fricative’</p>
<p>问题集示例参考 <a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/question.md" target="_blank" rel="noopener">MTTS问题集设计参考</a></p>
<p>值得注意的是，merlin中使用的问题集和HTS中有所不同，Merlin中新增加了CQS问题，Merlin处理Questions Set 的模块在merlin/src/frontend/label_normalisation 中的Class HTSLabelNormalisation</p>
<p><strong>Question Set 的格式是</strong></p>
<p>QS + 一个空格 + “question_name” + 任意空格+ {Answer1, answer2, answer3…} # 无论是QS还是CQS的answer中，前后的**不用加，加了也会被去掉 CQS + 一个空格 + “question_name” + 任意空格+ {Answer} #对于CQS，这里只能有一个answer 比如 CQS C-Syl-Tone {<em>(d+)+} merlin也支持浮点数类型，只需改为CQS C-Syl-Tone {</em>([d.]+)+}</p>
<p>参考 ：  <a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-10-11-merlin-mandarin-fronted-process/" data-id="ck4ifvdg90043ywje7lsq6709" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-10-02-opencv-svm-hog-loc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-10-02-opencv-svm-hog-loc/" class="article-date">
  <time datetime="2019-12-23T10:45:59.659Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-10-02-opencv-svm-hog-loc/">使用HOG+SVM+滑窗+NMS完成目标定位分类</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="0-概览"><a href="#0-概览" class="headerlink" title="0  概览"></a>0  概览</h2><p>整个过程如下:</p>
<ol>
<li>数据标注</li>
<li>抽取HOG</li>
<li>训练SVM</li>
<li>预测（滑窗，分类）</li>
<li>NMS</li>
</ol>
<h2 id="1-数据标注"><a href="#1-数据标注" class="headerlink" title="1 数据标注"></a>1 数据标注</h2><p>使用的是<code>extract_feature_from_fasterrcnn_labeled.py</code></p>
<p>本文直接使用了 YOLO的数据标注格式。YOLO的标注格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 一个有9个物体，9个物体总共属于3个分类</span><br><span class="line">9,3</span><br><span class="line"># 分类yinliao的类别是0，xmin,ymin,xmax,ymax分别是56,103,261,317</span><br><span class="line">56,103,261,317,0,yinliao</span><br><span class="line">261,0,442,120,2,kele</span><br><span class="line">465,16,645,180,2,kele</span><br><span class="line">209,138,345,266,2,kele</span><br></pre></td></tr></table></figure>

<p>根据此标注文件可以从图片中抠取目标物体的ROI。如下图，根据标注的文件可以抽取的ROI示例，左上角为提取的ROI：</p>
<p><img src="/images/blog/hog_svm_loc1.jpg" alt="yolov123"> </p>
<h2 id="2-抽取特征"><a href="#2-抽取特征" class="headerlink" title="2 抽取特征"></a>2 抽取特征</h2><h3 id="2-1-抽取HOG特征"><a href="#2-1-抽取HOG特征" class="headerlink" title="2.1 抽取HOG特征"></a>2.1 抽取HOG特征</h3><p>注意：</p>
<ul>
<li>我最开始使用的是 Opencv的<code>hog = cv2.HOGDescriptor</code>的方式，后来修改为<code>skimage.feature</code>。</li>
<li>hog特征如果想要固定长度的话，提取对象(ROI)必须也是固定长度的，所以需要做个resize。否则的话在训练时，会出现数据维度不一致的问题。我之前以为HOG会把任意尺寸的图像转换为相同长度的特征，发现并不是。提取完之后检查输出文件的大小是否一样。</li>
<li>提取ROI的时候注意 y在前，x在后。比如<code>roi = gray[int(rec[1]):int(rec[3]),int(rec[0]):int(rec[2])]</code>,rec里面是顺序的(xmin,ymin,xmax,ymax)</li>
<li>使用 <code>sklearn.externals.joblib.dump(hog特征，保存路径)</code>的方式存储提取的HOG特征</li>
</ul>
<p>ROI和其HOG特征示例如下（左边为ROI，右边为对应的HOG）：</p>
<p><img src="/images/blog/hog_svm_loc2.jpg" alt="yolov123"> </p>
<p>当前的做法是每次提取一个正样本，就生成5个或者10个负样本。使用<code>generate_neg_box</code>方法。</p>
<h3 id="2-2-SURF特征"><a href="#2-2-SURF特征" class="headerlink" title="2.2 SURF特征"></a>2.2 SURF特征</h3><p>参考</p>
<p> <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html" target="_blank" rel="noopener">opencv3.0+的 surf特征使用</a></p>
<p><a href="http://www.dummies.com/programming/big-data/data-science/how-to-visualize-the-classifier-in-an-svm-supervised-learning-model/" target="_blank" rel="noopener">使用SVM对图像数据分类</a></p>
<p><a href="https://docs.opencv.org/3.4.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html" target="_blank" rel="noopener">opencv surf特征一些参数的解释</a></p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def extract_surf(img,pca &#x3D; 100,visual &#x3D;False):</span><br><span class="line">    image &#x3D; cv2.imread(im)</span><br><span class="line">    image &#x3D; cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)</span><br><span class="line">    surf &#x3D; cv2.xfeatures2d.SURF_create(1000, extended&#x3D;False)</span><br><span class="line">    (kps, descs) &#x3D; surf.detectAndCompute(image, None)</span><br><span class="line">    if visual:</span><br><span class="line">        print(len(kps),descs.shape)</span><br><span class="line">        print(&quot;&#x3D;&#x3D;&quot; * 30)</span><br><span class="line">        print(descs)</span><br><span class="line">        img2 &#x3D; cv2.drawKeypoints(image, kps, None, (255, 0, 0), 4)</span><br><span class="line">        cv2.imshow(&quot;with surf feature&quot;, img2)</span><br><span class="line">        cv2.waitKey(0)</span><br><span class="line">    return kps, descs</span><br></pre></td></tr></table></figure>

<ul>
<li><p>SURF_create:第一个参数是Hessian矩阵阈值，此值越大，生成的特征点越少；第二个参数是extended是是否需要拓展，False时每个特征点维度是64，True时每个特征点维度是128.</p>
</li>
<li><p>detectAndCompute的结果：有两个值kps和descs。其实descs包含kps，descs是一个二维数组，行数即特征点数目（不固定），列数固定为64或128.如下图：</p>
</li>
</ul>
<p><img src="/images/blog/hog_svm_loc3.jpg" alt="yolov123"> </p>
<h2 id="3-训练SVM"><a href="#3-训练SVM" class="headerlink" title="3 训练SVM"></a>3 训练SVM</h2><p>代码在 <a href="">classifier.py</a>中。训练SVM很简单，如下：</p>
<p><img src="/images/blog/hog_svm_loc4.jpg" alt="yolov123"> </p>
<p>过程为：</p>
<ol>
<li><p>添加 <code>(正样本，正标签)</code>和<code>(负样本，负标签)</code>.</p>
</li>
<li><p>声明一个SVM分类器</p>
</li>
<li><p>拟合SVM分类器</p>
</li>
<li><p>保存模型</p>
</li>
</ol>
<p>训练过程非常快。</p>
<h2 id="4-预测-滑窗，分类"><a href="#4-预测-滑窗，分类" class="headerlink" title="4 预测(滑窗，分类)"></a>4 预测(滑窗，分类)</h2><p>测试过程相对麻烦。需要处理几个问题</p>
<ol>
<li>训练的时候的ROI都是固定尺寸的(做了resize)，但是测试的时候可能物体有<strong>尺寸变化</strong>，如何应对尺寸变换-&gt;对原图做图像<strong>金字塔缩放</strong>(会生成大约9张不同尺寸的图)</li>
<li>如何在一张图中<strong>搜索物体</strong>-&gt;使用固定尺寸的<strong>滑窗遍历</strong>图像</li>
<li>临近位置的ROI可能会预测<strong>多个结果</strong>，合并结果-&gt;<strong>NMS</strong></li>
</ol>
<p>代码<code>classifier.py</code>的<code>test_model</code>方法截图：</p>
<p><img src="/images/blog/hog_svm_loc5.jpg" alt="yolov123"><br><img src="/images/blog/hog_svm_loc6.jpg" alt="yolov123"> </p>
<h3 id="4-1-图像金字塔"><a href="#4-1-图像金字塔" class="headerlink" title="4.1 图像金字塔"></a>4.1 图像金字塔</h3><p>注意生成图像金字塔的时候并没有使用<code>skimage.transform.pyramid_gaussian</code>的，因为这种方法对使用HOG特征不太友好。所以此代码中重写了<code>pyramid</code></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><ol>
<li>不能使用opencv的hog描述子来计算和训练，否则在预测的时候会出现数据维度或格式错误’</li>
<li>HOG特征的长度是跟图像的尺寸有关的，所以在计算HOG特征之前要统一resize到固定尺寸才行。虽然HOG特征计算时声称，只跟</li>
<li>使用SVM做二分类的时候要注意，负样本可能需要多一点。不然在预测时会出现很多误判。我刚开始时使用另外一个分类的ROI作为负样本，事实表明效果很差，最后采取了随机在正样本周围取样，效果会变好一点。</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><strong>classifier.py</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line">#encoding:utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">@author:</span><br><span class="line">@time:2017&#x2F;3&#x2F;19 11:08</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">import numpy as np</span><br><span class="line">import glob</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">from skimage.transform import pyramid_gaussian</span><br><span class="line">import imutils</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from skimage.io import imread</span><br><span class="line">from extract_feature_from_fasterrcnn_labeled import get_hog</span><br><span class="line">from skimage import feature</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def train_model(pos_feat_path,neg_feat_path,model_path):</span><br><span class="line">    fds &#x3D; []</span><br><span class="line">    labels &#x3D; []</span><br><span class="line">    # Load the positive features</span><br><span class="line">    for feat_path in glob.glob(os.path.join(pos_feat_path, &quot;*.feat&quot;)):</span><br><span class="line">        fd &#x3D; joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(1)</span><br><span class="line"></span><br><span class="line">    # Load the negative features</span><br><span class="line">    for feat_path in glob.glob(os.path.join(neg_feat_path, &quot;*.feat&quot;)):</span><br><span class="line">        fd &#x3D; joblib.load(feat_path)</span><br><span class="line">        fds.append(fd)</span><br><span class="line">        labels.append(0)</span><br><span class="line"></span><br><span class="line">    clf &#x3D; LinearSVC()</span><br><span class="line">    print(&quot;Training a Linear SVM Classifier&quot;)</span><br><span class="line">    max_len &#x3D; 0</span><br><span class="line">    for fd in fds:</span><br><span class="line">        if len(fd)&gt;max_len:</span><br><span class="line">            max_len &#x3D;len(fd)</span><br><span class="line">    for i in range(len(fds)):</span><br><span class="line">        fd &#x3D; fds[i]</span><br><span class="line">        np.squeeze(fd[i],axis&#x3D;0)</span><br><span class="line">        if len(fd)&lt;max_len:</span><br><span class="line">            fds[i] &#x3D; np.concatenate((fds[i],np.array([0]*(max_len-len(fds[i])))),axis&#x3D;0)</span><br><span class="line">    clf.fit(fds, labels)</span><br><span class="line">    # If feature directories don&#39;t exist, create them</span><br><span class="line">    if not os.path.isdir(os.path.split(model_path)[0]):</span><br><span class="line">        os.makedirs(os.path.split(model_path)[0])</span><br><span class="line">    joblib.dump(clf, model_path)</span><br><span class="line">    print(&quot;Classifier saved to &#123;&#125;&quot;.format(model_path))</span><br><span class="line"></span><br><span class="line">def sliding_window(image, window_size, step_size):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    This function returns a patch of the input image &#96;image&#96; of size equal</span><br><span class="line">    to &#96;window_size&#96;. The first image returned top-left co-ordinates (0, 0)</span><br><span class="line">    and are increment in both x and y directions by the &#96;step_size&#96; supplied.</span><br><span class="line">    So, the input parameters are -</span><br><span class="line">    * &#96;image&#96; - Input Image</span><br><span class="line">    * &#96;window_size&#96; - Size of Sliding Window</span><br><span class="line">    * &#96;step_size&#96; - Incremented Size of Window</span><br><span class="line"></span><br><span class="line">    The function returns a tuple -</span><br><span class="line">    (x, y, im_window)</span><br><span class="line">    where</span><br><span class="line">    * x is the top-left x co-ordinate</span><br><span class="line">    * y is the top-left y co-ordinate</span><br><span class="line">    * im_window is the sliding window image</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    for y in range(0, image.shape[0], step_size[1]):</span><br><span class="line">        for x in range(0, image.shape[1], step_size[0]):</span><br><span class="line">            yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])</span><br><span class="line"></span><br><span class="line">def overlapping_area(detection_1, detection_2):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Function to calculate overlapping area&#39;si</span><br><span class="line">    &#96;detection_1&#96; and &#96;detection_2&#96; are 2 detections whose area</span><br><span class="line">    of overlap needs to be found out.</span><br><span class="line">    Each detection is list in the format -&gt;</span><br><span class="line">    [x-top-left, y-top-left, confidence-of-detections, width-of-detection, height-of-detection]</span><br><span class="line">    The function returns a value between 0 and 1,</span><br><span class="line">    which represents the area of overlap.</span><br><span class="line">    0 is no overlap and 1 is complete overlap.</span><br><span class="line">    Area calculated from -&gt;</span><br><span class="line">    http:&#x2F;&#x2F;math.stackexchange.com&#x2F;questions&#x2F;99565&#x2F;simplest-way-to-calculate-the-intersect-area-of-two-rectangles</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # Calculate the x-y co-ordinates of the</span><br><span class="line">    # rectangles</span><br><span class="line">    x1_tl &#x3D; detection_1[0]</span><br><span class="line">    x2_tl &#x3D; detection_2[0]</span><br><span class="line">    x1_br &#x3D; detection_1[0] + detection_1[3]</span><br><span class="line">    x2_br &#x3D; detection_2[0] + detection_2[3]</span><br><span class="line">    y1_tl &#x3D; detection_1[1]</span><br><span class="line">    y2_tl &#x3D; detection_2[1]</span><br><span class="line">    y1_br &#x3D; detection_1[1] + detection_1[4]</span><br><span class="line">    y2_br &#x3D; detection_2[1] + detection_2[4]</span><br><span class="line">    # Calculate the overlapping Area</span><br><span class="line">    x_overlap &#x3D; max(0, min(x1_br, x2_br)-max(x1_tl, x2_tl))</span><br><span class="line">    y_overlap &#x3D; max(0, min(y1_br, y2_br)-max(y1_tl, y2_tl))</span><br><span class="line">    overlap_area &#x3D; x_overlap * y_overlap</span><br><span class="line">    area_1 &#x3D; detection_1[3] * detection_2[4]</span><br><span class="line">    area_2 &#x3D; detection_2[3] * detection_2[4]</span><br><span class="line">    total_area &#x3D; area_1 + area_2 - overlap_area</span><br><span class="line">    return overlap_area &#x2F; float(total_area)</span><br><span class="line"></span><br><span class="line">def nms(detections, threshold&#x3D;.5):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    This function performs Non-Maxima Suppression.</span><br><span class="line">    &#96;detections&#96; consists of a list of detections.</span><br><span class="line">    Each detection is in the format -&gt;</span><br><span class="line">    [x-top-left, y-top-left, confidence-of-detections, width-of-detection, height-of-detection]</span><br><span class="line">    If the area of overlap is greater than the &#96;threshold&#96;,</span><br><span class="line">    the area with the lower confidence score is removed.</span><br><span class="line">    The output is a list of detections.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    if len(detections) &#x3D;&#x3D; 0:</span><br><span class="line">        return []</span><br><span class="line">    # Sort the detections based on confidence score</span><br><span class="line">    detections &#x3D; sorted(detections, key&#x3D;lambda detections: detections[2],</span><br><span class="line">            reverse&#x3D;True)</span><br><span class="line">    # Unique detections will be appended to this list</span><br><span class="line">    new_detections&#x3D;[]</span><br><span class="line">    # Append the first detection</span><br><span class="line">    new_detections.append(detections[0])</span><br><span class="line">    # Remove the detection from the original list</span><br><span class="line">    del detections[0]</span><br><span class="line">    # For each detection, calculate the overlapping area</span><br><span class="line">    # and if area of overlap is less than the threshold set</span><br><span class="line">    # for the detections in &#96;new_detections&#96;, append the</span><br><span class="line">    # detection to &#96;new_detections&#96;.</span><br><span class="line">    # In either case, remove the detection from &#96;detections&#96; list.</span><br><span class="line">    for index, detection in enumerate(detections):</span><br><span class="line">        for new_detection in new_detections:</span><br><span class="line">            if overlapping_area(detection, new_detection) &gt; threshold:</span><br><span class="line">                del detections[index]</span><br><span class="line">                break</span><br><span class="line">        else:</span><br><span class="line">            new_detections.append(detection)</span><br><span class="line">            del detections[index]</span><br><span class="line">    return new_detections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pyramid(image, scale&#x3D;1.5, minSize&#x3D;(30, 30)):</span><br><span class="line">    # yield the original image</span><br><span class="line">    yield image</span><br><span class="line"></span><br><span class="line">    # keep looping over the pyramid</span><br><span class="line">    while True:</span><br><span class="line">        # compute the new dimensions of the image and resize it</span><br><span class="line">        w &#x3D; int(image.shape[1] &#x2F; scale)</span><br><span class="line">        image &#x3D; imutils.resize(image, width&#x3D;w)</span><br><span class="line"></span><br><span class="line">        # if the resized image does not meet the supplied minimum</span><br><span class="line">        # size, then stop constructing the pyramid</span><br><span class="line">        if image.shape[0] &lt; minSize[1] or image.shape[1] &lt; minSize[0]:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        # yield the next image in the pyramid</span><br><span class="line">        yield image</span><br><span class="line"></span><br><span class="line">def test_model(model_path,image):</span><br><span class="line">    im &#x3D; cv2.cvtColor(cv2.imread(image), cv2.COLOR_RGB2GRAY)</span><br><span class="line">    min_wdw_sz &#x3D; (50, 100)</span><br><span class="line">    step_size &#x3D; (30, 30)</span><br><span class="line">    downscale &#x3D; 1.25</span><br><span class="line">    visualize_det &#x3D; True</span><br><span class="line">    # Load the classifier</span><br><span class="line">    clf &#x3D; joblib.load(model_path)</span><br><span class="line">    visualize_test &#x3D; True</span><br><span class="line">    # List to store the detections</span><br><span class="line">    detections &#x3D; []</span><br><span class="line">    # The current scale of the image</span><br><span class="line">    scale &#x3D; 0</span><br><span class="line">    fourcc &#x3D; cv2.VideoWriter_fourcc(*&#39;XVID&#39;)</span><br><span class="line">    video_out &#x3D; cv2.VideoWriter(&#39;D:&#x2F;data&#x2F;test_carmera&#x2F;svm&#x2F;hog_svm_slide_wid.avi&#39;, -1, 20.0, (480,600))</span><br><span class="line"></span><br><span class="line">    # Downscale the image and iterate</span><br><span class="line">    #for im_scaled in pyramid_gaussian(im, downscale&#x3D;downscale):</span><br><span class="line">    for (i, im_scaled) in enumerate(pyramid(im, scale&#x3D;1.25)):</span><br><span class="line">        # This list contains detections at the current scale</span><br><span class="line">        cd &#x3D; []</span><br><span class="line">        # If the width or height of the scaled image is less than</span><br><span class="line">        # the width or height of the window, then end the iterations.</span><br><span class="line">        if im_scaled.shape[0] &lt; min_wdw_sz[1] or im_scaled.shape[1] &lt; min_wdw_sz[0]:</span><br><span class="line">            break</span><br><span class="line">        for (x, y, im_window) in sliding_window(im_scaled, min_wdw_sz, step_size):</span><br><span class="line">            if im_window.shape[0] !&#x3D; min_wdw_sz[1] or im_window.shape[1] !&#x3D; min_wdw_sz[0]:</span><br><span class="line">               continue</span><br><span class="line">            #Calculate the HOG features</span><br><span class="line">            #fd &#x3D; get_hog(im_window)</span><br><span class="line">            #im_window &#x3D; imutils.auto_canny(im_window)</span><br><span class="line">            im_window &#x3D;  cv2.resize(im_window,(200,250))</span><br><span class="line">            fd &#x3D; feature.hog(im_window, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">            print(fd.shape)</span><br><span class="line">            if len(fd)&gt;1:</span><br><span class="line">                #fd &#x3D; np.transpose(fd)</span><br><span class="line">                fd &#x3D; fd.reshape(1, -1)</span><br><span class="line">                pred &#x3D; clf.predict(fd)</span><br><span class="line">                print(&quot;prediction:\t &quot;,pred)</span><br><span class="line">                if pred &#x3D;&#x3D; 1:# and clf.decision_function(fd)&gt;1:</span><br><span class="line">                    print(&quot;Detection:: Location -&gt; (&#123;&#125;, &#123;&#125;)&quot;.format(x, y))</span><br><span class="line">                    print(&quot;Scale -&gt;  &#123;&#125; | Confidence Score &#123;&#125; \n&quot;.format(scale, clf.decision_function(fd)))</span><br><span class="line">                    detections.append((x, y, clf.decision_function(fd),</span><br><span class="line">                                   int(min_wdw_sz[0] * (downscale ** scale)),</span><br><span class="line">                                   int(min_wdw_sz[1] * (downscale ** scale))))</span><br><span class="line"></span><br><span class="line">                    cd.append(detections[-1])</span><br><span class="line">                # If visualize is set to true, display the working</span><br><span class="line">                # of the sliding window</span><br><span class="line">                if visualize_det:</span><br><span class="line">                    clone &#x3D; im_scaled.copy()</span><br><span class="line">                    for x1, y1, _, _, _ in cd:</span><br><span class="line">                        # Draw the detections at this scale</span><br><span class="line">                        cv2.rectangle(clone, (x1, y1), (x1 + im_window.shape[1], y1 +</span><br><span class="line">                                                        im_window.shape[0]), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">                    cv2.rectangle(clone, (x, y), (x + im_window.shape[1], y +</span><br><span class="line">                                                  im_window.shape[0]), (255, 255, 255), thickness&#x3D;2)</span><br><span class="line">                    cv2.imshow(&quot;Sliding Window in Progress&quot;, clone)</span><br><span class="line">                    out_img_tempt&#x3D; cv2.resize(clone,(480,600))</span><br><span class="line">                    video_out.write(out_img_tempt)</span><br><span class="line">                    cv2.waitKey(10)</span><br><span class="line">        # Move the the next scale</span><br><span class="line">        scale +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    # Display the results before performing NMS</span><br><span class="line">    clone &#x3D; im.copy()</span><br><span class="line">    for (x_tl, y_tl, _, w, h) in detections:</span><br><span class="line">        # Draw the detections</span><br><span class="line">        cv2.rectangle(im, (x_tl, y_tl), (x_tl + w, y_tl + h), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">    cv2.imshow(&quot;Raw Detections before NMS&quot;, im)</span><br><span class="line">    cv2.waitKey()</span><br><span class="line"></span><br><span class="line">    # Perform Non Maxima Suppression</span><br><span class="line">    detections &#x3D; nms(detections, 0.5)</span><br><span class="line"></span><br><span class="line">    # Display the results after performing NMS</span><br><span class="line">    for (x_tl, y_tl, _, w, h) in detections:</span><br><span class="line">        # Draw the detections</span><br><span class="line">        cv2.rectangle(clone, (x_tl, y_tl), (x_tl + w, y_tl + h), (0, 0, 0), thickness&#x3D;2)</span><br><span class="line">    cv2.imshow(&quot;Final Detections after applying NMS&quot;, clone)</span><br><span class="line">    out_img_tempt &#x3D; cv2.resize(clone,(480,600))</span><br><span class="line">    #out_img_tempt[0:clone.shape[0], 0:clone.shape[1]] &#x3D; clone[:, :]</span><br><span class="line">    video_out.write(out_img_tempt)</span><br><span class="line">    cv2.waitKey()</span><br><span class="line">    video_out.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    hog_svm_model &#x3D; r&quot;D:\data\imgs\hog_svm_model_kele.model&quot;</span><br><span class="line">    pos_feat_path &#x3D; r&quot;D:\data\imgs\hog_feats\2&quot;</span><br><span class="line">    neg_feat_path &#x3D; r&quot;D:\data\imgs\hog_feats\2_neg&quot;</span><br><span class="line"></span><br><span class="line">    # hog_svm_model &#x3D; r&quot;D:\data\test_carmera\svm\hog_svm_model.model&quot;</span><br><span class="line">    # pos_feat_path &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats\0&quot;</span><br><span class="line">    # neg_feat_path &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats\0_neg&quot;</span><br><span class="line"></span><br><span class="line">    train_model(pos_feat_path,neg_feat_path,hog_svm_model)</span><br><span class="line">    test_img &#x3D; &quot;D:&#x2F;data&#x2F;test_carmera&#x2F;svm&#x2F;images&#x2F;frmaes_2.jpg&quot;</span><br><span class="line">    test_img1 &#x3D; r&quot;D:\data\imgs\images\0b24fb0ee9947292ffbb88c6e7c22a08.jpg&quot;</span><br><span class="line">    test_model(hog_svm_model,test_img1)</span><br><span class="line"></span><br><span class="line">    # gray &#x3D; cv2.cvtColor(cv2.imread(test_img), cv2.COLOR_BGR2GRAY)</span><br><span class="line">    # edged &#x3D; imutils.auto_canny(gray)</span><br><span class="line">    # cv2.imshow(&quot;edges&quot;,edged)</span><br><span class="line">    # # find contours in the edge map, keeping only the largest one which</span><br><span class="line">    # # is presumed to be the car logo</span><br><span class="line">    # (a,cnts, _) &#x3D; cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">    # c &#x3D; max(cnts, key&#x3D;cv2.contourArea)</span><br><span class="line">    # # extract the logo of the car and resize it to a canonical width</span><br><span class="line">    # # and height</span><br><span class="line">    # (x, y, w, h) &#x3D; cv2.boundingRect(c)</span><br><span class="line">    # logo &#x3D; gray[y:y + h, x:x + w]</span><br><span class="line">    # cv2.imshow(&quot;rect&quot;,logo)</span><br><span class="line">    # cv2.waitKey(0)</span><br><span class="line">    # t0 &#x3D; time.time()</span><br><span class="line">    # clf_type &#x3D; &#39;LIN_SVM&#39;</span><br><span class="line">    # fds &#x3D; []</span><br><span class="line">    # labels &#x3D; []</span><br><span class="line">    # num &#x3D; 0</span><br><span class="line">    # total &#x3D; 0</span><br><span class="line">    # for feat_path in glob.glob(os.path.join(train_feat_path, &#39;*.feat&#39;)):</span><br><span class="line">    #     data &#x3D; joblib.load(feat_path)</span><br><span class="line">    #     fds.append(data[:-1])</span><br><span class="line">    #     labels.append(data[-1])</span><br><span class="line">    # if clf_type is &#39;LIN_SVM&#39;:</span><br><span class="line">    #     clf &#x3D; LinearSVC()</span><br><span class="line">    #     print(&quot;Training a Linear SVM Classifier.&quot;)</span><br><span class="line">    #     clf.fit(fds, labels)</span><br><span class="line">    #     # If feature directories don&#39;t exist, create them</span><br><span class="line">    #     # if not os.path.isdir(os.path.split(model_path)[0]):</span><br><span class="line">    #     #     os.makedirs(os.path.split(model_path)[0])</span><br><span class="line">    #     # joblib.dump(clf, model_path)</span><br><span class="line">    #     # clf &#x3D; joblib.load(model_path)</span><br><span class="line">    #     print(&quot;Classifier saved to &#123;&#125;&quot;.format(model_path))</span><br><span class="line">    #     for feat_path in glob.glob(os.path.join(test_feat_path, &#39;*.feat&#39;)):</span><br><span class="line">    #         total +&#x3D; 1</span><br><span class="line">    #         data_test &#x3D; joblib.load(feat_path)</span><br><span class="line">    #         data_test_feat &#x3D; data_test[:-1].reshape((1, -1))</span><br><span class="line">    #         result &#x3D; clf.predict(data_test_feat)</span><br><span class="line">    #         if int(result) &#x3D;&#x3D; int(data_test[-1]):</span><br><span class="line">    #             num +&#x3D; 1</span><br><span class="line">    #     rate &#x3D; float(num)&#x2F;total</span><br><span class="line">    #     t1 &#x3D; time.time()</span><br><span class="line">    #     print(&#39;The classification accuracy is %f&#39;%rate)</span><br><span class="line">    #     print(&#39;The cast of time is :%f&#39;%(t1-t0))</span><br><span class="line">    #</span><br></pre></td></tr></table></figure>

<p><strong>extract_feature_from_fasterrcnn_labeled.py</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">extract image hog feature from image labeled  for fasterrcnn</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">from skimage import feature,exposure</span><br><span class="line">from config import *</span><br><span class="line">import imutils</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def read_label_info_from_file(txt_file):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    read label information from txt file.</span><br><span class="line">    content of file should looks like:</span><br><span class="line">            9,3</span><br><span class="line">            56,103,261,317,0,yinliao</span><br><span class="line">            261,0,442,120,2,kele</span><br><span class="line">            ...</span><br><span class="line">    :param txt_file:</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    id_and_rect &#x3D; &#123;&#125;</span><br><span class="line">    with open(txt_file,&#39;r&#39;) as label_info:</span><br><span class="line">        lines &#x3D; label_info.readlines()</span><br><span class="line">        for line in lines[1:]:</span><br><span class="line">            infos &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">            xmin,ymin,xmax,ymax,class_id &#x3D; int(infos[0].strip()),int(infos[1].strip()),int(infos[2].strip()),int(infos[3].strip()),str(infos[4].strip())</span><br><span class="line">            if not class_id  in id_and_rect:</span><br><span class="line">                id_and_rect[class_id] &#x3D; [(xmin,ymin,xmax,ymax)]</span><br><span class="line">            else:</span><br><span class="line">                id_and_rect[class_id].append([xmin, ymin, xmax, ymax])</span><br><span class="line">    return id_and_rect</span><br><span class="line"></span><br><span class="line">def generate_neg_box(im_width,im_height,pos_boxes,times &#x3D; 5):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">        generate negative roi from image. negative roi should not in pos_boxes</span><br><span class="line"></span><br><span class="line">    :param im_height:   height of original image</span><br><span class="line">    :param im_width:    width of original image</span><br><span class="line">    :param pos_boxes:   positive roi boxes(xmin,ymin,xmax,ymax)</span><br><span class="line">    :param times:       times of negative vs positive boxes</span><br><span class="line">    :return:            a list of negative roi [(xmin,ymin,xmax,ymax),...]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    neg_boxes &#x3D; []</span><br><span class="line">    min_size &#x3D; min(im_width,im_height)</span><br><span class="line">    mask &#x3D; np.ones((im_width,im_height))</span><br><span class="line">    for (xmin,ymin,xmax,ymax) in pos_boxes:</span><br><span class="line">        mask[xmin:xmax,ymin:ymax] &#x3D; 0</span><br><span class="line"></span><br><span class="line">    for (xmin,ymin,xmax,ymax) in pos_boxes:</span><br><span class="line">        tmp_width &#x3D;  xmax-xmin</span><br><span class="line">        tmp_height &#x3D; ymax-ymin</span><br><span class="line">        # get $times times of negative boxes for every positive box</span><br><span class="line">        for _ in range(times):</span><br><span class="line">            flag &#x3D; True</span><br><span class="line">            neg_x_min &#x3D; 0</span><br><span class="line">            neg_y_min &#x3D; 0</span><br><span class="line">            start_time &#x3D; time.time()</span><br><span class="line">            while flag:</span><br><span class="line">                neg_x_min &#x3D; random.randint(0,min_size-tmp_width)</span><br><span class="line">                neg_y_min &#x3D; random.randint(0,min_size-tmp_height)</span><br><span class="line">                tmp_rect &#x3D; mask[neg_x_min:(neg_x_min+tmp_width),neg_y_min:(neg_y_min+tmp_height)]</span><br><span class="line">                flag &#x3D; np.any(tmp_rect &#x3D;&#x3D; 0)   # overlap with positive boxes</span><br><span class="line">                end_time &#x3D; time.time()</span><br><span class="line">                #print(end_time-start_time)</span><br><span class="line">                if (end_time-start_time)&gt;5:        # if takes more than 10 seconds,this should be stop</span><br><span class="line">                    print(&quot;time takes more than 10 secs&quot;)</span><br><span class="line">                    flag &#x3D; False</span><br><span class="line"></span><br><span class="line">            neg_boxes.append((neg_x_min,neg_y_min,(neg_x_min+tmp_width),(neg_y_min+tmp_height)))</span><br><span class="line"></span><br><span class="line">    return neg_boxes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def extract_hog_feature(img_file,label_info_file,feat_save_path,with_neg &#x3D; False):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    extract hog feature from one image</span><br><span class="line">    attention: one image may contains several classes of object</span><br><span class="line"></span><br><span class="line">    :param img_file:        image to be extracted</span><br><span class="line">    :param label_info_file: object label information file</span><br><span class="line">    :param feat_save_path: where to save the hog feature file</span><br><span class="line">    :return:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    id_and_rect &#x3D; read_label_info_from_file(label_info_file)</span><br><span class="line">    im &#x3D; cv2.imread(img_file)</span><br><span class="line">    gray &#x3D; cv2.cvtColor(im,cv2.COLOR_RGB2GRAY)</span><br><span class="line">    print(gray.shape)</span><br><span class="line">    i &#x3D; 0</span><br><span class="line">    basename &#x3D; os.path.basename(img_file).split(&quot;.&quot;)[0]</span><br><span class="line">    for (id,rect) in id_and_rect.items():</span><br><span class="line">        print(&quot;id&#x3D; &quot;+ str(id))</span><br><span class="line">        target_path &#x3D; os.path.join(feat_save_path,str(id))</span><br><span class="line">        if not os.path.exists(target_path):</span><br><span class="line">            os.mkdir(target_path)</span><br><span class="line">        for rec in rect:</span><br><span class="line">            #print(rec[0],rec[1],rec[2],rec[3])</span><br><span class="line">            roi &#x3D; gray[int(rec[1]):int(rec[3]),int(rec[0]):int(rec[2])]  # caution : the array sequence</span><br><span class="line">            if with_neg and int(id)&lt;3:</span><br><span class="line">                neg_target_path &#x3D; os.path.join(feat_save_path, str(id) + &quot;_neg&quot;)</span><br><span class="line">                if not os.path.exists(neg_target_path):</span><br><span class="line">                    os.mkdir(neg_target_path)</span><br><span class="line">                neg_boxes &#x3D; generate_neg_box(gray.shape[0],gray.shape[1],rect)</span><br><span class="line">                for neg_box in neg_boxes:</span><br><span class="line">                    (xmin, ymin, xmax, ymax) &#x3D; neg_box</span><br><span class="line">                    neg_roi &#x3D; gray[ymin:ymax,xmin:xmax]   # cautious sequence of x and y</span><br><span class="line">                    # print(xmin, ymin, xmax, ymax)</span><br><span class="line">                    # cv2.imshow(&quot;neg_roi&quot;,neg_roi)</span><br><span class="line">                    # cv2.waitKey(0)</span><br><span class="line">                    neg_roi &#x3D; cv2.resize(neg_roi, (200, 250))</span><br><span class="line">                    feat &#x3D; feature.hog(neg_roi, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),</span><br><span class="line">                                       cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">                    joblib.dump(feat, os.path.join(neg_target_path, basename + str(i) + &quot;.feat&quot;))</span><br><span class="line">                    cv2.imwrite(os.path.join(neg_target_path, basename + str(i) + &quot;.jpg&quot;),neg_roi)</span><br><span class="line">                    i &#x3D; i + 1</span><br><span class="line">            # for neg_box in neg_boxes:</span><br><span class="line">            #     (xmin,ymin,xmax,ymax) &#x3D; neg_box</span><br><span class="line">            #     cv2.rectangle(gray,(xmin,ymin),(xmax,ymax),(0,255,0))</span><br><span class="line">            cv2.rectangle(gray,(int(rec[0]),int(rec[1])),(int(rec[2]),int(rec[3])),(255,255,120))</span><br><span class="line">            cv2.imshow(&quot;samples &quot;,gray)</span><br><span class="line">            cv2.imshow(&quot;roi&quot;,roi)</span><br><span class="line">            cv2.waitKey(0)</span><br><span class="line">            roi &#x3D; cv2.resize(roi,(200,250))</span><br><span class="line">            print(&quot;lable is:\t&quot;,id)</span><br><span class="line">            #feat &#x3D; get_hog(roi)</span><br><span class="line">            feat &#x3D; feature.hog(roi, orientations&#x3D;9, pixels_per_cell&#x3D;(10, 10),</span><br><span class="line">                        cells_per_block&#x3D;(2, 2), transform_sqrt&#x3D;True)</span><br><span class="line">            joblib.dump(feat, os.path.join(target_path,basename+str(i)+&quot;.feat&quot;))</span><br><span class="line">            i &#x3D; i+1</span><br><span class="line"></span><br><span class="line">        # cv2.imshow(&quot;samples &quot;,gray)</span><br><span class="line">        # #cv2.imshow(&quot;roi&quot;,roi)</span><br><span class="line">        # cv2.waitKey(0)</span><br><span class="line"></span><br><span class="line">def get_hog(image):</span><br><span class="line">    winSize &#x3D; (64,64)</span><br><span class="line">    image &#x3D; cv2.resize(image,(200,250))</span><br><span class="line">    #winSize &#x3D; (image.shape[1], image.shape[0])</span><br><span class="line">    blockSize &#x3D; (8,8)</span><br><span class="line">    # blockSize &#x3D; (16,16)</span><br><span class="line">    blockStride &#x3D; (8,8)</span><br><span class="line">    cellSize &#x3D; (8,8)</span><br><span class="line">    nbins &#x3D; 9</span><br><span class="line">    derivAperture &#x3D; 1</span><br><span class="line">    winSigma &#x3D; 4.</span><br><span class="line">    histogramNormType &#x3D; 0</span><br><span class="line">    L2HysThreshold &#x3D; 2.0000000000000001e-01</span><br><span class="line">    gammaCorrection &#x3D; 0</span><br><span class="line">    nlevels &#x3D; 64</span><br><span class="line">    hog &#x3D; cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,</span><br><span class="line">                            histogramNormType,L2HysThreshold,gammaCorrection,nlevels)</span><br><span class="line">    winStride &#x3D; (8,8)</span><br><span class="line">    padding &#x3D; (8,8)</span><br><span class="line">    locations &#x3D; [] # (10, 10)# ((10,20),)</span><br><span class="line">    hist &#x3D; hog.compute(image,winStride,padding,locations)</span><br><span class="line">    return hist</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # img_dir &#x3D; r&quot;D:\data\imgs\images&quot;</span><br><span class="line">    # feat_dir &#x3D; r&quot;D:\data\imgs\hog_feats&quot;</span><br><span class="line">    img_dir &#x3D; r&quot;D:\data\test_carmera\svm\images&quot;</span><br><span class="line">    feat_dir &#x3D; r&quot;D:\data\test_carmera\svm\hog_feats&quot;</span><br><span class="line">    for file in os.listdir(img_dir):</span><br><span class="line">        image &#x3D; os.path.join(img_dir,file)</span><br><span class="line">        txt_file &#x3D; image.replace(&quot;images&quot;,&quot;labels&quot;).replace(&quot;.jpg&quot;,&quot;.xml.txt&quot;)</span><br><span class="line">        extract_hog_feature(image,txt_file,feat_dir,with_neg &#x3D;True)</span><br><span class="line">        print(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;    extract hog feature from file %s  done..&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;%image)</span><br><span class="line">    print(&quot;&#x3D;&#x3D;&#x3D;&#x3D;all file have extract done...&quot;)</span><br></pre></td></tr></table></figure>

<p>参考:</p>
<p><a href="http://blog.csdn.net/yjl9122/article/details/72765959" target="_blank" rel="noopener">http://blog.csdn.net/yjl9122/article/details/72765959</a></p>
<p><a href="https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/</a></p>
<p><a href="https://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-10-02-opencv-svm-hog-loc/" data-id="ck4ifvdga0045ywje5q0ycg75" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-09-26-merlin-mandarin-text-process-work" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-09-26-merlin-mandarin-text-process-work/" class="article-date">
  <time datetime="2019-12-23T10:45:59.657Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-09-26-merlin-mandarin-text-process-work/">merlin语音合成中文前端处理2-实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="0-数据示例"><a href="#0-数据示例" class="headerlink" title="0  数据示例"></a>0  数据示例</h2><p>以 THSCH-30数据集为例子。THSCH-30数据集分为两部分<strong>音频</strong>和<strong>文本</strong>。音频文件列表如下:</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_1.png" alt="mtts_mandarin_text">  </p>
<p>文本内容，全部文本存放在一个文件内。内容如下：</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_2.png" alt="mtts_mandarin_text">  </p>
<h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1 数据预处理"></a>1 数据预处理</h2><p>我们先处理文本内容，第一步是数据预处理。主要完成以下操作：</p>
<ol>
<li>去除所有标点符号。</li>
<li>去掉所有数字和字母</li>
<li>替换所有句子结束的标点符号为 <code>#4</code>，即 <code>re.sub(&#39;[,.，。]&#39;, &#39;#4&#39;, txt)</code>。其中的<code>逗号</code>,<code>点号</code>,<code>句号</code>,都替换为符号<code>#4</code>，此处的<code>#4</code>代表了不同的韵律标注层次。具体的不同层次，参考下面的说明</li>
</ol>
<ul>
<li><strong>#0</strong>:  stands for word segment</li>
<li><strong>#1</strong> : stands for prosodic word</li>
<li><strong>#2</strong>:  stands for stressful word (actually in this project we regrad it as #1)</li>
<li><strong>#3</strong>:  stands for prosodic phrase</li>
<li><strong>#4</strong>:   stands for intonational phrase</li>
</ul>
<p>以 下面这句为示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;A11_0 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然&#39;</span><br></pre></td></tr></table></figure>
<p>经过第一步的处理，其中<code>A11_0</code>是句子编号，不会被处理。以空格分割之后会处理后面的中文文本，经过处理，后面的文本没有变化。没有数字和字母，也没有标点符号。</p>
<h2 id="2-添加拼音"><a href="#2-添加拼音" class="headerlink" title="2 添加拼音"></a>2 添加拼音</h2><p>第二步是给每个中文添加拼音标注。使用的是<code>pypinyin</code>。示例代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">numstr, txt &#x3D; line.split(&#39; &#39;)</span><br><span class="line">       txt &#x3D; re.sub(&#39;#\d&#39;, &#39;&#39;, txt)</span><br><span class="line">       pinyin_list &#x3D; pinyin(txt, style&#x3D;Style.TONE3)</span><br><span class="line">       new_pinyin_list &#x3D; []</span><br><span class="line">       for item in pinyin_list:</span><br><span class="line">           if not item:</span><br><span class="line">               logger.warning(</span><br><span class="line">                   &#39;&#123;file_num&#125; do not generate right pinyin&#39;.format(numstr))</span><br><span class="line">           if not item[0][-1].isdigit(): # 对于没有 声调的拼音，统一添加声调5</span><br><span class="line">               phone &#x3D; item[0] + &#39;5&#39;</span><br><span class="line">           else:</span><br><span class="line">               phone &#x3D; item[0]</span><br><span class="line">           new_pinyin_list.append(phone)</span><br><span class="line">       lab_file &#x3D; os.path.join(wav_dir_path, numstr + &#39;.lab&#39;)</span><br><span class="line">       with open(lab_file, &#39;w&#39;) as oid:</span><br><span class="line">           oid.write(&#39; &#39;.join(new_pinyin_list))</span><br></pre></td></tr></table></figure>
<p>这样，上面的一句中文对应的拼音内容为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2</span><br></pre></td></tr></table></figure>
<p>可以看到，每个字都有对应的拼音。其中，有一步处理是给没有音调的字统一添加为音调5。有些词比如<code>的</code>是没有音调的，统一被添加音调5。这一步会将拼音内容写入一个标注文本<code>A11_0.lab</code></p>
<h2 id="3-强制对齐"><a href="#3-强制对齐" class="headerlink" title="3 强制对齐"></a>3 强制对齐</h2><p>这一步使用了语音识别模型，对语音进行强制对齐。语音识别模型的作用是识别语音中每个字的发音起止时间，并存储为TextGrid格式，这个格式是语音标注软件Praat的标注格式。<br>此步骤依赖以下内容：</p>
<h3 id="3-1-拼音词典"><a href="#3-1-拼音词典" class="headerlink" title="3.1 拼音词典"></a>3.1 拼音词典</h3><p>此步骤依赖的拼音词典是 <code>mandarin_mtts.lexicon</code>，其中的内容如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a1 a1</span><br><span class="line">a2 a2</span><br><span class="line">a3 a3</span><br><span class="line">a4 a4</span><br><span class="line">a5 a5</span><br><span class="line">ai1 ai1</span><br><span class="line">ai2 ai2</span><br><span class="line">ai3 ai3</span><br><span class="line">ai4 ai4</span><br><span class="line">ai5 ai5</span><br><span class="line">an1 an1</span><br><span class="line">an2 an2</span><br><span class="line">an3 an3</span><br><span class="line">an4 an4</span><br><span class="line">an5 an5</span><br><span class="line">ang1 ang1</span><br><span class="line">ang2 ang2</span><br><span class="line">ang3 ang3</span><br><span class="line">ang4 ang4</span><br><span class="line">ang5 ang5</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="3-2-强制对齐工具"><a href="#3-2-强制对齐工具" class="headerlink" title="3.2 强制对齐工具"></a>3.2 强制对齐工具</h3><p>MTTS所使用的强制对齐工具为 <a href="https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/releases/download/v1.0.0/montreal-forced-aligner_linux.tar.gz" target="_blank" rel="noopener">Montreal-Forced-Aligner</a></p>
<h3 id="3-3-强制对齐模型"><a href="#3-3-强制对齐模型" class="headerlink" title="3.3 强制对齐模型"></a>3.3 强制对齐模型</h3><p><strong>预备知识</strong></p>
<p>汉字按照长度可以划分为：<strong>句子</strong>，<strong>短语</strong>，<strong>汉字（音节）</strong>，<strong>音素</strong>。而音素由<strong>声母</strong>，<strong>韵母</strong>，<strong>元音</strong>，<strong>静音</strong>组成。</p>
<p>对齐模型使用的是由THSCH-30数据集所训练的中文语音识别模型，下载地址为 <a href="https://github.com/Jackiexiao/MTTS/releases/download/v0.1/thchs30.zip" target="_blank" rel="noopener">THSCH-30语音识别模型</a></p>
<h3 id="3-4-输出结果"><a href="#3-4-输出结果" class="headerlink" title="3.4 输出结果"></a>3.4 输出结果</h3><p>这一步主要是对语音音频文件处理，并得到识别结果。识别的结果是<strong>直接到音素</strong>，存储为TextGrid格式，示例文本对应的音频文件 <code>A11_0.wav</code>所得到的TextGrid标注文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">File type &#x3D; &quot;ooTextFile&quot;</span><br><span class="line">Object class &#x3D; &quot;TextGrid&quot;</span><br><span class="line"></span><br><span class="line">xmin &#x3D; 0.0</span><br><span class="line">xmax &#x3D; 7.8</span><br><span class="line">tiers? &lt;exists&gt;</span><br><span class="line">size &#x3D; 2</span><br><span class="line">item []:</span><br><span class="line">    item [1]:</span><br><span class="line">        class &#x3D; &quot;IntervalTier&quot;</span><br><span class="line">        name &#x3D; &quot;words&quot;</span><br><span class="line">        xmin &#x3D; 0.0</span><br><span class="line">        xmax &#x3D; 7.8</span><br><span class="line">        intervals: size &#x3D; 33</span><br><span class="line">            intervals [1]:</span><br><span class="line">                xmin &#x3D; 0.0</span><br><span class="line">                xmax &#x3D; 1.100</span><br><span class="line">                text &#x3D; &quot;&quot;</span><br><span class="line">            intervals [2]:</span><br><span class="line">                xmin &#x3D; 1.100</span><br><span class="line">                xmax &#x3D; 1.350</span><br><span class="line">                text &#x3D; &quot;lv4&quot;</span><br><span class="line">            intervals [3]:</span><br><span class="line">                xmin &#x3D; 1.350</span><br><span class="line">                xmax &#x3D; 1.460</span><br><span class="line">                text &#x3D; &quot;shi4&quot;</span><br><span class="line">         ....</span><br><span class="line">            intervals [59]:</span><br><span class="line">                xmin &#x3D; 6.980</span><br><span class="line">                xmax &#x3D; 7.150</span><br><span class="line">                text &#x3D; &quot;ang4&quot;</span><br><span class="line">            intervals [60]:</span><br><span class="line">                xmin &#x3D; 7.150</span><br><span class="line">                xmax &#x3D; 7.280</span><br><span class="line">                text &#x3D; &quot;r&quot;</span><br><span class="line">            intervals [61]:</span><br><span class="line">                xmin &#x3D; 7.280</span><br><span class="line">                xmax &#x3D; 7.430</span><br><span class="line">                text &#x3D; &quot;an2&quot;</span><br><span class="line">            intervals [62]:</span><br><span class="line">                xmin &#x3D; 7.430</span><br><span class="line">                xmax &#x3D; 7.780</span><br><span class="line">                text &#x3D; &quot;sp&quot;</span><br><span class="line">            intervals [63]:</span><br><span class="line">                xmin &#x3D; 7.780</span><br><span class="line">                xmax &#x3D; 7.8</span><br><span class="line">                text &#x3D; &quot;&quot;</span><br></pre></td></tr></table></figure>

<p>可以看到，一共识别到了64个字。其中解析如下：</p>
<ul>
<li>*<em>xmin *</em>:当前字的开始时间，单位为秒</li>
<li><strong>xmax</strong>：当前字的结束时间，单位为秒</li>
<li><strong>text</strong>：当前字的拼音和音调。</li>
</ul>
<h2 id="4-TextGrid标注格式转换为SFS格式"><a href="#4-TextGrid标注格式转换为SFS格式" class="headerlink" title="4 TextGrid标注格式转换为SFS格式"></a>4 TextGrid标注格式转换为SFS格式</h2><p>SFS即为声韵母标注，主要将每个字的音素标注为以下三类：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>s</strong></td>
<td>时长超过100ms的静音</td>
<td>sil,sp</td>
</tr>
<tr>
<td><strong>d</strong></td>
<td>时长短于100ms的静音</td>
<td>-</td>
</tr>
<tr>
<td><strong>a</strong></td>
<td>辅音</td>
<td>包含```b’, ‘p’, ‘m’, ‘f’, ‘d’, ‘t’, ‘n’, ‘l’, ‘g’, ‘k’, ‘h’, ‘j’, ‘q’, ‘x’, ‘zh’, ‘ch’, ‘sh’, ‘r’, ‘z’, ‘c’, ‘s’, ‘y’, ‘w’</td>
</tr>
<tr>
<td><strong>b</strong></td>
<td>元音</td>
<td>-</td>
</tr>
</tbody></table>
<p>同时将发音起止时间的单位从秒更改为 纳秒，即乘以10的6次方。上一步得到的TextGrid格式的标注转换为sfs格式之后，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">11000000 s</span><br><span class="line">12400000 a</span><br><span class="line">13500000 b</span><br><span class="line">....</span><br><span class="line">67500000 b</span><br><span class="line">68500000 a</span><br><span class="line">69800000 b</span><br><span class="line">71500000 b</span><br><span class="line">72800000 a</span><br><span class="line">74300000 b</span><br><span class="line">77800000 s</span><br></pre></td></tr></table></figure>

<h2 id="5-sfs到真实标注文件"><a href="#5-sfs到真实标注文件" class="headerlink" title="5 sfs到真实标注文件"></a>5 sfs到真实标注文件</h2><p>此步骤依赖于<strong>sfs标注文件</strong>和<strong>原始中文文本</strong>。上面的步骤示例中的A11_0文本和sfs标注即可。得到的标注结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">12400000 13500000 sil^l-v4+sh&#x3D;ih4@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">13500000 14300000 l^v4-sh+ih4&#x3D;y@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">14300000 14600000 v4^sh-ih4+y&#x3D;iang2@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">...</span><br><span class="line">66900000 67500000 ei4^sh-ih1+y&#x3D;i4@ih@&#x2F;A:4-1^4@&#x2F;B:26+3@1^2^27+4#27-4-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">67500000 68500000 sh^ih1-y+i4&#x3D;ang4@i@&#x2F;A:1-4^4@&#x2F;B:27+2@2^1^28+3#28-3-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">68500000 69800000 ih1^y-i4+ang4&#x3D;r@i@&#x2F;A:1-4^4@&#x2F;B:27+2@2^1^28+3#28-3-&#x2F;C:a_n^z#2+2+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">69800000 71500000 y^i4-ang4+r&#x3D;an2@ang@&#x2F;A:4-4^2@&#x2F;B:28+1@1^2^29+2#29-2-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">71500000 72800000 i4^ang4-r+an2&#x3D;sil@an@&#x2F;A:4-2^xx@&#x2F;B:29+0@2^1^30+1#30-1-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">72800000 74300000 ang4^r-an2+sil&#x3D;xx@an@&#x2F;A:4-2^xx@&#x2F;B:29+0@2^1^30+1#30-1-&#x2F;C:n_z^xx#2+2+xx&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">74300000 77800000 r^an2-sil+xx&#x3D;xx@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br></pre></td></tr></table></figure>

<h3 id="5-1-中文分词，词性标注，韵律标注"><a href="#5-1-中文分词，词性标注，韵律标注" class="headerlink" title="5.1 中文分词，词性标注，韵律标注"></a>5.1 中文分词，词性标注，韵律标注</h3><p>如果原中文内容里没有进行韵律标注，韵律标注以<strong>#</strong>分割。就会默认所有的分词结果里的每个词都是<strong>#0</strong>，但是最后一个是<code>#4</code>,即最后一个代表当前语句结束。参照第数据预处理部分的韵律不同值代表的不同意义。<br>第一步是对输入的原始中文进行分词，还是以上面步骤的示例文本为例，使用jieba分词的posseg分词和词性标注得到的结果如下：</p>
<ul>
<li><strong>分词结果</strong>：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;绿&#39;, &#39;是&#39;, &#39;阳春&#39;, &#39;烟景&#39;, &#39;大块文章&#39;, &#39;的&#39;, &#39;底色&#39;, &#39;四月&#39;, &#39;的&#39;, &#39;林峦&#39;, &#39;更是&#39;, &#39;绿&#39;, &#39;得&#39;, &#39;鲜活&#39;, &#39;秀媚&#39;, &#39;诗意&#39;, &#39;盎然&#39;]</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>词性标注结果</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;a&#39;, &#39;v&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;u&#39;, &#39;n&#39;, &#39;m&#39;, &#39;u&#39;, &#39;n&#39;, &#39;d&#39;, &#39;a&#39;, &#39;u&#39;, &#39;a&#39;, &#39;a&#39;, &#39;n&#39;, &#39;z&#39;]</span><br></pre></td></tr></table></figure></li>
<li><p><strong>韵律标注结果</strong>： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#4&#39;] （当前没有预标注韵律，默认所有分词都是#0，且最后一个为#4）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>音节分解结果</strong>：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#39;l&#39;, &#39;v4&#39;), (&#39;sh&#39;, &#39;ih4&#39;), (&#39;y&#39;, &#39;iang2&#39;), (&#39;ch&#39;, &#39;un1&#39;), (&#39;y&#39;, &#39;ian1&#39;), (&#39;j&#39;, &#39;ing3&#39;), (&#39;d&#39;, &#39;a4&#39;), (&#39;k&#39;, &#39;uai4&#39;), (&#39;w&#39;, &#39;uen2&#39;), (&#39;zh&#39;, &#39;ang1&#39;), (&#39;d&#39;, &#39;e5&#39;), (&#39;d&#39;, &#39;i3&#39;), (&#39;s&#39;, &#39;e4&#39;), (&#39;s&#39;, &#39;ic4&#39;), (&#39;y&#39;, &#39;ve4&#39;), (&#39;d&#39;, &#39;e5&#39;), (&#39;l&#39;, &#39;in2&#39;), (&#39;l&#39;, &#39;uan2&#39;), (&#39;g&#39;, &#39;eng4&#39;), (&#39;sh&#39;, &#39;ih4&#39;), (&#39;l&#39;, &#39;v4&#39;), (&#39;d&#39;, &#39;e2&#39;), (&#39;x&#39;, &#39;ian1&#39;), (&#39;h&#39;, &#39;uo2&#39;), (&#39;x&#39;, &#39;iu4&#39;), (&#39;m&#39;, &#39;ei4&#39;), (&#39;sh&#39;, &#39;ih1&#39;), (&#39;y&#39;, &#39;i4&#39;), (&#39;ang4&#39;,), (&#39;r&#39;, &#39;an2&#39;)]</span><br></pre></td></tr></table></figure>

<h3 id="5-2-获取音素类型和时间"><a href="#5-2-获取音素类型和时间" class="headerlink" title="5.2 获取音素类型和时间"></a>5.2 获取音素类型和时间</h3><p>分为两种情况，有sfs标注文件的和没有的。</p>
<p><strong>使用sfs标注文件</strong><br>sfs标注文件中每一行都是如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11000000 s</span><br><span class="line">12400000 a</span><br><span class="line">13500000 b</span><br><span class="line">..</span><br></pre></td></tr></table></figure>

<p>每一行以空格分割，分别代表了当前第i个音素的<strong>开始时间</strong>和<strong>音素类型</strong>。音素类型参考第4节的表。分别读取并保存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">音素类型列表：phs_type &#x3D;   [&#39;s&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;s&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;s&#39;]</span><br><span class="line">音素起止时间列表: times &#x3D; [&#39;0&#39;, 11000000, 12400000, 13500000, 14300000, 14600000, 15100000, 16500000, 17600000, 18300000, 19000000, 20400000, 21400000, 23100000, 24100000, 25200000, 26200000, 26900000, 27799999, 28700000, 29600000, 30299999, 31000000, 31500000, 32200000, 33200000, 34800000, 37400000, 37700000, 39100000, 39900000, 40700000, 41500000, 42200000, 42699999, 43500000, 44300000, 45500000, 47300000, 48300000, 49100000, 50900000, 52000000, 53000000, 53700000, 54300000, 54600000, 56000000, 57100000, 58700000, 59900000, 61500000, 62699999, 63900000, 65199999, 66900000, 67500000, 68500000, 69800000, 71500000, 72800000, 74300000, 77800000]</span><br></pre></td></tr></table></figure>
<p>音素起止时间列表(63)比音素类型列表(62)多一个，开始时间0。</p>
<p><strong>没有sfs标注文件</strong></p>
<p>如果没有sfs标注的时间，程序可以自动生成,方法是计算得到语句的所有因素长度，并将默认起止时间都设置为0.，音素类型都默认设置为<code>a</code>。</p>
<p>参考韵律列表。韵律列表其实就是当前语句被分词之后，每个词的停顿间隙。以<strong>#0,#1,#2,#3,#4</strong>标识，分别代表了不同层次的韵律。当前示例语句被分词为如下</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_3.png" alt="mtts_mandarin_text">  </p>
<p>原始语句如下，没有带任何标点符号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然</span><br></pre></td></tr></table></figure>
<p>分词结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">语句分词结果：  [&#39;绿&#39;, &#39;是&#39;, &#39;阳春&#39;, &#39;烟景&#39;, &#39;大块文章&#39;, &#39;的&#39;, &#39;底色&#39;, &#39;四月&#39;, &#39;的&#39;, &#39;林峦&#39;, &#39;更是&#39;, &#39;绿&#39;, &#39;得&#39;, &#39;鲜活&#39;, &#39;秀媚&#39;, &#39;诗意&#39;, &#39;盎然&#39;]</span><br></pre></td></tr></table></figure>
<p>对应的韵律列表如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#0&#39;, &#39;#4&#39;]</span><br></pre></td></tr></table></figure>
<p>比如对分词结果 <code>阳春</code>得到的音素为列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#39;y&#39;, &#39;iang2&#39;), (&#39;ch&#39;, &#39;un1&#39;)]</span><br></pre></td></tr></table></figure>
<p>长度为4，向音素类型列表中添加对应长度的默认值<code>a</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">phs_type &#x3D;  [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;] (后面四个是当前词&#96;阳春&#96;的音素对应的音素类型)</span><br></pre></td></tr></table></figure>
<p>最后得到的音素起止时间列表和音素类型列表分别为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">time&#x3D;  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">phs_type&#x3D;[&#39;s&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;s&#39;]</span><br></pre></td></tr></table></figure>
<p>可以看到，与有sfs对比，长度一致。都是对应音素数目的长度，只不过没有sfs文件的都是默认值。</p>
<h2 id="6-音素状态决策树"><a href="#6-音素状态决策树" class="headerlink" title="6 音素状态决策树"></a>6 音素状态决策树</h2><p>上面已经得到了音素对应的起止时间和音素类型，下一步是构建音素状态决策树。以英文为示意图，如下：</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_4.png" alt="mtts_mandarin_text">  </p>
<ol>
<li>首先，一整句话是有前后承接关系的，当<strong><em>绿是阳春</em></strong>这几个词出现时，<code>绿</code>字是<code>是</code>前缀，<code>阳</code>字是<code>是</code>字的后缀。这只是字面上的上下文关系，当前需要构建音素级别的上下文承接关系。所以需要进一步细化。</li>
<li>韵律标注层次有 ,由小到大（以<a href="https://github.com/Jackiexiao/MTTS" target="_blank" rel="noopener">MTTS</a>前端为例，其他标注格式不一定，<a href="http://www.data-baker.com/open_source.html" target="_blank" rel="noopener">标贝数据</a>只有,#1,#2,#3,#4）。<ul>
<li>音素: phn</li>
<li>音节:syllables</li>
<li>词:   #0</li>
<li>短语:  #1</li>
<li>句子: #3</li>
<li>句子结束: #4 </li>
</ul>
</li>
</ol>
<h3 id="6-1-MTTS的上下文标注"><a href="#6-1-MTTS的上下文标注" class="headerlink" title="6.1 MTTS的上下文标注"></a>6.1 MTTS的上下文标注</h3><p><strong>说明</strong></p>
<ul>
<li>没有设计语调短语层和段落层</li>
<li>也没有设置重音标注</li>
<li>@&amp;#$!^-+=以及/A:/B:…的使用主要是为了正则表达式匹配方便，10个符号(@&amp;#$!^-+=)共有100个匹配组合，即可以匹配100个属性</li>
<li>如果前后位置的基元不存在的话，用xx代替，例如 xx^sil-w+o=sh </li>
</ul>
<p>标注文件会标记不同韵律层次所有的上下文信息，详细可以参考下面的两张表：</p>
<p><strong>不同层级和对应的标注格式</strong></p>
<table>
<thead>
<tr>
<th>层级</th>
<th>标注格式</th>
</tr>
</thead>
<tbody><tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6@</td>
</tr>
<tr>
<td>声调层</td>
<td>/A:a1-a2^a3@</td>
</tr>
<tr>
<td>字/音节层</td>
<td>/B:b1+b2@b3^b4^b5+b6#b7-b8-</td>
</tr>
<tr>
<td>词层</td>
<td>/C:c1_c2^c3#c4+c5+c6&amp;</td>
</tr>
<tr>
<td>韵律词层</td>
<td>/D:d1=d2!d3@d4-d5&amp;</td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/E:e1</td>
</tr>
<tr>
<td>语句层</td>
<td>/F:f1^f2=f3_f4-f5!</td>
</tr>
</tbody></table>
<p>更加细致的划分（基元代表了不同层次的单元，可以是音素，也可以是音节，声调等）</p>
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前音节的元音</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>a1</td>
<td>前一音节/字的声调</td>
</tr>
<tr>
<td>a2</td>
<td>当前音节/字的声调</td>
</tr>
<tr>
<td>a3</td>
<td>后一音节/字的声调</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节/字到语句开始字的距离</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节/字到语句结束字的距离</td>
</tr>
<tr>
<td>b3</td>
<td>当前音节/字在词中的位置（正序）</td>
</tr>
<tr>
<td>b4</td>
<td>当前音节/字在词中的位置（倒序）</td>
</tr>
<tr>
<td>b5</td>
<td>当前音节/字在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节/字在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节/字在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节/字在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>c1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>c2</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>c3</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>c4</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>c5</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>c6</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>d1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>d2</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>d3</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>d4</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>d5</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>e1</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>e2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>e4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e5</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e6</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>e7</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>e8</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>f1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>f2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>f3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>f4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>f5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody></table>
<p>以英文单词 author为例</p>
<p><img src="/images/blog/mtts_mandarin_voice_text_5.png" alt="mtts_mandarin_text">  </p>
<p>体现在代码里面的标准公式化字符串如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">formation&#x3D;[</span><br><span class="line">    &#39; &#39;, &#39; &#39;,                                      #  开始时间，结束时间（上图中没有出现）</span><br><span class="line">    &#39;^&#39;, &#39;-&#39;, &#39;+&#39;, &#39;&#x3D;&#39;, &#39;@&#39;, &#39;@&#x2F;A:&#39;,                 # 连接符</span><br><span class="line">    &#39;-&#39;, &#39;^&#39;, &#39;@&#x2F;B:&#39;, </span><br><span class="line">    &#39;+&#39;, &#39;@&#39;, &#39;^&#39;, &#39;^&#39;, &#39;+&#39;, &#39;#&#39;, &#39;-&#39;, &#39;-&#x2F;C:&#39;, </span><br><span class="line">    &#39;_&#39;, &#39;^&#39;, &#39;#&#39;, &#39;+&#39;, &#39;+&#39;, &#39;&amp;&#x2F;D:&#39;, </span><br><span class="line">    &#39;&#x3D;&#39;, &#39;!&#39;, &#39;@&#39;, &#39;-&#39;, &#39;&amp;&#x2F;E:&#39;, </span><br><span class="line">    &#39;|&#39;, &#39;-&#39;, &#39;@&#39;, &#39;#&#39;, &#39;&amp;&#39;, &#39;!&#39;, &#39;-&#39;, &#39;#&#x2F;F:&#39;, </span><br><span class="line">    &#39;^&#39;, &#39;&#x3D;&#39;, &#39;_&#39;, &#39;-&#39;, &#39;!&#39;]</span><br></pre></td></tr></table></figure>
<p>示例中文的标注结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br><span class="line">12400000 13500000 sil^l-v4+sh&#x3D;ih4@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!&#39;</span><br><span class="line">13500000 14300000 l^v4-sh+ih4&#x3D;y@ih@&#x2F;A:4-4^2@&#x2F;B:1+28@1^1^2+29#2-29-&#x2F;C:a_v^n#1+1+2&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!&#39;</span><br><span class="line">...</span><br><span class="line">74300000 77800000 r^an2-sil+xx&#x3D;xx@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!&#39;</span><br></pre></td></tr></table></figure>
<p>以第一行和第二行为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 11000000 xx^xx-sil+l&#x3D;v4@xx@&#x2F;A:xx-xx^xx@&#x2F;B:xx+xx@xx^xx^xx+xx#xx-xx-&#x2F;C:xx_xx^xx#xx+xx+xx&amp;&#x2F;D:xx&#x3D;xx!xx@xx-xx&amp;&#x2F;E:xx|xx-xx@xx#xx&amp;xx!xx-xx#&#x2F;F:xx^xx&#x3D;xx_xx-xx!</span><br><span class="line">11000000 12400000 xx^sil-l+v4&#x3D;sh@v@&#x2F;A:xx-4^4@&#x2F;B:0+29@1^1^1+30#1-30-&#x2F;C:xx_a^v#xx+1+1&amp;&#x2F;D:xx&#x3D;30!xx@1-1&amp;&#x2F;E:xx|30-xx@xx#1&amp;xx!1-1#&#x2F;F:xx^30&#x3D;17_1-1!</span><br></pre></td></tr></table></figure>

<ul>
<li><p>注意：第一行其实是静音。因为任何一句话的开头都是静音，所以第一行的所有标注基元都是<strong>xx</strong>，代表了不存在的基元。</p>
</li>
<li><p>第二行才是字<strong>绿</strong>的开始，首先<strong>绿</strong>字被拆分为音素<strong>l</strong>和<strong>v4</strong></p>
</li>
<li><p>11000000和12400000代表了该字的开始和结束时间。</p>
</li>
<li><p><strong>xx^sil-l+v4=sh@v</strong>,是声母韵母层级的标注，(上表中的p)依次是：</p>
<ul>
<li><code>前前基元</code>（不存在，以xx代表）</li>
<li><code>连接符</code> :<strong>^</strong></li>
<li><code>前一基元</code>:<strong>sil</strong>（一句话的开始都是sil）</li>
<li><code>连接符</code>:<strong>-</strong></li>
<li><code>当前基元</code>:<strong>l</strong></li>
<li><code>连接符</code> :<em>*+</em></li>
<li><code>后一基元</code>: <strong>v4</strong></li>
<li><code>连接符</code>:<strong>=</strong></li>
<li><code>后后基元</code>:<strong>sh)</strong></li>
<li><code>连接符</code>:<strong>@</strong></li>
<li><code>当前音节的元音</code>:<strong>v</strong></li>
</ul>
</li>
<li><p><strong>@/A:xx-4^4</strong>，是声调层级标注(上标中的a): 依次是</p>
<ul>
<li><code>字调标注的开始</code>:<strong>@/A</strong></li>
<li><code>前一音节的字调</code>: <strong>xx</strong> (不存在的xx)</li>
<li><code>连接符</code>:<strong>-</strong></li>
<li><code>当前音节的字调</code>:<strong>4</strong>((绿)4)</li>
<li><code>连接符</code>:<strong>^</strong></li>
<li><code>后一个音节的字调</code>:<strong>4</strong>((是)4)</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-09-26-merlin-mandarin-text-process-work/" data-id="ck4ifvdg70041ywje0lf76tny" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-09-25-merlin-mandarin-text-process" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-09-25-merlin-mandarin-text-process/" class="article-date">
  <time datetime="2019-12-23T10:45:59.656Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-09-25-merlin-mandarin-text-process/">merlin语音合成中文前端处理1-理论</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="0-概述"><a href="#0-概述" class="headerlink" title="0 概述"></a>0 概述</h2><p>语音合成过程，需要处理两部分内容，分别是：</p>
<ul>
<li><p>文本(Text)处理： 假设我们的输入是<code>你好看啊</code></p>
</li>
<li><p>音频(speech)处理： 对应<code>你好看啊.wav</code></p>
</li>
</ul>
<h2 id="1-文本处理"><a href="#1-文本处理" class="headerlink" title="1 文本处理"></a>1 文本处理</h2><h3 id="1-1-规范化"><a href="#1-1-规范化" class="headerlink" title="1.1 规范化"></a>1.1 规范化</h3><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等</p>
<p>有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音[7]。例如</p>
<ul>
<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”；</li>
<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>
</ul>
<p>对于英文而言，如：</p>
<ul>
<li><strong>类别为年份（NYER）</strong>： 2011 $\rightarrow$ twenty eleven</li>
<li><strong>类别为货币(MONEY)</strong>: £100 $\rightarrow$  one hundred pounds</li>
<li><strong>类别为非单词，需要拟音(ASWD)</strong>:  IKEA $\rightarrow$  apply letter-to-sound</li>
<li><strong>类别为数字(NUM)</strong> : 100 NUM $\rightarrow$ one hundred</li>
<li><strong>类别为字母(LSEQ)</strong> :  DVD  $\rightarrow$ dee vee dee</li>
</ul>
<h3 id="1-2-转化为拼音"><a href="#1-2-转化为拼音" class="headerlink" title="1.2 转化为拼音"></a>1.2 转化为拼音</h3><p>参考<a href="http://www.moe.edu.cn/s78/A19/yxs_left/moe_810/s230/195802/t19580201_186000.html" target="_blank" rel="noopener">国家汉语拼音方案</a></p>
<p>使用一个汉语拼音词典，将<code>你好看啊</code>转换为： <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>。此过程需要注意有些多音词需要处理，可以只是使用python的<strong>pypinyin</strong></p>
<h3 id="1-3-拼音转换为音调表示"><a href="#1-3-拼音转换为音调表示" class="headerlink" title="1.3 拼音转换为音调表示"></a>1.3 拼音转换为音调表示</h3><p>目前支持将汉语拼音中的<code>一</code>,<code>二</code>,<code>三</code>,<code>四</code>声转换为 <code>1</code>,<code>2</code>,<code>3</code>,<code>4</code>,<code>5</code>（5代表轻声）</p>
<p> <code>nǐ</code>,<code>hǎo</code>,<code>kàn</code>,<code>ā</code>$\rightarrow$ <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<p>事实上<strong>pypinyin</strong>可以一步从<code>你好看啊</code>转换为 <code>ni3</code>,<code>hao3</code>,<code>kan4</code>,<code>a1</code></p>
<h3 id="1-4-将音节分解为音素"><a href="#1-4-将音节分解为音素" class="headerlink" title="1.4  将音节分解为音素"></a>1.4  将音节分解为音素</h3><p>音素为汉语拼音的最小单元。包括<code>声母</code>,<code>韵母</code>,但是其中还会有一些整体认读音节。(注意：下面所列并非官方标准版，不同情形可以采取不同取舍，参考<a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">MTTS文本分析</a>)</p>
<p><strong>整体认读音节</strong></p>
<p>16个整体认读音节分别是：<code>zhi 、chi、shi、ri、zi、ci、si、yi、wu、yu、ye、yue、yuan、yin 、yun、ying</code>，但是要注意没有yan，因为yan并不发作an音</p>
<p><strong>声母（23个）</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b p m f d t n l g k h j q x zh ch sh r z c s y w</span><br></pre></td></tr></table></figure>

<p><strong>韵母（39个）</strong></p>
<ul>
<li>单韵母 a、o、e、 ê、i、u、ü、-i（前）、-i（后）、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 üe、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 üan 、en、in、uen、 ün 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<p><strong>韵母（39个）（转换标注后）</strong></p>
<ul>
<li>单韵母 a、o、e、ea、i、u、v、ic、ih、er</li>
<li>复韵母 ai、ei、ao、ou、ia、ie、ua、uo、 ve、iao 、iou、uai、uei</li>
<li>鼻韵母 an、ian、uan、 van 、en、in、uen、 vn 、ang、iang、uang、eng、ing、ueng、ong、iong</li>
</ul>
<h3 id="1-5-结果"><a href="#1-5-结果" class="headerlink" title="1.5 结果"></a>1.5 结果</h3><p>此步骤的结果为 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#39;n&#39;, &#39;i3&#39;), (&#39;h&#39;, &#39;ao3&#39;), (&#39;k&#39;, &#39;an4&#39;), (&#39;a5&#39;,)]</span><br></pre></td></tr></table></figure>

<h2 id="2-合成基元选取"><a href="#2-合成基元选取" class="headerlink" title="2 合成基元选取"></a>2 合成基元选取</h2><p>合成基元就是合成语音所需的最小单元。由大到小来说：</p>
<ol>
<li>可以选择每个汉字，一共有6万多，会导致需要很大的训练集</li>
<li>可以选择所有拼音，数量会比汉字少很多</li>
<li>也可以选择声韵母，声韵母是组成音节的单元，21个声母+39个韵母，数据量大幅度减少。</li>
</ol>
<p>在实际语音中除了这些文本上的内容之外，还会存在开始和结束的<strong>静音</strong>，标点符号之间存在的<strong>短暂停顿</strong>。所以我们可以采取以下这套合成基元方案。</p>
<ul>
<li><strong>声母</strong>：  21个声母+wy（共23个）</li>
<li><strong>韵母</strong>： 39个韵母</li>
<li><strong>静音</strong>：<code>sil</code>, <code>pau</code>, <code>sp</code>。sil(silence) 表示句首和句尾的静音，pau(pause) 表示由逗号，顿号造成的停顿，句中其他的短停顿为sp(short pause)</li>
</ul>
<h2 id="3-上下文相关标注"><a href="#3-上下文相关标注" class="headerlink" title="3  上下文相关标注"></a>3  上下文相关标注</h2><p>上下文相关标注的规则要综合考虑有哪些上下文对当前音素发音的影响，总的来说，需要考虑发音基元及其前后基元的信息，以及发音基元所在的音节、词、韵律词、韵律短语、语句相关的信息。</p>
<p>此类标注对于不同任务可以自由设计，一种参考是<a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/mandarin_example_label.md" target="_blank" rel="noopener">MTTS普通话标注示例</a>。这里将参考中的一些内容作出一些解释：</p>
<table>
<thead>
<tr>
<th>层级（由小到达）</th>
<th>标注格式</th>
</tr>
</thead>
<tbody><tr>
<td>声韵母层</td>
<td>p1^p2-p3+p4=p5@p6_p7</td>
</tr>
<tr>
<td>.</td>
<td>/A:a1_a2-a3_a4#a5</td>
</tr>
<tr>
<td>音节层</td>
<td>/B:b1_b2!b3_b4#b5@b6!b7+b8@b9#b10_b11</td>
</tr>
<tr>
<td>.</td>
<td>/C:c1+c2-c3=c4#c5</td>
</tr>
<tr>
<td>词层</td>
<td>/D:d1-d2 /E:e1&amp;e2^e3_e4 /F:f1-f2</td>
</tr>
<tr>
<td>韵律层</td>
<td>/G:g1-g2 /H:h1-h2@h3+h4 /I:i1-i2</td>
</tr>
<tr>
<td>韵律短语层</td>
<td>/J:j1^j2=j3-j4 /K:k1=k2_k3^k4&amp;k5_k6 /L:l1^l2#l3-l4</td>
</tr>
<tr>
<td>语句层</td>
<td>/M:m1#m2+m3+m4!m5</td>
</tr>
</tbody></table>
<p>下面的（发音）基元指的是声韵母，HMM建模选用的单元是音节</p>
<table>
<thead>
<tr>
<th>标号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>p1</td>
<td>前前基元</td>
</tr>
<tr>
<td>p2</td>
<td>前一基元</td>
</tr>
<tr>
<td>p3</td>
<td>当前基元</td>
</tr>
<tr>
<td>p4</td>
<td>后一基元</td>
</tr>
<tr>
<td>p5</td>
<td>后后基元</td>
</tr>
<tr>
<td>p6</td>
<td>当前基元在当前音节的位置（正序）</td>
</tr>
<tr>
<td>p7</td>
<td>当前基元在当前音节的位置（倒序）</td>
</tr>
<tr>
<td>a1</td>
<td>前一音节的首基元</td>
</tr>
<tr>
<td>a2</td>
<td>前一音节的末基元</td>
</tr>
<tr>
<td>a3，a4</td>
<td>前一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>前一音节的基元数目</td>
</tr>
<tr>
<td>b1</td>
<td>当前音节的首基元</td>
</tr>
<tr>
<td>b2</td>
<td>当前音节的末基元</td>
</tr>
<tr>
<td>b3，b4</td>
<td>当前音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>a5</td>
<td>当前音节的基元数目</td>
</tr>
<tr>
<td>b6</td>
<td>当前音节在词中的位置（正序）</td>
</tr>
<tr>
<td>b7</td>
<td>当前音节在词中的位置（倒序）</td>
</tr>
<tr>
<td>b8</td>
<td>当前音节在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>b9</td>
<td>当前音节在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>b10</td>
<td>当前音节在韵律短语中的位置（正序）</td>
</tr>
<tr>
<td>b11</td>
<td>当前音节在韵律短语中的位置（倒序）</td>
</tr>
<tr>
<td>c1</td>
<td>后一音节的首基元</td>
</tr>
<tr>
<td>c2</td>
<td>后一音节的末基元</td>
</tr>
<tr>
<td>c3，c4</td>
<td>后一音节的声调类型（词典和文本分析，下同）</td>
</tr>
<tr>
<td>c5</td>
<td>后一音节的基元数目</td>
</tr>
<tr>
<td>d1</td>
<td>前一个词的词性</td>
</tr>
<tr>
<td>d2</td>
<td>前一个词的音节数目</td>
</tr>
<tr>
<td>e1</td>
<td>当前词的词性</td>
</tr>
<tr>
<td>e2</td>
<td>当前词中的音节数目</td>
</tr>
<tr>
<td>e3</td>
<td>当前词在韵律词中的位置（正序）</td>
</tr>
<tr>
<td>e4</td>
<td>当前词在韵律词中的位置（倒序）</td>
</tr>
<tr>
<td>f1</td>
<td>后一个词的词性</td>
</tr>
<tr>
<td>f2</td>
<td>后一个词的音节数目</td>
</tr>
<tr>
<td>g1</td>
<td>前一个韵律词的音节数目</td>
</tr>
<tr>
<td>g2</td>
<td>前一个韵律词的词数目</td>
</tr>
<tr>
<td>—</td>
<td>—-</td>
</tr>
<tr>
<td>h1</td>
<td>当前韵律词的音节数目</td>
</tr>
<tr>
<td>h2</td>
<td>当前韵律词的词数目</td>
</tr>
<tr>
<td>h3</td>
<td>当前韵律词在韵律短语的位置（正序）</td>
</tr>
<tr>
<td>h4</td>
<td>当前韵律词在韵律短语的位置（倒序）</td>
</tr>
<tr>
<td>—</td>
<td>–</td>
</tr>
<tr>
<td>i1</td>
<td>后一个韵律词的音节数目</td>
</tr>
<tr>
<td>i2</td>
<td>后一个韵律词的词数目</td>
</tr>
<tr>
<td>–</td>
<td>—</td>
</tr>
<tr>
<td>j1</td>
<td>前一韵律短语的语调类型</td>
</tr>
<tr>
<td>j2</td>
<td>前一韵律短语的音节数目</td>
</tr>
<tr>
<td>j3</td>
<td>前一韵律短语的词数目</td>
</tr>
<tr>
<td>j4</td>
<td>前一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>—</td>
<td>—-</td>
</tr>
<tr>
<td>k1</td>
<td>当前韵律短语的语调类型</td>
</tr>
<tr>
<td>k2</td>
<td>当前韵律短语的音节数目</td>
</tr>
<tr>
<td>k3</td>
<td>当前韵律短语的词数目</td>
</tr>
<tr>
<td>k4</td>
<td>当前韵律短语的韵律词个数</td>
</tr>
<tr>
<td>k5</td>
<td>当前韵律短语在语句中的位置（正序）</td>
</tr>
<tr>
<td>k6</td>
<td>当前韵律短语在语句中的位置（倒序）</td>
</tr>
<tr>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>l1</td>
<td>后一韵律短语的语调类型</td>
</tr>
<tr>
<td>l2</td>
<td>后一韵律短语的音节数目</td>
</tr>
<tr>
<td>l3</td>
<td>后一韵律短语的词数目</td>
</tr>
<tr>
<td>l4</td>
<td>后一韵律短语的韵律词个数</td>
</tr>
<tr>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>m1</td>
<td>语句的语调类型</td>
</tr>
<tr>
<td>m2</td>
<td>语句的音节数目</td>
</tr>
<tr>
<td>m3</td>
<td>语句的词数目</td>
</tr>
<tr>
<td>m4</td>
<td>语句的韵律词数目</td>
</tr>
<tr>
<td>m5</td>
<td>语句的韵律短语数目</td>
</tr>
</tbody></table>
<h2 id="4-问题集设计"><a href="#4-问题集设计" class="headerlink" title="4 问题集设计"></a>4 问题集设计</h2><p>问题集(Question Set)即是决策树中条件判断的设计。问题集通常很大，由几百个判断条件组成。</p>
<p>问题集的设计依赖于不同语言的语言学知识，而且<strong>与上下文标注文件相匹配，改变上下文标注方法也需要相应地改变问题集</strong>，对于中文语音合成而言，问题集的设计的规则有:</p>
<ul>
<li><strong>前前个，前个，当前，下个，下下个声韵母分别是某个合成基元吗</strong>，合成基元共有65个(23声母+39韵母+3静音)，例如判断是否是元音a QS “LL-a” QS “L-a” QS “C-a” QS “R-a” QS “RR-a”</li>
<li><strong>声母特征划分</strong>，例如声母可以划分成塞音，擦音，鼻音，唇音等，声母特征划分24个</li>
<li><strong>韵母特征划分</strong>，例如韵母可以划分成单韵母，复合韵母，分别包含aeiouv的韵母，韵母特征划分8个</li>
<li><strong>其他信息划分</strong>，词性划分，26个词性; 声调类型，5个; 是否是声母或者韵母或者静音，3个</li>
<li><strong>韵律特征划分</strong>，如是否是重音，重音和韵律词/短语的位置数量</li>
<li><strong>位置和数量特征划分</strong></li>
</ul>
<p>对于三音素模型而言，对于每个划分的特征，都会产生3个判断条件，该音素是否满足条件，它的左音素（声韵母）和右音素（声韵母）是否满足条件，有时会扩展到左左音素和右右音素的情况，这样就有5个问题。其中，每个问题都是以 QS 命令开头，问题集的答案可以有多个，中间以逗号隔开，答案是一个包含通配符的字符串。当问题表达式为真时，该字符串成功匹配标注文件中的某一行标注。格式如：</p>
<p>QS 问题表达式 {答案 1，答案 2，答案 3，……}</p>
<p>QS “LL==Fricative” {f^<em>,s^</em>,sh^<em>,x^</em>,h^<em>,lh^</em>,hy^<em>,hh^</em>}</p>
<p>对于3音素上下文相关的基元模型的3个问题，例如： * 判断当前，前接，后接音素/单元是否为擦音 * QS ‘C_Fricative’ * QS ‘L_Fricative’ * QS ‘R_Fricative’</p>
<p>问题集示例参考 <a href="https://github.com/Jackiexiao/MTTS/blob/master/docs/mddocs/question.md" target="_blank" rel="noopener">MTTS问题集设计参考</a></p>
<p>值得注意的是，merlin中使用的问题集和HTS中有所不同，Merlin中新增加了CQS问题，Merlin处理Questions Set 的模块在merlin/src/frontend/label_normalisation 中的Class HTSLabelNormalisation</p>
<p><strong>Question Set 的格式是</strong></p>
<p>QS + 一个空格 + “question_name” + 任意空格+ {Answer1, answer2, answer3…} # 无论是QS还是CQS的answer中，前后的**不用加，加了也会被去掉 CQS + 一个空格 + “question_name” + 任意空格+ {Answer} #对于CQS，这里只能有一个answer 比如 CQS C-Syl-Tone {<em>(d+)+} merlin也支持浮点数类型，只需改为CQS C-Syl-Tone {</em>([d.]+)+}</p>
<p>参考 ：  <a href="https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html" target="_blank" rel="noopener">https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-09-25-merlin-mandarin-text-process/" data-id="ck4ifvdg6003zywje44nx7tuq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-09-23-merlin-mandain-voice-op" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-09-23-merlin-mandain-voice-op/" class="article-date">
  <time datetime="2019-12-23T10:45:59.655Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-09-23-merlin-mandain-voice-op/">merlin语音合成方案mandarin_voice操作步骤</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="0-概览"><a href="#0-概览" class="headerlink" title="0 概览"></a>0 概览</h3><p>本文详细解释Merlin Mandarin_voice下脚本一步一步所做的事。</p>
<h3 id="01-setup"><a href="#01-setup" class="headerlink" title="01_setup"></a>01_setup</h3><p>脚本<code>merlin/egs/mandarin_voice/s1/01_setup.sh</code></p>
<p>主要工作是创建一个目录，做好准备工作。主要创建了如下文件夹:</p>
<ul>
<li>experiments</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">─ mandarin_voice(voice name)</span><br><span class="line">    ├── acoustic_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    ├── duration_model</span><br><span class="line">    │ ├── data</span><br><span class="line">    │ ├── gen</span><br><span class="line">    │ ├── inter_module</span><br><span class="line">    │ ├── log</span><br><span class="line">    │ └── nnets_model</span><br><span class="line">    └── test_synthesis</span><br><span class="line">        ├── gen-lab</span><br><span class="line">        ├── prompt-lab</span><br><span class="line">        ├── test_id_list.scp</span><br><span class="line">        └── wav</span><br></pre></td></tr></table></figure>

<ul>
<li>database</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> feats</span><br><span class="line">│ ├── bap</span><br><span class="line">│ ├── lf0</span><br><span class="line">│ └── mgc</span><br><span class="line">├── labels</span><br><span class="line">│ └── label_phone_align</span><br><span class="line">├── prompt-lab</span><br><span class="line">│ ├── A11_0.lab</span><br><span class="line">│ ├── A11_1.lab</span><br><span class="line">│ ├── A11_2.lab</span><br><span class="line">└── wav</span><br><span class="line">    ├── A11_0.wav</span><br><span class="line">    ├── A11_100.wav</span><br><span class="line">    ├── A11_101.wav</span><br></pre></td></tr></table></figure>


<p>将一些基本参数写入到<code>conf/global_setting.cfg</code>文件中</p>
<p><img src="/images/blog/merlin_mandarin_voice_op1.jpg" alt="merlinmandarin voice操作"></p>
<p><strong>注意：一定要在setup.sh里面定义好train,valid,test的数量，不然修改global_config.cfg里面的值也没用。这三者相加的值要等于（duration_model/FileIdList下）file_id_list.scp总行数</strong></p>
<h3 id="02-prepare-lab"><a href="#02-prepare-lab" class="headerlink" title="02_prepare_lab"></a>02_prepare_lab</h3><p>需要两个参数：</p>
<ul>
<li>lab_dir: 第一步中的标注目录 <code>database/labels</code></li>
<li>prompt_lab_dir :第一步中生成的<code>database/prompt-lab</code></li>
</ul>
<h4 id="2-1-准备文件夹"><a href="#2-1-准备文件夹" class="headerlink" title="2.1 准备文件夹"></a>2.1 准备文件夹</h4><ul>
<li><p>将 <code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件分别复制到<code>experiments/mandarin_voice/duration_model/data</code>（时域模型）和<code>experiments/mandarin_voice/acoustic_model/data</code>（声学模型）下。【用于训练】</p>
</li>
<li><p>将<code>database/prompt-lab</code>下的lab文件复制到<code>experiments/mandarin_voice/test_synthesis</code>下【用于测试（合成）】</p>
</li>
</ul>
<h4 id="2-2-生成文件列表"><a href="#2-2-生成文件列表" class="headerlink" title="2.2 生成文件列表"></a>2.2 生成文件列表</h4><ul>
<li><p>将<code>database/labels</code>目录下的<code>lab_phone_align</code>下的lab文件列表写入到<code>experiments/mandarin_voice/duration_model/FileIdList&#39;和</code>experiments/mandarin_voice/acoustic_model/FileIdList’。并移除文件后缀【训练集文件列表】</p>
</li>
<li><p>将<code>database/prompt-lab</code>下的lab文件列表写入到<code>experiments/mandarin_voice/test_synthesis/test_id_list.scp</code>文件中，并移除文件后缀【用于合成语音的文本列表】</p>
</li>
</ul>
<h3 id="03-prepare-acoustic-feature"><a href="#03-prepare-acoustic-feature" class="headerlink" title="03_prepare_acoustic_feature"></a>03_prepare_acoustic_feature</h3><p>需要两个参数</p>
<ul>
<li><strong>wav_dir</strong>: 使用的是第一步中的<code>database/wav</code>，下面存放的是所有的wav音频文件</li>
<li><strong>feat_dir</strong>:输出文件目录<code>database/feats</code>，是当前脚本输出的特征存放文件目录<h4 id="3-1-使用声码器抽取声学特征"><a href="#3-1-使用声码器抽取声学特征" class="headerlink" title="3.1 使用声码器抽取声学特征"></a>3.1 使用声码器抽取声学特征</h4></li>
</ul>
<p>使用<code>merlin/misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本抽取，注意，其中的声码器可以是<code>WORLD</code>也可以是其他的，比如<code>straight</code>,<code>WORLD_2</code>。其实依然是在python中调用以下脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">world &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;WORLD&quot;)</span><br><span class="line">sptk &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;SPTK-3.9&quot;)</span><br><span class="line">reaper &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;REAPER&quot;)</span><br></pre></td></tr></table></figure>

<p>生成的特征目录如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sp_dir &#x3D; os.path.join(feat_dir, &#39;sp&#39; )</span><br><span class="line">mgc_dir &#x3D; os.path.join(feat_dir, &#39;mgc&#39;)</span><br><span class="line">ap_dir &#x3D; os.path.join(feat_dir, &#39;ap&#39; )</span><br><span class="line">bap_dir &#x3D; os.path.join(feat_dir, &#39;bap&#39;)</span><br><span class="line">f0_dir &#x3D; os.path.join(feat_dir, &#39;f0&#39; )</span><br><span class="line">lf0_dir &#x3D; os.path.join(feat_dir, &#39;lf0&#39;)</span><br></pre></td></tr></table></figure>

<p>如果我们使用world作为vocoder的话，会使用<code>misc/scripts/vocoder/world/extract_features_for_merlin.py</code>脚本，生成步骤其实是：</p>
<ol>
<li>直接从原始wav文件，使用<code>world analysis</code>抽取 <code>sp</code>,<code>bapd</code>特征。<code>straight</code>vocoder 会产生 <code>ap</code>,如果使用reaper会产生<code>f0</code>特征。</li>
<li><code>f0</code>$\rightarrow$ <code>lf0</code>,<code>bapd</code>$\rightarrow$ <code>bap</code>,<code>sp</code>$\rightarrow$ <code>mgc</code></li>
</ol>
<h4 id="3-2-复制特征到声学特征目录下"><a href="#3-2-复制特征到声学特征目录下" class="headerlink" title="3.2 复制特征到声学特征目录下"></a>3.2 复制特征到声学特征目录下</h4><p>将所有<code>feat_dir</code>下的所有文件,包括<code>sp</code>,<code>mgc</code>,<code>ap</code>,<code>bap</code>,<code>f0</code>,<code>lf0</code>复制到<code>experiments/mandarin_voice/acoustic_model/data</code>下。</p>
<h2 id="04-prepare-conf-files"><a href="#04-prepare-conf-files" class="headerlink" title="04_prepare_conf_files"></a>04_prepare_conf_files</h2><p>执行<code>./scripts/prepare_config_files.sh</code></p>
<p><strong>duration相关配置</strong></p>
<ul>
<li><p>先从<code>merlin/misc/recipes/duration_demo.conf</code>复制一份到<code>conf/duration_mandarin_voice.conf</code>，并修改<code>conf/duration_mandarin_voice.conf</code>中的一些目录</p>
<ul>
<li>MerlinDir</li>
<li>WorkDir</li>
<li>TOPLEVEL</li>
<li>FileIdList</li>
</ul>
</li>
<li><p>修改Label相关的配置项【Labels】</p>
<ul>
<li>silence_pattern：修改为 <code>[&#39;*-sil+*&#39;]</code></li>
<li>label_type:<code>state_align</code> 或 <code>phone_align</code>，修改之后为<code>phone_align</code></li>
<li>label_align: 即配置音素对齐文件的目录<code>/experiments/mandarin_voice/duration_model/data/label_phone_align</code></li>
<li>question_file_name:<code>/misc/questions/questions-mandarin.hed</code>问题集</li>
</ul>
</li>
<li><p>修改输出配置【Outputs】，label_type有<code>state_align</code> 或 <code>phone_align</code>，如果是<code>state_align</code>会在【outputs】处指定<code>dur=5</code>,如果是<code>phone_align</code>则指定<code>dur=1</code></p>
</li>
<li><p>神经网络的架构配置，如果当前声音文件是<code>demo</code>则修改<code>hidden_layer_size</code> 【architechture】</p>
</li>
<li><p>修改训练、验证、测试数据数量。【data】</p>
<ul>
<li>train_file_number: 200</li>
<li>valid_file_number: 25</li>
<li>test_file_number: 25</li>
</ul>
</li>
</ul>
<p><strong>acoustic相关配置</strong></p>
<ul>
<li>复制文件<code>conf/acoustic_mandarin_voice.conf</code>，修改变量，label配置都和duration相关配置一样。</li>
<li>修改输出配置【outputs】<ul>
<li>mgc</li>
<li>dmgc</li>
<li>bap</li>
<li>dbap</li>
<li>lf0</li>
<li>dlf0</li>
</ul>
</li>
<li>波形文件设置【waveform】<ul>
<li>framelength</li>
<li>minimum_phase_order</li>
<li>fw_alpha</li>
</ul>
</li>
<li>其他的【architechture】和【data】都和duration相关配置一样。</li>
</ul>
<p>执行<code>./scripts/prepare_config_files_for_synthesis.sh</code>配置测试（或合成）语音相关的参数。基本和上面的<code>./scripts/prepare_config_files.sh</code>一样，需要配置<code>duration</code>和<code>ascoustic</code>参数。新增了【Processes】</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DurationModel: True</span><br><span class="line">GenTestList: True</span><br><span class="line"># sub-processes</span><br><span class="line">NORMLAB: True</span><br><span class="line">MAKEDUR: False</span><br><span class="line">MAKECMP: False</span><br><span class="line">NORMCMP: False</span><br><span class="line">TRAINDNN: False</span><br><span class="line">DNNGEN: True</span><br><span class="line">CALMCD: False</span><br></pre></td></tr></table></figure>

<h3 id="05-train-duration-model"><a href="#05-train-duration-model" class="headerlink" title="05_train_duration_model"></a>05_train_duration_model</h3><p>实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/duration_mandarin_voice.conf</code></p>
<p>其中<code>./scripts/submit.sh</code>是theano相关参数的配置。</p>
<h3 id="06-train-acoustic-model"><a href="#06-train-acoustic-model" class="headerlink" title="06_train_acoustic_model"></a>06_train_acoustic_model</h3><p>训练声学模型，实际执行的是<code>./scripts/submit.sh   merlin/src/run_merlin.py   conf/acoustic_mandarin_voice.conf</code></p>
<h3 id="07-run-merlin"><a href="#07-run-merlin" class="headerlink" title="07_run_merlin"></a>07_run_merlin</h3><p>需要两个参数</p>
<ul>
<li>test_dur_config_file: 语音合成的时域配置文件</li>
<li>test_synth_config_file:语音合成的</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-09-23-merlin-mandain-voice-op/" data-id="ck4ifvdg5003xywje4a551ps6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-08-13-merlin-tts-techmap3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-08-13-merlin-tts-techmap3/" class="article-date">
  <time datetime="2019-12-23T10:45:59.653Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-08-13-merlin-tts-techmap3/">merlin语音合成讲义三：系统回归</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-概览"><a href="#1-概览" class="headerlink" title="1 概览"></a>1 概览</h2><p>前馈神经网络</p>
<ul>
<li>概念上直白的</li>
<li>对每个输入帧frame<ul>
<li>执行回归得到对应的输出特征</li>
</ul>
</li>
<li>为避免更广(wider)的输入上下文，可以简单的将几个frame堆叠</li>
<li>需要注意的是：语言特征已经跨越(span)了几个时间尺度(timescale)</li>
</ul>
<h3 id="1-1-方向"><a href="#1-1-方向" class="headerlink" title="1.1 方向"></a>1.1 方向</h3><ul>
<li>前馈架构<ul>
<li>没有记忆</li>
</ul>
</li>
<li>简单的循环神经网络</li>
<li>梯度消失现象</li>
<li>LSTM神经元解决了梯度消失现象（其他类型的可能存在）</li>
</ul>
<p><strong>但是</strong></p>
<ul>
<li>输入和输出有相同的帧率(frame rate)</li>
<li>需要一个额外的时钟或者对齐机制来对输入做上采样</li>
</ul>
<h3 id="1-2-sequence-to-sequence"><a href="#1-2-sequence-to-sequence" class="headerlink" title="1.2 sequence-to-sequence"></a>1.2 sequence-to-sequence</h3><ul>
<li><p>下一步是，集成对齐机制到网络内部</p>
</li>
<li><p>当前：输入序列长度可能与输出序列长度不一致</p>
</li>
<li><p>例如：</p>
<ul>
<li>输入：上下文依赖的音素序列<ul>
<li>输出：声学帧(对于声码器vocoder)</li>
</ul>
</li>
</ul>
</li>
<li><p>概念上</p>
<ul>
<li>读取整个输入序列；使用一个固定长度的表征来记忆</li>
<li>给定表征，写输出序列</li>
</ul>
</li>
<li><p>encoder（编码器）</p>
</li>
<li><p>是一个循环神经网络，读入整个输入序列，然后用固定长度表征来summarises或者memorises他们。</p>
</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_1.png" alt="TTS merlin技术路线"></p>
<ul>
<li>encoder和decoder</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_2.png" alt="TTS merlin技术路线"></p>
<h3 id="1-3-sequence-to-sequence中的对齐"><a href="#1-3-sequence-to-sequence中的对齐" class="headerlink" title="1.3 sequence-to-sequence中的对齐"></a>1.3 sequence-to-sequence中的对齐</h3><ul>
<li>基本模型，输入和输出之间没有对齐</li>
<li>通过加入注意力模型来获得更好结果<ul>
<li>decoder可以接近输入序列<ul>
<li>decoder也可以在前一个时间步(time step)接近其输出</li>
</ul>
</li>
</ul>
</li>
<li>对齐像ASR模型。但是用声码器(vocoder)来做ASR效果不好<ul>
<li>因而我们期望通过使用ASR样式的声学特征(仅仅是模型的对齐部分)来获得更好效果</li>
</ul>
</li>
</ul>
<p><code>04_prepare_conf_files.sh</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;preparing config files for acoustic, duration models...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_config_files.sh $global_config_file</span><br><span class="line">echo &quot;preparing config files for synthesis...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_config_files_for_synthesis.sh $global_config_file</span><br></pre></td></tr></table></figure>

<p><code>05_train_duration_model.sh</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $duration_conf_file</span><br></pre></td></tr></table></figure>
<p><code>config files</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">Merlin: &lt;path to Merlin root directory&gt;</span><br><span class="line">TOPLEVEL: &lt;path where experiments are created&gt;</span><br><span class="line">[Paths]</span><br><span class="line"># where to place work files</span><br><span class="line">work: &lt;path where data, log, models and generated data are stored and created&gt;</span><br><span class="line"># where to find the data</span><br><span class="line">data: %(work)s&#x2F;data</span><br><span class="line"># where to find intermediate directories</span><br><span class="line">inter_data: %(work)s&#x2F;inter_module</span><br><span class="line"># list of file basenames, training and validation in a single list</span><br><span class="line">file_id_list: %(data)s&#x2F;file_id_list.scp</span><br><span class="line">test_id_list: %(data)s&#x2F;test_id_list.scp</span><br><span class="line">in_mgc_dir: %(data)s&#x2F;mgc</span><br><span class="line">in_bap_dir : %(data)s&#x2F;bap</span><br><span class="line">[Labels]</span><br><span class="line">enforce_silence: False</span><br><span class="line">silence_pattern: [&#39;*-sil+*&#39;]</span><br><span class="line"># options: state_align or phone_align</span><br><span class="line">label_type: state_align</span><br><span class="line">label_align: &lt;path to labels&gt;</span><br><span class="line">question_file_name: &lt;path to questions set&gt;</span><br><span class="line">add_frame_features: True</span><br><span class="line"># options: full, coarse_coding, minimal_frame, state_only, frame_only, none</span><br><span class="line">subphone_feats: full</span><br><span class="line">[Outputs]</span><br><span class="line"># dX should be 3 times X</span><br><span class="line">mgc : 60</span><br><span class="line">dmgc : 180</span><br><span class="line">bap : 1</span><br><span class="line">dbap : 3</span><br><span class="line">lf0 : 1</span><br><span class="line">dlf0 : 3</span><br><span class="line">[Waveform]</span><br><span class="line">[Outputs]</span><br><span class="line"># dX should be 3 times X</span><br><span class="line">mgc : 60</span><br><span class="line">dmgc : 180</span><br><span class="line">bap : 1</span><br><span class="line">dbap : 3</span><br><span class="line">lf0 : 1</span><br><span class="line">dlf0 : 3</span><br><span class="line">[Waveform]</span><br><span class="line">test_synth_dir: None</span><br><span class="line"># options: WORLD or STRAIGHT</span><br><span class="line">vocoder_type: WORLD</span><br><span class="line">samplerate: 16000</span><br><span class="line">framelength: 1024</span><br><span class="line"># Frequency warping coefficient used to compress the spectral envelope into MGC (or MCEP)</span><br><span class="line">fw_alpha: 0.58</span><br><span class="line">minimum_phase_order: 511</span><br><span class="line">use_cep_ap: True</span><br><span class="line">[Architecture]</span><br><span class="line">switch_to_keras: False</span><br><span class="line">hidden_layer_size : [1024, 1024, 1024, 1024, 1024, 1024]</span><br><span class="line">hidden_layer_type : [&#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;, &#39;TANH&#39;]</span><br><span class="line">model_file_name: feed_forward_6_tanh</span><br><span class="line">#if RNN or sequential training is used, please set sequential_training to True.</span><br><span class="line">sequential_training : False</span><br><span class="line">dropout_rate : 0.0</span><br><span class="line">batch_size : 256</span><br><span class="line"># options: -1 for exponential decay, 0 for constant learning rate, 1 for linear decay</span><br><span class="line">lr_decay : -1</span><br><span class="line">learning_rate : 0.002</span><br><span class="line"># options: sgd, adam, rprop</span><br><span class="line">optimizer : sgd</span><br><span class="line">warmup_epoch : 10</span><br><span class="line">training_epochs : 25</span><br><span class="line">[Processes]</span><br><span class="line"># Main processes</span><br><span class="line">AcousticModel : True</span><br><span class="line">GenTestList : False</span><br><span class="line"># sub-processes</span><br><span class="line">NORMLAB : True</span><br><span class="line">MAKECMP : True</span><br><span class="line">NORMCMP : True</span><br><span class="line">TRAINDNN : True</span><br><span class="line">DNNGEN : True</span><br><span class="line">GENWAV : True</span><br><span class="line">CALMCD : True</span><br></pre></td></tr></table></figure>

<p><code>06_train_acoustic_model.sh</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $acoustic_conf_file</span><br></pre></td></tr></table></figure>

<p><code>07_run_merlin.sh</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inp_txt&#x3D;$1</span><br><span class="line">test_dur_config_file&#x3D;$2</span><br><span class="line">test_synth_config_file&#x3D;$3</span><br><span class="line">echo &quot;preparing full-contextual labels using Festival frontend...&quot;</span><br><span class="line">lab_dir&#x3D;$(dirname $inp_txt)</span><br><span class="line">.&#x2F;scripts&#x2F;prepare_labels_from_txt.sh $inp_txt $lab_dir $global_config_file</span><br><span class="line">echo &quot;synthesizing durations...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $test_dur_config_file</span><br><span class="line">echo &quot;synthesizing speech...&quot;</span><br><span class="line">.&#x2F;scripts&#x2F;submit.sh $&#123;MerlinDir&#125;&#x2F;src&#x2F;run_merlin.py $test_synth_config_file</span><br></pre></td></tr></table></figure>

<h2 id="2-设计选择：声学模型"><a href="#2-设计选择：声学模型" class="headerlink" title="2 设计选择：声学模型"></a>2 设计选择：声学模型</h2><ul>
<li>直白的方式：如果输入和输出<strong>有相同长度并且是对齐的</strong></li>
<li>前馈神经网络</li>
<li>循环神经网络层</li>
<li>不那么直白的方式：对非对齐输入和输出序列<ul>
<li>使用sequence-to-sequence</li>
</ul>
</li>
<li>唯一的实践限制是，是使用什么技术，比如Theano,Tensorflow</li>
</ul>
<h3 id="2-1-方向"><a href="#2-1-方向" class="headerlink" title="2.1 方向"></a>2.1 方向</h3><ul>
<li>回归的输出是什么<ul>
<li>声学特征<ul>
<li>不是语音波形<br>所以还需要进一步</li>
</ul>
</li>
</ul>
</li>
<li>生成波形<ul>
<li>输入时声学特征</li>
<li>输出是语音波形</li>
</ul>
</li>
</ul>
<h2 id="3-波形生成-waveform-generator"><a href="#3-波形生成-waveform-generator" class="headerlink" title="3 波形生成(waveform generator)"></a>3 波形生成(waveform generator)</h2><h3 id="3-1-从声学-acoustic-特征回到原始声码器-vocoder-特征"><a href="#3-1-从声学-acoustic-特征回到原始声码器-vocoder-特征" class="headerlink" title="3.1 从声学(acoustic)特征回到原始声码器(vocoder)特征"></a>3.1 从声学(acoustic)特征回到原始声码器(vocoder)特征</h3><p><img src="/images/blog/merlin_tts_tch3_3.png" alt="TTS merlin技术路线"></p>
<h3 id="3-2-WORLD：periodic-excitation-using-a-pulse-train"><a href="#3-2-WORLD：periodic-excitation-using-a-pulse-train" class="headerlink" title="3.2 WORLD：periodic excitation using a pulse train"></a>3.2 WORLD：periodic excitation using a pulse train</h3><ul>
<li>脉冲位置的计算<ul>
<li>声音分割：每一个fundamental period(基本周期)创建一个脉冲，T0。从F0计算T0，其中F0之前被声学模型预测得到</li>
<li>非声音分割：固定频率 $T0=5ms$<h3 id="3-3-WORLD-obtain-spectral-envelope-at-exact-pulse-locations-by-interpolation-插值法在每个确定的脉冲位置获取频谱包络"><a href="#3-3-WORLD-obtain-spectral-envelope-at-exact-pulse-locations-by-interpolation-插值法在每个确定的脉冲位置获取频谱包络" class="headerlink" title="3.3 WORLD:obtain spectral envelope at exact pulse locations, by interpolation(插值法在每个确定的脉冲位置获取频谱包络)"></a>3.3 WORLD:obtain spectral envelope at exact pulse locations, by interpolation(插值法在每个确定的脉冲位置获取频谱包络)</h3></li>
</ul>
</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_4.png" alt="TTS merlin技术路线"></p>
<h3 id="3-4-WORLD：重构周期性和非周期性的幅度频谱-magnitude-spectra"><a href="#3-4-WORLD：重构周期性和非周期性的幅度频谱-magnitude-spectra" class="headerlink" title="3.4 WORLD：重构周期性和非周期性的幅度频谱(magnitude spectra)"></a>3.4 WORLD：重构周期性和非周期性的幅度频谱(magnitude spectra)</h3><p><img src="/images/blog/merlin_tts_tch3_5.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_6.png" alt="TTS merlin技术路线"></p>
<h3 id="3-5-WORLD-生成波形"><a href="#3-5-WORLD-生成波形" class="headerlink" title="3.5 WORLD:生成波形"></a>3.5 WORLD:生成波形</h3><p><img src="/images/blog/merlin_tts_tch3_7.png" alt="TTS merlin技术路线"></p>
<h2 id="4-拓展"><a href="#4-拓展" class="headerlink" title="4 拓展"></a>4 拓展</h2><ul>
<li>混合语音合成<ul>
<li>使用Merlin来预测声学特征，使用Festival来做单元选取(select unit)</li>
</ul>
</li>
<li>声音转换<ul>
<li>输入语音而非文本</li>
<li>训练数据是对齐的输入和输出语音（而不是音素标签和语音）</li>
</ul>
</li>
<li>讲话人调整<ul>
<li>增强输入</li>
<li>调整隐藏层</li>
<li>转换输出</li>
</ul>
</li>
</ul>
<h3 id="4-1-经典单元选取"><a href="#4-1-经典单元选取" class="headerlink" title="4.1 经典单元选取"></a>4.1 经典单元选取</h3><p>此处以音素单元为例，目标和join cost</p>
<p><img src="/images/blog/merlin_tts_tch3_8.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_9.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_10.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-独立特征形式-Independent-Feature-Formulation-IFF-目标损失"><a href="#4-2-独立特征形式-Independent-Feature-Formulation-IFF-目标损失" class="headerlink" title="4.2 独立特征形式(Independent Feature Formulation(IFF))目标损失"></a>4.2 独立特征形式(Independent Feature Formulation(IFF))目标损失</h3><p><img src="/images/blog/merlin_tts_tch3_11.png" alt="TTS merlin技术路线"></p>
<h3 id="4-3-声学空间形式-Acoustic-Space-Formulation-目标损失"><a href="#4-3-声学空间形式-Acoustic-Space-Formulation-目标损失" class="headerlink" title="4.3 声学空间形式(Acoustic Space Formulation)目标损失"></a>4.3 声学空间形式(Acoustic Space Formulation)目标损失</h3><p><img src="/images/blog/merlin_tts_tch3_12.png" alt="TTS merlin技术路线"></p>
<h3 id="4-4-混合语音合成就像使用Acoustic-Space-Formulation目标损失的单元选取"><a href="#4-4-混合语音合成就像使用Acoustic-Space-Formulation目标损失的单元选取" class="headerlink" title="4.4  混合语音合成就像使用Acoustic Space Formulation目标损失的单元选取"></a>4.4  混合语音合成就像使用Acoustic Space Formulation目标损失的单元选取</h3><p><img src="/images/blog/merlin_tts_tch3_13.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_14.png" alt="TTS merlin技术路线"></p>
<h3 id="4-5-混合语音合成就像：统计参数语音合成，使用声码器-vocoder-的替换"><a href="#4-5-混合语音合成就像：统计参数语音合成，使用声码器-vocoder-的替换" class="headerlink" title="4.5 混合语音合成就像：统计参数语音合成，使用声码器(vocoder)的替换"></a>4.5 混合语音合成就像：统计参数语音合成，使用声码器(vocoder)的替换</h3><p><img src="/images/blog/merlin_tts_tch3_15.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_16.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_17.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_18.png" alt="TTS merlin技术路线"></p>
<h3 id="4-6-混合语音合成就像：同时对目标和join-cost使用混合密度网络"><a href="#4-6-混合语音合成就像：同时对目标和join-cost使用混合密度网络" class="headerlink" title="4.6 混合语音合成就像：同时对目标和join cost使用混合密度网络"></a>4.6 混合语音合成就像：同时对目标和join cost使用混合密度网络</h3><p><img src="/images/blog/merlin_tts_tch3_19.png" alt="TTS merlin技术路线"></p>
<h3 id="7-声音转换"><a href="#7-声音转换" class="headerlink" title="7 声音转换"></a>7 声音转换</h3><p>将源声转换为另外一个人的声音，而不改变声音内容</p>
<p><img src="/images/blog/merlin_tts_tch3_20.png" alt="TTS merlin技术路线"></p>
<p>使用神经网络完成</p>
<p><img src="/images/blog/merlin_tts_tch3_21.png" alt="TTS merlin技术路线"></p>
<h3 id="7-1-输入和输出的声学特征的抽取和工程"><a href="#7-1-输入和输出的声学特征的抽取和工程" class="headerlink" title="7.1 输入和输出的声学特征的抽取和工程"></a>7.1 输入和输出的声学特征的抽取和工程</h3><p><img src="/images/blog/merlin_tts_tch3_22.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch3_23.png" alt="TTS merlin技术路线"></p>
<h3 id="7-2-输入输出的对齐"><a href="#7-2-输入输出的对齐" class="headerlink" title="7.2 输入输出的对齐"></a>7.2 输入输出的对齐</h3><ul>
<li>从波形waveform中声学特征</li>
<li>使用动态时间封装(Dynamic Time Wraping(DTW))</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_24.png" alt="TTS merlin技术路线"></p>
<h3 id="7-3-最简单的方法：对齐输入和输出特征-逐帧回归"><a href="#7-3-最简单的方法：对齐输入和输出特征-逐帧回归" class="headerlink" title="7.3 最简单的方法：对齐输入和输出特征+逐帧回归"></a>7.3 最简单的方法：对齐输入和输出特征+逐帧回归</h3><p><img src="/images/blog/merlin_tts_tch3_25.png" alt="TTS merlin技术路线"></p>
<h3 id="7-4-当然，我们也可以用前馈神经网络做得更好"><a href="#7-4-当然，我们也可以用前馈神经网络做得更好" class="headerlink" title="7.4 当然，我们也可以用前馈神经网络做得更好"></a>7.4 当然，我们也可以用前馈神经网络做得更好</h3><p><img src="/images/blog/merlin_tts_tch3_26.png" alt="TTS merlin技术路线"></p>
<p>我们可以使用<code>Merlin/egs/voice_conversion/s1/</code>目录下的脚本完成这个工作</p>
<p><code>03_align_src_with_target.sh</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">src_feat_dir&#x3D;$1</span><br><span class="line">tgt_feat_dir&#x3D;$2</span><br><span class="line">src_aligned_feat_dir&#x3D;$3</span><br><span class="line">src_mgc_dir&#x3D;$src_feat_dir&#x2F;mgc</span><br><span class="line">tgt_mgc_dir&#x3D;$tgt_feat_dir&#x2F;mgc</span><br><span class="line">echo &quot;Align source acoustic features with target acoustic features...&quot;</span><br><span class="line">python $&#123;MerlinDir&#125;&#x2F;misc&#x2F;scripts&#x2F;voice_conversion&#x2F;dtw_aligner_festvox.py $&#123;MerlinDir&#125;&#x2F;tools</span><br><span class="line">$&#123;src_feat_dir&#125; $&#123;tgt_feat_dir&#125; $&#123;src_aligned_feat_dir&#125; $&#123;bap_dim&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/merlin_tts_tch3_26.png" alt="TTS merlin技术路线"></p>
<h2 id="8-讲话人调整-Speaker-Adaptation"><a href="#8-讲话人调整-Speaker-Adaptation" class="headerlink" title="8 讲话人调整(Speaker Adaptation)"></a>8 讲话人调整(Speaker Adaptation)</h2><ul>
<li>只使用了目标讲话人一小段录音来创造一个新的声音</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_28.png" alt="TTS merlin技术路线"></p>
<h3 id="8-1-使用DNN方法的讲话人调整"><a href="#8-1-使用DNN方法的讲话人调整" class="headerlink" title="8.1 使用DNN方法的讲话人调整"></a>8.1 使用DNN方法的讲话人调整</h3><ul>
<li>需要额外的输入特征</li>
<li>应用转换（声音转换）到输出特征</li>
<li>学习一个模型参数的修改(LHUC)</li>
<li>共享层(hat swapping)</li>
<li>在目标讲述人数据上fine-tuning整个模型</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch3_29.png" alt="TTS merlin技术路线"></p>
<p>共享层和hot swapping</p>
<p><img src="/images/blog/merlin_tts_tch3_30.png" alt="TTS merlin技术路线"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-08-13-merlin-tts-techmap3/" data-id="ck4ifvdg4003vywje5n31doeb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-08-12-merlin-tts-techmap2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-08-12-merlin-tts-techmap2/" class="article-date">
  <time datetime="2019-12-23T10:45:59.629Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-08-12-merlin-tts-techmap2/">merlin语音合成讲义二：如何构建系统之数据准备</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一 前端部分</p>
<p><img src="/images/blog/merlin_tts_tch2_1.png" alt="TTS merlin技术路线"></p>
<p>如果存在已经训练好的前端工具，我们可以使用已经有的工具<code>Festival</code>,<code>MaryTTS</code>,<code>sSpeak</code><br>如果我们没有标注数据，我们可以使用<code>Ossian</code></p>
<h3 id="1-1-Ossian-Toolkit"><a href="#1-1-Ossian-Toolkit" class="headerlink" title="1.1 Ossian Toolkit"></a>1.1 Ossian Toolkit</h3><ul>
<li>使用训练数据，可以使用最少的speech+text<ul>
<li>句子或段落对齐</li>
</ul>
</li>
<li>可以利用用户的任何额外数据</li>
<li>提供<code>前端模块</code>和<code>胶水</code>来组合，Merlin DNNs</li>
</ul>
<p>我们将展示</p>
<ul>
<li>Ossian如何与Merlin结合来构建一个<code>Swahili</code>声音而不需要任何语言专家，只需要转录语音</li>
<li>引入Ossian的某些思路来管理，而不需要标注</li>
</ul>
<h4 id="1-1-1-Ossian-：Training-Data"><a href="#1-1-1-Ossian-：Training-Data" class="headerlink" title="1.1.1  Ossian ：Training Data"></a>1.1.1  Ossian ：Training Data</h4><p>我们仅需要UTF-8格式的文本和语音，同时匹配 <code>句子 除以 段落</code> 尺寸的chunks</p>
<p><img src="/images/blog/merlin_tts_tch2_2.png" alt="TTS merlin技术路线"></p>
<p><img src="/images/blog/merlin_tts_tch2_3.png" alt="TTS merlin技术路线"></p>
<p><strong>utt文件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;utt text &#x3D;&quot;Khartoum imejitenga na mzozo huo.&quot; waveform&#x3D;&quot;.&#x2F;wav&#x2F;pm_n2336.wav&quot;  utterance_name&#x3D;&quot;pm_n2236&quot;&gt;</span><br></pre></td></tr></table></figure>
<p>其中<code>utterance_name=&quot;pm_n2236&quot;</code>是一个XML格式的发生结构，在训练集语料库为每个句子构建的。<br>其内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;utt text&#x3D;&quot;Khartoum imejitenga na mzozo huo.&quot; waveform&#x3D;&quot;.&#x2F;wav&#x2F;pm_n2236.wav&quot;</span><br><span class="line">utterance_name&#x3D;&quot;pm_n2236&quot; processors_used&#x3D;&quot;,word_splitter&quot;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;_END_&quot; token_class&#x3D;&quot;_END_&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;Khartoum&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;imejitenga&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;na&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;mzozo&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot; &quot; token_class&#x3D;&quot;space&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;huo&quot; token_class&#x3D;&quot;word&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;.&quot; token_class&#x3D;&quot;punctuation&quot;&#x2F;&gt;</span><br><span class="line">&lt;token text&#x3D;&quot;_END_&quot; token_class&#x3D;&quot;_END_&quot;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;utt&gt;</span><br></pre></td></tr></table></figure>
<p>unicode字符属性用以无关语言的正则表达式来tokenise文本<br>正则表达式</p>
<p><img src="/images/blog/merlin_tts_tch2_4.png" alt="TTS merlin技术路线"></p>
<p>同时unicode用来将tokens分类为单词，空格和标点。</p>
<h3 id="1-1-2-词性标注-POS-Tagging"><a href="#1-1-2-词性标注-POS-Tagging" class="headerlink" title="1.1.2 词性标注(POS Tagging)"></a>1.1.2 词性标注(POS Tagging)</h3><p><img src="/images/blog/merlin_tts_tch2_5.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_6.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_7.png" alt="TTS merlin技术路线"></p>
<h4 id="4-1-3-分布式词向量作为POS-Part-Of-Speech-tag-替代品"><a href="#4-1-3-分布式词向量作为POS-Part-Of-Speech-tag-替代品" class="headerlink" title="4.1.3 分布式词向量作为POS(Part Of Speech) tag 替代品"></a>4.1.3 分布式词向量作为POS(Part Of Speech) tag 替代品</h4><p><img src="/images/blog/merlin_tts_tch2_8.png" alt="TTS merlin技术路线"><br>分别来看<br><img src="/images/blog/merlin_tts_tch2_9.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_10.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_11.png" alt="TTS merlin技术路线"></p>
<p>将所有词映射到词向量空间<br><img src="/images/blog/merlin_tts_tch2_12.png" alt="TTS merlin技术路线"></p>
<p>将词向量替代POS<br><img src="/images/blog/merlin_tts_tch2_13.png" alt="TTS merlin技术路线"></p>
<p>以字母替代音素的标注文件<br><img src="/images/blog/merlin_tts_tch2_14.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_15.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-强制对齐和-静音检测"><a href="#4-2-强制对齐和-静音检测" class="headerlink" title="4.2 强制对齐和 静音检测"></a>4.2 强制对齐和 静音检测</h3><p><img src="/images/blog/merlin_tts_tch2_16.png" alt="TTS merlin技术路线"></p>
<h3 id="4-3-phrasing-短语"><a href="#4-3-phrasing-短语" class="headerlink" title="4.3 phrasing(短语)"></a>4.3 phrasing(短语)</h3><p><img src="/images/blog/merlin_tts_tch2_17.png" alt="TTS merlin技术路线"></p>
<h2 id="5-语言特征工程：使用XPATHS-做flatten"><a href="#5-语言特征工程：使用XPATHS-做flatten" class="headerlink" title="5 语言特征工程：使用XPATHS 做flatten"></a>5 语言特征工程：使用XPATHS 做flatten</h2><p><img src="/images/blog/merlin_tts_tch2_18.png" alt="TTS merlin技术路线"><br>对应的详细标注<br><img src="/images/blog/merlin_tts_tch2_19.png" alt="TTS merlin技术路线"></p>
<h2 id="6-语言特征和工程"><a href="#6-语言特征和工程" class="headerlink" title="6 语言特征和工程"></a>6 语言特征和工程</h2><p><img src="/images/blog/merlin_tts_tch2_20.png" alt="TTS merlin技术路线"></p>
<h3 id="6-1-语言特征工程：flatten到上下文依赖的音素"><a href="#6-1-语言特征工程：flatten到上下文依赖的音素" class="headerlink" title="6.1 语言特征工程：flatten到上下文依赖的音素"></a>6.1 语言特征工程：flatten到上下文依赖的音素</h3><p><img src="/images/blog/merlin_tts_tch2_21.png" alt="TTS merlin技术路线"></p>
<p>注意看左下角，<code>ao-th+er</code>，当前音素为<code>th</code>，其前缀为<code>ao</code>,后缀为<code>er</code>。进一步</p>
<p><img src="/images/blog/merlin_tts_tch2_22.png" alt="TTS merlin技术路线"></p>
<p>得到一个完整的音素标注。再进一步</p>
<p><img src="/images/blog/merlin_tts_tch2_23.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_24.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_25.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_26.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_27.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_28.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_29.png" alt="TTS merlin技术路线"></p>
<p>详细解释上面的标注文件</p>
<p><img src="/images/blog/merlin_tts_tch2_30.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_31.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_32.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_33.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_34.png" alt="TTS merlin技术路线"></p>
<p>一个句子的完整标注文件<br><img src="/images/blog/merlin_tts_tch2_35.png" alt="TTS merlin技术路线"></p>
<h3 id="6-3-将每个上下文依赖的音素编码为向量"><a href="#6-3-将每个上下文依赖的音素编码为向量" class="headerlink" title="6.3 将每个上下文依赖的音素编码为向量"></a>6.3 将每个上下文依赖的音素编码为向量</h3><p><strong>示例</strong>：使用一个长度为1-40二进制码来编码quinphone</p>
<p><img src="/images/blog/merlin_tts_tch2_36.png" alt="TTS merlin技术路线"></p>
<p>对应的二进制码</p>
<p><img src="/images/blog/merlin_tts_tch2_37.png" alt="TTS merlin技术路线"></p>
<p>开始编码，以头<code>sil</code>开始：</p>
<p><img src="/images/blog/merlin_tts_tch2_38.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_39.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_40.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_41.png" alt="TTS merlin技术路线"></p>
<h3 id="6-4-语言特征工程：上采样到声学帧率-framerate"><a href="#6-4-语言特征工程：上采样到声学帧率-framerate" class="headerlink" title="6.4 语言特征工程：上采样到声学帧率(framerate)"></a>6.4 语言特征工程：上采样到声学帧率(framerate)</h3><p><img src="/images/blog/merlin_tts_tch2_42.png" alt="TTS merlin技术路线"></p>
<h3 id="6-5-语言特征工程：添加音素内-within-phone-位置特征"><a href="#6-5-语言特征工程：添加音素内-within-phone-位置特征" class="headerlink" title="6.5 语言特征工程：添加音素内(within-phone)位置特征"></a>6.5 语言特征工程：添加音素内(within-phone)位置特征</h3><p><img src="/images/blog/merlin_tts_tch2_43.png" alt="TTS merlin技术路线"></p>
<h2 id="7-时域到底来源于哪里"><a href="#7-时域到底来源于哪里" class="headerlink" title="7 时域到底来源于哪里"></a>7 时域到底来源于哪里</h2><p>先看脚本<br>02_prepare_labels.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># alignments can be state-level (like HTS) or phone-level</span><br><span class="line">if [ &quot;$Labels&quot; &#x3D;&#x3D; &quot;state_align&quot; ]</span><br><span class="line">.&#x2F;scripts&#x2F;run_state_aligner.sh $wav_dir $inp_txt $lab_dir $global_config_file</span><br><span class="line">elif [ &quot;$Labels&quot; &#x3D;&#x3D; &quot;phone_align&quot; ]</span><br><span class="line">.&#x2F;scripts&#x2F;run_phone_aligner.sh $wav_dir $inp_txt $lab_dir $global_config_file</span><br><span class="line"># the alignments will be used to train the duration model later</span><br><span class="line">cp -r $lab_dir&#x2F;label_$Labels $duration_data_dir</span><br><span class="line"># and to upsample the linguistic features to acoustic frame rate</span><br><span class="line"># when training the acoustic model</span><br><span class="line">cp -r $lab_dir&#x2F;label_$Labels $acoustic_data_dir</span><br></pre></td></tr></table></figure>
<p>run_state_aligner.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#do prepare full-contextual labels without timestamps</span><br><span class="line">echo &quot;preparing full-contextual labels using Festival frontend...&quot;</span><br><span class="line">bash $&#123;WorkDir&#125;&#x2F;scripts&#x2F;prepare_labels_from_txt.sh $inp_txt $lab_dir $global_config_file $train</span><br><span class="line"># do forced alignment using HVite from HTK</span><br><span class="line">python $aligner&#x2F;forced_alignment.py</span><br></pre></td></tr></table></figure>
<p>forced_alignment.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">aligner &#x3D; ForcedAlignment()</span><br><span class="line">aligner.prepare_training(file_id_list_name, wav_dir, lab_dir, work_dir, multiple_speaker)</span><br><span class="line">aligner.train_hmm(7, 32)</span><br><span class="line">aligner.align(work_dir, lab_align_dir)</span><br></pre></td></tr></table></figure>

<h2 id="8-声学特征抽取和工程"><a href="#8-声学特征抽取和工程" class="headerlink" title="8 声学特征抽取和工程"></a>8 声学特征抽取和工程</h2><h3 id="8-1-为何我们使用声学特征抽取-波形waveform"><a href="#8-1-为何我们使用声学特征抽取-波形waveform" class="headerlink" title="8.1 为何我们使用声学特征抽取-波形waveform"></a>8.1 为何我们使用声学特征抽取-波形waveform</h3><ul>
<li>音素 a: 的波形</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_44.png" alt="TTS merlin技术路线"></p>
<ul>
<li>音素a:的magnitude spectrum（幅度频谱）</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_45.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_46.png" alt="TTS merlin技术路线"></p>
<h3 id="8-2-术语"><a href="#8-2-术语" class="headerlink" title="8.2 术语"></a>8.2 术语</h3><ul>
<li>Spectral Envelope：频谱封装</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_47.png" alt="TTS merlin技术路线"></p>
<ul>
<li>F0</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_48.png" alt="TTS merlin技术路线"></p>
<ul>
<li>Aperiodic energy：非周期能量</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_49.png" alt="TTS merlin技术路线"></p>
<h3 id="8-5-典型声码器-WORLD"><a href="#8-5-典型声码器-WORLD" class="headerlink" title="8.5 典型声码器 WORLD"></a>8.5 典型声码器 WORLD</h3><ul>
<li>语音特征<ul>
<li>Spectral Envelope(使用CheapTrick评估)</li>
<li>F0：使用DIO评估</li>
<li>Band aperiodicties：使用D4C评估</li>
</ul>
</li>
</ul>
<h4 id="8-5-1-spectral-envelope-评估"><a href="#8-5-1-spectral-envelope-评估" class="headerlink" title="8.5.1 spectral envelope 评估"></a>8.5.1 spectral envelope 评估</h4><ul>
<li>Hanning窗长度3T0</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_50.png" alt="TTS merlin技术路线"></p>
<ul>
<li>使用一个<strong>长度为 2/3 F0</strong>移动平均过滤器</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_51.png" alt="TTS merlin技术路线"><br><img src="/images/blog/merlin_tts_tch2_52.png" alt="TTS merlin技术路线"></p>
<ul>
<li>使用一个<strong>长度为 2 F0</strong>移动平均过滤器</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_53.png" alt="TTS merlin技术路线"></p>
<ul>
<li>$SpEnv= q_0logSp(F)+q1logSp(F+F0)+q1logSp(F-F0)$</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_54.png" alt="TTS merlin技术路线"></p>
<h4 id="8-5-3-F0-评估"><a href="#8-5-3-F0-评估" class="headerlink" title="8.5.3 F0 评估"></a>8.5.3 F0 评估</h4><p><img src="/images/blog/merlin_tts_tch2_55.png" alt="TTS merlin技术路线"></p>
<h4 id="8-5-4-band-aperiodicities"><a href="#8-5-4-band-aperiodicities" class="headerlink" title="8.5.4 band aperiodicities"></a>8.5.4 band aperiodicities</h4><ul>
<li>能量和非能量之间的比率，对固定频率bands取平均</li>
<li>比如： 总功率/sine 波形功率(total power /sine wave power)</li>
<li>示例中，此比例为<ul>
<li>最低band： a</li>
<li>更多band： b</li>
<li>最高band： c</li>
</ul>
</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch2_56.png" alt="TTS merlin技术路线"></p>
<h3 id="8-6-声学特征工程"><a href="#8-6-声学特征工程" class="headerlink" title="8.6 声学特征工程"></a>8.6 声学特征工程</h3><p><img src="/images/blog/merlin_tts_tch2_57.png" alt="TTS merlin技术路线"></p>
<p>原始声学特征与实际使用的声学特征</p>
<p><img src="/images/blog/merlin_tts_tch2_58.png" alt="TTS merlin技术路线"></p>
<p>抽取一部分来分析</p>
<p><img src="/images/blog/merlin_tts_tch2_59.png" alt="TTS merlin技术路线"></p>
<p>再细致来看</p>
<p><img src="/images/blog/merlin_tts_tch2_60.png" alt="TTS merlin技术路线"></p>
<p>处理步骤如下</p>
<p><img src="/images/blog/merlin_tts_tch2_61.png" alt="TTS merlin技术路线"></p>
<p>可以直接运行脚本<code>03_prepare_acoustic_features.sh</code>得到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;MerlinDir&#125;&#x2F;misc&#x2F;scripts&#x2F;vocoder&#x2F;$&#123;Vocoder,,&#125;&#x2F;</span><br><span class="line">extract_features_for_merlin.py $&#123;MerlinDir&#125; $&#123;wav_dir&#125; $&#123;feat_dir&#125; $SamplingFreq</span><br></pre></td></tr></table></figure>
<p><code>extract_features_for_merlin.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># tools directory</span><br><span class="line">world &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;WORLD&quot;)</span><br><span class="line">sptk &#x3D; os.path.join(merlin_dir, &quot;tools&#x2F;bin&#x2F;SPTK-3.9&quot;)</span><br><span class="line">if fs &#x3D;&#x3D; 16000:</span><br><span class="line">nFFTHalf &#x3D; 1024</span><br><span class="line">alpha &#x3D; 0.58</span><br><span class="line">elif fs &#x3D;&#x3D; 48000:</span><br><span class="line">nFFTHalf &#x3D; 2048</span><br><span class="line">alpha &#x3D; 0.77</span><br><span class="line">mcsize&#x3D;59</span><br><span class="line">world_analysis_cmd &#x3D; &quot;%s %s %s %s %s&quot; % (os.path.join(world, &#39;analysis&#39;), \</span><br><span class="line">filename,</span><br><span class="line">os.path.join(f0_dir, file_id + &#39;.f0&#39;), \</span><br><span class="line">os.path.join(sp_dir, file_id + &#39;.sp&#39;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &#39;.bapd&#39;))</span><br><span class="line">os.system(world_analysis_cmd)</span><br><span class="line">### convert f0 to lf0 ###</span><br><span class="line">sptk_x2x_da_cmd &#x3D; &quot;%s +da %s &gt; %s&quot; % (os.path.join(sptk, &#39;x2x&#39;), \</span><br><span class="line">extract_features_for_merlin.py</span><br><span class="line">os.path.join(f0_dir, file_id + &#39;.f0a&#39;), \</span><br><span class="line">os.path.join(sptk, &#39;sopr&#39;) + &#39; -magic 0.0 -LN -MAGIC</span><br><span class="line">-1.0E+10&#39;, \</span><br><span class="line">os.path.join(lf0_dir, file_id + &#39;.lf0&#39;))</span><br><span class="line">os.system(sptk_x2x_af_cmd)</span><br><span class="line">### convert sp to mgc ###</span><br><span class="line">sptk_x2x_df_cmd1 &#x3D; &quot;%s +df %s | %s | %s &gt;%s&quot; % (os.path.join(sptk, &#39;x2x&#39;), \</span><br><span class="line">os.path.join(sp_dir, file_id + &#39;.sp&#39;), \</span><br><span class="line">os.path.join(sptk, &#39;sopr&#39;) + &#39; -R -m 32768.0&#39;, \</span><br><span class="line">os.path.join(sptk, &#39;mcep&#39;) + &#39; -a &#39; + str(alpha</span><br><span class="line">&#39; -m &#39; + str(</span><br><span class="line">mcsize) + &#39; -l &#39; + str(</span><br><span class="line">nFFTHalf) + &#39; -e 1.0E-8 -j 0 -f 0.0 -q 3 &#39;,</span><br><span class="line">os.path.join(mgc_dir, file_id + &#39;.mgc&#39;))</span><br><span class="line">os.system(sptk_x2x_df_cmd1)</span><br><span class="line">### convert bapd to bap ###</span><br><span class="line">sptk_x2x_df_cmd2 &#x3D; &quot;%s +df %s &gt; %s &quot; % (os.path.join(sptk, &quot;x2x&quot;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &quot;.bapd&quot;), \</span><br><span class="line">os.path.join(bap_dir, file_id + &#39;.bap&#39;))</span><br><span class="line">os.system(sptk_x2x_df_cmd2)</span><br></pre></td></tr></table></figure>

























      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-08-12-merlin-tts-techmap2/" data-id="ck4ifvdg3003rywjehq1id30u" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-08-11-merlin-tts-techmap1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-08-11-merlin-tts-techmap1/" class="article-date">
  <time datetime="2019-12-23T10:45:59.628Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-08-11-merlin-tts-techmap1/">merlin语音合成讲义一：技术路线概览</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一-概览"><a href="#一-概览" class="headerlink" title="一 概览"></a>一 概览</h2><p>经典的统计参数语音合成方法的三步</p>
<p><img src="/images/blog/merlin_tts_tch1.png" alt="TTS merlin技术路线"></p>
<p>单独看前端和后端</p>
<p><img src="/images/blog/merlin_tts_tch2.png" alt="TTS merlin技术路线"></p>
<p>那么统计模型的任务是</p>
<p><img src="/images/blog/merlin_tts_tch3.png" alt="TTS merlin技术路线"></p>
<p>可以看到其实统计模型的任务就是做一个sequence-to-sequence的回归</p>
<p><img src="/images/blog/merlin_tts_tch4.png" alt="TTS merlin技术路线"></p>
<p>即：输入序列(语义特征)回归到输出序列的声学特征。但是由于二者之间不同的声学始终频率而导致长度不一。</p>
<p><img src="/images/blog/merlin_tts_tch5.png" alt="TTS merlin技术路线"></p>
<h2 id="三-TTS的三个方向"><a href="#三-TTS的三个方向" class="headerlink" title="三 TTS的三个方向"></a>三 TTS的三个方向</h2><ul>
<li><p>目前为止<br>将TTS问题设置为一个<strong>sequence-tosequence</strong>的回归问题。这是一个有意为之的通用方法，这样易于理解</p>
<ul>
<li>用不同的方法来做回归，神经网络或者机器学习方法</li>
<li>选取不同的输入输出特征</li>
</ul>
</li>
<li><p>接下来<br>TTS是如何完成的，使用一个pre-built系统。可以快速完成整个pipeline，从文本到波形输出</p>
</li>
<li><p>进一步<br>如何构建上面说的pre-built系统。一个缓慢的，一步一步的运行整个pipeline，关注在如何创造一个新系统（对任何语言）</p>
</li>
</ul>
<h2 id="4-术语"><a href="#4-术语" class="headerlink" title="4 术语"></a>4 术语</h2><ol>
<li>前端<br>即<code>text</code>$\rightarrow$ <code>linguistic specification</code></li>
<li>统计模型回归<br><code>linguistic specification</code>$\rightarrow$<code>acoustic features</code></li>
<li>waveform geneator（波形语音生成）<br><code>acoustic features</code>$\rightarrow$<code>waveform</code></li>
<li>语言规范(Linguistic specification)<br>完整的事物</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch6.png" alt="TTS merlin技术路线"></p>
<ol start="5">
<li>语言特征<br>独立的元素。</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch7.png" alt="TTS merlin技术路线"></p>
<ol start="6">
<li>声学特征<br>帧序列<br><img src="/images/blog/merlin_tts_tch8.png" alt="TTS merlin技术路线"></li>
</ol>
<h2 id="4-从文本到语音"><a href="#4-从文本到语音" class="headerlink" title="4 从文本到语音"></a>4 从文本到语音</h2><ul>
<li>文本处理<ul>
<li>pipeline 架构</li>
<li>语言规范</li>
</ul>
</li>
<li>回归<ul>
<li>时域模型</li>
<li>声学特征</li>
</ul>
</li>
<li>波形生成<ul>
<li>声学特征</li>
<li>信号处理</li>
</ul>
</li>
</ul>
<h3 id="4-1-语言规范"><a href="#4-1-语言规范" class="headerlink" title="4.1 语言规范"></a>4.1 语言规范</h3><p><img src="/images/blog/merlin_tts_tch9.png" alt="TTS merlin技术路线"></p>
<p>使用前端工具从文本中抽取特征</p>
<p><img src="/images/blog/merlin_tts_tch10.png" alt="TTS merlin技术路线"></p>
<h3 id="4-2-文本预处理"><a href="#4-2-文本预处理" class="headerlink" title="4.2 文本预处理"></a>4.2 文本预处理</h3><p>对应的文本处理pipeline为<br><img src="/images/blog/merlin_tts_tch11.png" alt="TTS merlin技术路线"></p>
<p>而前端之中的文本预处理详细的划分为：<br><img src="/images/blog/merlin_tts_tch12.png" alt="TTS merlin技术路线"></p>
<p>需要注意的是，<code>tokenize</code>,<code>POS tag</code>,<code>LTS</code>,<code>Phrase breaks</code>,<code>intonation</code>等都是从标记数据中独立学习得到的。</p>
<h4 id="4-2-1-Tokenize-amp-Normalize"><a href="#4-2-1-Tokenize-amp-Normalize" class="headerlink" title="4.2.1 Tokenize &amp; Normalize"></a>4.2.1 Tokenize &amp; Normalize</h4><p><img src="/images/blog/merlin_tts_tch13.png" alt="TTS merlin技术路线"></p>
<ol>
<li><p>第一步：将输入流划分为token，即潜在的单词</p>
<ul>
<li>对于英语和其他语言</li>
</ul>
<ul>
<li>基于规则</li>
<li>空格和标点都是很好的特征</li>
</ul>
<ul>
<li>对于许多其他语言，特别是没有使用空格的</li>
</ul>
<ul>
<li>可能i更困难</li>
<li>需要其他技术</li>
</ul>
</li>
<li><p>第二步：对每个token分类，找到非标注词(Non-Standard Words)，需要做进一步预处理</p>
</li>
</ol>
<p><img src="/images/blog/merlin_tts_tch14.png" alt="TTS merlin技术路线"><br>3. 第三步： 对每一类非标准词(NSW)，使用一些特殊模块来处理。<br><img src="/images/blog/merlin_tts_tch15.png" alt="TTS merlin技术路线"></p>
<h4 id="4-2-2-POS-tagging-词性标注"><a href="#4-2-2-POS-tagging-词性标注" class="headerlink" title="4.2.2 POS tagging 词性标注"></a>4.2.2 POS tagging 词性标注</h4><p><img src="/images/blog/merlin_tts_tch16.png" alt="TTS merlin技术路线"></p>
<ul>
<li>Part-Of-Speech tagger</li>
<li>准确率可能很高</li>
<li>在标注过的数据集上训练</li>
<li>类别是为文本设计的，而非语音</li>
</ul>
<p><img src="/images/blog/merlin_tts_tch16_1.png" alt="TTS merlin技术路线"></p>
<h4 id="4-2-3-Pronuncication-LTS"><a href="#4-2-3-Pronuncication-LTS" class="headerlink" title="4.2.3  Pronuncication /LTS"></a>4.2.3  Pronuncication /LTS</h4><p><img src="/images/blog/merlin_tts_tch17.png" alt="TTS merlin技术路线"></p>
<ul>
<li><p>发音模型：</p>
<ul>
<li>查找词典，等等</li>
<li>单词到声音的模型</li>
</ul>
</li>
<li><p>但是：</p>
<ul>
<li>需要深层次的语言知识来设计发音集合</li>
<li>需要人类专家来撰写词典</li>
</ul>
<p>发音词典示例</p>
<p><img src="/images/blog/merlin_tts_tch18.png" alt="TTS merlin技术路线"></p>
</li>
</ul>
<h4 id="4-2-4-语言规范"><a href="#4-2-4-语言规范" class="headerlink" title="4.2.4 语言规范"></a>4.2.4 语言规范</h4><p>得到语言规范如下</p>
<p><img src="/images/blog/merlin_tts_tch19.png" alt="TTS merlin技术路线"></p>
<h2 id="5-语言特征工程"><a href="#5-语言特征工程" class="headerlink" title="5 语言特征工程"></a>5 语言特征工程</h2><p><img src="/images/blog/merlin_tts_tch20.png" alt="TTS merlin技术路线"></p>
<h3 id="5-1-术语"><a href="#5-1-术语" class="headerlink" title="5.1 术语"></a>5.1 术语</h3><ul>
<li>Flatten：<code>语言规范</code>$\rightarrow$ <code>上下文依赖的音素序列</code></li>
<li>Encode：<code>上下文依赖的音素序列</code>$\rightarrow$<code>向量序列</code></li>
<li>Upsample： <code>向量序列</code>$\rightarrow$<code>在声学特征framerate帧率上的向量序列</code></li>
</ul>
<h3 id="5-2-Flatten-amp-encode-将语言规范转换为向量序列"><a href="#5-2-Flatten-amp-encode-将语言规范转换为向量序列" class="headerlink" title="5.2 Flatten &amp; encode:将语言规范转换为向量序列"></a>5.2 Flatten &amp; encode:将语言规范转换为向量序列</h3><p><img src="/images/blog/merlin_tts_tch21.png" alt="TTS merlin技术路线"></p>
<h3 id="5-3-Upsample：添加时域信息"><a href="#5-3-Upsample：添加时域信息" class="headerlink" title="5.3 Upsample：添加时域信息"></a>5.3 Upsample：添加时域信息</h3><p><img src="/images/blog/merlin_tts_tch22.png" alt="TTS merlin技术路线"></p>
<h2 id="6-统计模型"><a href="#6-统计模型" class="headerlink" title="6 统计模型"></a>6 统计模型</h2><h3 id="6-1-声学模型：一个简单的前馈神经网络"><a href="#6-1-声学模型：一个简单的前馈神经网络" class="headerlink" title="6.1  声学模型：一个简单的前馈神经网络"></a>6.1  声学模型：一个简单的前馈神经网络</h3><p><img src="/images/blog/merlin_tts_tch23.png" alt="TTS merlin技术路线"></p>
<p>有向权重连接</p>
<p>这些网络层的不同作用：</p>
<p><img src="/images/blog/merlin_tts_tch24.png" alt="TTS merlin技术路线"></p>
<h3 id="6-2-用神经网络来合成"><a href="#6-2-用神经网络来合成" class="headerlink" title="6.2 用神经网络来合成"></a>6.2 用神经网络来合成</h3><p><img src="/images/blog/merlin_tts_tch25.png" alt="TTS merlin技术路线"></p>
<h2 id="7-波形生成（waveform-generator）"><a href="#7-波形生成（waveform-generator）" class="headerlink" title="7 波形生成（waveform generator）"></a>7 波形生成（waveform generator）</h2><h3 id="7-1-声学特征是什么"><a href="#7-1-声学特征是什么" class="headerlink" title="7.1 声学特征是什么"></a>7.1 声学特征是什么</h3><p><img src="/images/blog/merlin_tts_tch26.png" alt="TTS merlin技术路线"></p>
<h2 id="8-使用神经网络的TTS系统"><a href="#8-使用神经网络的TTS系统" class="headerlink" title="8 使用神经网络的TTS系统"></a>8 使用神经网络的TTS系统</h2><p>如果我们把所有的这一切综合起来的示意图如下：</p>
<p>第一步：<br><img src="/images/blog/merlin_tts_tch27.png" alt="TTS merlin技术路线"></p>
<p>第二步：<br><img src="/images/blog/merlin_tts_tch28.png" alt="TTS merlin技术路线"><br>第三步：<br><img src="/images/blog/merlin_tts_tch29.png" alt="TTS merlin技术路线"><br>第四步<br><img src="/images/blog/merlin_tts_tch30.png" alt="TTS merlin技术路线"><br>第五步<br><img src="/images/blog/merlin_tts_tch31.png" alt="TTS merlin技术路线"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-08-11-merlin-tts-techmap1/" data-id="ck4ifvdg4003tywje2uj2cxu2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-08-10-TTS_MSD_HMM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-08-10-TTS_MSD_HMM/" class="article-date">
  <time datetime="2019-12-23T10:45:59.627Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-08-10-TTS_MSD_HMM/">语音合成：MSD-HMM多空间概率分布HMM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一-概念"><a href="#一-概念" class="headerlink" title="一  概念"></a>一  概念</h2><p>从数据中抽取音高(Pitch)之后，我们发现无法对音高建模，因为数据中同时存在离散和连续的值。</p>
<h3 id="1-1-多空间概率分布"><a href="#1-1-多空间概率分布" class="headerlink" title="1.1 多空间概率分布"></a>1.1 多空间概率分布</h3><p>多空间概率分布的示意图如下:</p>
<p><img src="/images/blog/multi_space_pdf_2.png" alt="多空间概率分布示意图"></p>
<p>如上图M的一个样本空间由G个空间构成:，其中$\Omega _g^{g=1}$是一个 $n_g$维的空间$R^{n_g}$。每个空间都有自己的维度，其中一些可能有着相同的维度。每个空间$\Omega _g$都有其出现的概率$w_g$,即$P(\Omega)=w_g$,其中$\sum _{g=1} ^Gw_g=1$如果$n_g&gt;0$则每个空间都有一个概率密度函数$N_g(x),x\epsilon R^{n_g}$ 。如果$n_g$=0则认为$\Omega _g$仅包括一个样点,且概率$P(\Omega)$被定义为$P(\Omega)=1$。</p>
<p>一个具体问题来解释这个过程。</p>
<p><img src="/images/blog/multi_space_sample.png" alt="假设问题"></p>
<p> 一个人在池塘钓鱼。池塘中有红色的鱼,蓝色的鱼以及乌龟,此外,水中还有一些垃圾。当这个人钓到一条鱼的时候,他会观察鱼的种类和大小,如长度和重量当他钓到一只乌龟,则会观察乌龟壳的直径这里假设乌龟壳是圆形的而当他钓上一些垃圾的时候,则不会关心垃圾的任何属性。在这个例子中,我们可以看到全部的样本空间由四个空间组成。</p>
<ul>
<li>$\Omega _1$ ,二维的空间,代表着红色的鱼的长度及重量。</li>
<li>$\Omega _2$ ,二维的空间,代表着蓝色的鱼的长度及重量。</li>
<li>$\Omega _3$ ,一维的空间,代表着乌龟壳的直径。</li>
<li>$\Omega _4$ ,零维的空间,代表垃圾</li>
</ul>
<p>权重 $w_1,w_2,w_3,w_4$ 由池塘中红鱼蓝鱼,乌龟和垃圾所占的比例所决定。函数$N_1(\dot)$ 和 $N_2(\dot)$ 分别是关于红鱼和蓝鱼大小的二维概率密度函数长度和重量。函数 $N_3(\dot)$ 则是关于乌龟的一维的概率密度函数。例如这个人钓到一条红鱼,则观察向量 $o=(\lbrace 1\rbrace,x)$ 。其中 $x$ 为一个两维的向量,代表红鱼的长度和重量。假设这个人日夜不停的钓鱼,如果在夜晚,他无法区分鱼的颜色,只能丈量鱼的长度和重量,在这种情况下,鱼的观测向量$o=(\lbrace 1,2 \rbrace,x)$</p>
<h3 id="1-2-MSD-HMM"><a href="#1-2-MSD-HMM" class="headerlink" title="1.2 MSD-HMM"></a>1.2 MSD-HMM</h3><p>根据上述多空间概率分布的定义,人们提出了一MSD-HMM结构如下图所示。</p>
<p><img src="/images/blog/msd-hmm.png" alt="MSD_HMM示意图"></p>
<p> MSD-HMM的初始分布 $\pi$ 二及转移矩阵A都与其他的HMM相同,此处仅给出状态输出分布概率:</p>
<p>$$<br> b_i(o)=\sum <em>{g\belong S(o)}w</em>{ig}N_{ig}(V(o))<br>$$</p>
<p>实际上,MSD-HMM同时包含了连续HMM和离散HMM。当 $n_g=0$ 时MSD-HMM与离散HMM是完全相同的。当 $S(o_t)$ 代表且仅代表一个特定的空间时,MSD-HMM则与连续HMM相同。由于MSD-HMM的这个特性,它很适合用于描述语音信号中基音周期。</p>
<h2 id="二-上下文相关的声学建模技术"><a href="#二-上下文相关的声学建模技术" class="headerlink" title="二 上下文相关的声学建模技术"></a>二 上下文相关的声学建模技术</h2><p>在连续的语音中,人们的发音普遍会受到上下文的影响而发生变化,这就是连续语音之间的协同发音现象。上下文无关的建模方法对每个识别基元分别独立建模,忽略了这种协同发音的现象,因而采用上下文无关模型的语音合成系统,其合成语音会出现不连贯或一字一顿等现象,所以无法取得较高的自然度。解决这个问题的方法是使用上下文相关的建模方法。上下文相关的建模方法在语音识别中有很多的应用,在语音合成中,我们也可以采用这个方法。与上下文无关的建模方法相比,上下文相关建模方法需要考虑如下的几个问题：</p>
<ol>
<li><p>如何选取基本识别基元。对于汉语语音系统而言,常用的基本基元有音节、声韵母和音素。由于汉语有个无调音节,如考虑语调则超过个音节,如果考虑上下文相关的变化,则会由于基元数目太多而导致模型无法实现。而声韵母与音素的数目都相对很少,因此可以用来作为上下文相关模型的基元</p>
</li>
<li><p>如何降低模型的规模。即使采用声韵母或音素作为上下文相关模型的基元,模型的规模仍然非常巨大。假设基元的个数为,则有,个可能的上下文相关基元。如果每个基元分为五个状态,每个状态采用单个高斯分布来描述,系统中仍然有,个高斯分布,如此大规模的模型会导致系统的速度下降,模型存储空间占用巨大,而且在训练数据库不是足够大的情况下,很多基元会存在训练不充分的问题。解决的办法是采用参数共享的技术。例如进行状态共享建模,或者混合密度共享建模。</p>
</li>
<li><p>如何预测在训练数据中没有出现的基元。在上下文相关的声学模型中,由于训练数据的限制,有些基元可能在训练数据中完全不出现,但是可能出现在待合成的数据中。为了保证合成过程的顺利进行,我们必须采取的补救措施保证每个基元都能找到模型描述。通常使用的方法是基于决策树的策略,使用那些可见基元的分布来合成在训练数据中不可见的基元。</p>
</li>
</ol>
<p>在实际中的上下文相关声学建模技术中,通常采用决策树与状态共享相结合的策略,这样既可以降低模型规模,避免训练不充分问题,还可以有效合成那些训练数据中不可见的基元。</p>
<h3 id="2-1-合成基元的选择"><a href="#2-1-合成基元的选择" class="headerlink" title="2.1 合成基元的选择"></a>2.1 合成基元的选择</h3><p><strong>音节</strong>： 汉语约有个无调音节和多个有调音节。在进行上下文无关的声学建模时,选用音节作为基元可以取得比较好的性能。但如果使用上下文相关的建模,由前接一当前一后续所组成的元组的数目将过于庞大,故采用音节作为基元并不合适。</p>
<p><strong>音素</strong>：汉语有大约个音素,但音素并没有反映出汉语语音的特点,而且,相对于声韵母,音素显得十分不稳定,这就给标注带来了困难,进而影响声学建模。</p>
<p><strong>声韵母</strong></p>
<ul>
<li><p>声韵结构是汉语音节特有的结构,使用声韵母基元,可以利用已有的语言学知识,进而提高声学模型的性能</p>
</li>
<li><p>使用声韵母作为识别基元,上下文相关信息也变得比较确定</p>
</li>
<li><p>选择声韵母作为基元,使得语音段的长度和基元数目比较适当</p>
</li>
</ul>
<h3 id="声韵母作为基元"><a href="#声韵母作为基元" class="headerlink" title="声韵母作为基元"></a>声韵母作为基元</h3><p><strong>基元的定义</strong></p>
<table>
<thead>
<tr>
<th>基元</th>
<th>元素</th>
</tr>
</thead>
<tbody><tr>
<td>声母</td>
<td>b、p、m、f、d、t、n、l、g、k、h、j、q、x、zh、ch、sh、z、c、s 、 y、w、r，_A,_E,_I,_O,_U</td>
</tr>
<tr>
<td>韵母</td>
<td>ai 、ei、 ui 、ao、 ou、 iu 、ie 、ve、 er、 an 、en 、in、 un 、vn 、ang 、eng、 ing 、ong ，an 、en 、in、 un 、vn，ang 、eng、 ing 、ong</td>
</tr>
</tbody></table>
<h2 id="三-基于决策树的状态共享"><a href="#三-基于决策树的状态共享" class="headerlink" title="三 基于决策树的状态共享"></a>三 基于决策树的状态共享</h2><p>根据上面定义的基本基元,以这些基本基元为中心,考虑它们上下文相关的情况,我们可以将每个上下文相关的基元表示为 $l-c+r/env$ 的方式,其中 $c$ 为中心基元,$l$ 为左相关信息,$r$ 为右相关信息,$env$ 则表示该基元所在位置的一些环境特征。本系统中,环境特征包括前接音节字调,当前音节字调,后续音节字调,当前音节到前一自然停处的字数,当前音节到后一自然停顿处的字数,前接词的词性,当前词的词性,后续词的词性,当前音节在当前词中的位置,当前词的音节数,音节所在句的长度。<br>$$<br> L-C+R/A:a1_a2_a3/B:b1_b2/C:c1_c2_c3/D:d1_d2/E:e<br>$$</p>
<p>其中 $C$ 代表当前元,$L$ 代表前接基元,$R$ 代表后接基元,$ABCDE$ 几项代表当前基元的上下文相关的一些特征。$a1$ 为前接字字调,$a2$ 为当前字字调,$a3$ 为后接字字调,$b?$ 为基元所在的字在当前停顿段落短语或短句中的位置,$b1$ 为到段落开始字的距离,$b2$ 为到段落结束字的距离,$c1$ 为前接词的词性, $c2$ 为当前词的词性,$c3$ 为后接词的词性,$d1$ 为当前字在当前词中的位置,为 $d2$ 当前词的字数, $e$ 为句子的总字数。为了将发音相似的基元共享到一起。本系统中使用决策树来实现参数共享的策略。这样做的好处是</p>
<ul>
<li><p>一是降低模型的规模</p>
</li>
<li><p>二是避免由于训练数据的稀疏性而造成训练不充分的问题</p>
</li>
<li><p>三是可以近似合成那些在训练数据中不存在的基元。</p>
</li>
</ul>
<p><strong>示例</strong></p>
<p>例如“他见了人就甜牙吠咬,咬住就不撒嘴”一句中的“见”字的韵母的标注为:</p>
<p>$$<br>  j-ian4+1/A:1_4_5/B:2_3/C:r_v_u/D:1_1/E:15<br>$$</p>
<h3 id="3-1-决策树划分特征的确定"><a href="#3-1-决策树划分特征的确定" class="headerlink" title="3.1 决策树划分特征的确定"></a>3.1 决策树划分特征的确定</h3><p>决策树的分裂依赖于问题集的设计。为了定义问题集,我们首先来确认划分特征。划分特征包括两大类,发音相似性和基元的上下文相关信息。</p>
<p>其中发音相似性的特征有以下几类,韵母划分特征,声母划分特征,单音划分特征。<strong>此划分，每个人不一样</strong></p>
<p><strong>声母的特征划分</strong></p>
<table>
<thead>
<tr>
<th>划分特征</th>
<th>描述</th>
<th>基元列表</th>
</tr>
</thead>
<tbody><tr>
<td>Stop</td>
<td>塞音</td>
<td>b,d,g,p,t,k</td>
</tr>
<tr>
<td>Aspirated Stop</td>
<td>塞送气音</td>
<td>b,d,g</td>
</tr>
<tr>
<td>Unaspirated Stop</td>
<td>非塞送气音</td>
<td>p,t,k</td>
</tr>
<tr>
<td>Affricate</td>
<td>塞擦音</td>
<td>z,zh,j,c,ch,q</td>
</tr>
<tr>
<td>Aspirated Affricate</td>
<td>塞擦送气音</td>
<td>z,zh,j</td>
</tr>
<tr>
<td>Unaspirated  Affricate</td>
<td>非塞擦送气音</td>
<td>C,zh,q</td>
</tr>
<tr>
<td>Fricative</td>
<td>擦音</td>
<td>f,s,sh,x,h,r</td>
</tr>
<tr>
<td>Fricative 2</td>
<td>擦音2</td>
<td>f,s,sh,x,h,r,k</td>
</tr>
<tr>
<td>Voiceless Fricative</td>
<td>擦清音</td>
<td>f,s,sh,x,h</td>
</tr>
<tr>
<td>Voice Fricative</td>
<td>浊清音</td>
<td>r,k</td>
</tr>
<tr>
<td>Nasal</td>
<td>鼻音</td>
<td>m,n,</td>
</tr>
<tr>
<td>Nasal 2</td>
<td>鼻音2</td>
<td>m,n,l</td>
</tr>
<tr>
<td>Labial</td>
<td>唇音</td>
<td>B,p,m</td>
</tr>
<tr>
<td>Labial 2</td>
<td>唇音2</td>
<td>B,p,m,f</td>
</tr>
<tr>
<td>Apical</td>
<td>顶音</td>
<td>Z,c,s,d,t,n,l,zh,sh,r</td>
</tr>
<tr>
<td>Apical Front</td>
<td>顶前音</td>
<td>Z,c,s</td>
</tr>
<tr>
<td>Apical 1</td>
<td>顶音1</td>
<td>D,t,n,l</td>
</tr>
<tr>
<td>Apical 2</td>
<td>顶音2</td>
<td>D,t</td>
</tr>
<tr>
<td>Apical 3</td>
<td>顶音3</td>
<td>N,l</td>
</tr>
<tr>
<td>Apical End</td>
<td>顶后音1</td>
<td>Zh,ch,sh</td>
</tr>
<tr>
<td>Apical End 2</td>
<td>顶后音2</td>
<td>Zh,ch,sh</td>
</tr>
<tr>
<td>Tongue Top</td>
<td>舌前音</td>
<td>J,q,x</td>
</tr>
<tr>
<td>Tongue Root</td>
<td>舌根音</td>
<td>G,k,h</td>
</tr>
<tr>
<td>Zero</td>
<td>零声母</td>
<td>_A,_E,_I,_O,_U,_V</td>
</tr>
<tr>
<td>XFuyin</td>
<td>全部声母（包含零声母）</td>
<td>全部</td>
</tr>
<tr>
<td>Fuyin</td>
<td>全部声母（不包含零声母）</td>
<td>不包含零声母</td>
</tr>
</tbody></table>
<p><strong>韵母的划分特征</strong></p>
<table>
<thead>
<tr>
<th>划分特征</th>
<th>描述</th>
<th>基元特征</th>
</tr>
</thead>
<tbody><tr>
<td>Single Yun</td>
<td>单韵母</td>
<td>A,I,u,e,v,ic,ih</td>
</tr>
<tr>
<td>Com Yun</td>
<td>复合韵母</td>
<td>An,ai,ang,…vn</td>
</tr>
<tr>
<td>Type A</td>
<td>含 a的韵母</td>
<td>A,ia,an,ang,ai,ua,ao</td>
</tr>
<tr>
<td>Type E</td>
<td>含e的韵母</td>
<td>E,ie,ve,ei,uei</td>
</tr>
<tr>
<td>Type I</td>
<td>含I的韵母</td>
<td>I,ai,ei,uei,ia,ian,iang,iao,ie,in,ing,iong,iou</td>
</tr>
<tr>
<td>Type O</td>
<td>含o的韵母</td>
<td>O,ao,uo,ou,ong,iou</td>
</tr>
<tr>
<td>Type U</td>
<td>含u的韵母</td>
<td>U,ua,uen,u,ueng,uo,iou</td>
</tr>
<tr>
<td>Type V</td>
<td>含v的韵母</td>
<td>V,vn,ve</td>
</tr>
</tbody></table>
<p>为了使得决策树的分裂更加细致,我们将每个声韵母作为一个划分特征,这就是单基元划分特征。最后再加上句首尾静音SIL,句中的由逗号和顿号造成的停顿PAU,句中其他的短停顿sp。</p>
<p><strong>上下文相关信息划分特征</strong></p>
<table>
<thead>
<tr>
<th>上下文相关信息划分特征</th>
</tr>
</thead>
<tbody><tr>
<td>基基元所在音节的前接音节的声调调</td>
</tr>
<tr>
<td>基基元所在的音节为的声调调</td>
</tr>
<tr>
<td>基基元所在音节的后接音节的声调调</td>
</tr>
<tr>
<td>基基元所在的音节在韵律短语中的位置正向</td>
</tr>
<tr>
<td>基基元所在的音节在韵律短语中的位置反向</td>
</tr>
<tr>
<td>基基元所在词的前接词的词性性</td>
</tr>
<tr>
<td>基基元所在词的词性性</td>
</tr>
<tr>
<td>基基元所在词的后接词的词性性</td>
</tr>
<tr>
<td>基基元在其所在词中的位置置</td>
</tr>
<tr>
<td>基基元所在词的音节数字数</td>
</tr>
</tbody></table>
<h3 id="3-2-决策树问题集的定义"><a href="#3-2-决策树问题集的定义" class="headerlink" title="3.2 决策树问题集的定义"></a>3.2 决策树问题集的定义</h3><p>在确定了划分特征后,我们根据划分特征来定义决策树的问题集。对于发音相似性的特征,每个特征都会对应三个问题左问题,中心问题和右问题。其中,对于单基元特征和声母的划分特征,其对应问题的答案是对称的。例如塞音(Stop)对应的三个问题为：</p>
<p><strong>发音相似的特征</strong></p>
<p>$$<br>QS’L_Stop’ \quad \lbrace b- ^{\star} ,d- ^{\star},g- ^{\star},p- ^{\star},t- ^{\star},k- ^{\star} \rbrace  \<br>QS’R_Stop’\quad \lbrace ^{\star}+b/ ^{\star} ,^{\star}+d/ ^{\star},^{\star}g/^{\star},^{\star}p+/ ^{\star},^{\star}t+/ ^{\star},^{\star}k+/ ^{\star} \rbrace  \<br>QS’C_Stop’\quad \lbrace b- ^{\star} ,d- ^{\star},g- ^{\star},p- ^{\star},t- ^{\star},k- ^{\star} \rbrace<br>$$</p>
<p>其中单引号中的部分为问题的标识,而大括号内的部分为问题的答案’,<strong>$\star$</strong>和”’<strong>?</strong>‘为通配符,如<strong>“$b-\star$”</strong>则代表所有以<strong>“$b-$”</strong>开头的上下文相关基元</p>
<p><strong>部分韵母的划分特征</strong></p>
<p>而对于部分韵母的划分特征,其问题的答案是非对称的。如</p>
<p>$$<br> QS’L_Type_A’\quad \lbrace a?- ^{\star},ia?-^{\star},ua?-^{\star},A-^{\star}\rbrace \<br>QS’R_Type_A’\quad \lbrace ^{\star}+a?/^{\star},^{\star}+ai?/^{\star},^{\star}+an?/^{\star},^{\star}+ang/^{\star},^{\star}+ao?/^{\star},^{\star}+_A/^{\star}<br>$$</p>
<p>因为这类问题的意思是左邻的发音是“$a$”,在这类问题中,复合韵母一般是不对称的。</p>
<p><strong>上下文相关信息的划分特征</strong></p>
<p>对于上下文相关信息的划分特征,问题的设计方式为首先对每个单独的划分特征建立各自的问题,然后,对关系密切的划分特征建立联合的问题。如</p>
<p>$$<br> QS’C_tone1’\quad \lbrace ^{\star}A:?_1_?/B^{\star} \rbrace<br>$$</p>
<p>代表所有当前音节为一声的基元</p>
<p>$$<br> QS’C_tone3_3’\quad \lbrace ^{\star}A:?_3_3?/B^{\star} \rbrace<br>$$</p>
<p>则代表当前音节为三声而后续音节也为三声的基元。这样设计的好处是可以把汉语中一些变调的规则加入问题集中,经过训练,上下文相关的基元中可以包含变调的声音,最终提高合成语音的自然度。</p>
<h3 id="3-3-决策树的构建"><a href="#3-3-决策树的构建" class="headerlink" title="3.3 决策树的构建"></a>3.3 决策树的构建</h3><p>问题集建立后,则开始构造决策树。考虑到合成基元的拓扑结构其第一个状态和最后一个状态分别为起始状态和结束状态,他们不能驻留,只在模型中起辅助作用。其余的状态可以驻留或者转移到下一个状态。因此,真正起作用的是中间的几个状态。因此在构造决策树的时候,我们只考虑中间的几个状态。</p>
<p>决策树的的构造有两种方法</p>
<ol>
<li>方法对每个中心基元的每个状态分别构造决策树。这种方法假设当基元的中心音素不同时,基元之间相互独立,因此首先根据中心音素对所有的基元进行分类,然后在利用决策树来进行状态共享。图一给出了中心基元为的所有基元的状态组成的决策树示意图。</li>
</ol>
<p><img src="/images/blog/method1_descion_tree1.png" alt="决策树构建"></p>
<ol start="2">
<li>方法对所有基元的同一个状态构造决策树。这种方法假设当中心音素不同时,基元之间仍然有一定的重叠。即使基元的中心音素不同,它们之间的状态仍然有可能共享。基元之间的状态共享情况完全依靠基于决策树的分类策略。图一给出了所有基元的状态组成的决策树示意图。</li>
</ol>
<p><img src="/images/blog/method1_descion_tree2.png" alt="决策树构建"></p>
<blockquote>
<p>这个树结构的意思是静音是第一个问题，就是对这个树分类影响最大的一个问题，后面的问题依次减弱，然后可能会一直分叉，直到决策树判断截止为止，所以的单元走到那个叶节点就截止了，能走到那个叶节点的单元都共享一个状态了，这个状态用高斯分布描述</p>
</blockquote>
<p>于方法1共需要构造基元总数有效状态数棵不同的决策树,这样,只有相同基元的状态才会被共享,这样对保证最后合成语音的单音清晰度有所帮助。而对于方法工,决策树的数量与基元的有效状态数相同,在这里,所有基元的状态进行共享,不同基元中一些发音相似的状态亦被共享到一起,有助于减小最终模型的规模,并且可以在一定程度上提高对训练集中未出现基元的鲁棒性。实验表明,当训练数据较少时句,方法的清晰度明显高于方法工,但在训练数据增加后句,两种方法合成语音的质量十分接近。考虑到方法近似合成未知基元的能力较好,本系统最终选择使用方法2进行决策树的构建。</p>
<p>决策树由自顶而下的顺序生成,首先,将所有的状态放入根节点中,然后进行节点分裂。节点分裂依赖于评估函数。决策树评估函数用来估计决策树的节点上的样本相似性。这里,我们定义对数似然概率作为为节点分裂的评估函数。在每个节点进行分裂的时候,我们从问题集中选择一个问题,然后根据此问题把节点分成两个子节点并且计算评估函数的增量,我们选择具有最大增量的问题,并且根据此问题把节点划分成两部分。当所有问题的增量都低于某个闭值的时候,节点上的分裂过程将停止。最终,同一个叶子节点中的状态将被共享到一起。</p>
<h2 id="四-模型的训练流程"><a href="#四-模型的训练流程" class="headerlink" title="四 模型的训练流程"></a>四 模型的训练流程</h2><p>训练部分主要依靠HTK工具包(加入HTS1.1补丁)加入补丁和SPTK3.0工具包完成。</p>
<ol>
<li>从语音文件中提取基音周期参数（使用auto-corelation方法）。</li>
<li>使用SPTK工具提取语音文件的MFCC及能量</li>
<li>将上述两步中得到的数据组合,计算差分,最终加入HTK文件头,得到格式的训练文件</li>
<li>使用上下文无关标注文件和训练文件进行上下文无关基元的HMM训练(使用HTK工具包中的HInit和HRest工具)</li>
<li>使用上下文相关标注文件和训练文件进行5次嵌入式训练。使用HeRest工具</li>
<li>引入问题集,对频谱HMM和基音周期HMM进行基于决策树的状态共享。使用HHed工具</li>
<li>使用上下文相关标注文件和训练文件进行5次嵌入式训练,在最后一次<br>迭代中,根据HMM的状态转移矩阵得到基元状态时长的HMM。使用HeRest工具</li>
<li>引入问题集,对状态时长HMM进行基于决策树的状态共享。使用HHed工具</li>
<li>输出二进制的模型文件</li>
</ol>
<h2 id="五-语音合成流程"><a href="#五-语音合成流程" class="headerlink" title="五 语音合成流程"></a>五 语音合成流程</h2><ol>
<li><p>使用进行分词,标注。</p>
</li>
<li><p>音字转换,同时获得声韵母基元的上下文环境信息</p>
</li>
<li><p>生成合成模块所需的格式文件</p>
</li>
<li><p>解析文字处理模块的文件,将其转换为带有环境信息的上下文相关基元序列</p>
</li>
<li><p>根据每个带有环境信息的上下文相关基元搜索并得到其相应的状态时长、基音周期和频谱的HMM。</p>
</li>
<li><p>由状态时长HMM得到基元个状态的持续时长。</p>
</li>
<li><p>根据状态的时长、基音周期HMM和频谱HMM,进行参数生成,得到每祯的基音周期、对数能量和MFCC参数。</p>
</li>
<li><p>将每祯的基音周期、对数能量和MFCC参数送入基于MSLA滤波器的合成器,得到合成语音</p>
<p><img src="/images/blog/tts_chn_paper.png" alt="合成流程"></p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-08-10-TTS_MSD_HMM/" data-id="ck4ifvdg2003pywje1gfrg28v" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2018-06-02-transfer-learning-pratice" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/23/2018-06-02-transfer-learning-pratice/" class="article-date">
  <time datetime="2019-12-23T10:45:59.605Z" itemprop="datePublished">2019-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/blog/">blog</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/23/2018-06-02-transfer-learning-pratice/">迁移学习实践-Tensorflow分类任务</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>摘自 : <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" target="_blank" rel="noopener">medium transfer learning</a></p>
<h2 id="1-说明和准备"><a href="#1-说明和准备" class="headerlink" title="1 说明和准备"></a>1 说明和准备</h2><h3 id="1-1-任务问题"><a href="#1-1-任务问题" class="headerlink" title="1.1 任务问题"></a>1.1 任务问题</h3><p>我们要对只有4000张图片(3000张训练，1000张验证)的数据集做图像分类，分为<code>猫</code>和<code>狗</code>两类。图片数据可以从<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">kaggle 猫狗分类挑战</a>上下载到25000张，不过为了演示迁移学习，假定只有4000张图片。</p>
<h3 id="1-2-数据准备"><a href="#1-2-数据准备" class="headerlink" title="1.2 数据准备"></a>1.2 数据准备</h3><p>首先下载全部的数据集，然后筛选出其中的4000张。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">np.random.seed(42)</span><br><span class="line">files &#x3D; glob.glob(&#39;train&#x2F;*&#39;)</span><br><span class="line"># 载入全部的猫狗图片</span><br><span class="line">cat_files &#x3D; [fn for fn in files if &#39;cat&#39; in fn]</span><br><span class="line">dog_files &#x3D; [fn for fn in files if &#39;dog&#39; in fn]</span><br><span class="line">print(len(cat_files), len(dog_files))</span><br><span class="line"># (12500, 12500)</span><br><span class="line"></span><br><span class="line"># 筛选出其中的4000张图片</span><br><span class="line">cat_train &#x3D; np.random.choice(cat_files, size&#x3D;1500, replace&#x3D;False)</span><br><span class="line">dog_train &#x3D; np.random.choice(dog_files, size&#x3D;1500, replace&#x3D;False)</span><br><span class="line">cat_files &#x3D; list(set(cat_files) - set(cat_train))</span><br><span class="line">dog_files &#x3D; list(set(dog_files) - set(dog_train))</span><br><span class="line"></span><br><span class="line">cat_val &#x3D; np.random.choice(cat_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">dog_val &#x3D; np.random.choice(dog_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">cat_files &#x3D; list(set(cat_files) - set(cat_val))</span><br><span class="line">dog_files &#x3D; list(set(dog_files) - set(dog_val))</span><br><span class="line"></span><br><span class="line">cat_test &#x3D; np.random.choice(cat_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line">dog_test &#x3D; np.random.choice(dog_files, size&#x3D;500, replace&#x3D;False)</span><br><span class="line"></span><br><span class="line">print(&#39;Cat datasets:&#39;, cat_train.shape, cat_val.shape, cat_test.shape)</span><br><span class="line">print(&#39;Dog datasets:&#39;, dog_train.shape, dog_val.shape, dog_test.shape)</span><br><span class="line"># Cat datasets: (1500,) (500,) (500,)</span><br><span class="line"># Dog datasets: (1500,) (500,) (500,)</span><br></pre></td></tr></table></figure>
<p>将数据子集单独放到其他文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_dir &#x3D; &#39;training_data&#39;</span><br><span class="line">val_dir &#x3D; &#39;validation_data&#39;</span><br><span class="line">test_dir &#x3D; &#39;test_data&#39;</span><br><span class="line"></span><br><span class="line">train_files &#x3D; np.concatenate([cat_train, dog_train])</span><br><span class="line">validate_files &#x3D; np.concatenate([cat_val, dog_val])</span><br><span class="line">test_files &#x3D; np.concatenate([cat_test, dog_test])</span><br><span class="line"></span><br><span class="line">os.mkdir(train_dir) if not os.path.isdir(train_dir) else None</span><br><span class="line">os.mkdir(val_dir) if not os.path.isdir(val_dir) else None</span><br><span class="line">os.mkdir(test_dir) if not os.path.isdir(test_dir) else None</span><br><span class="line"></span><br><span class="line">for fn in train_files:</span><br><span class="line">    shutil.copy(fn, train_dir)</span><br><span class="line"></span><br><span class="line">for fn in validate_files:</span><br><span class="line">    shutil.copy(fn, val_dir)</span><br><span class="line">    </span><br><span class="line">for fn in test_files:</span><br><span class="line">    shutil.copy(fn, test_dir)</span><br></pre></td></tr></table></figure>

<p>为模型准备数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img</span><br><span class="line"></span><br><span class="line">IMG_DIM &#x3D; (150, 150)</span><br><span class="line"></span><br><span class="line">train_files &#x3D; glob.glob(&#39;training_data&#x2F;*&#39;)</span><br><span class="line">train_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in train_files]</span><br><span class="line">train_imgs &#x3D; np.array(train_imgs)</span><br><span class="line">train_labels &#x3D; [fn.split(&#39;\\&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in train_files]</span><br><span class="line"></span><br><span class="line">validation_files &#x3D; glob.glob(&#39;validation_data&#x2F;*&#39;)</span><br><span class="line">validation_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in validation_files]</span><br><span class="line">validation_imgs &#x3D; np.array(validation_imgs)</span><br><span class="line">validation_labels &#x3D; [fn.split(&#39;\\&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in validation_files]</span><br><span class="line"></span><br><span class="line">print(&#39;Train dataset shape:&#39;, train_imgs.shape, </span><br><span class="line">      &#39;\tValidation dataset shape:&#39;, validation_imgs.shape)</span><br><span class="line"># Train dataset shape: (3000, 150, 150, 3)  </span><br><span class="line"># Validation dataset shape: (1000, 150, 150, 3)</span><br></pre></td></tr></table></figure>
<p>现在，我们得到了3000张训练集和1000张验证集，图像长宽为$150\times 150$，接下来，我们要将图片像素矩阵值取值范围缩放到0到1.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_imgs_scaled &#x3D; train_imgs.astype(&#39;float32&#39;)</span><br><span class="line">validation_imgs_scaled &#x3D; validation_imgs.astype(&#39;float32&#39;)</span><br><span class="line">train_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line">validation_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line"></span><br><span class="line">print(train_imgs[0].shape)</span><br><span class="line">array_to_img(train_imgs[0])</span><br></pre></td></tr></table></figure>

<p>一些基本参数，同时将字符型的分类类别改为数值型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_shape &#x3D; (150, 150, 3)</span><br><span class="line"></span><br><span class="line"># encode text category labels</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line"></span><br><span class="line">le &#x3D; LabelEncoder()</span><br><span class="line">le.fit(train_labels)</span><br><span class="line">train_labels_enc &#x3D; le.transform(train_labels)</span><br><span class="line">validation_labels_enc &#x3D; le.transform(validation_labels)</span><br><span class="line"></span><br><span class="line">print(train_labels[1495:1505], train_labels_enc[1495:1505])</span><br><span class="line"># [&#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;dog&#39;] [0 0 0 0 0 1 1 1 1 1]</span><br></pre></td></tr></table></figure>

<h2 id="2-基准模型"><a href="#2-基准模型" class="headerlink" title="2 基准模型"></a>2 基准模型</h2><p>先手写个基准的CNN模型，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Conv2D(16, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;, </span><br><span class="line">                 input_shape&#x3D;input_shape))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(64, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>模型架构如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Layer (type) Output Shape Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_1 (Conv2D) (None, 148, 148, 16) 448       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 (None, 74, 74, 16) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D) (None, 72, 72, 64) 9280      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D) (None, 34, 34, 128) 73856     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten) (None, 36992) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense) (None, 512) 18940416  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense) (None, 1) 513       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 19,024,513</span><br><span class="line">Trainable params: 19,024,513</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>

<p>我们设置<code>batch_size=30</code>，总共有3000张图片，也就是一个epoch需要100次迭代。我们训练30个epoch，然后验证模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size &#x3D; 30</span><br><span class="line">num_classes &#x3D; 2</span><br><span class="line">epochs &#x3D; 30</span><br><span class="line">history &#x3D; model.fit(x&#x3D;train_imgs_scaled, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_imgs_scaled, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>训练过程的输出如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Train on 3000 samples, validate on 1000 samples</span><br><span class="line">Epoch 1&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 10s - loss: 0.7583 - acc: 0.5627 - val_loss: 0.7182 - val_acc: 0.5520</span><br><span class="line">Epoch 2&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.6343 - acc: 0.6533 - val_loss: 0.5891 - val_acc: 0.7190</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 29&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.0314 - acc: 0.9950 - val_loss: 2.7014 - val_acc: 0.7140</span><br><span class="line">Epoch 30&#x2F;30</span><br><span class="line">3000&#x2F;3000 - 8s - loss: 0.0147 - acc: 0.9967 - val_loss: 2.4963 - val_acc: 0.7220</span><br></pre></td></tr></table></figure>
<p>训练集都接近100%准确率了，但是验证集准确率还只有72%。模型可能存在过拟合。可以使用如下的代码画出训练和验证过程的loss下降和准确率变化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) &#x3D; plt.subplots(1, 2, figsize&#x3D;(12, 4))</span><br><span class="line">t &#x3D; f.suptitle(&#39;Basic CNN Performance&#39;, fontsize&#x3D;12)</span><br><span class="line">f.subplots_adjust(top&#x3D;0.85, wspace&#x3D;0.3)</span><br><span class="line"></span><br><span class="line">epoch_list &#x3D; list(range(1,31))</span><br><span class="line">ax1.plot(epoch_list, history.history[&#39;acc&#39;], label&#x3D;&#39;Train Accuracy&#39;)</span><br><span class="line">ax1.plot(epoch_list, history.history[&#39;val_acc&#39;], label&#x3D;&#39;Validation Accuracy&#39;)</span><br><span class="line">ax1.set_xticks(np.arange(0, 31, 5))</span><br><span class="line">ax1.set_ylabel(&#39;Accuracy Value&#39;)</span><br><span class="line">ax1.set_xlabel(&#39;Epoch&#39;)</span><br><span class="line">ax1.set_title(&#39;Accuracy&#39;)</span><br><span class="line">l1 &#x3D; ax1.legend(loc&#x3D;&quot;best&quot;)</span><br><span class="line"></span><br><span class="line">ax2.plot(epoch_list, history.history[&#39;loss&#39;], label&#x3D;&#39;Train Loss&#39;)</span><br><span class="line">ax2.plot(epoch_list, history.history[&#39;val_loss&#39;], label&#x3D;&#39;Validation Loss&#39;)</span><br><span class="line">ax2.set_xticks(np.arange(0, 31, 5))</span><br><span class="line">ax2.set_ylabel(&#39;Loss Value&#39;)</span><br><span class="line">ax2.set_xlabel(&#39;Epoch&#39;)</span><br><span class="line">ax2.set_title(&#39;Loss&#39;)</span><br><span class="line">l2 &#x3D; ax2.legend(loc&#x3D;&quot;best&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_1.png" alt=""></p>
<p>上图左可以看到，3个epoch之后就开始过拟合了，训练准确率一直上升，但是验证准确率保持不变了。</p>
<h3 id="2-1-简单的优化模型"><a href="#2-1-简单的优化模型" class="headerlink" title="2.1 简单的优化模型"></a>2.1 简单的优化模型</h3><p>上面的CNN是个基本的架构，接下来我们做一些优化策略，网络架构上加入正则化，使用一定概率的dropout。只修改网络结构部分，其他训练代码不变。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Conv2D(16, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;, </span><br><span class="line">                 input_shape&#x3D;input_shape))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(64, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Conv2D(128, kernel_size&#x3D;(3, 3), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(MaxPooling2D(pool_size&#x3D;(2, 2)))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])          </span><br><span class="line">history &#x3D; model.fit(x&#x3D;train_imgs_scaled, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_imgs_scaled, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>使用上面的画图代码，画出训练曲线。</p>
<p><img src="/images/blog/transfer_learning_pratice_2.png" alt=""></p>
<p>有所改善，但是效果不明显。依然是过拟合，究其原因，数据量太少，可以使用部分的数据集增强策略增加数据多样性。</p>
<h3 id="2-2-使用数据增强策略"><a href="#2-2-使用数据增强策略" class="headerlink" title="2.2 使用数据增强策略"></a>2.2 使用数据增强策略</h3><p>keras的<code>ImageDataGenerator</code>自带了一些数据增强方法，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br></pre></td></tr></table></figure>
<p>当然，我们还可以使用 <a href="https://github.com/albu/albumentations" target="_blank" rel="noopener">Albumentations</a>来做更多的增强策略。我们先来看看<code>ImageDataGenerator</code>增强之后的图片效果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mg_id &#x3D; 2595</span><br><span class="line">cat_generator &#x3D; train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],</span><br><span class="line">                                   batch_size&#x3D;1)</span><br><span class="line">cat &#x3D; [next(cat_generator) for i in range(0,5)]</span><br><span class="line">fig, ax &#x3D; plt.subplots(1,5, figsize&#x3D;(16, 6))</span><br><span class="line">print(&#39;Labels:&#39;, [item[1][0] for item in cat])</span><br><span class="line">l &#x3D; [ax[i].imshow(cat[i][0][0]) for i in range(0,5)]</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/transfer_learning_pratice_3.png" alt=""></p>
<p>再次使用上面的基准模型(加了dropout层的)，此次我们将学习率稍微改小点，将默认的<code>1e-3</code>改为<code>1e-4</code>，防止模型过拟合，此时的数据量增多了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-4),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])        </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>打印训练曲线<br><img src="/images/blog/transfer_learning_pratice_4.png" alt=""></p>
<p>模型的准确率提升到<strong>82%</strong>，而且已经不再过拟合了。</p>
<h2 id="3-使用其他模型做迁移学习"><a href="#3-使用其他模型做迁移学习" class="headerlink" title="3 使用其他模型做迁移学习"></a>3 使用其他模型做迁移学习</h2><h3 id="3-1-VGG-16模型"><a href="#3-1-VGG-16模型" class="headerlink" title="3.1 VGG-16模型"></a>3.1 VGG-16模型</h3><p>分类模型，我们选用VGG16为例。首先，我们需要理解VGG16的模型架构，如下</p>
<p><img src="/images/blog/transfer_learning_pratice_5.png" alt=""></p>
<p>13个$3\times 3$的卷积，5个maxpooling缩减了网络输入尺寸。在两个全连接层之前的输出是4096个神经元，全连接都是1000个神经元(代表了1000个分类)。由于我们要做的是做猫狗分类，最后三层是不需要的。我们更关心前5个blocks(下图)，我们可以将VGG模型看做一个特征抽取器。下图是VGG模型的三种用法</p>
<p><img src="/images/blog/transfer_learning_pratice_6.png" alt=""></p>
<ul>
<li>如果我们只是作为特征抽取器，则按照图中中间的示例，冻结所有的blocks(5个)，在训练过程中，这些blocks中的所有参数都不会更新。</li>
<li>如果我们做fine-tuning，可以考虑按照图右冻结前3个blocks，更新后面两个blocks(4和5)的参数（每个训练epoch过程都会）。</li>
</ul>
<h3 id="3-2-将预训练模型作为特征抽取器"><a href="#3-2-将预训练模型作为特征抽取器" class="headerlink" title="3.2 将预训练模型作为特征抽取器"></a>3.2 将预训练模型作为特征抽取器</h3><p>3.1节中最后一张图的中间的架构，冻结所有blocks层的参数的用法。下面是对应的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from keras.applications import vgg16</span><br><span class="line">from keras.models import Model</span><br><span class="line">import keras</span><br><span class="line"></span><br><span class="line">vgg &#x3D; vgg16.VGG16(include_top&#x3D;False, weights&#x3D;&#39;imagenet&#39;, </span><br><span class="line">                                     input_shape&#x3D;input_shape)</span><br><span class="line"></span><br><span class="line">output &#x3D; vgg.layers[-1].output</span><br><span class="line">output &#x3D; keras.layers.Flatten()(output)</span><br><span class="line">vgg_model &#x3D; Model(vgg.input, output)</span><br><span class="line"></span><br><span class="line">vgg_model.trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    layer.trainable &#x3D; False</span><br><span class="line">    </span><br><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&#39;max_colwidth&#39;, -1)</span><br><span class="line">layers &#x3D; [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]</span><br><span class="line">pd.DataFrame(layers, columns&#x3D;[&#39;Layer Type&#39;, &#39;Layer Name&#39;, &#39;Layer Trainable&#39;])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_7.png" alt=""></p>
<p>此处，将VGG模型看做SURF或者HOG特征之类的东西就可以，使用过程不更新参数，直接输入图片，预测得到特征。用法如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bottleneck_feature_example &#x3D; vgg.predict(train_imgs_scaled[0:1])</span><br><span class="line">print(bottleneck_feature_example.shape)</span><br><span class="line">plt.imshow(bottleneck_feature_example[0][:,:,0])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_8.png" alt=""></p>
<p>从训练数据和验证数据中使用VGG16抽取特征如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def get_bottleneck_features(model, input_imgs):</span><br><span class="line">    features &#x3D; model.predict(input_imgs, verbose&#x3D;0)</span><br><span class="line">    return features</span><br><span class="line"></span><br><span class="line">train_features_vgg &#x3D; get_bottleneck_features(vgg_model, train_imgs_scaled)</span><br><span class="line">validation_features_vgg &#x3D; get_bottleneck_features(vgg_model, validation_imgs_scaled)</span><br><span class="line">print(&#39;Train Bottleneck Features:&#39;, train_features_vgg.shape, </span><br><span class="line">      &#39;\tValidation Bottleneck Features:&#39;, validation_features_vgg.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Bottleneck Features: (3000, 8192)  </span><br><span class="line">Validation Bottleneck Features: (1000, 8192)</span><br></pre></td></tr></table></figure>
<p>接下来，我们可以以VGG作为特征抽取器重新构建一个训练模型。其实，在抽取特征之后直接接一个SVM或者KNN分类器也是一样的。下面以keras代码，重新构建CNN模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"># 此时的vgg_model 已经设置了trainable&#x3D;False</span><br><span class="line">input_shape &#x3D; vgg_model.output_shape[1]</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(InputLayer(input_shape&#x3D;(input_shape,)))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-4),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>网络结构如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type) Output Shape Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">input_2 (InputLayer) (None, 8192) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense) (None, 512) 4194816   </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout) (None, 512) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense) (None, 512) 262656    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout) (None, 512) 0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense) (None, 1) 513       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 4,457,985</span><br><span class="line">Trainable params: 4,457,985</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>
<p><strong>注意，此时的训练代码中输入数据不再是图片，而是VGG抽取的特征了</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history &#x3D; model.fit(x&#x3D;train_features_vgg, y&#x3D;train_labels_enc,</span><br><span class="line">                    validation_data&#x3D;(validation_features_vgg, validation_labels_enc),</span><br><span class="line">                    batch_size&#x3D;batch_size,</span><br><span class="line">                    epochs&#x3D;epochs,</span><br><span class="line">                    verbose&#x3D;1)</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/transfer_learning_pratice_9.png" alt=""></p>
<p>验证准确率提升到<strong>88%</strong>，虽然看起来依然过拟合了。</p>
<h3 id="3-3-使用数据增强-VGG作为特征抽取器"><a href="#3-3-使用数据增强-VGG作为特征抽取器" class="headerlink" title="3.3 使用数据增强+VGG作为特征抽取器"></a>3.3 使用数据增强+VGG作为特征抽取器</h3><p>由于我们使用data generator，此处不再用VGG作为特征抽取器.此部分与上面的区别在于，上面的VGG模型不是网络的一部分，属于数据处理部分，用vgg将处理图片(特征抽取)之后的数据传入了新的小网络。而当前是将VGG作为网络的一部分，与新的网络层，构建了一个新模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line"></span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br><span class="line">train_generator &#x3D; train_datagen.flow(train_imgs, train_labels_enc, batch_size&#x3D;30)</span><br><span class="line">val_generator &#x3D; val_datagen.flow(validation_imgs, validation_labels_enc, batch_size&#x3D;20)</span><br></pre></td></tr></table></figure>
<p>网络构建部分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from keras.applications import vgg16</span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line">from keras.models import Model</span><br><span class="line">import keras</span><br><span class="line"></span><br><span class="line">vgg &#x3D; vgg16.VGG16(include_top&#x3D;False, weights&#x3D;&#39;imagenet&#39;, </span><br><span class="line">                                     input_shape&#x3D;input_shape)</span><br><span class="line"></span><br><span class="line">output &#x3D; vgg.layers[-1].output</span><br><span class="line">output &#x3D; keras.layers.Flatten()(output)</span><br><span class="line">vgg_model &#x3D; Model(vgg.input, output)</span><br><span class="line"></span><br><span class="line">vgg_model.trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    layer.trainable &#x3D; False</span><br><span class="line"># 下面是我们新加的网络层，将VGG放在前面</span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(vgg_model)</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"># 学习率变小了</span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;2e-5),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">              </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>此时的学习曲线，如下</p>
<p><img src="/images/blog/transfer_learning_pratice_10.png" alt=""></p>
<p>此时的验证准确率提升到了90%，而且没有过拟合。</p>
<h3 id="3-5-fine-tuning-预训练的VGG模型-数据增强"><a href="#3-5-fine-tuning-预训练的VGG模型-数据增强" class="headerlink" title="3.5 fine-tuning 预训练的VGG模型+数据增强"></a>3.5 fine-tuning 预训练的VGG模型+数据增强</h3><p>此部分，可以参考3.1节部分VGG示意图的最右边那张图，此时vgg模型中某些blocks的参数也在训练过程中得到更新。如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vgg_model.trainable &#x3D; True</span><br><span class="line"></span><br><span class="line">set_trainable &#x3D; False</span><br><span class="line">for layer in vgg_model.layers:</span><br><span class="line">    if layer.name in [&#39;block5_conv1&#39;, &#39;block4_conv1&#39;]:</span><br><span class="line">        set_trainable &#x3D; True</span><br><span class="line">    if set_trainable:</span><br><span class="line">        layer.trainable &#x3D; True</span><br><span class="line">    else:</span><br><span class="line">        layer.trainable &#x3D; False</span><br><span class="line">        </span><br><span class="line">layers &#x3D; [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]</span><br><span class="line">pd.DataFrame(layers, columns&#x3D;[&#39;Layer Type&#39;, &#39;Layer Name&#39;, &#39;Layer Trainable&#39;])</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/transfer_learning_pratice_11.png" alt=""></p>
<p>可以看到<code>block4</code>和<code>block5</code>已经变成可以训练了。此时再次减小学习率，同时使用了增强了的数据处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">train_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255, zoom_range&#x3D;0.3, rotation_range&#x3D;50,</span><br><span class="line">                                   width_shift_range&#x3D;0.2, height_shift_range&#x3D;0.2, shear_range&#x3D;0.2, </span><br><span class="line">                                   horizontal_flip&#x3D;True, fill_mode&#x3D;&#39;nearest&#39;)</span><br><span class="line">val_datagen &#x3D; ImageDataGenerator(rescale&#x3D;1.&#x2F;255)</span><br><span class="line">train_generator &#x3D; train_datagen.flow(train_imgs, train_labels_enc, batch_size&#x3D;30)</span><br><span class="line">val_generator &#x3D; val_datagen.flow(validation_imgs, validation_labels_enc, batch_size&#x3D;20)</span><br><span class="line"></span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras import optimizers</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(vgg_model)</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;, input_dim&#x3D;input_shape))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(512, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(Dropout(0.3))</span><br><span class="line">model.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line"></span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">              optimizer&#x3D;optimizers.RMSprop(lr&#x3D;1e-5),</span><br><span class="line">              metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">              </span><br><span class="line">history &#x3D; model.fit_generator(train_generator, steps_per_epoch&#x3D;100, epochs&#x3D;100,</span><br><span class="line">                              validation_data&#x3D;val_generator, validation_steps&#x3D;50, </span><br><span class="line">                              verbose&#x3D;1)</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/transfer_learning_pratice_12.png" alt=""></p>
<p>可以看到，验证准确率已经提升到了<strong>96%</strong>，与基准模型相比，提升了24%。</p>
<h2 id="4-测试模型"><a href="#4-测试模型" class="headerlink" title="4 测试模型"></a>4 测试模型</h2><p>接下来在测试集上测试上面的5种模型</p>
<ol>
<li>基准CNN模型</li>
<li>使用了数据增强的基准CNN模型</li>
<li>迁移学习：使用VGG16作为特征抽取器【VGG只用在数据处理上】</li>
<li>迁移学习：使用VGG作为模型的一部分，并且使用了数据增强策略</li>
<li>迁移学习：对VGG模型微调，让其<code>block4</code>和<code>block5</code>参数可更新</li>
</ol>
<p>测试代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">IMG_DIM &#x3D; (150, 150)</span><br><span class="line"></span><br><span class="line">test_files &#x3D; glob.glob(&#39;test_data&#x2F;*&#39;)</span><br><span class="line">test_imgs &#x3D; [img_to_array(load_img(img, target_size&#x3D;IMG_DIM)) for img in test_files]</span><br><span class="line">test_imgs &#x3D; np.array(test_imgs)</span><br><span class="line">test_labels &#x3D; [fn.split(&#39;&#x2F;&#39;)[1].split(&#39;.&#39;)[0].strip() for fn in test_files]</span><br><span class="line"></span><br><span class="line">test_imgs_scaled &#x3D; test_imgs.astype(&#39;float32&#39;)</span><br><span class="line">test_imgs_scaled &#x2F;&#x3D; 255</span><br><span class="line">test_labels_enc &#x3D; class2num_label_transformer(test_labels)</span><br><span class="line"></span><br><span class="line">print(&#39;Test dataset shape:&#39;, test_imgs.shape)</span><br><span class="line">print(test_labels[0:5], test_labels_enc[0:5])</span><br></pre></td></tr></table></figure>
<p>测试基准模型的代码如下（其他模型类似）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictions &#x3D; basic_cnn.predict_classes(test_imgs_scaled, verbose&#x3D;0)</span><br><span class="line">predictions &#x3D; num2class_label_transformer(predictions)</span><br><span class="line">meu.display_model_performance_metrics(true_labels&#x3D;test_labels, predicted_labels&#x3D;predictions, </span><br><span class="line">                                      classes&#x3D;list(set(test_labels)))</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/transfer_learning_pratice_13.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shartoo.github.com/2019/12/23/2018-06-02-transfer-learning-pratice/" data-id="ck4ifvdg1003nywje5089279b" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/12/23/template/">博客题目</a>
          </li>
        
          <li>
            <a href="/2019/12/23/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-11-26-model-pruning/">模型剪枝和优化-torch和Tensorflow为例</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-10-28--understand-pytorch/">理解pytorch的计算逻辑</a>
          </li>
        
          <li>
            <a href="/2019/12/23/2019-09-24-outlier-detection/">使用pyod做离群点检测</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 shartoo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>